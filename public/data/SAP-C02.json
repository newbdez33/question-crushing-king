{
  "questions": [
    {
      "id": "a0ad180fae024d66993ed1d875970e05",
      "questionNumber": 1,
      "type": "single",
      "content": "Question #1<p>A company needs to architect a hybrid DNS solution. This solution will use an Amazon Route 53 private hosted zone for the domain cloud.example.com for the resources stored within VPCs.</p><p>The company has the following DNS resolution requirements:</p><p>On-premises systems should be able to resolve and connect to cloud.example.com.</p><p>All VPCs should be able to resolve cloud.example.com.</p><p>There is already an AWS Direct Connect connection between the on-premises corporate network and AWS Transit Gateway.</p><p>Which architecture should the company use to meet these requirements with the HIGHEST performance?</p>",
      "options": [
        {
          "label": "A",
          "content": "Associate the private hosted zone to all the VPCs. Create a Route 53 inbound resolver in the shared services VPC. Attach all VPCs to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the inbound resolver."
        },
        {
          "label": "B",
          "content": "Associate the private hosted zone to all the VPCs. Deploy an Amazon EC2 conditional forwarder in the shared services VPC. Attach all VPCs to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the conditional forwarder."
        },
        {
          "label": "C",
          "content": "Associate the private hosted zone to the shared services VPC. Create a Route 53 outbound resolver in the shared services VPC. Attach all VPCs to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the outbound resolver."
        },
        {
          "label": "D",
          "content": "&nbsp;Associate the private hosted zone to the shared services VPC. Create a Route 53 inbound resolver in the shared services VPC. Attach the shared services VPC to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the inbound resolver.&nbsp;"
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>The correct answer is A. Here's the detailed explanation:</p><p>Why Option A is the Best Solution?</p><p>The requirements are:</p><p>1.On-premises systems must resolve `cloud.example.com`.</p><p>2.All VPCs must resolve `cloud.example.com`.</p><p>3.High performance is required (minimal latency and complexity).</p><p>Option A meets these requirements by:</p><p>1.Associating the private hosted zone with all VPCs → Ensures VPC resources can resolve `cloud.example.com` natively via Route 53.</p><p>2.Creating a Route 53 inbound resolver in the shared services VPC → Provides a centralized DNS resolution endpoint for on-premises queries.</p><p>3.Using AWS Transit Gateway for connectivity → Ensures low-latency communication between VPCs and on-premises.</p><p>4.Forwarding on-premises DNS queries to the inbound resolver → Allows on-premises systems to resolve `cloud.example.com` efficiently.</p><p>Why Other Options Are Incorrect?</p><p>-Option B: Uses anEC2-based conditional forwarder, which introduces unnecessary management overhead (scaling, patching) compared to Route 53 Resolver (fully managed).</p><p>-Option C: Associates the private hosted zoneonly with the shared services VPC, meaning other VPCs cannot resolve `cloud.example.com` natively (requires outbound resolver, which adds complexity).</p><p>-Option D: Similar to Option C, but uses aninbound resolver while still restricting the hosted zone to one VPC, breaking resolution for other VPCs.</p><p>Key Considerations for Performance</p><p>-Route 53 private hosted zones provide the fastest DNS resolution within VPCs when associated directly.</p><p>-Route 53 Resolver (inbound) is fully managed and optimized for hybrid DNS, unlike manual EC2 solutions.</p><p>-Transit Gateway ensures high-speed connectivity between VPCs and on-premises.</p><p>Conclusion</p><p>Option A is the most efficient, scalable, and performant solution, leveraging native AWS services (Route 53 private hosted zones + inbound resolver) without introducing unnecessary complexity.</p><p>ref:: https://aws.amazon.com/blogs/networking-and-content-delivery/centralized-dns-management-of-hybrid-cloud-withamazon-route-53-and-aws-transit-gateway/ </p><p><br></p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "d9ff533cc2e2461a98b648ab6504f6a2",
      "questionNumber": 2,
      "type": "single",
      "content": "<p>Question #2</p><p>A company is providing weather data over a REST-based API to several customers. The API is hosted by Amazon API Gateway and is integrated with different AWS Lambda functions for each API operation. The company uses Amazon Route 53 for DNS and has created a resource record of weather.example.com. The company stores data for the API in Amazon DynamoDB tables. The company needs a solution that will give the API the ability to fail over to a different AWS Region.</p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Deploy a new set of Lambda functions in a new Region. Update the API Gateway API to use an edge-optimized API endpoint with Lambda functions from both Regions as targets. Convert the DynamoDB tables to global tables."
        },
        {
          "label": "B",
          "content": "Deploy a new API Gateway API and Lambda functions in another Region. Change the Route 53 DNS record to a multivalue answer. Add both API Gateway APIs to the answer. Enable target health monitoring. Convert the DynamoDB tables to global tables."
        },
        {
          "label": "C",
          "content": "Deploy a new API Gateway API and Lambda functions in another Region. Change the Route 53 DNS record to a failover record. Enable target health monitoring. Convert the DynamoDB tables to global tables."
        },
        {
          "label": "D",
          "content": "Deploy a new API Gateway API in a new Region. Change the Lambda functions to global functions. Change the Route 53 DNS record to a multivalue answer. Add both API Gateway APIs to the answer. Enable target health monitoring. Convert the DynamoDB tables to global tables."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the information provided, the best solution for implementing a multi-region failover capability for the company's REST API would be:</p><p>C. Deploy a new API Gateway API and Lambda functions in another Region. Change the Route 53 DNS record to a failover record. Enable target health monitoring. Convert the DynamoDB tables to global tables.</p><p>This approach provides a complete disaster recovery solution by:</p><p>Deploying the entire API infrastructure (API Gateway and Lambda functions) in a second region</p><p>Using Route 53 failover records with health monitoring to automatically redirect traffic when needed</p><p>Converting DynamoDB tables to global tables to ensure data consistency across regions</p><p>Option A is incomplete because it only deploys new Lambda functions in the new region and updates the existing API Gateway to use an edge-optimized endpoint. This doesn't provide true regional failover since the API Gateway itself would still be a single point of failure in one region.</p><p>The correct solution (C) creates a fully redundant setup across regions with automatic failover capabilities through Route 53 health checks and global tables for data replication.</p><p>Sources</p><p>Implementing multi-Region failover for Amazon API Gateway | AWS Compute Blog (https://aws.amazon.com/cn/blogs/compute/implementing-multi-region-failover-for-amazon-api-gateway/）</p><p>Deploy a reference architecture that models a serverless active/passive workload with asynchronous replication of application data and failover from a primary to a secondary AWS Region - Multi-Region Application Architecture Solution （ https://docs.aws.amazon.com/solutions/latest/multi-region-application-architecture/welcome.html）</p><p>AWS Backup vs global table/database for cost-effective RTO,RPO requirements | AWS re:Post （https://repost.aws/questions/QUWf0GuonKSvW8U2N4o8huHw/aws-backup-vs-global-table-database-for-cost-effective-rto-rpo-requirements）</p><p>Configure custom health checks for DNS failover for an API Gateway API - Amazon API Gateway （https://docs.aws.amazon.com/apigateway/latest/developerguide/dns-failover.html）</p><p>Community | Scale across borders: build a multi-region architecture while maintaining data residency （https://community.aws/content/2dhVhtsciD5gVBlCKUlHoszrDzU/scale-beyond-borders）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "6794a7c0df9546a9a4f3fe370533447d",
      "questionNumber": 3,
      "type": "single",
      "content": "Question #3<p>A company uses AWS Organizations with a single OU named Production to manage multiple accounts. All accounts are members of the Production OU. Administrators use deny list SCPs in the root of the organization to manage access to restricted services.</p><p>The company recently acquired a new business unit and invited the new unit’s existing AWS account to the organization. Once onboarded, the administrators of the new business unit discovered that they are not able to update existing AWS Config rules to meet the company’s policies.</p><p>Which option will allow administrators to make changes and continue to enforce the current policies without introducing additional long-term maintenance?</p>",
      "options": [
        {
          "label": "A",
          "content": "Remove the organization&rsquo;s root SCPs that limit access to AWS Config. Create AWS Service Catalog products for the company&rsquo;s standard AWS Config rules and deploy them throughout the organization, including the new account."
        },
        {
          "label": "B",
          "content": "Create a temporary OU named Onboarding for the new account. Apply an SCP to the Onboarding OU to allow AWS Config actions. Move the new account to the Production OU when adjustments to AWS Config are complete."
        },
        {
          "label": "C",
          "content": "Convert the organization&rsquo;s root SCPs from deny list SCPs to allow list SCPs to allow the required services only. Temporarily apply an SCP to the organization&rsquo;s root that allows AWS Config actions for principals only in the new account."
        },
        {
          "label": "D",
          "content": "Create a temporary OU named Onboarding for the new account. Apply an SCP to the Onboarding OU to allow AWS Config actions. Move the organization&rsquo;s root SCP to the Production OU. Move the new account to the Production OU when adjustments to AWS Config are complete."
        }
      ],
      "correctAnswer": "D",
      "explanation": "To allow the new business unit's administrators to make changes to AWS Config rules while maintaining the existing policies, the solution is to create a temporary OU named Onboarding for the new account, as described in option D. By applying an SCP to this OU that allows AWS Config actions, the new account can adjust its AWS Config rules as needed. Once the adjustments are complete, moving the root SCP to the Production OU ensures that the current policies continue to be enforced across all accounts, and moving the new account to the Production OU integrates it with the rest of the organization while applying the same policies.<p>ref:https://docs.aws.amazon.com/whitepapers/latest/organizing-your-aws-environment/transitional-ou.html</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "fd8161c5ec9d4f5595763cf3312aa727",
      "questionNumber": 4,
      "type": "single",
      "content": "<p>Question #4</p><p>A company is running a two-tier web-based application in an on-premises data center. The application layer consists of a single server running a stateful application. The application connects to a PostgreSQL database running on a separate server. The application’s user base is expected to grow significantly, so the company is migrating the application and database to AWS. The solution will use Amazon Aurora PostgreSQL, Amazon EC2 Auto Scaling, and Elastic Load Balancing.</p><p>Which solution will provide a consistent user experience that will allow the application and database tiers to scale?</p>",
      "options": [
        {
          "label": "A",
          "content": "Enable Aurora Auto Scaling for Aurora Replicas. Use a Network Load Balancer with the least outstanding requests routing algorithm and sticky sessions enabled."
        },
        {
          "label": "B",
          "content": "Enable Aurora Auto Scaling for Aurora writers. Use an Application Load Balancer with the round robin routing algorithm and sticky sessions enabled."
        },
        {
          "label": "C",
          "content": "Enable Aurora Auto Scaling for Aurora Replicas. Use an Application Load Balancer with the round robin routing and sticky sessions enabled."
        },
        {
          "label": "D",
          "content": "Enable Aurora Scaling for Aurora writers. Use a Network Load Balancer with the least outstanding requests routing algorithm and sticky sessions enabled."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the information provided, the best solution for migrating the stateful application to AWS while ensuring scalability and a consistent user experience would be:</p><p>C. Enable Aurora Auto Scaling for Aurora Replicas. Use an Application Load Balancer with the round robin routing and sticky sessions enabled.</p><p>This solution is optimal because:</p><p>Aurora Auto Scaling for Aurora Replicas will allow the database tier to automatically scale read capacity based on demand, which is essential as the user base grows significantly.</p><p>The Application Load Balancer (ALB) is the appropriate choice for HTTP/HTTPS traffic of a web-based application, offering more advanced routing capabilities than a Network Load Balancer.</p><p>Sticky sessions are crucial for maintaining a consistent user experience with a stateful application, as they ensure that a user's session remains connected to the same EC2 instance throughout their session.</p><p>Round robin routing with the ALB will distribute incoming traffic evenly across the healthy EC2 instances in the Auto Scaling group, providing efficient load distribution.</p><p>Option A is less suitable because Network Load Balancers operate at the transport layer (TCP/UDP) and are typically used for extreme performance requirements rather than standard web applications. Additionally, NLBs don't natively support sticky sessions in the same way ALBs do, which is critical for this stateful application.</p><p>Sources</p><p>REL07-BP01 Use automation when obtaining or scaling resources - AWS Well-Architected Framework（https://docs.aws.amazon.com/wellarchitected/2023-10-03/framework/rel_adapt_to_changes_autoscale_adapt.html）</p><p>HA and automatic scaling for ASP.NET Web Forms applications on AWS - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-asp-net-web-forms/ha-scaling.html）</p><p>Hyperscaling Aurora MySQL-Compatible to handle sudden traffic growth - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/hyperscale-aurora-mysql/introduction.html）</p><p>Replatforming backend databases - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-replatforming-cots-applications/replatforming-backend-databases.html）</p><p>Navigating the Cloud: Joby Aviation’s PLM migration to AWS | AWS for Industries （https://aws.amazon.com/cn/blogs/industries/navigating-the-cloud-joby-aviations-plm-migration-to-aws/）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "43b9a74695df4caf9bedea54f179acb9",
      "questionNumber": 5,
      "type": "single",
      "content": "Question #5<p>A company uses a service to collect metadata from applications that the company hosts on premises. Consumer devices such as TVs and internet radios access the applications. Many older devices do not support certain HTTP headers and exhibit errors when these headers are present in responses. The company has configured an on-premises load balancer to remove the unsupported headers from responses sent to older devices, which the company identified by the User-Agent headers.</p><p>The company wants to migrate the service to AWS, adopt serverless technologies, and retain the ability to support older devices. The company has already migrated the applications into a set of AWS Lambda functions.</p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an Amazon CloudFront distribution for the metadata service. Create an Application Load Balancer (ALB). Configure the CloudFront distribution to forward requests to the ALB. Configure the ALB to invoke the correct Lambda function for each type of request. Create a CloudFront function to remove the problematic headers based on the value of the User-Agent header."
        },
        {
          "label": "B",
          "content": "Create an Amazon API Gateway REST API for the metadata service. Configure API Gateway to invoke the correct Lambda function for each type of request. Modify the default gateway responses to remove the problematic headers based on the value of the User-Agent header."
        },
        {
          "label": "C",
          "content": "Create an Amazon API Gateway HTTP API for the metadata service. Configure API Gateway to invoke the correct Lambda function for each type of request. Create a response mapping template to remove the problematic headers based on the value of the User-Agent. Associate the response data mapping with the HTTP API."
        },
        {
          "label": "D",
          "content": "Create an Amazon CloudFront distribution for the metadata service. Create an Application Load Balancer (ALB). Configure the CloudFront distribution to forward requests to the ALB. Configure the ALB to invoke the correctLambda function for each type of request. Create a Lambda@Edge function that will remove the problematic headers in response to viewer requests based on the value of the User-Agent header."
        }
      ],
      "correctAnswer": "A",
      "explanation": "The company needs to migrate its metadata service to AWS with the following requirements:<ol><li>Serverless Architecture: The company wants to adopt serverless technologies.</li><li>Backward Compatibility: Older consumer devices, which do not support certain HTTP headers, must still be supported.</li><li>Header Removal Logic: Specific HTTP headers, identified by the User-Agent, must be removed for compatibility with older devices.</li></ol><p>Options Analysis:</p><ul><li><p>A: Creating an Amazon CloudFront distribution and an Application Load Balancer (ALB) allows requests to be routed efficiently. CloudFront can remove the problematic HTTP headers using a CloudFront function based on the User-Agent value. The ALB can invoke the appropriate Lambda function for request processing. This solution aligns well with the requirements and involves minimal effort.</p></li><li><p>B: Using an API Gateway REST API and configuring the gateway to handle header removal introduces unnecessary complexity. While functional, it requires creating additional mapping templates or modifying responses, which adds operational overhead compared to the simplicity of CloudFront and ALB.</p></li><li><p>C: Using an API Gateway HTTP API to remove headers requires managing response mapping templates and associating them with the HTTP API. Similar to option B, this adds complexity and operational effort.</p></li><li><p>D: Using a Lambda@Edge function with a CloudFront distribution is a viable alternative but adds latency since Lambda@Edge executes at the edge locations. This may impact performance compared to CloudFront functions, which are natively integrated and faster for lightweight tasks like header removal.</p></li></ul><p>Key Benefits of Option A:</p><ul><li>Seamless Header Removal: CloudFront functions provide a lightweight mechanism to remove headers based on the User-Agent value.</li><li>Efficient Routing: The ALB efficiently routes requests to the correct Lambda function for processing.</li><li>Serverless Design: Fully aligns with the serverless architecture requirement while ensuring backward compatibility.</li></ul><p>Thus, Option A is the correct answer.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "3890d3eab2a94c02ba61b5a9011b22b1",
      "questionNumber": 6,
      "type": "multiple",
      "content": "Question #6<p>A retail company needs to provide a series of data files to another company, which is its business partner. These files are saved in an Amazon S3 bucket under Account A, which belongs to the retail company. The business partner company wants one of its IAM users, User_DataProcessor, to access the files from its own AWS account (Account B).</p><p>Which combination of steps must the companies take so that User_DataProcessor can access the S3 bucket successfully? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Turn on the cross-origin resource sharing (CORS) feature for the S3 bucket in Account A."
        },
        {
          "label": "B",
          "content": "In Account A, set the S3 bucket policy to the following:<img src=\"images/sap/26dc0168e1d87a53fb5e3b1ec70bd818d068a834.png\" >"
        },
        {
          "label": "C",
          "content": "In Account A, set the S3 bucket policy to the following:<img src=\"images/sap/f72d638327f404d54ec27fb6aeb9de801b2a1289.png\" >"
        },
        {
          "label": "D",
          "content": "In Account B, set the permissions of User_DataProcessor to the following:<img src=\"images/sap/f1d421a4bd9c09ce357fcffbb01f0fbf0a6c78cc.png\" >"
        },
        {
          "label": "E",
          "content": "&nbsp;In Account B, set the permissions of User_DataProcessor to the following:<img src=\"images/sap/0df71cf187317eeb945f1544749574d395f3d2f4.png\" >"
        }
      ],
      "correctAnswer": "CD",
      "explanation": "To allow User_DataProcessor to access the S3 bucket from Account B, the following steps need to be taken: <p>In Account A, set the S3 bucket policy to allow access to the bucket from the IAM user in Account B. This is done by adding a statement to the </p><p>bucket policy that allows the IAM user in Account B to perform the necessary actions (GetObject and ListBucket) on the bucket and its contents. </p><p>In Account B, create an IAM policy that allows the IAM user (User_DataProcessor) to perform the necessary actions (GetObject and ListBucket) on </p><p>the S3 bucket and its contents. The policy should reference the ARN of the S3 bucket and the actions that the user is allowed to perform. </p><p>Note: turning on the cross-origin resource sharing (CORS) feature for the S3 bucket in Account A is not necessary for this scenario as it is typically </p><p>used for allowing web browsers to access resources from different domains. </p><p><br></p><p>此题具有一定的争议，可自行确认。建议C&amp;D</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "c92b505af3d348358b671f331f1807fb",
      "questionNumber": 7,
      "type": "single",
      "content": "<p>Question #7</p><p>A company is running a traditional web application on Amazon EC2 instances. The company needs to refactor the application as microservices that run on containers. Separate versions of the application exist in two distinct environments: production and testing. Load for the application is variable, but the minimum load and the maximum load are known. A solutions architect needs to design the updated application with a serverless architecture that minimizes operational complexity.</p><p>Which solution will meet these requirements MOST cost-effectively?</p>",
      "options": [
        {
          "label": "A",
          "content": "Upload the container images to AWS Lambda as functions. Configure a concurrency limit for the associated Lambda functions to handle the expected peak load. Configure two separate Lambda integrations within Amazon API Gateway: one for production and one for testing."
        },
        {
          "label": "B",
          "content": "Upload the container images to Amazon Elastic Container Registry (Amazon ECR). Configure two auto scaled Amazon Elastic Container Service (Amazon ECS) clusters with the Fargate launch type to handle the expected load. Deploy tasks from the ECR images. Configure two separate Application Load Balancers to direct traffic to the ECS clusters."
        },
        {
          "label": "C",
          "content": "Upload the container images to Amazon Elastic Container Registry (Amazon ECR). Configure two auto scaled Amazon Elastic Kubernetes Service (Amazon EKS) clusters with the Fargate launch type to handle the expected load. Deploy tasks from the ECR images. Configure two separate Application Load Balancers to direct traffic to the EKS clusters."
        },
        {
          "label": "D",
          "content": "Upload the container images to AWS Elastic Beanstalk. In Elastic Beanstalk, create separate environments and deployments for production and testing. Configure two separate Application Load Balancers to direct traffic to the Elastic Beanstalk deployments."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Themost cost-effective solution that meets the requirements (minimizing operational complexity while supporting variable load with microservices in containers) is: &nbsp;</p><p>Option B &nbsp;</p><p>Upload the container images to Amazon Elastic Container Registry (Amazon ECR). Configure two auto-scaled Amazon Elastic Container Service (Amazon ECS) clusters with the Fargate launch type to handle the expected load. Deploy tasks from the ECR images. Configure two separate Application Load Balancers to direct traffic to the ECS clusters. &nbsp;</p><p>#Why Option B? &nbsp;</p><p>1.Serverless & Low Operational Overhead: &nbsp;</p><p> &nbsp; -ECS with Fargate is serverless, eliminating the need to manage EC2 instances. &nbsp;</p><p> &nbsp; - Auto-scaling ensures cost efficiency by adjusting to variable load. &nbsp;</p><p>2.Container Support: &nbsp;</p><p> &nbsp; - ECR stores container images, and ECS deploys them seamlessly. &nbsp;</p><p>3.Separate Environments: &nbsp;</p><p> &nbsp; - Two distinct ECS clusters (production & testing) ensure isolation. &nbsp;</p><p> &nbsp; - Application Load Balancers (ALBs) route traffic appropriately. &nbsp;</p><p>4.Cost-Effectiveness: &nbsp;</p><p> &nbsp; - Fargate scales automatically, avoiding over-provisioning. &nbsp;</p><p> &nbsp; - No Kubernetes overhead (unlike EKS in Option C). &nbsp;</p><p>#Why Not Other Options? &nbsp;</p><p>-Option A (Lambda with containers): Lambda is not ideal for long-running microservices (cold starts, concurrency limits). &nbsp;</p><p>-Option C (EKS with Fargate): EKS introduces unnecessary Kubernetes complexity for this use case. &nbsp;</p><p>-Option D (Elastic Beanstalk): Not fully serverless, and less suited for microservices compared to ECS Fargate. &nbsp;</p><p>Conclusion:Option B provides the best balance of cost efficiency, scalability, and operational simplicity. ✅</p><p><br></p><p>ref:https://www.densify.com/eks-best-practices/aws-ecs-vs-eks/</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "49cc5192c49541dfb7ce91f9a9b6447d",
      "questionNumber": 8,
      "type": "single",
      "content": "<p>Question #8</p><p>A company has a multi-tier web application that runs on a fleet of Amazon EC2 instances behind an Application Load Balancer (ALB). The instances are in an Auto Scaling group. The ALB and the Auto Scaling group are replicated in a backup AWS Region. The minimum value and the maximum value for the Auto Scaling group are set to zero. An Amazon RDS Multi-AZ DB instance stores the application’s data. The DB instance has a read replica in the backup Region. The application presents an endpoint to end users by using an Amazon Route 53 record.</p><p>The company needs to reduce its RTO to less than 15 minutes by giving the application the ability to automatically fail over to the backup Region.The company does not have a large enough budget for an active-active strategy. </p><p>What should a solutions architect recommend to meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Reconfigure the application&rsquo;s Route 53 record with a latency-based routing policy that load balances traffic between the two ALBs. Create an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group values. Create an Amazon CloudWatch alarm that is based on the HTTPCode_Target_5XX_Count metric for the ALB in the primary Region. Configure the CloudWatch alarm to invoke the Lambda function."
        },
        {
          "label": "B",
          "content": "&nbsp;Create an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group values. Configure Route 53 with a health check that monitors the web application and sends an Amazon Simple Notification Service (Amazon SNS) notification to the Lambda function when the health check status is unhealthy. Update the application&rsquo;s Route 53 record with a failover policy that routes traffic to the ALB in the backup Region when a health check failure occurs."
        },
        {
          "label": "C",
          "content": "Configure the Auto Scaling group in the backup Region to have the same values as the Auto Scaling group in the primary Region. Reconfigure the application&rsquo;s Route 53 record with a latency-based routing policy that load balances traffic between the two ALBs. Remove the read replica. Replace the read replica with a standalone RDS DB instance. Configure Cross-Region Replication between the RDS DB instances by using snapshots and Amazon S3."
        },
        {
          "label": "D",
          "content": "Configure an endpoint in AWS Global Accelerator with the two ALBs as equal weighted targets. Create an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group values. Create an Amazon CloudWatch alarm that is based on the HTTPCode_Target_5XX_Count metric for the ALB in the primary Region. Configure the CloudWatch alarm to invoke the Lambda function."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>To reduce the RTO to less than 15 minutes and enable automatic failover to the backup Region, the solution recommended in option B should be implemented. By creating an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group values, the company can ensure that the backup Region is ready to take over in case of a failure. Configuring Route 53 with a health check that monitors the web application and triggers the Lambda function when the health check status is unhealthy allows for immediate action in the event of a failure. Updating the Route 53 record with a failover policy ensures that traffic is automatically routed to the ALB in the backup Region when the health check fails, facilitating a quick recovery.</p><p>The correct answer isB. Here's why:</p><p>Requirements:</p><p>1.Reduce RTO to &lt;15 minutes with automatic failover to the backup Region.</p><p>2.No active-active strategy (due to budget constraints).</p><p>3. Current setup:</p><p> &nbsp; - Multi-tier web app on EC2 (Auto Scaling + ALB) in primary Region.</p><p> &nbsp; - Backup Region has ALB + Auto Scaling (min/max = 0).</p><p> &nbsp; - RDS Multi-AZ in primary Region + read replica in backup Region.</p><p> &nbsp; - Route 53 manages the application endpoint.</p><p>Why Option B?</p><p>-Route 53 Failover Routing Policy: This is the most cost-effective way to automate failover. Route 53 can monitor the health of the primary Region and switch traffic to the backup Region when unhealthy.</p><p>-Lambda Function: Promotes the RDS read replica to a standalone instance and modifies the Auto Scaling group in the backup Region (scaling from 0 to desired capacity).</p><p>-Health Check + SNS: Route 53 health checks can trigger the Lambda function via SNS when the primary Region fails.</p><p>-Meets RTO Requirement: The combination of Route 53 failover and Lambda automation ensures fast recovery (&lt;15 minutes).</p><p>Why Not Other Options?</p><p>-A: Latency-based routing doesn’t provide failover; CloudWatch alarms alone don’t ensure Route 53 traffic redirection.</p><p>-C: Active-active setup (latency routing + equal Auto Scaling) is expensive and against budget constraints. Cross-Region replication via snapshots is slow.</p><p>-D: Global Accelerator improves performance but doesn’t inherently handle database failover or Auto Scaling adjustments.</p><p>Conclusion:</p><p>B is the most cost-effective and automated solution for achieving&lt;15-minute RTO withpassive standby in the backup Region. &nbsp;</p><p>Answer: B</p><p>ref:https://docs.amazonaws.cn/en_us/Route53/latest/DeveloperGuide/welcome-health-checks.html &nbsp;and https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "95f9e4055e1940a0b7ca08a7a4e05f49",
      "questionNumber": 9,
      "type": "multiple",
      "content": "<p>Question #9</p><p>A company is hosting a critical application on a single Amazon EC2 instance. The application uses an Amazon ElastiCache for Redis single-node cluster for an in-memory data store. The application uses an Amazon RDS for MariaDB DB instance for a relational database. For the application to function, each piece of the infrastructure must be healthy and must be in an active state.</p><p>A solutions architect needs to improve the application's architecture so that the infrastructure can automatically recover from failure with the least possible downtime.</p><p><br></p><p>Which combination of steps will meet these requirements? (Choose three.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Use an Elastic Load Balancer to distribute traffic across multiple EC2 instances. Ensure that the EC2 instances are part of an Auto Scaling group that has a minimum capacity of two instances."
        },
        {
          "label": "B",
          "content": "Use an Elastic Load Balancer to distribute traffic across multiple EC2 instances. Ensure that the EC2 instances are configured in unlimited mode."
        },
        {
          "label": "C",
          "content": "Modify the DB instance to create a read replica in the same Availability Zone. Promote the read replica to be the primary DB instance in failure scenarios."
        },
        {
          "label": "D",
          "content": "Modify the DB instance to create a Multi-AZ deployment that extends across two Availability Zones."
        },
        {
          "label": "E",
          "content": "Create a replication group for the ElastiCache for Redis cluster. Configure the cluster to use an Auto Scaling group that has a minimum capacity of two instances."
        },
        {
          "label": "F",
          "content": "Create a replication group for the ElastiCache for Redis cluster. Enable Multi-AZ on the cluster."
        }
      ],
      "correctAnswer": "ADF",
      "explanation": "<p>The correct combination of steps to improve the application's architecture for automatic recovery from failure with minimal downtime is:</p><p>A,D, andF. &nbsp;</p><p> Explanation: &nbsp;</p><p>1.A – Using anElastic Load Balancer (ELB) with anAuto Scaling group (minimum of two instances) ensures high availability for the EC2 instances. If one instance fails, the other can continue serving traffic, and Auto Scaling will replace the failed instance. &nbsp;</p><p>2.D – Modifying theRDS for MariaDB to a Multi-AZ deployment ensures automatic failover to a standby replica in another Availability Zone if the primary DB instance fails, minimizing downtime. &nbsp;</p><p>3.F – Creating areplication group for ElastiCache for Redis with Multi-AZ enabled ensures high availability by maintaining a standby replica in a different Availability Zone, allowing automatic failover if the primary node fails. &nbsp;</p><p> Why not the others? &nbsp;</p><p>-B – \"Unlimited mode\" for EC2 instances is not relevant to high availability. &nbsp;</p><p>-C – A read replica in thesame AZ does not provide automatic failover (Multi-AZ does). &nbsp;</p><p>-E – ElastiCache does not use Auto Scaling groups; instead, it usesreplication groups with Multi-AZ. &nbsp;</p><p>Thus, the best combination isA, D, and F. ✅</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "872591509a184e3aaad487e21e11ecc3",
      "questionNumber": 10,
      "type": "multiple",
      "content": "<p>Question #10</p><p>A retail company is operating its ecommerce application on AWS. The application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The company uses an Amazon RDS DB instance as the database backend. Amazon CloudFront is configured with one origin that points to the ALB. Static content is cached. Amazon Route 53 is used to host all public zones.</p><p>After an update of the application, the ALB occasionally returns a 502 status code (Bad Gateway) error. The root cause is malformed HTTP headers that are returned to the ALB. The webpage returns successfully when a solutions architect reloads the webpage immediately after the error occurs.</p><p>While the company is working on the problem, the solutions architect needs to provide a custom error page instead of the standard ALB error page to visitors.</p><p>Which combination of steps will meet this requirement with the LEAST amount of operational overhead? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an Amazon S3 bucket. Configure the S3 bucket to host a static webpage. Upload the custom error pages to Amazon S3."
        },
        {
          "label": "B",
          "content": "Create an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health check response Target.FailedHealthChecks is greater than 0. Configure the Lambda function to modify the forwarding rule at the ALB to point to a publicly accessible web server."
        },
        {
          "label": "C",
          "content": "Modify the existing Amazon Route 53 records by adding health checks. Configure a fallback target if the health check fails. Modify DNS records to point to a publicly accessible webpage."
        },
        {
          "label": "D",
          "content": "Create an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health check response Elb.InternalError is greater than 0. Configure the Lambda function to modify the forwarding rule at the ALB to point to a public accessible web server."
        },
        {
          "label": "E",
          "content": "&nbsp;Add a custom error response by confi guring a CloudFront custom error page. Modify DNS records to point to a publicly accessible web page.&nbsp;"
        }
      ],
      "correctAnswer": "AE",
      "explanation": "<p>To provide a custom error page with the least amount of operational overhead, the solutions architect should create an Amazon S3 bucket to host a static webpage with the custom error pages (option A) and configure Amazon CloudFront to use those custom error pages (option E). This approach allows for a quick and straightforward implementation without the need to modify the underlying infrastructure or manage additional services like AWS Lambda or Route 53 health checks. The custom error pages can be easily updated and maintained in the S3 bucket, and CloudFront can be quickly reconfigured to point to the new error pages, minimizing the impact on the ongoing operations.</p><p>The correct answers areA andE. &nbsp;</p><p>Explanation: &nbsp;</p><p>The requirement is todisplay a custom error page when the ALB returns a502 error (Bad Gateway) due to malformed HTTP headers, with theleast operational overhead. &nbsp;</p><p>#Option A: &nbsp;</p><p>-Create an S3 bucket to host a static custom error page. &nbsp;</p><p>- This is a simple and cost-effective way to serve a static error page without additional infrastructure. &nbsp;</p><p>#Option E: &nbsp;</p><p>-Configure CloudFront to return a custom error page when a 502 error occurs. &nbsp;</p><p>- Since CloudFront is already in front of the ALB, this is the most straightforward solution. CloudFront allows you to definecustom error pages for specific HTTP status codes (like 502) and point them to an S3 bucket or another origin. &nbsp;</p><p>Why not the other options? &nbsp;</p><p>-B & D: Using Lambda to modify ALB rules introduces unnecessary complexity and operational overhead. &nbsp;</p><p>-C: Modifying Route 53 health checks and DNS records is not ideal because it would require switching traffic away from the application entirely, which is overkill for handling transient 502 errors. &nbsp;</p><p>Best Solution: &nbsp;</p><p>1.Upload a custom error page to S3 (A). &nbsp;</p><p>2.Configure CloudFront to serve this custom error page when a 502 occurs (E). &nbsp;</p><p>This approach isserverless, low-maintenance, and leverages existing AWS services (CloudFront + S3) without requiring additional infrastructure changes. &nbsp;</p><p>Final Answer:A & E</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "e497a62553f9478b88f0249fedcd5307",
      "questionNumber": 11,
      "type": "multiple",
      "content": "<p>Question #11</p><p>A company has many AWS accounts and uses AWS Organizations to manage them. A solutions architect must implement a solution that the company can use to share a common network across multiple accounts.</p><p><br></p><p>The company’s infrastructure team has a dedicated infrastructure account that has a VPC. The infrastructure team must use this account to manage the network. Individual accounts cannot have the ability to manage their own networks. However, individual accounts must be able to create AWS resources within subnets.</p><p><br>Which combination of actions should the solutions architect perform to meet these requirements? (Choose two.) </p>",
      "options": [
        {
          "label": "A",
          "content": "Create a transit gateway in the infrastructure account."
        },
        {
          "label": "B",
          "content": "Enable resource sharing from the AWS Organizations management account."
        },
        {
          "label": "C",
          "content": "Create VPCs in each AWS account within the organization in AWS Organizations. Configure the VPCs to share the same CIDR range and subnets as the VPC in the infrastructure account. Peer the VPCs in each individual account with the VPC in the infrastructure account."
        },
        {
          "label": "D",
          "content": "Create a resource share in AWS Resource Access Manager in the infrastructure account. Select the specific AWS Organizations OU that will use the shared network. Select each subnet to associate with the resource share."
        },
        {
          "label": "E",
          "content": "Create a resource share in AWS Resource Access Manager in the infrastructure account. Select the specific AWS Organizations OU that will use the shared network. Select each prefix list to associate with the resource share."
        }
      ],
      "correctAnswer": "BD",
      "explanation": "<p>Based on the information provided, to implement a solution for sharing a common network across multiple AWS accounts while ensuring the infrastructure team maintains control of the network management, the following two actions should be taken:</p><p>B. Enable resource sharing from the AWS Organizations management account.</p><p>D. Create a resource share in AWS Resource Access Manager in the infrastructure account. Select the specific AWS Organizations OU that will use the shared network. Select each subnet to associate with the resource share.</p><p>This combination works because:</p><p>First, you need to enable resource sharing at the AWS Organizations level (option B). This is a prerequisite step that must be performed in the management account of the organization before any resources can be shared between accounts.</p><p>Then, in the infrastructure account that owns the VPC, you need to create a resource share using AWS Resource Access Manager (RAM) and specify which organizational units (OUs) should have access to the shared network resources (option D). By selecting specific subnets to associate with the resource share, you're allowing the individual accounts to create resources within those subnets while maintaining network management control in the infrastructure account.</p><p>This approach satisfies all requirements:</p><p>The infrastructure team maintains control of network management through their dedicated account</p><p>Individual accounts cannot manage the network themselves</p><p>Individual accounts can create AWS resources within the shared subnets</p><p>The common network is shared across multiple accounts in the organization</p><p>The other options would either not fulfill all requirements or implement the sharing incorrectly.</p><p>Sources</p><p>Shareable AWS resources - AWS Resource Access Manager （https://docs.aws.amazon.com/ram/latest/userguide/shareable.html）</p><p>SEC03-BP08 Share resources securely - AWS Well-Architected Framework (2022-03-31) （https://docs.aws.amazon.com/wellarchitected/2022-03-31/framework/sec_permissions_share_securely.html）</p><p>Share your VPC subnets with other accounts - Amazon Virtual Private Cloud （https://docs.aws.amazon.com/vpc/latest/userguide/vpc-sharing.html）</p><p>SEC03-BP08 Share resources securely within your organization - AWS Well-Architected Framework （https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_permissions_share_securely.html）</p><p>VPC sharing - Building a Scalable and Secure Multi-VPC AWS Network Infrastructure （https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/amazon-vpc-sharing.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "c89d938fb1e44d33a0546a35d6b613e8",
      "questionNumber": 12,
      "type": "single",
      "content": "<p>Question #12</p><p>A company wants to use a third-party software-as-a-service (SaaS) application. The third-party SaaS application is consumed through several API calls. The third-party SaaS application also runs on AWS inside a VPC.</p><p>The company will consume the third-party SaaS application from inside a VPC. The company has internal security policies that mandate the use of private connectivity that does not traverse the internet. No resources that run in the company VPC are allowed to be accessed from outside the company’s VPC. All permissions must conform to the principles of least privilege.<br>Which solution meets these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an AWS PrivateLink interface VPC endpoint. Connect this endpoint to the endpoint service that the third-party SaaS application provides. Create a security group to limit the access to the endpoint. Associate the security group with the endpoint."
        },
        {
          "label": "B",
          "content": "Create an AWS Site-to-Site VPN connection between the third-party SaaS application and the company VPC. Configure network ACLs to limit access across the VPN tunnels."
        },
        {
          "label": "C",
          "content": "Create a VPC peering connection between the third-party SaaS application and the company VPUpdate route tables by adding the needed routes for the peering connection."
        },
        {
          "label": "D",
          "content": "Create an AWS PrivateLink endpoint service. Ask the third-party SaaS provider to create an interface VPC endpoint for this endpoint service. Grant permissions for the endpoint service to the specific account of the third-party SaaS provider."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>The correct answer isA. Here's why:</p><p>Requirements Summary:</p><p>1.Private connectivity (no internet traversal).</p><p>2.No inbound access from outside the company’s VPC.</p><p>3.Least privilege permissions must be enforced.</p><p>Why Option A is Correct:</p><p>-AWS PrivateLink (Interface VPC Endpoint) allows private connectivity between the company’s VPC and the third-party SaaS applicationwithout traversing the internet.</p><p>- The company creates aninterface VPC endpoint in its own VPC and connects it to the SaaS provider’sendpoint service.</p><p>-Security groups can restrict access to only the necessary resources, enforcing least privilege.</p><p>-No inbound access is allowed from outside the company’s VPC (meets security policy).</p><p>Why Other Options Are Incorrect:</p><p>-B. Site-to-Site VPN: &nbsp;</p><p> &nbsp;- Traffic still traverses an encrypted VPN over the internet (does not fully meet \"no internet traversal\" requirement). &nbsp;</p><p> &nbsp;- Network ACLs are less granular than security groups for least privilege.</p><p>-C. VPC Peering: &nbsp;</p><p> &nbsp;- Requires routing configuration that could inadvertently allow unwanted access. &nbsp;</p><p> &nbsp;- Peering does not inherently enforce least privilege as strictly as PrivateLink. &nbsp;</p><p> &nbsp;- SaaS provider’s VPC and company VPC must be in the same region (not always feasible).</p><p>-D. PrivateLink Endpoint Service (Incorrect Direction): &nbsp;</p><p> &nbsp;- This would allow the SaaS provider to initiate connections into the company’s VPC (violates \"no inbound access\" policy). &nbsp;</p><p> &nbsp;- The company should be the consumer, not the provider, of the endpoint service.</p><p>Conclusion: &nbsp;</p><p>Option A is the only solution that fully meets the requirements of private connectivity, no internet traversal, no inbound access, and least privilege. &nbsp;</p><p>Answer: A</p><p>ref: https://docs.aws.amazon.com/vpc/latest/privatelink/privatelink-access-saas.html </p><p><br></p><p>- The interface VPC Endpoint will provide local access to the SaaS service from within the company's VPC. Moreover, traffic to and acces from the SaaS VPC will traverse the AWS network rather than the internet. This is considered private traffic . ref:</p><p>https://docs.aws.amazon.com/vpc/latest/privatelink/privatelink-access-saas.html https://aws.amazon.com/blogs/apn/enabling-new-saas-strategies-with-aws-privatelink/ &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "16ff896812ab4d9897b5b360596bef4c",
      "questionNumber": 13,
      "type": "single",
      "content": "<p>Question #13<br>A company needs to implement a patching process for its servers. The on-premises servers and Amazon EC2 instances use a variety of tools to perform patching. Management requires a single report showing the patch status of all the servers and instances.</p><p><br>Which set of actions should a solutions architect take to meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use AWS Systems Manager to manage patches on the on-premises servers and EC2 instances. Use Systems Manager to generate patch compliance reports."
        },
        {
          "label": "B",
          "content": "Use AWS OpsWorks to manage patches on the on-premises servers and EC2 instances. Use Amazon QuickSight integration with OpsWorks to generate patch compliance reports."
        },
        {
          "label": "C",
          "content": "Use an Amazon EventBridge rule to apply patches by scheduling an AWS Systems Manager patch remediation job. Use Amazon Inspector to generate patch compliance reports."
        },
        {
          "label": "D",
          "content": "Use AWS OpsWorks to manage patches on the on-premises servers and EC2 instances. Use AWS X-Ray to post the patch status to AWS Systems Manager OpsCenter to generate patch compliance reports."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Based on the information provided, the best answer for implementing a patching process with unified reporting across on-premises servers and EC2 instances is:</p><p>A. Use AWS Systems Manager to manage patches on the on-premises servers and EC2 instances. Use Systems Manager to generate patch compliance reports.</p><p>This is the correct approach because:</p><p>AWS Systems Manager is specifically designed to manage both on-premises servers and EC2 instances through the Systems Manager Agent (SSM Agent), which can be installed on both types of environments.</p><p>Systems Manager Patch Manager provides a unified patching solution that works across different operating systems and environments, addressing the company's need to consolidate their variety of patching tools.</p><p>Systems Manager includes built-in patch compliance reporting capabilities that can generate a single comprehensive report showing the patch status of all servers and instances, which directly fulfills the management requirement.</p><p>Option B is incorrect because AWS OpsWorks is primarily a configuration management service that uses Chef or Puppet, and while it can help with some aspects of server management, it's not specifically designed for cross-platform patching with unified reporting.</p><p>Option C is incomplete because while Amazon EventBridge can schedule Systems Manager patch jobs, Amazon Inspector is focused on vulnerability assessment rather than providing comprehensive patch compliance reporting across both on-premises and EC2 environments.</p><p>AWS Systems Manager (Option A) provides the most comprehensive and purpose-built solution for this specific requirement.</p><p>Sources</p><p>Increase visibility and governance on cloud with AWS Cloud Operations services – Part 2 | AWS Cloud Operations & Migrations Blog （https://aws.amazon.com/cn/blogs/mt/increase-visibility-and-governance-aws-cloud-operations-part-2/）</p><p>AWS Systems Manager Patch Manager - AWS Systems Manager （https://docs.aws.amazon.com/systems-manager/latest/userguide/patch-manager.html）</p><p>Getting Started with Patch Manager and Amazon EC2 Systems Manager | AWS Cloud Operations & Migrations Blog （https://aws.amazon.com/cn/blogs/mt/getting-started-with-patch-manager-and-amazon-ec2-systems-manager/）</p><p>Automate Systems Manager patching reports via email and Slack notifications in an AWS Organization | AWS Cloud Operations Blog （https://aws.amazon.com/cn/blogs/mt/automate-systems-manager-patching-reports-via-email-and-slack-notifications-in-an-aws-organization/）</p><p>Patching solution design for mutable EC2 instances - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/patch-management-hybrid-cloud/design-standard.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "676b0f8353ea4fe5921bfad7a5286925",
      "questionNumber": 14,
      "type": "single",
      "content": "<p>Question #14<br>A company is running an application on several Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer. The load on the application varies throughout the day, and EC2 instances are scaled in and out on a regular basis. Log files from the EC2 instances are copied to a central Amazon S3 bucket every 15 minutes. The security team discovers that log files are missing from some of the terminated EC2 instances.<br><br>Which set of actions will ensure that log files are copied to the central S3 bucket from the terminated EC2 instances?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a script to copy log files to Amazon S3, and store the script in a file on the EC2 instance. Create an Auto Scaling lifecycle hook and an Amazon EventBridge rule to detect lifecycle events from the Auto Scaling group. Invoke an AWS Lambda function on the autoscaling:EC2_INSTANCE_TERMINATING transition to send ABANDON to the Auto Scaling group to prevent termination, run the script to copy the log files, and terminate the instance using the AWS SDK."
        },
        {
          "label": "B",
          "content": "Create an AWS Systems Manager document with a script to copy log files to Amazon S3. Create an Auto Scaling lifecycle hook and an Amazon EventBridge rule to detect lifecycle events from the Auto Scaling group. Invoke an AWS Lambda function on the autoscaling:EC2_INSTANCE_TERMINATING transition to call the AWS Systems Manager API SendCommand operation to run the document to copy the log files and send CONTINUE to the Auto Scaling group to terminate the instance."
        },
        {
          "label": "C",
          "content": "Change the log delivery rate to every 5 minutes. Create a script to copy log files to Amazon S3, and add the script to EC2 instance user data. Create an Amazon EventBridge rule to detect EC2 instance termination. Invoke an AWS Lambda function from the EventBridge rule that uses the AWS CLI to run the user-data script to copy the log files and terminate the instance."
        },
        {
          "label": "D",
          "content": "Create an AWS Systems Manager document with a script to copy log files to Amazon S3. Create an Auto Scaling lifecycle hook that publishes a message to an Amazon Simple Notification Service (Amazon SNS) topic. From the SNS notification, call the AWS Systems Manager API SendCommand operation to run the document to copy the log files and send ABANDON to the Auto Scaling group to terminate the instance."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Option B is the correct solution as it uses an Auto Scaling lifecycle hook and an Amazon EventBridge rule to detect lifecycle events from the Auto Scaling group. An AWS Lambda function is then invoked on the autoscaling:EC2_INSTANCE_TERMINATING transition to call the AWS Systems Manager API SendCommand operation to run the document to copy the log files and send CONTINUE to the Auto Scaling group to terminate the instance. This ensures that log files are copied to S3 before the instance is terminated.</p><p>Problem Analysis:</p><p>- EC2 instances in an Auto Scaling group are terminated before log files are fully copied to S3.</p><p>- Logs are copied every 15 minutes, but some instances are terminated before the next sync.</p><p>- Need a mechanism to ensure logs are copiedbefore termination.</p><p>Why Option B is Correct:</p><p>1.Auto Scaling Lifecycle Hook: &nbsp;</p><p> &nbsp; - A lifecycle hook (`autoscaling:EC2_INSTANCE_TERMINATING`) pauses instance termination, allowing time to perform actions.</p><p>2.AWS Systems Manager (SSM) Document: &nbsp;</p><p> &nbsp; - Uses `SendCommand` to execute a predefined script (stored in SSM) to copy logs to S3.</p><p> &nbsp; - SSM ensures the script runs even if the instance is in the process of termination.</p><p>3.EventBridge + Lambda Integration: &nbsp;</p><p> &nbsp; - EventBridge detects the termination event.</p><p> &nbsp; - Lambda triggers the SSM `SendCommand` to copy logs.</p><p> &nbsp; - After completion, Lambda sends `CONTINUE` to Auto Scaling to proceed with termination.</p><p>Why Other Options Are Incorrect:</p><p>-A: Uses `ABANDON` instead of `CONTINUE`, which prevents Auto Scaling from terminating the instance, requiring manual termination (inefficient and error-prone).</p><p>-C: Changing the log delivery rate to 5 minutes doesn’t guarantee logs are copied before termination. Also, user-data scripts run at launch, not termination.</p><p>-D: Uses `ABANDON` (incorrect for this use case) and relies on SNS, which adds unnecessary complexity compared to EventBridge + Lambda.</p><p>Key AWS Services Used:</p><p>-Auto Scaling Lifecycle Hooks (to delay termination).</p><p>-AWS Systems Manager (SSM) (to reliably run commands on instances).</p><p>-Amazon EventBridge (to detect lifecycle events).</p><p>-AWS Lambda (to trigger SSM and resume termination).</p><p>Thus,B is the most reliable and automated solution. &nbsp;</p><p>Answer: B</p><p>https://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/ </p><p>https://www.examtopics.com/discussions/amazon/view/69532-exam-aws-certified-solutions-architect-professional-topic-1/ &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "cac0dd6fb1ff461384ad4e55412d963c",
      "questionNumber": 15,
      "type": "multiple",
      "content": "<p>Question #15 <br>A company is using multiple AWS accounts. The DNS records are stored in a private hosted zone for Amazon Route 53 in Account A. The company’s applications and databases are running in Account B.</p><p><br></p><p>A solutions architect will deploy a two-tier application in a new VPC. To simplify the confi guration, the db.example.com CNAME record set for the Amazon RDS endpoint was created in a private hosted zone for Amazon Route 53. </p><p><br></p><p>During deployment, the application failed to start. Troubleshooting revealed that db.example.com is not resolvable on the Amazon EC2 instance. </p><p><br></p><p>The solutions architect confi rmed that the record set was created correctly in Route 53. </p><p><br></p><p>Which combination of steps should the solutions architect take to resolve this issue? (Choose two.) </p>",
      "options": [
        {
          "label": "A",
          "content": "Deploy the database on a separate EC2 instance in the new VPC. Create a record set for the instance&rsquo;s private IP in the private hosted zone."
        },
        {
          "label": "B",
          "content": "Use SSH to connect to the application tier EC2 instance. Add an RDS endpoint IP address to the /etc/resolv.conf file."
        },
        {
          "label": "C",
          "content": "Create an authorization to associate the private hosted zone in Account A with the new VPC in Account B."
        },
        {
          "label": "D",
          "content": "Create a private hosted zone for the example com domain in Account B. Configure Route 53 replication between AWS accounts."
        },
        {
          "label": "E",
          "content": "Associate a new VPC in Account B with a hosted zone in Account A. Delete the association authorization in Account A."
        }
      ],
      "correctAnswer": "CE",
      "explanation": "<p>The correct answers are:</p><p>C. Create an authorization to associate the private hosted zone in Account A with the new VPC in Account B. &nbsp;</p><p>E. Associate a new VPC in Account B with a hosted zone in Account A. Delete the association authorization in Account A.</p><p> Explanation:</p><p>1.Private Hosted Zone Sharing Across Accounts: &nbsp;</p><p> &nbsp; - The private hosted zone (`db.example.com`) is inAccount A, but the application and database inAccount B need to resolve this DNS record. &nbsp;</p><p> &nbsp; - To allowAccount B to resolve records fromAccount A's private hosted zone, you must: &nbsp;</p><p> &nbsp; &nbsp; -Authorize the association inAccount A (the hosted zone owner). &nbsp;</p><p> &nbsp; &nbsp; -Associate the VPC inAccount B with the hosted zone inAccount A. &nbsp;</p><p> &nbsp; - After the association is complete, the authorization can be deleted (optional but recommended for security). &nbsp;</p><p>2.Why Other Options Are Incorrect: &nbsp;</p><p> &nbsp; -A: Deploying the database on a separate EC2 instance is unnecessary—the issue is DNS resolution, not the database itself. &nbsp;</p><p> &nbsp; -B: Manually modifying `/etc/resolv.conf` is not a scalable or recommended solution. &nbsp;</p><p> &nbsp; -D: Creating a duplicate private hosted zone inAccount B and setting up replication is overly complex—cross-account VPC association is the correct approach. &nbsp;</p><p> Steps to Resolve:</p><p>1. InAccount A, authorizeAccount B's VPC to associate with the private hosted zone. &nbsp;</p><p>2. InAccount B, associate the VPC with the hosted zone. &nbsp;</p><p>3. (Optional) Delete the authorization inAccount A after association. &nbsp;</p><p>This ensures the EC2 instances inAccount B can resolve `db.example.com` via the shared private hosted zone. &nbsp;</p><p>Final Answer: C & E</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "7c058267b8ab499abcc864ae9c83998e",
      "questionNumber": 16,
      "type": "single",
      "content": "<p>Question #16<br>A company used Amazon EC2 instances to deploy a web fleet to host a blog site. The EC2 instances are behind an Application Load Balancer (ALB) and are configured in an Auto Scaling group. The web application stores all blog content on an Amazon EFS volume.</p><p><br></p><p>The company recently added a feature for bloggers to add video to their posts, attracting 10 times the previous user traffic. At peak times of day, users report buffering and timeout issues while attempting to reach the site or watch videos.<br><br>Which is the MOST cost-effi cient and scalable deployment that will resolve the issues for users?</p>",
      "options": [
        {
          "label": "A",
          "content": "Reconfigure Amazon EFS to enable maximum I/O."
        },
        {
          "label": "B",
          "content": "Update the blog site to use instance store volumes for storage. Copy the site contents to the volumes at launch and to Amazon S3 at shutdown."
        },
        {
          "label": "C",
          "content": "Configure an Amazon CloudFront distribution. Point the distribution to an S3 bucket, and migrate the videos from EFS to Amazon S3."
        },
        {
          "label": "D",
          "content": "Set up an Amazon CloudFront distribution for all site contents, and point the distribution at the ALB."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the information provided, the most cost-efficient and scalable deployment that will resolve the buffering and timeout issues for users is:</p><p>C. Configure an Amazon CloudFront distribution. Point the distribution to an S3 bucket, and migrate the videos from EFS to Amazon S3.</p><p>This solution is optimal because:</p><p>Video content delivery is the primary cause of the performance issues. CloudFront is a content delivery network (CDN) that caches content at edge locations worldwide, significantly reducing latency for video streaming.</p><p>Amazon S3 is designed for high-throughput storage and retrieval of objects like video files, making it much more suitable than EFS for this use case.</p><p>This approach offloads the video content delivery from the EC2 instances and EFS, allowing the web servers to focus on serving the blog's dynamic content.</p><p>CloudFront with S3 provides automatic scaling to handle the 10x increase in traffic without requiring manual intervention or complex configurations.</p><p>The pay-as-you-go pricing model of CloudFront and S3 is more cost-effective for serving video content compared to scaling up EC2 instances or upgrading EFS performance modes.</p><p>Option A (reconfiguring EFS for maximum I/O) would increase costs significantly without addressing the fundamental issue that EFS isn't optimized for serving video content to many simultaneous users.</p><p>Option B (using instance store volumes) would be complex to manage, wouldn't persist through instance terminations without additional work, and wouldn't solve the fundamental issue of efficiently delivering video content to users across different geographical locations.</p><p>Sources</p><p>Videos are lagging and too slow to load | AWS re:Post （https://repost.aws/questions/QUwi8VdIPqSCGjqnpuy0xf5w/videos-are-lagging-and-too-slow-to-load）</p><p>Video on demand and live streaming video with CloudFront - Amazon CloudFront （https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/on-demand-streaming-video.html）</p><p>Evolving the architecture with Amazon CloudFront - Hosting Static Websites on AWS （https://docs.aws.amazon.com/whitepapers/latest/build-static-websites-aws/evolving-the-architecture-with-amazon-cloudfront.html）</p><p>Seeking Cost Saving options For ALB with Autoscaling | AWS re:Post （https://repost.aws/questions/QUSbPCZMSxTaKYWC07NXryuQ/seeking-cost-saving-options-for-alb-with-autoscaling）</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "5dcd5aa330004deba2ff2a3e731bf27c",
      "questionNumber": 17,
      "type": "single",
      "content": "<p>Question #17<br>A company with global offices has a single 1 Gbps AWS Direct Connect connection to a single AWS Region. The company’s on-premises network uses the connection to communicate with the company’s resources in the AWS Cloud. The connection has a single private virtual interface that connects to a single VPC.<br><br></p><p>A solutions architect must implement a solution that adds a redundant Direct Connect connection in the same Region. The solution also must provide connectivity to other Regions through the same pair of Direct Connect connections as the company expands into other Regions. </p><p><br></p><p>Which solution meets these requirements? </p>",
      "options": [
        {
          "label": "A",
          "content": "Provision a Direct Connect gateway. Delete the existing private virtual interface from the existing connection. Create the second Direct Connect connection. Create a new private virtual interface on each connection, and connect both private virtual interfaces to the Direct Connect gateway. Connect the Direct Connect gateway to the single VPC."
        },
        {
          "label": "B",
          "content": "Keep the existing private virtual interface. Create the second Direct Connect connection. Create a new private virtual interface on the new connection, and connect the new private virtual interface to the single VPC."
        },
        {
          "label": "C",
          "content": "Keep the existing private virtual interface. Create the second Direct Connect connection. Create a new public virtual interface on the new connection, and connect the new public virtual interface to the single VPC."
        },
        {
          "label": "D",
          "content": "Provision a transit gateway. Delete the existing private virtual interface from the existing connection. Create the second Direct Connect connection. Create a new private virtual interface on each connection, and connect both private virtual interfaces to the transit gateway. Associate the transit gateway with the single VPC."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>The correct answer isA. Here's why:</p><p> Requirements:</p><p>1.Add a redundant Direct Connect (DX) connection in the same Region – Ensures high availability.</p><p>2.Provide connectivity to other Regions through the same pair of DX connections – Requires a solution that allows cross-Region connectivity.</p><p>3.Use the same pair of DX connections as the company expands – Avoids needing additional DX connections for future Regions.</p><p> Why Option A is Correct:</p><p>-Direct Connect Gateway (DX Gateway) allows you to connect multiple VPCs (including those in other Regions) to an on-premises network over a single or redundant DX connection.</p><p>- Steps in Option A:</p><p> &nbsp;1.Provision a DX Gateway – This enables multi-Region connectivity.</p><p> &nbsp;2.Delete the existing private VIF – Prepares for a new configuration.</p><p> &nbsp;3.Create a second DX connection – Provides redundancy.</p><p> &nbsp;4.Create a new private VIF on each connection and attach them to the DX Gateway – Ensures both connections are used.</p><p> &nbsp;5.Connect the DX Gateway to the single VPC – Maintains current connectivity while enabling future expansion.</p><p>-Benefits:</p><p> &nbsp;-Redundancy: Two DX connections in the same Region.</p><p> &nbsp;-Multi-Region Support: The DX Gateway allows future expansion to other Regions without additional DX connections.</p><p> Why Other Options Are Incorrect:</p><p>-Option B: Only connects the second DX to the same VPC. Doesnot support multi-Region connectivity.</p><p>-Option C: Uses apublic VIF, which is not suitable for private VPC connectivity (and does not address multi-Region needs).</p><p>-Option D: Uses aTransit Gateway, which can connect multiple VPCs but doesnot natively support cross-Region DX connectivity (unlike a DX Gateway).</p><p> Key Takeaway:</p><p>ADirect Connect Gateway is the correct solution because it enables bothredundancy andmulti-Region connectivity through the same pair of DX connections.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "44a0a78cf2314ebfb10900701a58ef20",
      "questionNumber": 18,
      "type": "single",
      "content": "<p>Question #18<br>A company has a web application that allows users to upload short videos. The videos are stored on Amazon EBS volumes and analyzed by custom recognition software for categorization. The website contains static content that has variable traffic with peaks in certain months. The architecture consists of Amazon EC2 instances running in an Auto Scaling group for the web application and EC2 instances running in an Auto Scaling group to process an Amazon SQS queue.<br><br></p><p>The company wants to re-architect the application to reduce operational overhead using AWS managed services where possible and remove dependencies on third-party software. </p><p><br></p><p>Which solution meets these requirements? </p>",
      "options": [
        {
          "label": "A",
          "content": "Use Amazon ECS containers for the web application and Spot instances for the Auto Scaling group that processes the SQS queue. Replace the custom software with Amazon Rekognition to categorize the videos."
        },
        {
          "label": "B",
          "content": "Store the uploaded videos in Amazon EFS and mount the file system to the EC2 instances for the web application. Process the SQS queue with an AWS Lambda function that calls the Amazon Rekognition API to categorize the videos."
        },
        {
          "label": "C",
          "content": "Host the web application in Amazon S3. Store the uploaded videos in Amazon S3. Use S3 event notification to publish events to the SQS queue. Process the SQS queue with an AWS Lambda function that calls the Amazon Rekognition API to categorize the videos."
        },
        {
          "label": "D",
          "content": "Use AWS Elastic Beanstalk to launch EC2 instances in an Auto Scaling group for the web application and launch a worker environment to process the SQS queue. Replace the custom software with Amazon Rekognition to categorize the videos."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>The correct answer isC. Here's why it meets the requirements best:</p><p>Key Requirements:</p><p>1.Reduce operational overhead – Use managed services where possible.</p><p>2.Remove third-party software dependencies – Replace custom recognition software with AWS services.</p><p>3.Handle variable traffic efficiently – The solution should scale seamlessly.</p><p>4.Store and process videos effectively – Should leverage AWS services for storage and analysis.</p><p>Why Option C is Best:</p><p>-Host static web content in Amazon S3 – S3 is a fully managed, highly scalable service for static websites, reducing the need for EC2 instances.</p><p>-Store uploaded videos in Amazon S3 – More scalable and cost-effective than EBS or EFS for object storage.</p><p>-S3 Event Notifications + SQS Queue – When a video is uploaded, S3 can trigger an SQS message automatically.</p><p>-Lambda + Amazon Rekognition – Lambda is serverless (no EC2 management) and integrates seamlessly with Rekognition for video analysis.</p><p>-Fully managed services – Eliminates the need for maintaining EC2 instances for processing.</p><p>Why Other Options Are Less Optimal:</p><p>-A – Still uses EC2 (operational overhead) and Spot Instances (not fully managed).</p><p>-B – EFS is overkill for video storage (better for shared file systems) and still uses EC2 for the web app.</p><p>-D – Elastic Beanstalk still involves managing EC2 instances (not fully serverless).</p><p>Conclusion:</p><p>Option C is the most serverless, scalable, and operationally efficient solution, fully leveraging AWS managed services (S3, Lambda, Rekognition, SQS). &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "f2022239081c4ef1ada60f965eca8aac",
      "questionNumber": 19,
      "type": "single",
      "content": "<p>Question #19<br>A company has a serverless application comprised of Amazon CloudFront, Amazon API Gateway, and AWS Lambda functions. The current deployment process of the application code is to create a new version number of the Lambda function and run an AWS CLI script to update. If the new function version has errors, another CLI script reverts by deploying the previous working version of the function. The company would like to decrease the time to deploy new versions of the application logic provided by the Lambda functions, and also reduce the time to detect and revert when errors are identified.<br><br>How can this be accomplished?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create and deploy nested AWS CloudFormation stacks with the parent stack consisting of the AWS CloudFront distribution and API Gateway, and the child stack containing the Lambda function. For changes to Lambda, create an AWS CloudFormation change set and deploy; if errors are triggered, revert the AWS CloudFormation change set to the previous version."
        },
        {
          "label": "B",
          "content": "Use AWS SAM and built-in AWS CodeDeploy to deploy the new Lambda version, gradually shift traffic to the new version, and use pre-traffic and post-traffic test functions to verify code. Rollback if Amazon CloudWatch alarms are triggered."
        },
        {
          "label": "C",
          "content": "Refactor the AWS CLI scripts into a single script that deploys the new Lambda version. When deployment is completed, the script tests execute. If errors are detected, revert to the previous Lambda version."
        },
        {
          "label": "D",
          "content": "Create and deploy an AWS CloudFormation stack that consists of a new API Gateway endpoint that references the new Lambda version. Change the CloudFront origin to the new API Gateway endpoint, monitor errors and if detected, change the AWS CloudFront origin to the previous API Gateway endpoint."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>The best answer to decrease the time to deploy new versions of the Lambda functions and reduce the time to detect and revert errors is:</p><p>B. Use AWS SAM and built-in AWS CodeDeploy to deploy the new Lambda version, gradually shift traffic to the new version, and use pre-traffic and post-traffic test functions to verify code. Rollback if Amazon CloudWatch alarms are triggered.</p><p> Explanation:</p><p>-AWS Serverless Application Model (SAM) simplifies the deployment of serverless applications (including Lambda, API Gateway, etc.).</p><p>-AWS CodeDeploy integration with SAM enablescanary or linear traffic shifting, allowing gradual rollout of new Lambda versions while monitoring for errors.</p><p>-Pre-traffic and post-traffic test functions can validate the new version before and after shifting traffic.</p><p>-Automated rollback occurs if CloudWatch alarms (e.g., error rates) are triggered, reducing downtime and manual intervention.</p><p>- This approach improves deployment speed (compared to CLI scripts) and provides better error detection and rollback mechanisms.</p><p> Why not the other options?</p><p>-A: While CloudFormation helps with infrastructure management, change sets are slower and less automated for Lambda deployments compared to SAM/CodeDeploy.</p><p>-C: Refactoring CLI scripts still requires manual testing and rollback, which is less efficient than automated traffic shifting and monitoring.</p><p>-D: Changing CloudFront origins and API Gateway endpoints is overly complex for Lambda version updates and introduces unnecessary overhead.</p><p>Thus,B is the most efficient and automated solution.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "61f8440f216c43bca196f243d02bf030",
      "questionNumber": 20,
      "type": "single",
      "content": "Question #20 <p>A company is planning to store a large number of archived documents and make the documents available to employees through the corporate intranet. Employees will access the system by connecting through a client VPN service that is attached to a VPC. The data must not be accessible to the public.<br><br>The documents that the company is storing are copies of data that is held on physical media elsewhere. The number of requests will be low. Availability and speed of retrieval are not concerns of the company.<br><br>Which solution will meet these requirements at the LOWEST cost?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an Amazon S3 bucket. Configure the S3 bucket to use the S3 One Zone-Infrequent Access (S3 One Zone-IA) storage class as default. Configure the S3 bucket for website hosting. Create an S3 interface endpoint. Configure the S3 bucket to allow access only through that endpoint."
        },
        {
          "label": "B",
          "content": "&nbsp;Launch an Amazon EC2 instance that runs a web server. Attach an Amazon Elastic File System (Amazon EFS) file system to store the archived data in the EFS One Zone-Infrequent Access (EFS One Zone-IA) storage class. Configure the instance security groups to allow access only from private networks."
        },
        {
          "label": "C",
          "content": "Launch an Amazon EC2 instance that runs a web server. Attach an Amazon Elastic Block Store (Amazon EBS) volume to store the archived data. Use the Cold HDD (sc1) volume type. Configure the instance security groups to allow access only from private networks."
        },
        {
          "label": "D",
          "content": "Create an Amazon S3 bucket. Configure the S3 bucket to use the S3 Glacier Deep Archive storage class as default. Configure the S3 bucket for website hosting. Create an S3 interface endpoint. Configure the S3 bucket to allow access only through that endpoint."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>- Option A: S3 One Zone - IA is designed for less frequently accessed data, but it is more expensive than S3 Glacier Deep Archive for long - term storage. Since the requests are low and availability/speed of retrieval are not concerns, using S3 One Zone - IA will cost more than necessary. So, this option is not the lowest - cost solution.</p><p> - Option B: Launching an EC2 instance with an EFS file system in the EFS One Zone - IA storage class incurs costs for the EC2 instance (including compute, storage, and networking) and the EFS file system. Managing an EC2 instance also has operational overhead. This is a more complex and costly solution compared to using S3 Glacier Deep Archive in an S3 bucket. So, this option is not the best choice.</p><p> - Option C: Using an EC2 instance with an EBS Cold HDD (sc1) volume also has costs associated with the EC2 instance and the EBS volume. Additionally, managing the EC2 instance requires more effort. Similar to Option B, this is more expensive and operationally complex than using S3 Glacier Deep Archive. So, this option is not optimal.</p><p> - Option D: S3 Glacier Deep Archive is the lowest - cost storage class in S3, suitable for long - term archived data with infrequent access. Creating an S3 bucket with S3 Glacier Deep Archive as the default storage class, configuring it for website hosting, and restricting access through an S3 interface endpoint meets the requirements. It is a cost - effective solution as it leverages S3's low - cost storage for archived data and provides a secure way to access the data through the corporate intranet. So, this option is the best choice to meet the requirements at the lowest cost. </p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "c82e1ad85c144d6785f87780c36cdf37",
      "questionNumber": 21,
      "type": "single",
      "content": "<p>Question #21</p><p>A company is using an on-premises Active Directory service for user authentication. The company wants to use the same authentication service to sign in to the company’s AWS accounts, which are using AWS Organizations. AWS Site-to-Site VPN connectivity already exists between the on-premises environment and all the company’s AWS accounts. </p><p><br></p><p>The company’s security policy requires conditional access to the accounts based on user groups and roles. User identities must be managed in a single location.<br><br>Which solution will meet these requirements? </p>",
      "options": [
        {
          "label": "A",
          "content": "Configure AWS IAM Identity Center (AWS Single Sign-On) to connect to Active Directory by using SAML 2.0. Enable automatic provisioning by using the System for Cross-domain Identity Management (SCIM) v2.0 protocol. Grant access to the AWS accounts by using attribute-based access controls (ABACs)."
        },
        {
          "label": "B",
          "content": "Configure AWS IAM Identity Center (AWS Single Sign-On) by using IAM Identity Center as an identity source. Enable automatic provisioning by using the System for Cross-domain Identity Management (SCIM) v2.0 protocol. Grant access to the AWS accounts by using IAM Identity Center permission sets."
        },
        {
          "label": "C",
          "content": "&nbsp;In one of the company&rsquo;s AWS accounts, configure AWS Identity and Access Management (IAM) to use a SAML 2.0 identity provider. Provision IAM users that are mapped to the federated users. Grant access that corresponds to appropriate groups in Active Directory. Grant access to the required AWS accounts by using cross-account IAM users."
        },
        {
          "label": "D",
          "content": "In one of the company&rsquo;s AWS accounts, configure AWS Identity and Access Management (IAM) to use an OpenID Connect (OIDC) identity provider. Provision IAM roles that grant access to the AWS account for the federated users that correspond to appropriate groups in Active Directory. Grant access to the required AWS accounts by using cross-account IAM roles."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>The correct answer isA. &nbsp;</p><p> Explanation: &nbsp;</p><p>The requirements are: &nbsp;</p><p>1.Use on-premises Active Directory for authentication across AWS accounts (under AWS Organizations). &nbsp;</p><p>2.Conditional access based on user groups and roles. &nbsp;</p><p>3.Centralized identity management (user identities must be managed in a single location). &nbsp;</p><p>4.Existing Site-to-Site VPN ensures connectivity between on-premises AD and AWS. &nbsp;</p><p>Why Option A is correct: &nbsp;</p><p>-AWS IAM Identity Center (AWS Single Sign-On) + SAML 2.0 allows integration with on-premises Active Directory. &nbsp;</p><p>-SCIM v2.0 enables automatic provisioning of users and groups from AD to AWS IAM Identity Center. &nbsp;</p><p>-Attribute-Based Access Control (ABAC) allows conditional access based on user attributes (e.g., group membership). &nbsp;</p><p>- This setup meets the requirement of managing identities in a single location (Active Directory) while enabling federated access to AWS accounts. &nbsp;</p><p>Why other options are incorrect: &nbsp;</p><p>-B: Uses IAM Identity Center as the identity source (not on-premises AD). &nbsp;</p><p>-C & D: Require manual IAM user/role provisioning (not centralized) and do not leverage AWS IAM Identity Center for seamless multi-account access. &nbsp;</p><p>Thus,A is the best solution. &nbsp;</p><p>AWS IAM Identity Center (AWS SSO) allows for the integration of on-premises Active Directory with AWS accounts, enabling users to sign in with their existing credentials. By using SAML 2.0, the company can establish a trust relationship between AWS SSO and Active Directory. The SCIM v2.0 protocol can be used for automatic provisioning and de-provisioning of user identities across AWS accounts. ABACs can be implemented to grant access to AWS resources based on user attributes, aligning with the company's security policy for conditional access. This centralized approach ensures that user identities are managed in a single location, meeting the company's requirements.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "5be88f23e68a41dda7499b3025ef2d0c",
      "questionNumber": 22,
      "type": "single",
      "content": "<p>Question #22 <br>A software company has deployed an application that consumes a REST API by using Amazon API Gateway, AWS Lambda functions, and an Amazon DynamoDB table. The application is showing an increase in the number of errors during PUT requests. Most of thePUT calls come from a small number of clients that are authenticated with specific API keys. </p><p><br></p><p>A large number of the PUT requests originate from one client. The API is noncritical, and clients can tolerate retries of unsuccessful calls. However, the errors are displayed to customers and are causing damage to the API’s reputation.</p><p><br></p><p>What should the solutions architect recommend to improve the customer experience?</p>",
      "options": [
        {
          "label": "A",
          "content": "Implement retry logic with exponential backoff and irregular variation in the client application. Ensure that the errors are caught and handled with descriptive error messages."
        },
        {
          "label": "B",
          "content": "Implement API throttling through a usage plan at the API Gateway level. Ensure that the client application handles code 429 replies without error."
        },
        {
          "label": "C",
          "content": "Turn on API caching to enhance responsiveness for the production stage. Run 10-minute load tests. Verify that the cache capacity is appropriate for the workload."
        },
        {
          "label": "D",
          "content": "Implement reserved concurrency at the Lambda function level to provide the resources that are needed during sudden increases in traffic."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>The correct answer isB. Implement API throttling through a usage plan at the API Gateway level. Ensure that the client application handles code 429 replies without error.</p><p> Explanation:</p><p>-Problem Identification: A small number of clients (especially one client) are generating a large number of PUT requests, leading to errors that affect the API’s reputation. The API is non-critical, and clients can tolerate retries.</p><p>-Solution Approach: </p><p> &nbsp;-API Throttling with Usage Plans: API Gateway allows you to set throttling limits (rate and burst limits) via usage plans. This ensures that no single client can overwhelm the system with excessive requests.</p><p> &nbsp;-Handling 429 (Too Many Requests): When throttling is applied, API Gateway returns a `429` status code. The client application should gracefully handle this response (e.g., by implementing retries) instead of showing errors to users.</p><p>-Why Not Other Options?:</p><p> &nbsp;-A: While retry logic with exponential backoff is good practice, it doesn’t address the root cause (excessive requests from a single client).</p><p> &nbsp;-C: API caching improves responsiveness but does not solve throttling issues for PUT requests (caching is typically used for idempotent GET requests).</p><p> &nbsp;-D: Reserved concurrency in Lambda ensures resource availability but doesn’t prevent API overload from excessive client requests.</p><p>Thus,B is the best solution to improve customer experience by enforcing fair usage and ensuring clients handle throttling gracefully.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "98af7196b87b4b43a94b44a5f0e83ea9",
      "questionNumber": 23,
      "type": "single",
      "content": "<p>Question #23</p><p>A company is running a data-intensive application on AWS. The application runs on a cluster of hundreds of Amazon EC2 instances. A shared file system also runs on several EC2 instances that store 200 TB of data. The application reads and modifies the data on the shared file system and generates a report. The job runs once monthly, reads a subset of the files from the shared file system, and takes about 72 hours to complete. The compute instances scale in an Auto Scaling group, but the instances that host the shared file system run continuously. The compute and storage instances are all in the same AWS Region. </p><p><br></p><p>A solutions architect needs to reduce costs by replacing the shared file system instances. The file system must provide high performance access to the needed data for the duration of the 72-hour run.<br><br>Which solution will provide the LARGEST overall cost reduction while meeting these requirements? </p>",
      "options": [
        {
          "label": "A",
          "content": "Migrate the data from the existing shared file system to an Amazon S3 bucket that uses the S3 Intelligent-Tiering storage class. Before the job runs each month, use Amazon FSx for Lustre to create a new file system with the data from Amazon S3 by using lazy loading. Use the new file system as the shared storage for the duration of the job. Delete the file system when the job is complete."
        },
        {
          "label": "B",
          "content": "Migrate the data from the existing shared file system to a large Amazon Elastic Block Store (Amazon EBS) volume with Multi-Attach enabled. Attach the EBS volume to each of the instances by using a user data script in the Auto Scaling group launch template. Use the EBS volume as the shared storage for the duration of the job. Detach the EBS volume when the job is complete."
        },
        {
          "label": "C",
          "content": "Migrate the data from the existing shared file system to an Amazon S3 bucket that uses the S3 Standard storage class. Before the job runs each month, use Amazon FSx for Lustre to create a new file system with the data from Amazon S3 by using batch loading. Use the new file system as the shared storage for the duration of the job. Delete the file system when the job is complete."
        },
        {
          "label": "D",
          "content": "Migrate the data from the existing shared file system to an Amazon S3 bucket. Before the job runs each month, use AWS Storage Gateway to create a file gateway with the data from Amazon S3. Use the file gateway as the shared storage for the job. Delete the file gateway when the job is complete."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>The correct answer isA. Migrate the data from the existing shared file system to an Amazon S3 bucket that uses the S3 Intelligent-Tiering storage class. Before the job runs each month, use Amazon FSx for Lustre to create a new file system with the data from Amazon S3 by using lazy loading. Use the new file system as the shared storage for the duration of the job. Delete the file system when the job is complete.</p><p> Explanation:</p><p>1.Cost Efficiency:</p><p> &nbsp; -S3 Intelligent-Tiering automatically optimizes storage costs by moving data between tiers based on access patterns, which is ideal for infrequently accessed data (monthly job).</p><p> &nbsp; -FSx for Lustre (lazy loading) only loads data when it is accessed, reducing initial setup costs and time compared to batch loading (which pre-loads all data upfront).</p><p> &nbsp; - Since the job runs only once a month,deleting the FSx for Lustre file system after the job avoids ongoing costs.</p><p>2.Performance:</p><p> &nbsp; - FSx for Lustre provideshigh-performance, low-latency access needed for data-intensive applications, especially when dealing with large datasets (200 TB).</p><p>3.Alternatives Analysis:</p><p> &nbsp; -Option B (EBS Multi-Attach): While EBS Multi-Attach allows shared storage, it is not cost-effective for large-scale (200 TB) storage and does not scale as well as FSx for Lustre for high-performance workloads.</p><p> &nbsp; -Option C (FSx for Lustre with batch loading): Batch loading pre-loads all data upfront, which is slower and more expensive than lazy loading (since not all data may be needed).</p><p> &nbsp; -Option D (Storage Gateway): File Gateway is not designed for high-performance compute workloads and would introduce latency compared to FSx for Lustre.</p><p> Conclusion:</p><p>Option A provides thelargest cost reduction while meeting performance requirements by leveraging S3 Intelligent-Tiering for long-term storage and FSx for Lustre (lazy loading) for temporary high-performance access.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "1d5c669a59be480a95dc205979d245ba",
      "questionNumber": 24,
      "type": "single",
      "content": "<p>Question #24 <br>A company is developing a new service that will be accessed using TCP on a static port. A solutions architect must ensure that the service is highly available, has redundancy across Availability Zones, and is accessible using the DNS name my.service.com, which is publicly accessible. The service must use fixed address assignments so other companies can add the addresses to their allow lists.</p><p><br></p><p>Assuming that resources are deployed in multiple Availability Zones in a single Region, which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "&nbsp;Create Amazon EC2 instances with an Elastic IP address for each instance. Create a Network Load Balancer (NLB) and expose the static TCP port. Register EC2 instances with the NLB. Create a new name server record set named my.service.com, and assign the Elastic IP addresses of the EC2 instances to the record set. Provide the Elastic IP addresses of the EC2 instances to the other companies to add to their allow lists."
        },
        {
          "label": "B",
          "content": "Create an Amazon ECS cluster and a service definition for the application. Create and assign public IP addresses for the ECS cluster. Create a Network Load Balancer (NLB) and expose the TCP port. Create a target group and assign the ECS cluster name to the NLB. Create a new A record set named my.service.com, and assign the public IP addresses of the ECS cluster to the record set. Provide the public IP addresses of the ECS cluster to the other companies to add to their allow lists."
        },
        {
          "label": "C",
          "content": "Create Amazon EC2 instances for the service. Create one Elastic IP address for each Availability Zone. Create a Network Load Balancer (NLB) and expose the assigned TCP port. Assign the Elastic IP addresses to the NLB for each Availability Zone. Create a target group and register the EC2 instances with the NLB. Create a new A (alias) record set named my.service.com, and assign the NLB DNS name to the record set."
        },
        {
          "label": "D",
          "content": "Create an Amazon ECS cluster and a service definition for the application. Create and assign public IP address for each host in the cluster. Create an Application Load Balancer (ALB) and expose the static TCP port. Create a target group and assign the ECS service definition name to the ALB. Create a new CNAME record set and associate the public IP addresses to the record set. Provide the Elastic IP addresses of the Amazon EC2 instances to the other companies to add to their allow lists."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>The correct answer isC. &nbsp;</p><p> Explanation: &nbsp;</p><p>The requirements are: &nbsp;</p><p>1.High availability & redundancy across Availability Zones (AZs) – This is achieved by deploying resources in multiple AZs and using a Network Load Balancer (NLB). &nbsp;</p><p>2.Accessible via DNS name (`my.service.com`) – AnA (alias) record pointing to the NLB's DNS name satisfies this. &nbsp;</p><p>3.Static TCP port & fixed IP addresses for allow lists – The NLB can be assignedElastic IPs (EIPs) per AZ, providing stable IPs that other companies can whitelist. &nbsp;</p><p>4.Public accessibility – The NLB is internet-facing and uses public IPs. &nbsp;</p><p> Why Option C is Correct: &nbsp;</p><p>- UsesNLB (supports static TCP ports and retains source IP). &nbsp;</p><p>- AssignsEIPs per AZ to the NLB (fixed IPs for allow lists). &nbsp;</p><p>-A (alias) record points to the NLB's DNS name (best practice for load balancers). &nbsp;</p><p>- EC2 instances are registered with the NLB (ensures traffic distribution). &nbsp;</p><p> Why Other Options Are Wrong: &nbsp;</p><p>-A: Uses EC2 instance EIPs directly (not scalable; no load balancing if an instance fails). &nbsp;</p><p>-B: Uses ECS cluster public IPs (not fixed; NLB should have EIPs, not the cluster). &nbsp;</p><p>-D: UsesALB (does not support static TCP ports) and incorrect DNS setup (CNAME with IPs). &nbsp;</p><p>To meet the requirements of high availability, redundancy, and fixed address assignments, the best solution is to use Amazon EC2 instances with Elastic IP addresses and a Network Load Balancer (NLB). By assigning an Elastic IP address to each Availability Zone and attaching them to the NLB, the service can maintain a fixed address that can be added to allow lists by other companies. The NLB ensures that traffic is distributed across healthy instances and provides redundancy across Availability Zones. Using an A (alias) record set for the DNS name my.service.com and pointing it to the NLB DNS name ensures that the service is accessible using the specified DNS name.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "aa7922fb4af14f11805a137b17781971",
      "questionNumber": 25,
      "type": "single",
      "content": "Question #25<br>A company uses an on-premises data analytics platform. The system is highly available in a fully redundant configuration across 12 servers in the company’s data center. <p><br></p><p>The system runs scheduled jobs, both hourly and daily, in addition to one-time requests from users. Scheduled jobs can take between 20 minutes and 2 hours to finish running and have tight SLAs. The scheduled jobs account for 65% of the system usage. User jobs typically finish running in less than 5 minutes and have no SLA. The user jobs account for 35% of system usage. During system failures, scheduled jobs must continue to meet SLAs. However, user jobs can be delayed. </p><p><br></p><p>A solutions architect needs to move the system to Amazon EC2 instances and adopt a consumption-based model to reduce costs with no long-term commitments. The solution must maintain high availability and must not affect the SLAs.</p><p><br></p><p>Which solution will meet these requirements MOST cost-effectively?</p>",
      "options": [
        {
          "label": "A",
          "content": "Split the 12 instances across two Availability Zones in the chosen AWS Region. Run two instances in each Availability Zone as On-Demand Instances with Capacity Reservations. Run four instances in each Availability Zone as Spot Instances."
        },
        {
          "label": "B",
          "content": "Split the 12 instances across three Availability Zones in the chosen AWS Region. In one of the Availability Zones, run all four instances as On-Demand Instances with Capacity Reservations. Run the remaining instances as Spot Instances."
        },
        {
          "label": "C",
          "content": "Split the 12 instances across three Availability Zones in the chosen AWS Region. Run two instances in each Availability Zone as On-Demand Instances with a Savings Plan. Run two instances in each Availability Zone as Spot Instances."
        },
        {
          "label": "D",
          "content": "Split the 12 instances across three Availability Zones in the chosen AWS Region. Run three instances in each Availability Zone as On-Demand Instances with Capacity Reservations. Run one instance in each Availability Zone as a Spot Instance."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>To solve this question, we first map the key requirements from the scenario and evaluate each option against them:</p><p><br></p><p><br></p><p> 1. Critical Term Definitions</p><p>- On-Demand Instances with Capacity Reservations: Guarantee capacity in a specific AZ (hourly billing, no long-term commitments) — ideal for SLA-bound jobs. &nbsp;</p><p>- Spot Instances: 50-90% cheaper than On-Demand but can be interrupted — ideal for non-SLA jobs. &nbsp;</p><p>- Savings Plans: Require 1/3-year commitments — violates \"no long-term commitments.\" &nbsp;</p><p><br></p><p><br></p><p> 2. Option Evaluation</p><p>We eliminate options that fail to meet core requirements first:</p><p><br></p><p>Option A: 12 instances across 2 AZs (2 On-Demand/Capacity Res + 4 Spot per AZ)</p><p>- Failure: Losing 1 AZ leaves 2 On-Demand + 4 Spot = 6 total instances. &nbsp;</p><p>- Issue: 6 instances &lt; 7.8 required for scheduled jobs → SLA breach. &nbsp;</p><p>- Eliminated.</p><p><br></p><p><br></p><p>Option B: 12 instances across 3 AZs (4 On-Demand/Capacity Res in 1 AZ, 8 Spot in others)</p><p>- Failure: If the AZ with On-Demand instances fails, only 8 Spot instances remain. &nbsp;</p><p>- Issue: Spot instances are interruptible — no guarantee 8 instances will be available for scheduled jobs → SLA breach. &nbsp;</p><p>- Eliminated.</p><p><br></p><p><br></p><p>Option C: 12 instances across 3 AZs (2 On-Demand/Savings Plan + 2 Spot per AZ)</p><p>- Issue: Savings Plans require 1/3-year commitments — violates \"no long-term commitments.\" &nbsp;</p><p>- Eliminated.</p><p><br></p><p><br></p><p>Option D: 12 instances across 3 AZs (3 On-Demand/Capacity Res + 1 Spot per AZ)</p><p>- HA Compliance: 3 AZs (tolerates 1 AZ failure) with even distribution (4 instances/AZ). &nbsp;</p><p>- Normal Operations: 9 total On-Demand instances cover the 7.8 required for scheduled jobs; 3 Spot instances handle user jobs. &nbsp;</p><p>- Failure Scenario: Losing 1 AZ leaves: &nbsp;</p><p> &nbsp;- 6 On-Demand instances (3 AZs - 1 = 2 AZs × 3) + 2 Spot instances (2 AZs × 1) = 8 total instances. &nbsp;</p><p> &nbsp;- 8 instances &gt; 7.8 required for scheduled jobs; user jobs are delayed (per requirement). &nbsp;</p><p>- Cost-Effective: Uses cheaper Spot for non-SLA jobs and On-Demand/Capacity Res for SLA jobs (no long-term commitments). &nbsp;</p><p>- Meets All Requirements.</p><p><br></p><p><br></p><p> 3. Conclusion</p><p>Only Option D satisfies all requirements: no long-term commitments, high availability, SLA compliance for scheduled jobs, and cost-effectiveness.</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "43c8dc25b7d34cd4aa62bd40d4d1dcd6",
      "questionNumber": 26,
      "type": "single",
      "content": "<p>Question #26</p><p>A security engineer determined that an existing application retrieves credentials to an Amazon RDS for MySQL database from an encrypted file in Amazon S3. For the next version of the application, the security engineer wants to implement the following application design changes to improve security: </p><p>The database must use strong, randomly generated passwords stored in a secure AWS managed service. </p><p>The application resources must be deployed through AWS CloudFormation. </p><p>The application must rotate credentials for the database every 90 days. </p><p>A solutions architect will generate a CloudFormation template to deploy the application. </p><p><br></p><p>Which resources specified in the CloudFormation template will meet the security engineer’s requirements with the LEAST amount of operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Generate the database password as a secret resource using AWS Secrets Manager. Create an AWS Lambda function resource to rotate the database password. Specify a Secrets Manager RotationSchedule resource to rotate the database password every 90 days."
        },
        {
          "label": "B",
          "content": "Generate the database password as a SecureString parameter type using AWS Systems Manager Parameter Store. Create an AWS Lambda function resource to rotate the database password. Specify a Parameter Store RotationSchedule resource to rotate the database password every 90 days."
        },
        {
          "label": "C",
          "content": "Generate the database password as a secret resource using AWS Secrets Manager. Create an AWS Lambda function resource to rotate the database password. Create an Amazon EventBridge scheduled rule resource to trigger the Lambda function password rotation every 90 days."
        },
        {
          "label": "D",
          "content": "Generate the database password as a SecureString parameter type using AWS Systems Manager Parameter Store. Specify an AWS AppSync DataSource resource to automatically rotate the database password every 90 days."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>The correct answer isA. &nbsp;</p><p> Explanation: &nbsp;</p><p>The security engineer's requirements are: &nbsp;</p><p>1.Store credentials securely in an AWS-managed service – AWS Secrets Manager is designed for this purpose (better than Parameter Store for secrets management). &nbsp;</p><p>2.Deploy via AWS CloudFormation – All options use CloudFormation, so this is satisfied. &nbsp;</p><p>3.Rotate credentials every 90 days – Secrets Manager has built-in rotation capabilities with `RotationSchedule`, reducing operational overhead. &nbsp;</p><p> Why Option A is Best: &nbsp;</p><p>-AWS Secrets Manager securely stores and manages the database password. &nbsp;</p><p>-RotationSchedule resource automates rotation every 90 days (no need for manual EventBridge rules). &nbsp;</p><p>-Lambda function handles the rotation logic (Secrets Manager provides a rotation template). &nbsp;</p><p> Why Other Options Are Incorrect: &nbsp;</p><p>-B: Parameter Store (SecureString) is not ideal for automatic rotation (Secrets Manager is better suited). &nbsp;</p><p>-C: While possible, using EventBridge for scheduling adds unnecessary complexity compared to Secrets Manager’s built-in rotation. &nbsp;</p><p>-D: AppSync is not used for credential rotation, and Parameter Store lacks native rotation support like Secrets Manager. &nbsp;</p><p>Thus,A meets all requirements with theleast operational overhead. </p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "487a36798186492c980b9840c4c034a4",
      "questionNumber": 27,
      "type": "multiple",
      "content": "<p>Question #27<br>A company is storing data in several Amazon DynamoDB tables. A solutions architect must use a serverless architecture to make the data accessible publicly through a simple API over HTTPS. The solution must scale automatically in response to demand.<br><br>Which solutions meet these requirements? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an Amazon API Gateway REST API. Configure this API with direct integrations to DynamoDB by using API Gateway&rsquo;s AWS integration type."
        },
        {
          "label": "B",
          "content": "Create an Amazon API Gateway HTTP API. Configure this API with direct integrations to DynamoDB by using API Gateway&rsquo;s AWS integration type."
        },
        {
          "label": "C",
          "content": "Create an Amazon API Gateway HTTP API. Configure this API with integrations to AWS Lambda functions that return data from the DynamoDB tables."
        },
        {
          "label": "D",
          "content": "Create an accelerator in AWS Global Accelerator. Configure this accelerator with AWS Lambda@Edge function integrations that return data from the DynamoDB tables."
        },
        {
          "label": "E",
          "content": "Create a Network Load Balancer. Configure listener rules to forward requests to the appropriate AWS Lambda functions."
        }
      ],
      "correctAnswer": "AC",
      "explanation": "<p>Based on the information provided, the two solutions that meet the requirements for making DynamoDB data accessible through a serverless API over HTTPS that scales automatically are:</p><p>A. Create an Amazon API Gateway REST API. Configure this API with direct integrations to DynamoDB by using API Gateway's AWS integration type.</p><p>C. Create an Amazon API Gateway HTTP API. Configure this API with integrations to AWS Lambda functions that return data from the DynamoDB tables.</p><p>Here's why these two options are correct:</p><p>Option A: API Gateway REST API with direct DynamoDB integration is a valid approach that allows you to connect API Gateway directly to DynamoDB using the AWS service integration type. This eliminates the need for Lambda functions as intermediaries and can be configured using mapping templates. This is a fully serverless solution that scales automatically with demand.</p><p>Option C: API Gateway HTTP API with Lambda integration is another valid approach where Lambda functions act as intermediaries between the API Gateway and DynamoDB. The Lambda functions can contain business logic to process the data before returning it to the client. This solution is also fully serverless and scales automatically.</p><p>Option B is incorrect because API Gateway HTTP APIs do not support direct AWS service integrations with DynamoDB. HTTP APIs can only integrate with Lambda functions, HTTP endpoints, or private resources in a VPC, but not directly with AWS services like DynamoDB.</p><p>Both options A and C provide serverless architectures that make DynamoDB data accessible through HTTPS and scale automatically in response to demand, meeting all the requirements specified in the question.</p><p>Sources</p><p>Tutorial: Create a CRUD HTTP API with Lambda and DynamoDB - Amazon API Gateway （https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-dynamo-db.html）</p><p>Is Direct Integration API Gateway to DDB Still Supported? | AWS re:Post （https://repost.aws/questions/QUMtrOZH6XR0WhWaBf2qRtTg/is-direct-integration-api-gateway-to-ddb-still-supported）</p><p>API Gateway use cases - Amazon API Gateway （https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-overview-developer-experience.html）</p><p>Get started with API Gateway - Serverless （https://docs.aws.amazon.com/serverless/latest/devguide/starter-apigw.html）</p><p>Building a Serverless Architecture on AWS | AWS re:Post （https://repost.aws/articles/ARTqFF9AQmSgqrx2BNH-wkbw/building-a-serverless-architecture-on-aws）</p><p>Use PutIntegration with an AWS SDK or CLI - AWS SDK Code Examples （https://docs.aws.amazon.com/code-library/latest/ug/api-gateway_example_api-gateway_PutIntegration_section.html）</p><p>Community | Using API Gateway mapping templates for direct DynamoDB integrations （https://community.aws/content/2f0C1U6QRHgVMRaN4L8dhMur3EB/using-api-gateway-mapping-templates-for-direct-dynamodb-integrations）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "a2098e2c6b7748edb3eac533dee6744d",
      "questionNumber": 28,
      "type": "multiple",
      "content": "<p>Question #28</p><p>A company has registered 10 new domain names. The company uses the domains for online marketing. The company needs a solution that will redirect online visitors to a specific URL for each domain. </p><p><br></p><p>All domains and target URLs are defined in a JSON document. All DNS records are managed by Amazon Route 53. A solutions architect must implement a redirect service that accepts HTTP and HTTPS requests.<br><br></p><p>Which combination of steps should the solutions architect take to meet these requirements with the LEAST amount of operational effort? (Choose three.) </p>",
      "options": [
        {
          "label": "A",
          "content": "&nbsp;Create a dynamic webpage that runs on an Amazon EC2 instance. Configure the webpage to use the JSON document in combination with the event message to look up and respond with a redirect URL."
        },
        {
          "label": "B",
          "content": "Create an Application Load Balancer that includes HTTP and HTTPS listeners."
        },
        {
          "label": "C",
          "content": "Create an AWS Lambda function that uses the JSON document in combination with the event message to look up and respond with a redirect URL."
        },
        {
          "label": "D",
          "content": "Use an Amazon API Gateway API with a custom domain to publish an AWS Lambda function."
        },
        {
          "label": "E",
          "content": "Create an Amazon CloudFront distribution. Deploy a Lambda@Edge function."
        },
        {
          "label": "F",
          "content": "Create an SSL certificate by using AWS Certificate Manager (ACM). Include the domains as Subject Alternative Names."
        }
      ],
      "correctAnswer": "CEF",
      "explanation": "<p>Based on the information provided, to implement a redirect service for multiple domains with the least operational effort, the following three steps should be taken:</p><p>C. Create an AWS Lambda function that uses the JSON document in combination with the event message to look up and respond with a redirect URL.</p><p>E. Create an Amazon CloudFront distribution. Deploy a Lambda@Edge function.</p><p>F. Create an SSL certificate by using AWS Certificate Manager (ACM). Include the domains as Subject Alternative Names.</p><p>This combination provides the most efficient solution because:</p><p>Lambda@Edge (options C and E together) allows you to run code at AWS edge locations in response to CloudFront events. The Lambda function can use the JSON document to look up the appropriate redirect URL based on the incoming domain name in the request. This is a serverless approach that requires minimal operational effort.</p><p>CloudFront (option E) provides a global content delivery network that can handle both HTTP and HTTPS requests, which is a requirement. It also integrates seamlessly with Lambda@Edge for the redirect logic.</p><p>AWS Certificate Manager (option F) allows you to create and manage SSL certificates for all the domains with minimal effort. By including all domains as Subject Alternative Names (SANs) in a single certificate, you can secure all 10 domains without having to manage individual certificates. This certificate can then be attached to the CloudFront distribution to enable HTTPS.</p><p>This solution meets all the requirements:</p><p>Handles both HTTP and HTTPS requests</p><p>Uses the JSON document to map domains to target URLs</p><p>Works with Route 53 for DNS management</p><p>Requires minimal operational effort through serverless components</p><p>Scales automatically to handle visitor traffic</p><p>The other options would either require more operational effort or wouldn't fully meet the requirements specified in the question.</p><p>Sources</p><p>DNS forwarder externally | AWS re:Post （https://repost.aws/questions/QULgvWEouMTuGFdPl53nCB0A/dns-forwarder-externally）</p><p>Point a domain to your Lightsail instance using Amazon Route 53 - Amazon Lightsail （https://docs.aws.amazon.com/lightsail/latest/userguide/amazon-lightsail-using-route-53-to-point-a-domain-to-an-instance.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "6c853ffa12f0469a9131f3630eac0f64",
      "questionNumber": 29,
      "type": "single",
      "content": "<p>Question #29 <br>A company that has multiple AWS accounts is using AWS Organizations. The company’s AWS accounts host VPCs, Amazon EC2 instances, and containers. The company’s compliance team has deployed a security tool in each VPC where the company has deployments. The security tools run on EC2 instances and send information to the AWS account that is dedicated for the compliance team. The company has tagged all the compliance related resources with a key of “costCenter” and a value of “compliance”.</p><p><br></p><p>The company wants to identify the cost of the security tools that are running on the EC2 instances so that the company can charge the compliance team’s AWS account. The cost calculation must be as accurate as possible.<br><br>What should a solutions architect do to meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "In the management account of the organization, activate the costCenter user-defined tag. Configure monthly AWS Cost and Usage Reports to save to an Amazon S3 bucket in the management account. Use the tag breakdown in the report to obtain the total cost for the costCenter tagged resources."
        },
        {
          "label": "B",
          "content": "In the member accounts of the organization, activate the costCenter user-defined tag. Configure monthly AWS Cost and Usage Reports to save to an Amazon S3 bucket in the management account. Schedule a monthly AWS Lambda function to retrieve the reports and calculate the total cost for the costCenter tagged resources."
        },
        {
          "label": "C",
          "content": "In the member accounts of the organization activate the costCenter user-defined tag. From the management account, schedule a monthly AWS Cost and Usage Report. Use the tag breakdown in the report to calculate the total cost for the costCenter tagged resources."
        },
        {
          "label": "D",
          "content": "Create a custom report in the organization view in AWS Trusted Advisor. Configure the report to generate a monthly billing summary for the costCenter tagged resources in the compliance team&rsquo;s AWS account."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>The correct answer isA. Here's the detailed explanation:</p><p>Why Option A is Correct:</p><p>1.Activate the `costCenter` tag in the management account: &nbsp;</p><p> &nbsp; - AWS Organizations allows you to activateuser-defined cost allocation tags (like `costCenter`) in themanagement account. Once activated, these tags become available for cost allocation tracking across all member accounts in the organization.</p><p>2.Configure AWS Cost and Usage Reports (CUR): &nbsp;</p><p> &nbsp; - TheAWS Cost and Usage Report (CUR) provides the most detailed breakdown of AWS costs, including resource-level tagging information.</p><p> &nbsp; - By saving the CUR to anAmazon S3 bucket in the management account, you can analyze costs across all member accounts.</p><p> &nbsp; - The report includes atag breakdown, allowing you to filter costs by the `costCenter:compliance` tag.</p><p>3.Accurate Cost Calculation: &nbsp;</p><p> &nbsp; - The CUR provides granular data, ensuring the cost calculation for the security tools isas accurate as possible.</p><p> &nbsp; - Since the compliance team’s resources are tagged consistently, the report can accurately attribute costs to them.</p><p>Why Other Options Are Incorrect:</p><p>-Option B: &nbsp;</p><p> &nbsp;- While activating the tag in member accounts is possible, it’snot necessary—the management account can activate the tag for the whole organization. &nbsp;</p><p> &nbsp;- Using a Lambda function to manually calculate costs isless efficient than using CUR’s built-in tag breakdown.</p><p>-Option C: &nbsp;</p><p> &nbsp;- TheAWS Cost and Usage Report must be configured in the payer (management) account, not member accounts. &nbsp;</p><p> &nbsp;- The phrasing \"schedule a monthly report\" is misleading—CUR is continuous, not scheduled.</p><p>-Option D: &nbsp;</p><p> &nbsp;-Trusted Advisor does not provide cost allocation tagging reports. It focuses on performance, security, and fault tolerance, not detailed billing breakdowns.</p><p>Conclusion: &nbsp;</p><p>Option A is the best solution because it leverages AWS Organizations' tag activation and the detailed cost breakdown provided by AWS Cost and Usage Reports (CUR) to accurately track and allocate costs for the compliance team’s resources. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "7c4bac3d8e3a4d58acb5362501db92bc",
      "questionNumber": 30,
      "type": "multiple",
      "content": "<p>Question #30 <br>A company has 50 AWS accounts that are members of an organization in AWS Organizations. Each account contains multiple VPCs. The company wants to use AWS Transit Gateway to establish connectivity between the VPCs in each member account. Each time a new member account is created, the company wants to automate the process of creating a new VPC and a transit gateway attachment.<br><br>Which combination of steps will meet these requirements? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "From the management account, share the transit gateway with member accounts by using AWS Resource Access Manager."
        },
        {
          "label": "B",
          "content": "From the management account, share the transit gateway with member accounts by using an AWS Organizations SCP."
        },
        {
          "label": "C",
          "content": "Launch an AWS CloudFormation stack set from the management account that automatically creates a new VPC and a VPC transit gateway attachment in a member account. Associate the attachment with the transit gateway in the management account by using the transit gateway ID."
        },
        {
          "label": "D",
          "content": "Launch an AWS CloudFormation stack set from the management account that automatically creates a new VPC and a peering transit gateway attachment in a member account. Share the attachment with the transit gateway in the management account by using a transit gateway service-linked role."
        },
        {
          "label": "E",
          "content": "From the management account, share the transit gateway with member accounts by using AWS Service Catalog."
        }
      ],
      "correctAnswer": "AC",
      "explanation": "<p>Based on the information provided, to automate the process of creating a new VPC and transit gateway attachment each time a new member account is created in an AWS Organization, the following two steps should be taken:</p><p>A. From the management account, share the transit gateway with member accounts by using AWS Resource Access Manager.</p><p>C. Launch an AWS CloudFormation stack set from the management account that automatically creates a new VPC and a VPC transit gateway attachment in a member account. Associate the attachment with the transit gateway in the management account by using the transit gateway ID.</p><p>This combination works because:</p><p>AWS Resource Access Manager (RAM) is the correct service to share resources like transit gateways across accounts within an AWS Organization. By sharing the transit gateway from the management account using RAM, all member accounts can access and attach to it. This is a prerequisite for establishing connectivity between VPCs across different accounts.</p><p>AWS CloudFormation stack sets provide the automation capability needed to deploy the same resources (in this case, a VPC and transit gateway attachment) across multiple accounts. By launching a stack set from the management account, you can automatically create these resources in each member account and associate the attachment with the shared transit gateway using its ID.</p><p>Option B is incorrect because Service Control Policies (SCPs) are used to manage permissions and restrict actions within an organization, not to share resources between accounts.</p><p>This solution meets all the requirements by:</p><p>Enabling connectivity between VPCs across all 50 member accounts</p><p>Automating the process of creating a new VPC and transit gateway attachment when new member accounts are created</p><p>Using the management account to orchestrate and share the necessary resources</p><p>Sources</p><p>Automate setting up and managing your transit networks with AWS Transit Gateway - Network Orchestration for AWS Transit Gateway （https://docs.aws.amazon.com/solutions/latest/network-orchestration-aws-transit-gateway/solution-overview.html）</p><p>Automate Networking foundation in multi-account environments | Networking & Content Delivery （https://aws.amazon.com/cn/blogs/networking-and-content-delivery/automate-networking-foundation-in-multi-account-environments/）</p><p>Securely scale multi-account architecture with AWS Network Firewall and AWS Control Tower | AWS Cloud Operations & Migrations Blog （https://aws.amazon.com/cn/blogs/mt/scale-multi-account-architecture-aws-network-firewall-and-aws-control-tower/）</p><p>Transit gateway attachment configuration | AWS re:Post （https://repost.aws/questions/QUvX0QyO3pRQWkBQspwcgVKg/transit-gateway-attachment-configuration）</p><p>Automating AWS Transit Gateway attachments to a transit gateway in a central account | Networking & Content Delivery （https://aws.amazon.com/cn/blogs/networking-and-content-delivery/automating-aws-transit-gateway-attachments-to-a-transit-gateway-in-a-central-account/）</p><p>Automate your network setup in AWS Control Tower using Aviatrix | AWS Marketplace （https://aws.amazon.com/cn/blogs/awsmarketplace/automate-your-network-setup-in-aws-control-tower-using-aviatrix/）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "440d91a0a5f44f129a45dd259af6c790",
      "questionNumber": 31,
      "type": "single",
      "content": "<p>Question #31</p><p>An enterprise company wants to allow its developers to purchase third-party software through AWS Marketplace. The company uses an AWS Organizations account structure with full features enabled, and has a shared services account in each organizational unit (OU) that will be used by procurement managers. The procurement team’s policy indicates that developers should be able to obtain third-party software from an approved list only and use Private Marketplace in AWS Marketplace to achieve this requirement. The procurement team wants administration of Private Marketplace to be restricted to a role named procurement-manager-role, which could be assumed by procurement managers. Other IAM users, groups, roles, and account administrators in the company should be denied Private Marketplace administrative access.<br><br>What is the MOST effi cient way to design an architecture to meet these requirements? </p>",
      "options": [
        {
          "label": "A",
          "content": "Create an IAM role named procurement-manager-role in all AWS accounts in the organization. Add the PowerUserAccess managed policy to the role. Apply an inline policy to all IAM users and roles in every AWS account to deny permissions on the AWSPrivateMarketplaceAdminFullAccess managed policy."
        },
        {
          "label": "B",
          "content": "Create an IAM role named procurement-manager-role in all AWS accounts in the organization. Add the AdministratorAccess managed policy to the role. Define a permissions boundary with the AWSPrivateMarketplaceAdminFullAccess managed policy and attach it to all the developer roles."
        },
        {
          "label": "C",
          "content": "Create an IAM role named procurement-manager-role in all the shared services accounts in the organization. Add the AWSPrivateMarketplaceAdminFullAccess managed policy to the role. Create an organization root-level SCP to deny permissions to administer Private Marketplace to everyone except the role named procurement-manager-role. Create another organization root-level SCP to deny permissions to create an IAM role named procurement-manager-role to everyone in the organization."
        },
        {
          "label": "D",
          "content": "Create an IAM role named procurement-manager-role in all AWS accounts that will be used by developers. Add the AWSPrivateMarketplaceAdminFullAccess managed policy to the role. Create an SCP in Organizations to deny permissions to administer Private Marketplace to everyone except the role named procurement-manager-role. Apply the SCP to all the shared services accounts in the organization."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>TheMOST efficient way to design the architecture to meet the given requirements is:</p><p>Option C &nbsp;</p><p>Explanation: &nbsp;</p><p>1.IAM Role in Shared Services Accounts: &nbsp;</p><p> &nbsp; - The procurement-manager-role is created only in theshared services accounts (not all accounts), which aligns with the requirement that procurement managers use these accounts. &nbsp;</p><p> &nbsp; - The role is granted the`AWSPrivateMarketplaceAdminFullAccess` managed policy, allowing it to administer Private Marketplace.</p><p>2.SCP at Root Level to Restrict Private Marketplace Admin Access: &nbsp;</p><p> &nbsp; - AService Control Policy (SCP) at theroot level denies permissions to administer Private Marketplacefor everyone except `procurement-manager-role`. &nbsp;</p><p> &nbsp; - This ensures no other IAM user, role, or administrator can modify Private Marketplace settings.</p><p>3.SCP to Prevent Creation of Unauthorized Roles: &nbsp;</p><p> &nbsp; - A secondSCP denies permissions tocreate an IAM role named `procurement-manager-role` to prevent unauthorized users from bypassing restrictions.</p><p>Why Not Other Options? &nbsp;</p><p>-A: Incorrect because: &nbsp;</p><p> &nbsp;- `PowerUserAccess` does not grant Private Marketplace admin permissions. &nbsp;</p><p> &nbsp;- Applying inline deny policies to every IAM user/role is inefficient and hard to manage. &nbsp;</p><p>-B: Incorrect because: &nbsp;</p><p> &nbsp;- `AdministratorAccess` is overly permissive. &nbsp;</p><p> &nbsp;- Permissions boundaries on developer roles do not restrict other admins from accessing Private Marketplace. &nbsp;</p><p>-D: Incorrect because: &nbsp;</p><p> &nbsp;- The role should be inshared services accounts, not all developer accounts. &nbsp;</p><p> &nbsp;- The SCP should be applied at theroot level, not just shared services accounts, to cover the entire organization. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "d143301989344c09a1ba521e11a17d1b",
      "questionNumber": 32,
      "type": "single",
      "content": "<p>Question #32</p><p>A company is in the process of implementing AWS Organizations to constrain its developers to use only Amazon EC2, Amazon S3, and Amazon DynamoDB. The developers account resides in a dedicated organizational unit (OU). The solutions architect has implemented the following SCP on the developers account: <img src=\"images/sap/596f5d9e1c60e22d1ba6918d6bc05b134a3c5a36.png\" alt=\"61faadc126fb4fc55ec1eba8ee369d53.png\" data-href=\"\" style=\"\"/></p><p>When this policy is deployed, IAM users in the developers account are still able to use AWS services that are not listed in the policy. </p><p>What should the solutions architect do to eliminate the developers’ ability to use services outside the scope of this policy? </p>",
      "options": [
        {
          "label": "A",
          "content": "Create an explicit deny statement for each AWS service that should be constrained."
        },
        {
          "label": "B",
          "content": "Remove the FullAWSAccess SCP from the developers account&rsquo;s OU."
        },
        {
          "label": "C",
          "content": "Modify the FullAWSAccess SCP to explicitly deny all services."
        },
        {
          "label": "D",
          "content": "Add an explicit deny statement using a wildcard to the end of the SCP."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Remove the FullAWSAccess SCP from the developers account's OU.</p><p>This is the correct solution because:</p><p>AWS Organizations applies a default SCP called \"FullAWSAccess\" to all accounts and OUs when SCPs are first enabled. This default policy allows all actions on all AWS services.</p><p>SCPs work on a \"deny by default\" principle, where permissions must be explicitly allowed. However, the default FullAWSAccess SCP is overriding this by allowing all services.</p><p>When multiple SCPs apply to an account, they work using a logical AND operation. This means that even though the custom SCP is trying to restrict services to only EC2, S3, and DynamoDB, the FullAWSAccess SCP is still allowing all other services.</p><p>By removing the FullAWSAccess SCP from the developers account's OU, only the custom SCP will apply, which explicitly allows only EC2, S3, and DynamoDB. All other services will be implicitly denied because they're not explicitly allowed in any applicable SCP.</p><p>Option A (creating explicit deny statements for each service) would be impractical and difficult to maintain as AWS regularly adds new services.</p><p>Option C (modifying the FullAWSAccess SCP) is incorrect because the FullAWSAccess SCP is a managed policy that cannot be modified.</p><p>Option D (adding an explicit deny with a wildcard) is unnecessary and could potentially cause unintended consequences. The proper approach is to leverage the implicit deny behavior of SCPs by removing the FullAWSAccess SCP.</p><p>Sources</p><p>Restrict outbound network for accounts in my AWS organizations using SCP | AWS re:Post （https://repost.aws/questions/QUvA8rKb94QV2ag83SiQ1o2A/restrict-outbound-network-for-accounts-in-my-aws-organizations-using-scp）</p><p>Enforcing enterprise-wide preventive controls with AWS Organizations | AWS Cloud Operations Blog （）https://aws.amazon.com/cn/blogs/mt/enforcing-enterprise-wide-preventive-controls-with-aws-organizations/</p><p>Service control policy examples - AWS Organizations （https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples.html）</p><p>Using AWS Organizations for security - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/security-reference-architecture/organizations-security.html）</p><p>SCP not matching implicit Deny | AWS re:Post （https://repost.aws/questions/QU9g5-9ZIHSjWa9_xe5006Vw/scp-not-matching-implicit-deny）</p><p>Service control policies (SCPs) - AWS Organizations （https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "f88a738f218642c3836b218315ef6abe",
      "questionNumber": 33,
      "type": "single",
      "content": "<p>Question #33<br>A company is hosting a monolithic REST-based API for a mobile app on five Amazon EC2 instances in public subnets of a VPC. Mobile clients connect to the API by using a domain name that is hosted on Amazon Route 53. The company has created a Route 53 multivalue answer routing policy with the IP addresses of all the EC2 instances. Recently, the app has been overwhelmed by large and sudden increases to traffic.</p><p>A solutions architect needs to implement a solution so that the app can handle the new and varying load.<br><br>Which solution will meet these requirements with the LEAST operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Separate the API into individual AWS Lambda functions. Configure an Amazon API Gateway REST API with Lambda integration for the backend. Update the Route 53 record to point to the API Gateway API."
        },
        {
          "label": "B",
          "content": "Containerize the API logic. Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. Run the containers in the cluster by using Amazon EC2. Create a Kubernetes ingress. Update the Route 53 record to point to the Kubernetes ingress."
        },
        {
          "label": "C",
          "content": "Create an Auto Scaling group. Place all the EC2 instances in the Auto Scaling group. Configure the Auto Scaling group to perform scaling actions that are based on CPU utilization. Create an AWS Lambda function that reacts to Auto Scaling group changes and updates the Route 53 record."
        },
        {
          "label": "D",
          "content": "Create an Application Load Balancer (ALB) in front of the API. Move the EC2 instances to private subnets in the VPC. Add the EC2 instances as targets for the ALB. Update the Route 53 record to point to the ALB."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p><span style=\"color: rgb(31, 35, 41); background-color: rgb(255, 255, 255); font-size: 16px;\"> Answer: A </span></p><p><span style=\"color: rgb(31, 35, 41); background-color: rgb(255, 255, 255); font-size: 16px;\">Analysis of the Core Requirements in the Question The task is to resolve the issue of \"the application being unable to handle sudden and significant traffic surges\" while satisfying the requirement of **minimal operational overhead**. </span></p><p><span style=\"color: rgb(31, 35, 41); background-color: rgb(255, 255, 255); font-size: 16px;\">Option A - Logic: Split the monolithic API into individual AWS Lambda functions (serverless), configure an Amazon API Gateway REST API with Lambda integration as the backend, and update the Route 53 record to point to the API Gateway API. - Reasons for meeting the requirements: - Lambda requires no server management and can automatically scale elastically to cope with traffic fluctuations; - API Gateway is a fully managed service, so no infrastructure maintenance is needed; - The overall operational overhead of this solution is nearly zero, which fully aligns with the core requirement of \"minimal operational overhead.\" </span></p><p><span style=\"color: rgb(31, 35, 41); background-color: rgb(255, 255, 255); font-size: 16px;\">Option B- Issue: Containerizing the API logic + using Amazon Elastic Kubernetes Service (EKS) requires managing the Kubernetes cluster (including node maintenance, cluster configuration, container orchestration, etc.), which involves substantial operational effort and fails to meet the \"minimal operational overhead\" requirement. </span></p><p><span style=\"color: rgb(31, 35, 41); background-color: rgb(255, 255, 255); font-size: 16px;\">Option C - Issue: Even with an Auto Scaling group, EC2 instances still need to be managed (e.g., system patching, instance configuration), and the AWS Lambda function for updating the Route 53 record also requires maintenance. The operational overhead is significantly higher than that of the serverless solution. </span></p><p><span style=\"color: rgb(31, 35, 41); background-color: rgb(255, 255, 255); font-size: 16px;\">Option D- Issue: While the Application Load Balancer (ALB) is a managed service, EC2 instances still require manual management (e.g., instance maintenance, post-scaling configuration). The operational overhead is higher than that of the serverless Lambda + API Gateway solution.</span></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "4713834550d04d8ab23961eb7f268ebe",
      "questionNumber": 34,
      "type": "single",
      "content": "<p>Question #34</p><p>A company has created an OU in AWS Organizations for each of its engineering teams. Each OU owns multiple AWS accounts. The organization has hundreds of AWS accounts.</p><p>A solutions architect must design a solution so that each OU can view a breakdown of usage costs across its AWS accounts.</p><p>Which solution meets these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an AWS Cost and Usage Report (CUR) for each OU by using AWS Resource Access Manager. Allow each team to visualize the CUR through an Amazon QuickSight dashboard."
        },
        {
          "label": "B",
          "content": "Create an AWS Cost and Usage Report (CUR) from the AWS Organizations management account. Allow each team to visualize the CUR through an Amazon QuickSight dashboard."
        },
        {
          "label": "C",
          "content": "Create an AWS Cost and Usage Report (CUR) in each AWS Organizations member account. Allow each team to visualize the CUR through an Amazon QuickSight dashboard."
        },
        {
          "label": "D",
          "content": "Create an AWS Cost and Usage Report (CUR) by using AWS Systems Manager. Allow each team to visualize the CUR through SystemsManager OpsCenter dashboards."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Create an AWS Cost and Usage Report (CUR) from the AWS Organizations management account. Allow each team to visualize the CUR through an Amazon QuickSight dashboard.</p><p>This is the most efficient and recommended approach for the following reasons:</p><p>Centralized Cost Management: Creating the AWS Cost and Usage Report (CUR) from the management account provides a comprehensive view of costs across the entire organization, including all OUs and their member accounts.</p><p>Consolidated Data: The management account in AWS Organizations has visibility into the billing and usage data for all member accounts through consolidated billing, making it the ideal place to generate organization-wide cost reports.</p><p>Scalability: With hundreds of AWS accounts, creating and managing individual CURs in each member account (option C) would be extremely inefficient and difficult to maintain.</p><p>Data Visualization: Amazon QuickSight can connect to the CUR data and create dashboards that filter the data by OU, allowing each engineering team to see only their relevant cost information.</p><p>Access Control: Through proper IAM permissions and QuickSight dashboard sharing, each team can be granted access to view only the cost data relevant to their OU.</p><p>Option A is incorrect because AWS Resource Access Manager (RAM) cannot be used to share Cost and Usage Reports between accounts. RAM is designed for sharing AWS resources like subnets, transit gateways, and License Manager configurations, not billing data.</p><p>Option C is incorrect because it would create unnecessary complexity and management overhead by requiring the creation and maintenance of separate CURs in each of the hundreds of member accounts, rather than leveraging the consolidated view available from the management account.</p><p>Sources</p><p>CUR report for Organisation | AWS re:Post （https://repost.aws/questions/QURjr672h1QPGj4ja2fGGnRA/cur-report-for-organisation）</p><p>Is it possible to view the actual spending and budget values for all accounts through CUDOS? | AWS re:Post （https://repost.aws/questions/QUGtzcr2LcTbObILBiM63Fgw/is-it-possible-to-view-the-actual-spending-and-budget-values-for-all-accounts-through-cudos）</p><p>Introducing custom billing views: tailored cost and usage view for your stakeholders | AWS Cloud Financial Management （https://aws.amazon.com/cn/blogs/aws-cloud-financial-management/introducing-custom-billing-views-tailored-cost-and-usage-view-relevant-for-your-stakeholders/）</p><p>Access to CUR and Billing Info | AWS re:Post （https://repost.aws/questions/QUAjlooP76TaGBvVSZlyRHyQ/access-to-cur-and-billing-info）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "db637b155bc34da2ad1c4e6deba3ce4f",
      "questionNumber": 35,
      "type": "single",
      "content": "<p>Question #35</p><p>A company is storing data on premises on a Windows file server. The company produces 5 GB of new data daily. The company migrated part of its Windows-based workload to AWS and needs the data to be available on a file system in the cloud. The company already has established an AWS Direct Connect connection between the on-premises network and AWS.</p><p>Which data migration strategy should the company use?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use the file gateway option in AWS Storage Gateway to replace the existing Windows file server, and point the existing file share to the new file gateway."
        },
        {
          "label": "B",
          "content": "Use AWS DataSync to schedule a daily task to replicate data between the on-premises Windows file server and Amazon FSx."
        },
        {
          "label": "C",
          "content": "Use AWS Data Pipeline to schedule a daily task to replicate data between the on-premises Windows file server and Amazon Elastic File System (Amazon EFS)."
        },
        {
          "label": "D",
          "content": "Use AWS DataSync to schedule a daily task to replicate data between the on-premises Windows file server and Amazon Elastic File System (Amazon EFS)."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the information provided, the best data migration strategy for the company is:</p><p>B. Use AWS DataSync to schedule a daily task to replicate data between the on-premises Windows file server and Amazon FSx.</p><p>This is the optimal solution because:</p><p>AWS DataSync is specifically designed for efficient data transfer and synchronization between on-premises storage and AWS cloud storage services. It's ideal for the company's scenario of needing to transfer 5 GB of new data daily.</p><p>Amazon FSx provides a fully managed native Windows file system in the cloud, which is the perfect target for migrating data from a Windows file server. This ensures compatibility with Windows-based workloads that have been migrated to AWS.</p><p>DataSync can leverage the existing AWS Direct Connect connection to provide secure, fast, and reliable data transfer between the on-premises environment and AWS.</p><p>The solution allows for scheduled, automated daily transfers to keep the cloud file system up to date with the new data being produced on-premises.</p><p>Option A (using Storage Gateway's file gateway) is less suitable because it's designed more as a hybrid storage solution rather than a migration tool. While it could work, it would replace the existing file server rather than replicate data from it.</p><p>Option C (using AWS Data Pipeline with Amazon EFS) is incorrect because Data Pipeline is not optimized for file server migration scenarios, and Amazon EFS is a Linux-based file system that wouldn't be compatible with Windows-based workloads without additional configuration.</p><p>AWS DataSync with Amazon FSx provides the most straightforward, efficient, and compatible solution for this Windows file server migration scenario.</p><p>Sources</p><p>Migrating existing file storage to FSx for Windows File Server - Amazon FSx for Windows File Server（https://docs.aws.amazon.com/fsx/latest/WindowsGuide/migrate-files-fsx.html） </p><p>Which aws service i can use to continuesly sync data from onpremises to aws | AWS re:Post （https://repost.aws/questions/QUL4bbFAh0QUy51quD4Foa0w/which-aws-service-i-can-use-to-continuesly-sync-data-from-onpremises-to-aws）</p><p>Field Notes: Migrating File Servers to Amazon FSx and Integrating with AWS Managed Microsoft AD | AWS Architecture Blog （https://aws.amazon.com/cn/blogs/architecture/field-notes-migrating-file-servers-to-amazon-fsx-and-integrating-with-aws-managed-microsoft-ad/）</p><p>AWS Storage sessions at AWS re:Invent 2020-2021: Week 2 | AWS Storage Blog （https://aws.amazon.com/cn/blogs/storage/aws-storage-sessions-at-aws-reinvent-2020-week-2/）</p><p>Migrating existing files to FSx for Windows File Server using AWS DataSync - Amazon FSx for Windows File Server （https://docs.aws.amazon.com/fsx/latest/WindowsGuide/migrate-files-to-fsx-datasync.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "6c58a3be1a824fcd94c733054bcfef32",
      "questionNumber": 36,
      "type": "single",
      "content": "<p>Question #36</p><p>A company’s solutions architect is reviewing a web application that runs on AWS. The application references static assets in an Amazon S3 bucket in the us-east-1 Region. The company needs resiliency across multiple AWS Regions. The company already has created an S3 bucket in a second Region.</p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Configure the application to write each object to both S3 buckets. Set up an Amazon Route 53 public hosted zone with a record set using a weighted routing policy for each S3 bucket. Configure the application to reference the objects by using the Route 53 DNS name."
        },
        {
          "label": "B",
          "content": "Create an AWS Lambda function to copy objects from the S3 bucket in us-east-1 to the S3 bucket in the second Region. Invoke the Lambda function each time an object is written to the S3 bucket in us-east-1. Set up an Amazon CloudFront distribution with an origin group that contains the two S3 buckets as origins."
        },
        {
          "label": "C",
          "content": "Configure replication on the S3 bucket in us-east-1 to replicate objects to the S3 bucket in the second Region. Set up an Amazon CloudFront distribution with an origin group that contains the two S3 buckets as origins."
        },
        {
          "label": "D",
          "content": "Configure replication on the S3 bucket in us-east-1 to replicate objects to the S3 bucket in the second Region. If failover is required, update the application code to load S3 objects from the S3 bucket in the second Region."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the information provided, the best solution with the least operational overhead is:</p><p>C. Configure replication on the S3 bucket in us-east-1 to replicate objects to the S3 bucket in the second Region. Set up an Amazon CloudFront distribution with an origin group that contains the two S3 buckets as origins.</p><p>This solution is optimal because:</p><p>S3 Cross-Region Replication (CRR) automatically copies objects from the source bucket in us-east-1 to the destination bucket in the second region, ensuring data consistency across regions with minimal manual intervention.</p><p>CloudFront with origin groups provides built-in failover capability. If the primary S3 bucket becomes unavailable, CloudFront will automatically route requests to the secondary bucket in the other region.</p><p>This approach requires minimal ongoing operational overhead as the replication is handled automatically by AWS, and the failover mechanism is managed by CloudFront.</p><p>CloudFront also provides additional benefits like edge caching, which improves performance for end users by serving content from locations closer to them.</p><p>Option A (writing objects to both buckets and using Route 53 weighted routing) would require more operational overhead because:</p><p>The application would need to be modified to write each object twice (to both regions)</p><p>It would require managing DNS records and routing policies in Route 53</p><p>It doesn't provide automatic synchronization between buckets if one write operation fails</p><p>The CloudFront with S3 Cross-Region Replication approach provides a more streamlined, automated solution that meets the multi-region resilience requirements while minimizing the operational burden on the company.</p><p>Sources</p><p>Replicating objects within and across Regions - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html）</p><p>Data protection in Amazon S3 - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/data-protection.html）</p><p>Cross region S3 backup and restore v/s cross region S3 replication | AWS re:Post （https://repost.aws/questions/QUd92uKIKbSqqEO1hStW-AzA/cross-region-s3-backup-and-restore-v-s-cross-region-s3-replication）</p><p>Configuring replication for use with Multi-Region Access Points - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/MultiRegionAccessPointBucketReplication.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": true,
      "noteContent": "CloudFront with origin groups provides built-in failover capability."
    },
    {
      "id": "c4f8bb6b1d7641648475e8a836261408",
      "questionNumber": 37,
      "type": "single",
      "content": "<p>Question #37</p><p>A company is hosting a three-tier web application in an on-premises environment. Due to a recent surge in traffic that resulted in downtime and a significant financial impact, company management has ordered that the application be moved to AWS. The application is written in .NET and has a dependency on a MySQL database. A solutions architect must design a scalable and highly available solution to meet the demand of 200,000 daily users.</p><p>Which steps should the solutions architect take to design an appropriate solution?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use AWS Elastic Beanstalk to create a new application with a web server environment and an Amazon RDS MySQL Multi-AZ DB instance. The environment should launch a Network Load Balancer (NLB) in front of an Amazon EC2 Auto Scaling group in multiple Availability Zones. Use an Amazon Route 53 alias record to route traffic from the company&rsquo;s domain to the NLB."
        },
        {
          "label": "B",
          "content": "Use AWS CloudFormation to launch a stack containing an Application Load Balancer (ALB) in front of an Amazon EC2 Auto Scaling group spanning three Availability Zones. The stack should launch a Multi-AZ deployment of an Amazon Aurora MySQL DB cluster with a Retain deletion policy. Use an Amazon Route 53 alias record to route traffic from the company&rsquo;s domain to the ALB."
        },
        {
          "label": "C",
          "content": "Use AWS Elastic Beanstalk to create an automatically scaling web server environment that spans two separate Regions with an Application Load Balancer (ALB) in each Region. Create a Multi-AZ deployment of an Amazon Aurora MySQL DB cluster with a cross-Region read replica. Use Amazon Route 53 with a geoproximity routing policy to route traffic between the two Regions."
        },
        {
          "label": "D",
          "content": "Use AWS CloudFormation to launch a stack containing an Application Load Balancer (ALB) in front of an Amazon ECS cluster of Spot instances spanning three Availability Zones. The stack should launch an Amazon RDS MySQL DB instance with a Snapshot deletion policy. Use an Amazon Route 53 alias record to route traffic from the company&rsquo;s domain to the ALB."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>The correct answer isB. Here's why:</p><p> Key Requirements:</p><p>1.Scalable & Highly Available: The solution must handle 200,000 daily users with no downtime.</p><p>2.Three-Tier .NET Application: Requires a web tier, application tier, and database tier.</p><p>3.MySQL Dependency: The database must be highly available and scalable.</p><p>4.On-Premises to AWS Migration: The solution should leverage AWS best practices for resilience and performance.</p><p> Why Option B is Correct:</p><p>-Application Load Balancer (ALB): Distributes traffic across multiple Availability Zones (AZs) and is ideal for web applications (supports HTTP/HTTPS).</p><p>-EC2 Auto Scaling Group (Multi-AZ): Ensures the application scales horizontally and remains available even if one AZ fails.</p><p>-Amazon Aurora MySQL (Multi-AZ): Provides high availability with automatic failover and better performance than standard RDS MySQL.</p><p>-Retain Deletion Policy: Ensures the database is not accidentally deleted during stack updates/deletions.</p><p>-Route 53 Alias Record: Efficiently routes traffic to the ALB.</p><p> Why Other Options Are Incorrect:</p><p>-Option A: Uses aNetwork Load Balancer (NLB), which is better suited for low-latency/TCP traffic rather than web applications. Elastic Beanstalk is acceptable, but CloudFormation (Option B) offers more control for enterprise architectures.</p><p>-Option C: Overcomplicates the solution by introducingmulti-Region deployment, which is unnecessary for handling 200,000 daily users (a single Region with Multi-AZ is sufficient). Geoproximity routing is not required here.</p><p>-Option D: UsesSpot Instances for ECS, which is risky for production workloads due to potential interruptions. Also, aSnapshot deletion policy is less robust than aRetain policy for critical databases.</p><p> Conclusion:</p><p>Option B provides the most scalable, highly available, and cost-effective solution using ALB, Multi-AZ Aurora MySQL, and CloudFormation for infrastructure-as-code reliability. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "3b31d7de119c4f67b3b16cc7b170fa2d",
      "questionNumber": 38,
      "type": "single",
      "content": "<p>Question #38</p><p>A company is using AWS Organizations to manage multiple AWS accounts. For security purposes, the company requires the creation of an Amazon Simple Notification Service (Amazon SNS) topic that enables integration with a third-party alerting system in all the Organizations member accounts.</p><p>A solutions architect used an AWS CloudFormation template to create the SNS topic and stack sets to automate the deployment of CloudFormation stacks. Trusted access has been enabled in Organizations.</p><p>What should the solutions architect do to deploy the CloudFormation StackSets in all AWS accounts?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a stack set in the Organizations member accounts. Use service-managed permissions. Set deployment options to deploy to an organization. Use CloudFormation StackSets drift detection."
        },
        {
          "label": "B",
          "content": "Create stacks in the Organizations member accounts. Use self-service permissions. Set deployment options to deploy to an organization."
        },
        {
          "label": "C",
          "content": "Create a stack set in the Organizations management account. Use service-managed permissions. Set deployment options to deploy to the organization. Enable CloudFormation StackSets automatic deployment."
        },
        {
          "label": "D",
          "content": "Create stacks in the Organizations management account. Use service-managed permissions. Set deployment options to deploy to the organization. Enable CloudFormation StackSets drift detection."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>The correct answer is: &nbsp;</p><p>C. Create a stack set in the Organizations management account. Use service-managed permissions. Set deployment options to deploy to the organization. Enable CloudFormation StackSets automatic deployment. &nbsp;</p><p> Explanation: &nbsp;</p><p>1.Stack Sets must be created in the management account – Since the company is using AWS Organizations, the management account is responsible for deploying StackSets across member accounts. &nbsp;</p><p>2.Use service-managed permissions – This allows AWS Organizations to handle permissions automatically, ensuring the StackSet can deploy to all specified accounts. &nbsp;</p><p>3.Deploy to the organization – This ensures the StackSet is deployed to all (or selected) member accounts within the organization. &nbsp;</p><p>4.Enable automatic deployment – This ensures that new accounts added to the organization in the future will automatically receive the StackSet deployment. &nbsp;</p><p> Why not the other options? &nbsp;</p><p>-A: Incorrect because StackSets must be created in themanagement account, not member accounts. &nbsp;</p><p>-B: Incorrect becauseself-service permissions require manual intervention, whereasservice-managed permissions are needed for automated deployment across an organization. &nbsp;</p><p>-D: Incorrect becausestacks (not StackSets) are created in individual accounts, but the requirement is to deploy acrossall member accounts using StackSets. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "ad701a1aab5e47689aba0ee9a5dc7cd6",
      "questionNumber": 39,
      "type": "multiple",
      "content": "<p>Question #39<br>A company wants to migrate its workloads from on premises to AWS. The workloads run on Linux and Windows. The company has a large on-premises infrastructure that consists of physical machines and VMs that host numerous applications.</p><p>The company must capture details about the system configuration, system performance, running processes, and network connections of its on-premises workloads. The company also must divide the on-premises applications into groups for AWS migrations. The company needs recommendations for Amazon EC2 instance types so that the company can run its workloads on AWS in the most cost-effective manner.</p><p>Which combination of steps should a solutions architect take to meet these requirements? (Choose three.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Assess the existing applications by installing AWS Application Discovery Agent on the physical machines and VMs."
        },
        {
          "label": "B",
          "content": "Assess the existing applications by installing AWS Systems Manager Agent on the physical machines and VMs."
        },
        {
          "label": "C",
          "content": "Group servers into applications for migration by using AWS Systems Manager Application Manager."
        },
        {
          "label": "D",
          "content": "Group servers into applications for migration by using AWS Migration Hub."
        },
        {
          "label": "E",
          "content": "Generate recommended instance types and associated costs by using AWS Migration Hub."
        },
        {
          "label": "F",
          "content": "Import data about server sizes into AWS Trusted Advisor. Follow the recommendations for cost optimization."
        }
      ],
      "correctAnswer": "ADE",
      "explanation": "<p>The correct combination of steps to meet the company's requirements is:</p><p>A. Assess the existing applications by installing AWS Application Discovery Agent on the physical machines and VMs. &nbsp;</p><p>D. Group servers into applications for migration by using AWS Migration Hub. &nbsp;</p><p>E. Generate recommended instance types and associated costs by using AWS Migration Hub. &nbsp;</p><p> Explanation: &nbsp;</p><p>1.AWS Application Discovery Agent (Option A) is designed to collect system configuration, performance, processes, and network connection details from on-premises servers (both physical and VMs). This data is crucial for assessing and planning migrations. &nbsp;</p><p>2.AWS Migration Hub (Option D) allows you to group discovered servers into applications for migration tracking. &nbsp;</p><p>3.AWS Migration Hub (Option E) provides recommendations for EC2 instance types and cost estimates based on the collected discovery data. &nbsp;</p><p> Why not the other options? &nbsp;</p><p>-B. AWS Systems Manager Agent (SSM Agent) is used for managing instances post-migration, not for discovery and migration planning. &nbsp;</p><p>-C. AWS Systems Manager Application Manager is for operational management, not migration grouping. &nbsp;</p><p>-F. Trusted Advisor provides general AWS cost optimization tips but does not generate migration-specific instance recommendations. &nbsp;</p><p>Thus, the best choices areA, D, and E.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "69f15d92e2e54f9ea2494a5645ca0602",
      "questionNumber": 40,
      "type": "single",
      "content": "<p>Question #40<br>A company is hosting an image-processing service on AWS in a VPC. The VPC extends across two Availability Zones. Each Availability Zone contains one public subnet and one private subnet.</p><p><br></p><p>The service runs on Amazon EC2 instances in the private subnets. An Application Load Balancer in the public subnets is in front of the service.The service needs to communicate with the internet and does so through two NAT gateways. The service uses Amazon S3 for image storage. The EC2 instances retrieve approximately 1 TB of data from an S3 bucket each day.</p><p><br></p><p>The company has promoted the service as highly secure. A solutions architect must reduce cloud expenditures as much as possible without compromising the service’s security posture or increasing the time spent on ongoing operations.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Replace the NAT gateways with NAT instances."
        },
        {
          "label": "B",
          "content": "Move the EC2 instances to the public subnets. Remove the NAT gateways."
        },
        {
          "label": "C",
          "content": "Set up an S3 gateway VPC endpoint in the VPC. Attach an endpoint policy to the endpoint to allow the required actions on the S3 bucket."
        },
        {
          "label": "D",
          "content": "Attach an Amazon Elastic File System (Amazon EFS) volume to the EC2 instances. Host the images on the EFS volume."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>The correct answer isC. Set up an S3 gateway VPC endpoint in the VPC. Attach an endpoint policy to the endpoint to allow the required actions on the S3 bucket.</p><p> Explanation:</p><p>1.Current Cost Issue: The service uses NAT gateways for internet access, which incur costs based on data processed and hourly usage. Additionally, the EC2 instances retrieve1 TB of data daily from S3, which traverses the NAT gateways, increasing costs unnecessarily.</p><p>2.Security Requirement: The service is promoted as highly secure, so changes must not weaken security or increase operational overhead.</p><p>3.Optimization:</p><p> &nbsp; -S3 Gateway VPC Endpoint: </p><p> &nbsp; &nbsp; - Allows private EC2 instances to communicate directly with S3without going through NAT gateways or the public internet, reducing NAT gateway costs.</p><p> &nbsp; &nbsp; - Data transfer via the endpoint isfree (no additional charges for S3 traffic within the same region).</p><p> &nbsp; &nbsp; - Maintains security since traffic stays within the AWS network.</p><p> &nbsp; &nbsp; - An endpoint policy can restrict access to only the required S3 bucket and actions.</p><p> &nbsp; -No Operational Overhead: VPC endpoints are fully managed by AWS, requiring no maintenance.</p><p> Why Not the Other Options?</p><p>-A. Replace NAT gateways with NAT instances:</p><p> &nbsp;- NAT instances areless reliable (not highly available like NAT gateways) and requiremanual management (scaling, patching), increasing operational overhead.</p><p> &nbsp;- Cost savings are minimal compared to eliminating NAT gateway usage for S3 traffic.</p><p>-B. Move EC2 instances to public subnets and remove NAT gateways:</p><p> &nbsp;- Exposing EC2 instances to the public internetcompromises security, violating the requirement.</p><p>-D. Use Amazon EFS instead of S3:</p><p> &nbsp;- EFS ismore expensive for storage and throughput compared to S3, and it doesn’t address the NAT gateway cost issue.</p><p> &nbsp;- Adds complexity without solving the core problem.</p><p> Conclusion:</p><p>Option C is the most cost-effective and secure solution, eliminating NAT gateway costs for S3 traffic while maintaining security and operational simplicity. </p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "de318b0e36124f269cdd829cda8e3d77",
      "questionNumber": 41,
      "type": "single",
      "content": "<p>Question #41</p><p>A company recently deployed an application on AWS. The application uses Amazon DynamoDB. The company measured the application load and configured the RCUs and WCUs on the DynamoDB table to match the expected peak load. The peak load occurs once a week for a 4-hour period and is double the average load. The application load is close to the average load for the rest of the week. The access pattern includes many more writes to the table than reads of the table.</p><p><br></p><p>A solutions architect needs to implement a solution to minimize the cost of the table.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use AWS Application Auto Scaling to increase capacity during the peak period. Purchase reserved RCUs and WCUs to match the average load."
        },
        {
          "label": "B",
          "content": "Configure on-demand capacity mode for the table."
        },
        {
          "label": "C",
          "content": "Configure DynamoDB Accelerator (DAX) in front of the table. Reduce the provisioned read capacity to match the new peak load on the table."
        },
        {
          "label": "D",
          "content": "Configure DynamoDB Accelerator (DAX) in front of the table. Configure on-demand capacity mode for the table."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Based on the information provided, the most cost-effective solution for minimizing the cost of the DynamoDB table is:</p><p>A. Use AWS Application Auto Scaling to increase capacity during the peak period. Purchase reserved RCUs and WCUs to match the average load.</p><p>This solution is optimal because:</p><p>Reserved capacity provides significant cost savings (up to 70%) compared to on-demand pricing for the predictable portion of the workload, which in this case is the average load that occurs most of the time.</p><p>AWS Application Auto Scaling can automatically adjust the provisioned capacity to handle the peak load that occurs once a week for 4 hours, ensuring the application performs well during these periods without having to pay for that extra capacity all the time.</p><p>Since the peak load is predictable (once a week for 4 hours) and the rest of the time the load is consistent (close to average), this combination of reserved capacity for the baseline and auto-scaling for the peaks provides the most cost-effective approach.</p><p>Option B (on-demand capacity mode) would be simpler to manage but more expensive for this workload pattern. On-demand mode is typically more cost-effective for unpredictable workloads, but in this case, the workload has a predictable pattern with a consistent baseline.</p><p>Option C (using DAX) would not be cost-effective because:</p><p>DAX is designed to improve read performance, but the scenario states the access pattern includes \"many more writes to the table than reads\"</p><p>DAX would add additional costs without addressing the core issue of optimizing the write capacity costs, which is the primary concern given the write-heavy workload</p><p>The combination of reserved capacity for the predictable baseline load and auto-scaling for the weekly peak provides the most cost-effective solution for this specific workload pattern.</p><p>Sources</p><p>Choose the right throughput strategy for Amazon DynamoDB applications | AWS Database Blog （https://aws.amazon.com/cn/blogs/database/choose-the-right-throughput-strategy-for-amazon-dynamodb-applications/）</p><p>Evaluate your DynamoDB table's capacity mode - Amazon DynamoDB （https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/CostOptimization_TableCapacityMode.html）</p><p>Real time Database Dynamodb - Scaling Error | AWS re:Post （https://repost.aws/questions/QUpwt70SzaT1qhk6lEhnW9nw/real-time-database-dynamodb-scaling-error）</p><p>Demystifying Amazon DynamoDB on-demand capacity mode | AWS Database Blog （https://aws.amazon.com/cn/blogs/database/demystifying-amazon-dynamodb-on-demand-capacity-mode/）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "e58dafe609104a079e30b8381accfb3f",
      "questionNumber": 42,
      "type": "single",
      "content": "<p>Question #42</p><p>A solutions architect needs to advise a company on how to migrate its on-premises data processing application to the AWS Cloud. Currently, users upload input files through a web portal. The web server then stores the uploaded files on NAS and messages the processing server over a message queue. Each media file can take up to 1 hour to process. The company has determined that the number of media files awaiting processing is significantly higher during business hours, with the number of files rapidly declining after business hours.</p><p>What is the MOST cost-effective migration recommendation?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a queue using Amazon SQS. Configure the existing web server to publish to the new queue. When there are messages in the queue, invoke an AWS Lambda function to pull requests from the queue and process the files. Store the processed files in an Amazon S3 bucket."
        },
        {
          "label": "B",
          "content": "Create a queue using Amazon MQ. Configure the existing web server to publish to the new queue. When there are messages in the queue, create a new Amazon EC2 instance to pull requests from the queue and process the files. Store the processed files in Amazon EFS. Shut down the EC2 instance after the task is complete."
        },
        {
          "label": "C",
          "content": "Create a queue using Amazon MQ. Configure the existing web server to publish to the new queue. When there are messages in the queue, invoke an AWS Lambda function to pull requests from the queue and process the files. Store the processed files in Amazon EFS."
        },
        {
          "label": "D",
          "content": "Create a queue using Amazon SQS. Configure the existing web server to publish to the new queue. Use Amazon EC2 instances in an EC2 Auto Scaling group to pull requests from the queue and process the files. Scale the EC2 instances based on the SQS queue length. Store the processed files in an Amazon S3 bucket."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Themost cost-effective migration recommendation for this scenario is: &nbsp;</p><p>Answer: D &nbsp;</p><p>Explanation: &nbsp;</p><p>1.Amazon SQS is a fully managed, cost-effective queue service that eliminates the need to manage your own message broker (like Amazon MQ). It is the better choice for this migration. &nbsp;</p><p>2.EC2 Auto Scaling is ideal for processing tasks that take up to1 hour per file, as Lambda has a maximum execution time of15 minutes (which rules out options A and C). Auto Scaling allows the system to: &nbsp;</p><p> &nbsp; - Scaleup during business hours when the queue is long. &nbsp;</p><p> &nbsp; - Scaledown after business hours when the queue is short, reducing costs. &nbsp;</p><p>3.Amazon S3 is the most cost-effective and scalable storage solution for processed files (compared to Amazon EFS, which is more expensive for this use case). &nbsp;</p><p>Why Not the Other Options? &nbsp;</p><p>-A & C: Lambda has a15-minute execution limit, making it unsuitable for 1-hour processing tasks. &nbsp;</p><p>-B: Amazon MQ is more expensive than SQS (since it requires managing a broker). Also, manually launching EC2 instances is less efficient than Auto Scaling. &nbsp;</p><p>-C: UsingEFS for storage is more expensive and unnecessary compared to S3 for processed files. &nbsp;</p><p>Thus,Option D provides thebest balance of cost efficiency, scalability, and AWS-native services for this workload.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "70806b3c5582404b9f9961c2bbb001e5",
      "questionNumber": 43,
      "type": "single",
      "content": "<p>Question #43</p><p>A company is using Amazon OpenSearch Service to analyze data. The company loads data into an OpenSearch Service cluster with 10 data nodes from an Amazon S3 bucket that uses S3 Standard storage. The data resides in the cluster for 1 month for read-only analysis. After 1 month, the company deletes the index that contains the data from the cluster. For compliance purposes, the company must retain a copy of all input data.</p><p><br></p><p>The company is concerned about ongoing costs and asks a solutions architect to recommend a new solution.</p><p><br></p><p>Which solution will meet these requirements MOST cost-effectively?</p>",
      "options": [
        {
          "label": "A",
          "content": "Replace all the data nodes with UltraWarm nodes to handle the expected capacity. Transition the input data from S3 Standard to S3 Glacier Deep Archive when the company loads the data into the cluster."
        },
        {
          "label": "B",
          "content": "Reduce the number of data nodes in the cluster to 2. Add UltraWarm nodes to handle the expected capacity. Configure the indexes to transition to UltraWarm when OpenSearch Service ingests the data. Transition the input data to S3 Glacier Deep Archive after 1 month by using an S3 Lifecycle policy."
        },
        {
          "label": "C",
          "content": "Reduce the number of data nodes in the cluster to 2. Add UltraWarm nodes to handle the expected capacity. Configure the indexes to transition to UltraWarm when OpenSearch Service ingests the data. Add cold storage nodes to the cluster. Transition the indexes from UltraWarm to cold storage. Delete the input data from the S3 bucket after 1 month by using an S3 Lifecycle policy."
        },
        {
          "label": "D",
          "content": "Reduce the number of data nodes in the cluster to 2. Add instance-backed data nodes to handle the expected capacity. Transition the input data from S3 Standard to S3 Glacier Deep Archive when the company loads the data into the cluster."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Themost cost-effective solution that meets the requirements is: &nbsp;</p><p>Option B &nbsp;</p><p>Reduce the number of data nodes in the cluster to 2. Add UltraWarm nodes to handle the expected capacity. Configure the indexes to transition to UltraWarm when OpenSearch Service ingests the data. Transition the input data to S3 Glacier Deep Archive after 1 month by using an S3 Lifecycle policy. &nbsp;</p><p>Why Option B? &nbsp;</p><p>1.Cost Optimization with UltraWarm Nodes &nbsp;</p><p> &nbsp; - UltraWarm nodes arecheaper than standard data nodes for read-only workloads (like analytics). &nbsp;</p><p> &nbsp; - By reducing the number of standard data nodes to2 (minimum for high availability) and using UltraWarm for storage, the company saves costs. &nbsp;</p><p>2.Compliance Requirement (Data Retention in S3) &nbsp;</p><p> &nbsp; - The input data must be retained, butS3 Standard is expensive for long-term storage. &nbsp;</p><p> &nbsp; - Moving data toS3 Glacier Deep Archive after 1 month (via S3 Lifecycle policy) is thecheapest storage option for compliance. &nbsp;</p><p>3.No Need for Cold Storage Nodes (Option C is Overkill) &nbsp;</p><p> &nbsp; - Cold storage nodes in OpenSearch arenot necessary since UltraWarm already provides cost-efficient storage for read-only data. &nbsp;</p><p>4.Option A is Incorrect &nbsp;</p><p> &nbsp; - Replacingall data nodes with UltraWarm isnot feasible because UltraWarm cannot handleingestion—onlyquerying. &nbsp;</p><p>5.Option D is Incorrect &nbsp;</p><p> &nbsp; - Instance-backed data nodes arenot cost-effective compared to UltraWarm for read-only workloads. &nbsp;</p><p>Conclusion: &nbsp;</p><p>Option B provides thebest balance of cost savings and compliance by leveraging UltraWarm for OpenSearch and S3 Glacier Deep Archive for long-term retention. &nbsp;</p><p>Answer: B</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "df87a73d9480486fa9b531acf7d96b8c",
      "questionNumber": 44,
      "type": "single",
      "content": "<p>Question #44</p><p>A company has 10 accounts that are part of an organization in AWS Organizations. AWS Config is configured in each account. All accounts belong to either the Prod OU or the NonProd OU.</p><p><br></p><p>The company has set up an Amazon EventBridge rule in each AWS account to notify an Amazon Simple Notification Service (Amazon SNS) topic when an Amazon EC2 security group inbound rule is created with 0.0.0.0/0 as the source. The company’s security team is subscribed to the SNS topic.</p><p><br></p><p>For all accounts in the NonProd OU, the security team needs to remove the ability to create a security group inbound rule that includes 0.0.0.0/0 as the source.</p><p><br></p><p>Which solution will meet this requirement with the LEAST operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Modify the EventBridge rule to invoke an AWS Lambda function to remove the security group inbound rule and to publish to the SNS topic.Deploy the updated rule to the NonProd OU.&nbsp;"
        },
        {
          "label": "B",
          "content": "Add the vpc-sg-open-only-to-authorized-ports AWS Config managed rule to the NonProd OU."
        },
        {
          "label": "C",
          "content": "Configure an SCP to allow the ec2:AuthorizeSecurityGroupIngress action when the value of the aws:SourceIp condition key is not 0.0.0.0/0.Apply the SCP to the NonProd OU.&nbsp;"
        },
        {
          "label": "D",
          "content": "Configure an SCP to deny the ec2:AuthorizeSecurityGroupIngress action when the value of the aws:SourceIp condition key is 0.0.0.0/0.Apply the SCP to the NonProd OU."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. Configure an SCP to deny the ec2:AuthorizeSecurityGroupIngress action when the value of the aws:SourceIp condition key is 0.0.0.0/0. Apply the SCP to the NonProd OU.</p><p>This solution is the most effective with the least operational overhead because:</p><p>Service Control Policies (SCPs) are preventive controls that can be applied at the OU level to deny specific actions. In this case, denying the ec2:AuthorizeSecurityGroupIngress action with a condition that checks for 0.0.0.0/0 as the source IP will prevent users from creating open security group rules.</p><p>SCPs are enforced at the AWS Organizations level and apply to all accounts within the NonProd OU, providing centralized management with minimal configuration.</p><p>Once configured, the SCP will immediately prevent the creation of non-compliant security group rules without requiring any additional monitoring or remediation steps.</p><p>This approach has minimal operational overhead as it's a one-time setup that doesn't require enabling or configuring additional services beyond AWS Organizations, which is already in use.</p><p>Option B (adding the vpc-sg-open-only-to-authorized-ports AWS Config managed rule) would have higher operational overhead because:</p><p>AWS Config rules are detective controls, not preventive. They would detect non-compliant security groups after they've been created but wouldn't prevent their creation.</p><p>While AWS Config is already configured in each account, implementing and managing the rule would still require more operational effort than an SCP.</p><p>The Config rule would require additional remediation actions to fix non-compliant resources, whereas the SCP prevents the creation of non-compliant resources in the first place.</p><p>The SCP approach (Option D) provides a more direct and efficient solution with less operational overhead for preventing the creation of security group rules with 0.0.0.0/0 as the source in the NonProd OU.</p><p>Sources</p><p>Enforcing enterprise-wide preventive controls with AWS Organizations | AWS Cloud Operations Blog （https://aws.amazon.com/cn/blogs/mt/enforcing-enterprise-wide-preventive-controls-with-aws-organizations/）</p><p>Protect your AWS resources from unauthorized access using AWS Organizations integrations | AWS Cloud Operations Blog （https://aws.amazon.com/cn/blogs/mt/protect-your-aws-resources-from-unauthorized-access-using-aws-organizations-integrations/）</p><p>Example SCPs for Amazon Elastic Compute Cloud (Amazon EC2) - AWS Organizations （https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples_ec2.html）</p><p>Application team example: Creating an AWS Config rule - AWS Prescriptive Guidance（https://docs.aws.amazon.com/prescriptive-guidance/latest/vulnerability-management/application-team-example.html） </p><p>SCP evaluation - AWS Organizations （https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_evaluation.html）</p><p>Amazon Elastic Compute Cloud (Amazon EC2) controls - AWS Control Tower （https://docs.aws.amazon.com/controltower/latest/controlreference/ec2-rules.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "78f10dfee0854548b0245811a0be5f85",
      "questionNumber": 45,
      "type": "single",
      "content": "<p>Question #45</p><p>A company hosts a Git repository in an on-premises data center. The company uses webhooks to invoke functionality that runs in the AWS Cloud.</p><p><br></p><p>The company hosts the webhook logic on a set of Amazon EC2 instances in an Auto Scaling group that the company set as a target for an Application Load Balancer (ALB). The Git server calls the ALB for the configured webhooks. The company wants to move the solution to a serverless architecture.</p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "For each webhook, create and configure an AWS Lambda function URL. Update the Git servers to call the individual Lambda function URLs."
        },
        {
          "label": "B",
          "content": "Create an Amazon API Gateway HTTP API. Implement each webhook logic in a separate AWS Lambda function. Update the Git servers to call the API Gateway endpoint."
        },
        {
          "label": "C",
          "content": "Deploy the webhook logic to AWS App Runner. Create an ALB, and set App Runner as the target. Update the Git servers to call the ALB endpoint."
        },
        {
          "label": "D",
          "content": "Containerize the webhook logic. Create an Amazon Elastic Container Service (Amazon ECS) cluster, and run the webhook logic in AWS Fargate. Create an Amazon API Gateway REST API, and set Fargate as the target. Update the Git servers to call the API Gateway endpoint."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the information provided, the best solution with the least operational overhead is:</p><p>B. Create an Amazon API Gateway HTTP API. Implement each webhook logic in a separate AWS Lambda function. Update the Git servers to call the API Gateway endpoint.</p><p>This solution is optimal for the following reasons:</p><p>Serverless Architecture: Both API Gateway and Lambda are fully managed serverless services, eliminating the need to manage any infrastructure like EC2 instances, Auto Scaling groups, or load balancers.</p><p>Centralized Management: Using a single API Gateway endpoint provides a centralized entry point for all webhooks, which simplifies the configuration on the Git server side. You only need to update the Git server to point to one endpoint instead of multiple function URLs.</p><p>Separation of Concerns: Implementing each webhook logic in a separate Lambda function allows for independent development, deployment, and scaling of each webhook handler.</p><p>Minimal Operational Overhead: API Gateway HTTP APIs are specifically designed for low-latency, cost-effective API proxying to Lambda functions. They require minimal configuration and maintenance compared to REST APIs.</p><p>Built-in Features: API Gateway provides built-in features like request validation, throttling, and monitoring that would otherwise need to be implemented separately.</p><p>Option A (Lambda function URLs) would require updating the Git server configuration for each webhook separately, increasing operational overhead and complexity in management.</p><p>Option C (AWS App Runner with ALB) introduces unnecessary components like an ALB, which adds complexity and operational overhead compared to the API Gateway approach. App Runner is more suitable for containerized applications rather than simple webhook handlers.</p><p>The API Gateway with Lambda functions approach provides the best balance of simplicity, scalability, and low operational overhead for this serverless migration.</p><p>Sources</p><p>AWS Lambda function URLs - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/choosing-the-right-aws-service-for-your-microservice-endpoints/function-urls.html）</p><p>Community | EC2 cost optimization:15 ways to save on Amazon EC2 （https://community.aws/content/2hQNGTh6vpwOHTgKELzL8mkqmKb/ec2-cost-optimization-15-ways-to-save-on-amazon-ec2）</p><p>Lambda Function URL Invoke with ALB | AWS re:Post （https://repost.aws/questions/QUYKEaKLPIQ2ui-DkJjHT-kQ/lambda-function-url-invoke-with-alb）</p><p>Least operational overhead to handle monolithic app | AWS re:Post （https://repost.aws/questions/QUuoRkJsZURTe6xML-FfpFNQ/least-operational-overhead-to-handle-monolithic-app）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "41e60e8b88a142e291ccaa5910e44843",
      "questionNumber": 46,
      "type": "single",
      "content": "<p>Question #46<br>A company is planning to migrate 1,000 on-premises servers to AWS. The servers run on several VMware clusters in the company’s data center. As part of the migration plan, the company wants to gather server metrics such as CPU details, RAM usage, operating system information, and running processes. The company then wants to query and analyze the data.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Deploy and configure the AWS Agentless Discovery Connector virtual appliance on the on-premises hosts. Configure Data Exploration in AWS Migration Hub. Use AWS Glue to perform an ETL job against the data. Query the data by using Amazon S3 Select."
        },
        {
          "label": "B",
          "content": "Export only the VM performance information from the on-premises hosts. Directly import the required data into AWS Migration Hub. Update any missing information in Migration Hub. Query the data by using Amazon QuickSight."
        },
        {
          "label": "C",
          "content": "Create a script to automatically gather the server information from the on-premises hosts. Use the AWS CLI to run the put-resource-attributes command to store the detailed server data in AWS Migration Hub. Query the data directly in the Migration Hub console."
        },
        {
          "label": "D",
          "content": "Deploy the AWS Application Discovery Agent to each on-premises server. Configure Data Exploration in AWS Migration Hub. Use Amazon Athena to run predefined queries against the data in Amazon S3."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. Deploy the AWS Application Discovery Agent to each on-premises server. Configure Data Exploration in AWS Migration Hub. Use Amazon Athena to run predefined queries against the data in Amazon S3.</p><p>This solution is the most appropriate for gathering detailed server metrics and analyzing the data for a large-scale migration because:</p><p>The AWS Application Discovery Agent provides comprehensive data collection capabilities, gathering detailed metrics about CPU, RAM usage, operating system information, and running processes directly from each server. This level of detail is essential for proper migration planning for 1,000 servers.</p><p>The agent continuously collects data over time, allowing the company to understand usage patterns and performance characteristics, which is crucial for right-sizing resources in AWS.</p><p>AWS Migration Hub's Data Exploration feature stores the collected data in Amazon S3, creating a centralized repository of all server information.</p><p>Amazon Athena provides powerful SQL-based querying capabilities against the data stored in S3, allowing the company to run complex analyses, filter data, and generate reports without having to set up additional infrastructure.</p><p>Option B is incorrect because:</p><p>Exporting only VM performance information would miss critical details like operating system information and running processes</p><p>Directly importing data into Migration Hub without using the Discovery Agent would require manual updates for missing information, which is impractical for 1,000 servers</p><p>This approach would not provide the comprehensive data collection and analysis capabilities needed for a migration of this scale</p><p>The AWS Application Discovery Agent with Data Exploration in Migration Hub and Amazon Athena provides the most complete solution for gathering, storing, and analyzing server metrics for a large-scale migration to AWS.</p><p>Sources</p><p>Discover on-premises resources using AWS Migration Hub discovery tools - AWS Migration Hub （https://docs.aws.amazon.com/migrationhub/latest/ug/gs-new-user-discovery.html）</p><p>What is AWS Application Discovery Service? - AWS Application Discovery Service （https://docs.aws.amazon.com/application-discovery/latest/userguide/what-is-appdiscovery.html）</p><p>Assess - Migration Lens （https://docs.aws.amazon.com/wellarchitected/latest/migration-lens/assess-cost.html）</p><p>Welcome - AWS Application Discovery Service （https://docs.aws.amazon.com/application-discovery/latest/APIReference/Welcome.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "93d846207cae47aa95659d4a5e467024",
      "questionNumber": 47,
      "type": "single",
      "content": "<p>Question #47</p><p>A company is building a serverless application that runs on an AWS Lambda function that is attached to a VPC. The company needs to integrate the application with a new service from an external provider. The external provider supports only requests that come from public IPv4 addresses that are in an allow list.</p><p><br></p><p>The company must provide a single public IP address to the external provider before the application can start using the new service.</p><p><br></p><p>Which solution will give the application the ability to access the new service?</p>",
      "options": [
        {
          "label": "A",
          "content": "Deploy a NAT gateway. Associate an Elastic IP address with the NAT gateway. Configure the VPC to use the NAT gateway."
        },
        {
          "label": "B",
          "content": "Deploy an egress-only internet gateway. Associate an Elastic IP address with the egress-only internet gateway. Configure the elastic network interface on the Lambda function to use the egress-only internet gateway."
        },
        {
          "label": "C",
          "content": "Deploy an internet gateway. Associate an Elastic IP address with the internet gateway. Configure the Lambda function to use the internet gateway."
        },
        {
          "label": "D",
          "content": "Deploy an internet gateway. Associate an Elastic IP address with the internet gateway. Configure the default route in the public VPC route table to use the internet gateway."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Deploy a NAT gateway. Associate an Elastic IP address with the NAT gateway. Configure the VPC to use the NAT gateway.</p><p>This solution is the most appropriate for enabling a Lambda function attached to a VPC to access an external service that requires a whitelisted public IP address because:</p><p>NAT Gateway with an Elastic IP: This setup allows Lambda functions in private subnets to initiate outbound connections to the internet through a consistent, static public IP address (the Elastic IP associated with the NAT gateway). This public IP can be provided to the external provider for their allow list.</p><p>Security Best Practices: This approach follows AWS security best practices by keeping the Lambda function in a private subnet while still allowing it to make outbound connections. The Lambda function itself remains protected from inbound internet traffic.</p><p>Proper VPC Architecture: The NAT gateway is designed specifically for this use case - allowing resources in private subnets to access the internet while maintaining a consistent source IP address for outbound traffic.</p><p>Option C (using an internet gateway with an Elastic IP) is incorrect because:</p><p>Internet gateways don't have Elastic IP addresses directly associated with them</p><p>Internet gateways provide bidirectional internet connectivity, which could expose the Lambda function to inbound traffic</p><p>Lambda functions cannot directly use internet gateways - they would need to be placed in public subnets with public IP addresses, which is not a recommended security practice</p><p>The NAT gateway solution provides the single, consistent public IP address required by the external provider while maintaining proper security architecture for the serverless application.</p><p>Sources</p><p>NAT Gateway for outbound request | AWS re:Post （https://repost.aws/questions/QU9RYFVH0ARXWaNvUt9LB2-w/nat-gateway-for-outbound-request）</p><p>Troubleshoot networking issues in Lambda - AWS Lambda （https://docs.aws.amazon.com/lambda/latest/dg/troubleshooting-networking.html）</p><p>Lambda in RDS VPC needs bi-directional inet access | AWS re:Post （https://repost.aws/questions/QUcNlaEOLcTkqL2lD2RW_WbA/lambda-in-rds-vpc-needs-bi-directional-inet-access）</p><p>Leverage VPC endpoints - Serverless Applications Lens （https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/leverage-vpc-endpoints.html）</p><p>InternalError when associating EIP with Lambda ENI | AWS re:Post （https://repost.aws/questions/QUiBy0C0gUQYSjrq4HbFrNQQ/internalerror-when-associating-eip-with-lambda-eni）</p><p>how to provide static ip address for external system to whitelist my ec2 in private subnet ip address in aws? | AWS re:Post （https://repost.aws/questions/QUwBqnAslLTU-TSzztE8_O5A/how-to-provide-static-ip-address-for-external-system-to-whitelist-my-ec2-in-private-subnet-ip-address-in-aws）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "34b1f108a9764e228bc63a206ed53dc4",
      "questionNumber": 48,
      "type": "multiple",
      "content": "<p>Question #48</p><p>A solutions architect has developed a web application that uses an Amazon API Gateway Regional endpoint and an AWS Lambda function. The consumers of the web application are all close to the AWS Region where the application will be deployed. The Lambda function only queries an Amazon Aurora MySQL database. The solutions architect has configured the database to have three read replicas.</p><p>During testing, the application does not meet performance requirements. Under high load, the application opens a large number of database connections. The solutions architect must improve the application’s performance.</p><p>Which actions should the solutions architect take to meet these requirements? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Use the cluster endpoint of the Aurora database."
        },
        {
          "label": "B",
          "content": "Use RDS Proxy to set up a connection pool to the reader endpoint of the Aurora database."
        },
        {
          "label": "C",
          "content": "Use the Lambda Provisioned Concurrency feature."
        },
        {
          "label": "D",
          "content": "Move the code for opening the database connection in the Lambda function outside of the event handler."
        },
        {
          "label": "E",
          "content": "Change the API Gateway endpoint to an edge-optimized endpoint."
        }
      ],
      "correctAnswer": "BD",
      "explanation": "<p>Based on the information provided, the two actions that the solutions architect should take to improve the application's performance are:</p><p>B. Use RDS Proxy to set up a connection pool to the reader endpoint of the Aurora database. </p><p>D. Move the code for opening the database connection in the Lambda function outside of the event handler.</p><p>These solutions directly address the performance issues caused by the large number of database connections under high load:</p><p>RDS Proxy (Option B):</p><p>RDS Proxy provides connection pooling for Aurora databases, which efficiently manages and reuses database connections.</p><p>By setting up RDS Proxy to the reader endpoint, read queries can be distributed across the three read replicas while maintaining a controlled number of actual database connections.</p><p>This significantly reduces connection overhead and prevents connection exhaustion, which is critical when the application is under high load.</p><p>RDS Proxy also handles connection multiplexing, allowing multiple Lambda invocations to share a smaller pool of database connections.</p><p>Moving connection code outside the event handler (Option D):</p><p>In Lambda functions, code outside the event handler executes only when a new execution environment is initialized, not on every invocation.</p><p>By moving the database connection code outside the handler, the connection can be reused across multiple invocations of the same Lambda instance.</p><p>This reduces the overhead of repeatedly establishing new database connections for each request, improving response times and reducing the total number of connections.</p><p>Option C (Lambda Provisioned Concurrency) would help reduce cold starts and improve initial response times, but it doesn't directly address the database connection issue. While it might be beneficial for overall application performance, it's not one of the most effective solutions for the specific problem of managing a large number of database connections.</p><p>The combination of RDS Proxy for connection pooling and moving the database connection code outside the Lambda event handler provides the most effective solution for improving the application's performance under high load by efficiently managing database connections.</p><p>Sources</p><p>Community | We Improved Our Lambda Warm Start Speed by 95%! Here’s How （https://community.aws/content/2e2OlZen47W4OziSbu1on004vFI/improved-lambda-warm-start-speed-95）</p><p>RDS Proxy causing a consistent increase in DB Connections from Lambda | AWS re:Post （https://repost.aws/questions/QU2hiJKCLUQcq5ApSro0ixMQ/rds-proxy-causing-a-consistent-increase-in-db-connections-from-lambda）</p><p>Which is preferred, Lambda-&gt;RDS Proxy-&gt;RDS or Lambda-&gt;RDS? | AWS re:Post （https://repost.aws/questions/QU1U72jyYWTcisPaIF3CSSqg/which-is-preferred-lambda-rds-proxy-rds-or-lambda-rds）</p><p>Aurora Read Replica Scale In Connection Issue with RDS Proxy and Lambda | AWS re:Post （https://repost.aws/questions/QUKpsQ6HemSxWm0AjL3BSaDQ/aurora-read-replica-scale-in-connection-issue-with-rds-proxy-and-lambda）</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "cfe889aa3665426aad85a5d326c7cad8",
      "questionNumber": 49,
      "type": "single",
      "content": "<p>Question #49</p><p>A company is planning to host a web application on AWS and wants to load balance the traffic across a group of Amazon EC2 instances. One of the security requirements is to enable end-to-end encryption in transit between the client and the web server.</p><p><br></p><p>Which solution will meet this requirement?</p>",
      "options": [
        {
          "label": "A",
          "content": "Place the EC2 instances behind an Application Load Balancer (ALB). Provision an SSL certificate using AWS Certificate Manager (ACM), and associate the SSL certificate with the ALB. Export the SSL certificate and install it on each EC2 instance. Configure the ALB to listen on port 443 and to forward traffic to port 443 on the instances."
        },
        {
          "label": "B",
          "content": "Associate the EC2 instances with a target group. Provision an SSL certificate using AWS Certificate Manager (ACM). Create an Amazon CloudFront distribution and configure it to use the SSL certificate. Set CloudFront to use the target group as the origin server."
        },
        {
          "label": "C",
          "content": "Place the EC2 instances behind an Application Load Balancer (ALB). Provision an SSL certificate using AWS Certificate Manager (ACM), and associate the SSL certificate with the ALB. Provision a third-party SSL certificate and install it on each EC2 instance. Configure the ALB to listen on port 443 and to forward traffic to port 443 on the instances."
        },
        {
          "label": "D",
          "content": "Place the EC2 instances behind a Network Load Balancer (NLB). Provision a third-party SSL certificate and install it on the NLB and on each EC2 instance. Configure the NLB to listen on port 443 and to forward traffic to port 443 on the instances."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the information provided, the correct answer for implementing end-to-end encryption in transit between the client and the web server is:</p><p>C. Place the EC2 instances behind an Application Load Balancer (ALB). Provision an SSL certificate using AWS Certificate Manager (ACM), and associate the SSL certificate with the ALB. Provision a third-party SSL certificate and install it on each EC2 instance. Configure the ALB to listen on port 443 and to forward traffic to port 443 on the instances.</p><p>This solution provides true end-to-end encryption because:</p><p>The connection between the client and the ALB is encrypted using the ACM certificate associated with the ALB, securing the first part of the communication path.</p><p>The connection between the ALB and the EC2 instances is also encrypted using the third-party SSL certificates installed on each EC2 instance, securing the second part of the communication path.</p><p>By configuring the ALB to listen on port 443 (HTTPS) and forward traffic to port 443 on the instances, the entire communication path remains encrypted without any point where the traffic is unencrypted.</p><p>Option B does not provide end-to-end encryption because:</p><p>While CloudFront would use an SSL certificate to encrypt traffic between the client and CloudFront, the question mentions using a target group as the origin.</p><p>Target groups don't support SSL termination directly - they would typically be used with an ALB.</p><p>The solution doesn't specify how the traffic would be encrypted between CloudFront and the EC2 instances.</p><p>True end-to-end encryption requires that the data remains encrypted throughout its entire journey from the client to the web server. Only option C ensures that both segments of the connection (client to ALB and ALB to EC2) are encrypted, meeting the security requirement for end-to-end encryption in transit.</p><p>Sources</p><p>Encryption-in-transit for public sector workloads with AWS Nitro Enclaves and AWS Certificate Manager | AWS Public Sector Blog （https://aws.amazon.com/cn/blogs/publicsector/encryption-in-transit-public-sector-workloads-aws-nitro-enclaves-aws-certificate-manager/）</p><p>SEC09-BP02 Enforce encryption in transit - Security Pillar （https://docs.aws.amazon.com/wellarchitected/latest/security-pillar/sec_protect_data_transit_encrypt.html）</p><p>SEC09-BP02 Enforce encryption in transit - AWS Well-Architected Framework (2023-04-10) （https://docs.aws.amazon.com/wellarchitected/2023-04-10/framework/sec_protect_data_transit_encrypt.html）</p><p>Activate end-to-end HTTPS connectivity with AWS PrivateLink | AWS re:Post （https://repost.aws/knowledge-center/privatelink-https-connectivity）</p><p>Configuring HTTPS for your Elastic Beanstalk environment - AWS Elastic Beanstalk （https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "d46a100f70ae4044ae4673ea5c65c5b9",
      "questionNumber": 50,
      "type": "single",
      "content": "<p>Question #50</p><p>A company wants to migrate its data analytics environment from on premises to AWS. The environment consists of two simple Node.js applications. One of the applications collects sensor data and loads it into a MySQL database. The other application aggregates the data into reports. When the aggregation jobs run, some of the load jobs fail to run correctly. </p><p><br></p><p>The company must resolve the data loading issue. The company also needs the migration to occur without interruptions or changes for the company’s customers. </p><p><br></p><p>What should a solutions architect do to meet these requirements? </p>",
      "options": [
        {
          "label": "A",
          "content": "Set up an Amazon Aurora MySQL database as a replication target for the on-premises database. Create an Aurora Replica for the Aurora MySQL database, and move the aggregation jobs to run against the Aurora Replica. Set up collection endpoints as AWS Lambda functions behind a Network Load Balancer (NLB), and use Amazon RDS Proxy to write to the Aurora MySQL database. When the databases are synced, disable the replication job and restart the Aurora Replica as the primary instance. Point the collector DNS record to the NLB."
        },
        {
          "label": "B",
          "content": "Set up an Amazon Aurora MySQL database. Use AWS Database Migration Service (AWS DMS) to perform continuous data replication from the on-premises database to Aurora. Move the aggregation jobs to run against the Aurora MySQL database. Set up collection endpoints behind an Application Load Balancer (ALB) as Amazon EC2 instances in an Auto Scaling group. When the databases are synced, point the collector DNS record to the ALDisable the AWS DMS sync task after the cutover from on premises to AWS.&nbsp;"
        },
        {
          "label": "C",
          "content": "Set up an Amazon Aurora MySQL database. Use AWS Database Migration Service (AWS DMS) to perform continuous data replication from the on-premises database to Aurora. Create an Aurora Replica for the Aurora MySQL database, and move the aggregation jobs to run against the Aurora Replica. Set up collection endpoints as AWS Lambda functions behind an Application Load Balancer (ALB), and use Amazon RDS Proxy to write to the Aurora MySQL database. When the databases are synced, point the collector DNS record to the ALB. Disable the AWS DMS sync task after the cutover from on premises to AWS."
        },
        {
          "label": "D",
          "content": "Set up an Amazon Aurora MySQL database. Create an Aurora Replica for the Aurora MySQL database, and move the aggregation jobs to run against the Aurora Replica. Set up collection endpoints as an Amazon Kinesis data stream. Use Amazon Kinesis Data Firehose to replicate the data to the Aurora MySQL database. When the databases are synced, disable the replication job and restart the Aurora Replica as the primary instance. Point the collector DNS record to the Kinesis data stream."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>The correct answer isC. Here's why:</p><p> Key Requirements:</p><p>1.Migrate without interruptions or changes for customers: This requires a seamless cutover with minimal downtime.</p><p>2.Resolve data loading issues: The aggregation jobs are causing load jobs to fail, so separating the workloads (aggregation vs. data collection) is critical.</p><p>3.Use AWS services effectively: The solution should leverage AWS managed services for scalability, reliability, and cost-efficiency.</p><p> Why Option C is Correct:</p><p>-AWS DMS for continuous replication: Ensures the on-premises MySQL database is synced with Aurora MySQL before cutover.</p><p>-Aurora Replica for aggregation jobs: Offloads the aggregation workload to a read replica, preventing interference with the primary database's write performance (resolving the load job failures).</p><p>-Lambda + ALB for collection endpoints: </p><p> &nbsp;- Lambda is serverless and scales automatically for the data collection application.</p><p> &nbsp;- ALB routes traffic to Lambda, providing a single endpoint for customers.</p><p>-RDS Proxy: Manages database connections efficiently, preventing connection pool issues for Lambda functions.</p><p>-Cutover process: </p><p> &nbsp;- After syncing, the DNS record is updated to point to the ALB (minimal downtime).</p><p> &nbsp;- AWS DMS is disabled post-cutover.</p><p> Why Other Options Are Incorrect:</p><p>-A: Uses a Network Load Balancer (NLB) instead of ALB, which is less optimal for HTTP-based Node.js apps. Also, restarting the replica as primary is unnecessary with Aurora's failover capabilities.</p><p>-B: Uses EC2 instances instead of Lambda, which adds unnecessary management overhead and doesn’t fully leverage serverless scalability.</p><p>-D: Uses Kinesis for data collection, which is overkill for this use case (simple sensor data loading) and doesn’t address the aggregation job issue as cleanly as an Aurora Replica.</p><p> Summary:</p><p>OptionC provides the most seamless migration, resolves the performance issue by separating read/write workloads, and uses the most appropriate AWS services (Lambda, ALB, RDS Proxy, and Aurora Replica). </p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "539073fe5d6443d183e105d5c064f618",
      "questionNumber": 51,
      "type": "single",
      "content": "Question #51<p>A health insurance company stores personally identifiable information (PII) in an Amazon S3 bucket. The company uses server-side encryption with S3 managed encryption keys (SSE-S3) to encrypt the objects. According to a new requirement, all current and future objects in the S3 bucket must be encrypted by keys that the company’s security team manages. The S3 bucket does not have versioning enabled.<br><br>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "In the S3 bucket properties, change the default encryption to SSE-S3 with a customer managed key. Use the AWS CLI to re-upload all objects in the S3 bucket. Set an S3 bucket policy to deny unencrypted PutObject requests."
        },
        {
          "label": "B",
          "content": "In the S3 bucket properties, change the default encryption to server-side encryption with AWS KMS managed encryption keys (SSE-KMS). Set an S3 bucket policy to deny unencrypted PutObject requests. Use the AWS CLI to re-upload all objects in the S3 bucket."
        },
        {
          "label": "C",
          "content": "In the S3 bucket properties, change the default encryption to server-side encryption with AWS KMS managed encryption keys (SSE-KMS). Set an S3 bucket policy to automatically encrypt objects on GetObject and PutObject requests."
        },
        {
          "label": "D",
          "content": "In the S3 bucket properties, change the default encryption to AES-256 with a customer managed key. Attach a policy to deny unencrypted PutObject requests to any entities that access the S3 bucket. Use the AWS CLI to re-upload all objects in the S3 bucket."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the given question and requirements, the correct solution is:</p><p>B. In the S3 bucket properties, change the default encryption to server-side encryption with AWS KMS managed encryption keys (SSE-KMS). Set an S3 bucket policy to deny unencrypted PutObject requests. Use the AWS CLI to re-upload all objects in the S3 bucket. [[1]](https://repost.aws/questions/QUa2uGgWvVTeCAkp6vKHBmdw/how-to-encrypt-your-object-in-s3)</p><p>This solution meets the requirements for the following reasons:</p><p>1. It changes the encryption method from SSE-S3 to SSE-KMS, allowing the company's security team to manage the encryption keys.</p><p>2. It ensures that all future objects will be encrypted using the new SSE-KMS method by changing the default encryption settings.</p><p>3. The S3 bucket policy that denies unencrypted PutObject requests adds an extra layer of security, preventing any unencrypted objects from being uploaded. [[2]](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingDSSEncryption.html)</p><p>4. Re-uploading all existing objects using the AWS CLI ensures that all current objects in the bucket are also encrypted with the new SSE-KMS method. [[3]](https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html)</p><p>Implementation steps:</p><p>1. Change the default encryption in the S3 bucket properties to SSE-KMS.</p><p>2. Create a customer-managed KMS key for the security team to manage.</p><p>3. Set up an S3 bucket policy to deny unencrypted PutObject requests.</p><p>4. Use the AWS CLI to download and then re-upload all existing objects in the bucket, which will apply the new encryption method.</p><p>It's important to note that this process may take some time depending on the number and size of objects in the bucket. Also, ensure that proper access controls are in place for the KMS key to maintain the principle of least privilege.</p><p>Option C is incorrect because it suggests using GetObject and PutObject requests to automatically encrypt objects, which is not a valid method for changing the encryption of existing objects in S3.</p><p>For the most up-to-date information on S3 encryption and best practices, please refer to the official AWS documentation.</p><p>Sources</p><p>[1] [[1] How to encrypt your object in S3 | AWS re:Post] (https://repost.aws/questions/QUa2uGgWvVTeCAkp6vKHBmdw/how-to-encrypt-your-object-in-s3)</p><p>[3] [[2] Using dual-layer server-side encryption with AWS KMS keys (DSSE-KMS) - Amazon Simple Storage Service] (https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingDSSEncryption.html)</p><p>[6] [[3] Protecting data with server-side encryption - Amazon Simple Storage Service] (https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html)</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "22cae5cb4b2c474898211cfccd215a77",
      "questionNumber": 52,
      "type": "single",
      "content": "<p>Question #52</p><p>A company is running a web application in the AWS Cloud. The application consists of dynamic content that is created on a set of Amazon EC2 instances. The EC2 instances run in an Auto Scaling group that is confi gured as a target group for an Application Load Balancer (ALB). </p><p><br></p><p>The company is using an Amazon CloudFront distribution to distribute the application globally. The CloudFront distribution uses the ALB as an origin. The company uses Amazon Route 53 for DNS and has created an A record of www.example.com for the CloudFront distribution. </p><p><br></p><p>A solutions architect must confi gure the application so that itis highly available and fault tolerant. </p><p><br></p><p>Which solution meets these requirements? </p>",
      "options": [
        {
          "label": "A",
          "content": "Provision a full, secondary application deployment in a different AWS Region. Update the Route 53 A record to be a failover record. Add both of the CloudFront distributions as values. Create Route 53 health checks."
        },
        {
          "label": "B",
          "content": "Provision an ALB, an Auto Scaling group, and EC2 instances in a different AWS Region. Update the CloudFront distribution, and create a second origin for the new ALB. Configure one origin as primary and one origin as secondary."
        },
        {
          "label": "C",
          "content": "Provision an Auto Scaling group and EC2 instances in a different AWS Region. Create a second target for the new Auto Scaling group in the ALB. Set up the failover routing algorithm on the ALB."
        },
        {
          "label": "D",
          "content": "Provision a full, secondary application deployment in a different AWS Region. Create a second CloudFront distribution, and add the new application setup as an origin. Create an AWS Global Accelerator accelerator. Add both of the CloudFront distributions as endpoints."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>The correct answer isB. &nbsp;</p><p>Explanation: &nbsp;</p><p>The question requires ahighly available and fault-tolerant solution for a web application using CloudFront with an ALB as the origin. The best approach is toset up a secondary origin in another AWS Region and configure CloudFront to failover to it if the primary origin is unhealthy. &nbsp;</p><p>-Option B suggests: &nbsp;</p><p> &nbsp;- Deploying a secondary ALB, Auto Scaling group, and EC2 instances in a different AWS Region. &nbsp;</p><p> &nbsp;- Updating the CloudFront distribution to include thesecond origin. &nbsp;</p><p> &nbsp;- Creating anorigin group (a feature that allows CloudFront to failover to a secondary origin if the primary fails). &nbsp;</p><p> &nbsp;- This ensures that if the primary Region fails, CloudFront automatically routes traffic to the secondary Region. &nbsp;</p><p>Why not the other options? &nbsp;</p><p>-Option A: Uses Route 53 failover with two CloudFront distributions, but this isnot optimal because CloudFront already has built-in origin failover capabilities. Also, Route 53 failover would require DNS propagation time, whereas CloudFront origin failover is faster. &nbsp;</p><p>-Option C: Suggests adding a second target group to thesame ALB, but this does not providecross-Region redundancy (ALB is Region-bound). &nbsp;</p><p>-Option D: Uses AWS Global Accelerator, which isunnecessary since CloudFront already provides global distribution. Also, it adds complexity without significant benefits over CloudFront's native failover. &nbsp;</p><p>Conclusion: &nbsp;</p><p>Option B is the best solution because it leverages CloudFront'sorigin failover capability, ensuring high availability and fault tolerance without relying on slower DNS-based failover. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "569caf69f8dc424597777119cb44e6ef",
      "questionNumber": 53,
      "type": "single",
      "content": "<p>Question #53</p><p>A company has an organization in AWS Organizations that has a large number of AWS accounts. One of the AWS accounts is designated as a transit account and has a transit gateway that is shared with all of the other AWS accounts. AWS Site-to-Site VPN connections are configured between all of the company’s global offices and the transit account. </p><p><br></p><p>The company has AWS Config enabled on all of its accounts. The company’s networking team needs to centrally manage a list of internal IP address ranges that belong to the global offices. Developers will reference this list to gain access to their applications securely.<br><br>Which solution meets these requirements with the LEAST amount of operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a JSON file that is hosted in Amazon S3 and that lists all of the internal IP address ranges. Configure an Amazon Simple Notification Service (Amazon SNS) topic in each of the accounts that can be invoked when the JSON file is updated. Subscribe an AWS Lambda function to the SNS topic to update all relevant security group rules with the updated IP address ranges."
        },
        {
          "label": "B",
          "content": "Create a new AWS Config managed rule that contains all of the internal IP address ranges. Use the rule to check the security groups in each of the accounts to ensure compliance with the list of IP address ranges. Configure the rule to automatically remediate any noncompliant security group that is detected."
        },
        {
          "label": "C",
          "content": "In the transit account, create a VPC prefix list with all of the internal IP address ranges. Use AWS Resource Access Manager to share the prefix list with all of the other accounts. Use the shared prefix list to configure security group rules in the other accounts."
        },
        {
          "label": "D",
          "content": "In the transit account, create a security group with all of the internal IP address ranges. Configure the security groups in the other accounts to reference the transit account&rsquo;s security group by using a nested security group reference of &ldquo;/sg-1a2b3c4d&rdquo;."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>The correct answer isC. &nbsp;</p><p>Explanation: &nbsp;</p><p>The requirement is tocentrally manage a list of internal IP address ranges with theleast operational overhead, allowing developers to reference this list in security group rules. &nbsp;</p><p>-Option A involves manual updates to an S3 file, SNS notifications, and Lambda functions to update security groups. This introduces complexity and operational overhead. &nbsp;</p><p>-Option B uses AWS Config rules for compliance checks and remediation, but this is reactive (detects and fixes misconfigurations rather than centrally managing the list). &nbsp;</p><p>-Option C usesVPC Prefix Lists (a native AWS feature) to store IP ranges and shares them across accounts usingAWS Resource Access Manager (RAM). This allows security groups in any account to reference the centrally managed prefix list, reducing overhead. &nbsp;</p><p>-Option D suggests referencing a security group from another account, butsecurity groups cannot be shared across accounts in this way (nested security group references only work within the same VPC). &nbsp;</p><p>Why C is the Best Choice: &nbsp;</p><p>✅Centralized management (single prefix list in the transit account). &nbsp;</p><p>✅Least operational overhead (shared via RAM, automatically available in all accounts). &nbsp;</p><p>✅Directly usable in security group rules (no need for custom automation). &nbsp;</p><p>Thus,C is the correct answer.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "4d8df9a48df6403585b7cdf23c4ea9bc",
      "questionNumber": 54,
      "type": "single",
      "content": "<p>Question #54</p><p>A company runs a new application as a static website in Amazon S3. The company has deployed the application toa production AWS account and uses Amazon CloudFront to deliver the website. The website calls an Amazon API Gateway REST API. An AWS Lambda function backs each API method. </p><p><br></p><p>The company wants to create a CSV report every 2 weeks to show each API Lambda function’s recommended configured memory, recommended cost, and the price difference between current configurations and the recommendations. The company will store the reports in an S3 bucket.<br><br>Which solution will meet these requirements with the LEAST development time? </p>",
      "options": [
        {
          "label": "A",
          "content": "Create a Lambda function that extracts metrics data for each API Lambda function from Amazon CloudWatch Logs for the 2-week period. Collate the data into tabular format. Store the data as a .csv file in an S3 bucket. Create an Amazon EventBridge rule to schedule the Lambda function to run every 2 weeks."
        },
        {
          "label": "B",
          "content": "&nbsp;Opt in to AWS Compute Optimizer. Create a Lambda function that calls the ExportLambdaFunctionRecommendations operation. Export the .csv file to an S3 bucket. Create an Amazon EventBridge rule to schedule the Lambda function to run every 2 weeks."
        },
        {
          "label": "C",
          "content": "Opt in to AWS Compute Optimizer. Set up enhanced infrastructure metrics. Within the Compute Optimizer console, schedule a job to export the Lambda recommendations to a .csv file. Store the file in an S3 bucket every 2 weeks."
        },
        {
          "label": "D",
          "content": "Purchase the AWS Business Support plan for the production account. Opt in to AWS Compute Optimizer for AWS Trusted Advisor checks. In the Trusted Advisor console, schedule a job to export the cost optimization checks to a .csv file. Store the file in an S3 bucket every 2 weeks."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>The correct answer isB. &nbsp;</p><p> Explanation: &nbsp;</p><p>The requirements are to generate a CSV report every 2 weeks showing: &nbsp;</p><p>1. Recommended memory configurations for API Lambda functions &nbsp;</p><p>2. Recommended cost &nbsp;</p><p>3. Price difference between current and recommended configurations &nbsp;</p><p>AWS Compute Optimizer is the best service for this task because it provides Lambda function memory recommendations based on historical usage. &nbsp;</p><p># WhyB is the best option: &nbsp;</p><p>-Opt in to AWS Compute Optimizer (required to get recommendations). &nbsp;</p><p>-ExportLambdaFunctionRecommendations API call retrieves the needed data in CSV format. &nbsp;</p><p>-Lambda function + EventBridge rule automates the report generation every 2 weeks. &nbsp;</p><p>-Least development effort (just calling an API and storing the result, no manual data processing). &nbsp;</p><p># Why the other options are incorrect: &nbsp;</p><p>-A: Requires manually extracting and processing CloudWatch Logs (high development effort). &nbsp;</p><p>-C: Compute Optimizer does not natively support scheduled CSV exports (must use API). &nbsp;</p><p>-D: AWS Business Support and Trusted Advisor do not provide Lambda memory recommendations in this way. &nbsp;</p><p>Thus,B meets the requirements with the least development time. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "b7a7a0e84c214a55b4a0a3948ed38fc8",
      "questionNumber": 55,
      "type": "multiple",
      "content": "<p>Question #55</p><p>A company’s factory and automation applications are running in a single VPC. More than 20 applications run on a combination of Amazon EC2, Amazon Elastic Container Service (Amazon ECS), and Amazon RDS. </p><p><br></p><p>The company has software engineers spread across three teams. One of the three teams owns each application, and each time is responsible for the cost and performance of all of its applications. Team resources have tags that represent their application and team. The teams use IAM access for daily activities. The company needs to determine which costs on the monthly AWS bill are attributable to each application or team. </p><p><br></p><p>The company also must be able to create reports to compare costs from the last 12 months and to help forecast costs for the next 12 months. A solutions architect must recommend an AWS Billing and Cost Management solution that provides these cost reports.<br><br>Which combination of actions will meet these requirements? (Choose three.) </p>",
      "options": [
        {
          "label": "A",
          "content": "Activate the user-deﬁned cost allocation tags that represent the application and the team."
        },
        {
          "label": "B",
          "content": "Activate the AWS generated cost allocation tags that represent the application and the team."
        },
        {
          "label": "C",
          "content": "Create a cost category for each application in Billing and Cost Management."
        },
        {
          "label": "D",
          "content": "Activate IAM access to Billing and Cost Management."
        },
        {
          "label": "E",
          "content": "Create a cost budget."
        },
        {
          "label": "F",
          "content": "Enable Cost Explorer."
        }
      ],
      "correctAnswer": "ACF",
      "explanation": "<p>The correct combination of actions to meet the requirements is: &nbsp;</p><p>A. Activate the user-defined cost allocation tags that represent the application and the team. &nbsp;</p><p>*(The teams already have tags for applications and teams, so enabling these as cost allocation tags will allow cost tracking by these dimensions.)* &nbsp;</p><p>C. Create a cost category for each application in Billing and Cost Management. &nbsp;</p><p>*(Cost Categories allow grouping costs by business dimensions, such as applications or teams, even if tags are inconsistent.)* &nbsp;</p><p>F. Enable Cost Explorer. &nbsp;</p><p>*(Cost Explorer provides cost visualization, historical trends over 12 months, and forecasting capabilities.)* &nbsp;</p><p> Why not the others? &nbsp;</p><p>-B. AWS-generated tags (like `aws:createdBy`) are not relevant here—the teams already have custom tags. &nbsp;</p><p>-D. IAM access to billing is unrelated to cost reporting. &nbsp;</p><p>-E. Cost budgets are for setting spending alerts, not historical analysis or forecasting. &nbsp;</p><p> Final Answer: &nbsp;A, C, F </p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "93e47a70b83042ff841a5d243b576f0a",
      "questionNumber": 56,
      "type": "single",
      "content": "<p>Question #56</p><p>An AWS customer has a web application that runs on premises. The web application fetches data from a third-party API that is behind a firewall. The third party accepts only one public CIDR block in each client’s allow list. </p><p><br></p><p>The customer wants to migrate their web application to the AWS Cloud. The application will be hosted on a set of Amazon EC2 instances behind an Application Load Balancer (ALB) in a VPC. The ALB is located in public subnets. The EC2 instances are located in private subnets. NAT gateways provide internet access to the private subnets. </p><p><br></p><p>How should a solutions architect ensure that the web application can continue to call the third-party API after the migration?</p>",
      "options": [
        {
          "label": "A",
          "content": "Associate a block of customer-owned public IP addresses to the VPC. Enable public IP addressing for public subnets in the VPC."
        },
        {
          "label": "B",
          "content": "Register a block of customer-owned public IP addresses in the AWS account. Create Elastic IP addresses from the address block and assign them to the NAT gateways in the VPC."
        },
        {
          "label": "C",
          "content": "Create Elastic IP addresses from the block of customer-owned IP addresses. Assign the static Elastic IP addresses to the ALB."
        },
        {
          "label": "D",
          "content": "Register a block of customer-owned public IP addresses in the AWS account. Set up AWS Global Accelerator to use Elastic IP addresses from the address block. Set the ALB as the accelerator endpoint."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>The correct answer is B. Register a block of customer-owned public IP addresses in the AWS account. Create Elastic IP addresses from the address block and assign them to the NAT gateways in the VPC.</p><p> Explanation:</p><p>1.Problem Context: &nbsp;</p><p> &nbsp; - The third-party API only accepts requests from a singlepublic CIDR block (a fixed set of IP addresses).</p><p> &nbsp; - The web application runs on EC2 instances inprivate subnets, which access the internet viaNAT gateways.</p><p> &nbsp; - The NAT gateways provide outbound internet connectivity for the EC2 instances, but by default, they useAWS-owned public IPs, which are not fixed or controllable by the customer.</p><p>2.Why Option B is Correct: &nbsp;</p><p> &nbsp; - Bybringing your own public IP addresses (BYOIP) and assigning them asElastic IPs (EIPs) to the NAT gateways, all outbound traffic from the EC2 instances will use these fixed IPs.</p><p> &nbsp; - The third-party API can then whitelist this CIDR block, ensuring the application can still access the API after migration.</p><p>3.Why Other Options Are Incorrect: &nbsp;</p><p> &nbsp; -A: Enabling public IPs for public subnets affects instances in those subnets, not the private subnet EC2 instances calling the API. &nbsp;</p><p> &nbsp; -C: ALBs do not support static Elastic IP assignments (they use AWS-managed IPs). &nbsp;</p><p> &nbsp; -D: Global Accelerator provides fixed IPs for incoming traffic, not outbound traffic from EC2 instances.</p><p> Conclusion: &nbsp;</p><p>Assigning customer-owned Elastic IPs to NAT gateways ensures that all outbound API calls from the EC2 instances originate from a known, whitelisted IP range. &nbsp;</p><p>Final Answer: B</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "37c29823a15a4b0e96367b51e63f5867",
      "questionNumber": 57,
      "type": "single",
      "content": "<p>Question #57</p><p>A company with several AWS accounts is using AWS Organizations and service control policies (SCPs). An administrator created the following SCP and has attached it to an organizational unit (OU) that contains AWS account 1111-1111-1111: <br><img src=\"images/sap/cdd378217987faf9b7fcf4c0a6b4425ee1465a70.png\" alt=\"84e247f2b716db7432e6c7e949e6e496.png\" data-href=\"\" style=\"\"/></p><p>Developers working in account 1111-1111-1111 complain that they cannot create Amazon S3 buckets. How should the administrator address this problem?</p>",
      "options": [
        {
          "label": "A",
          "content": "Add s3:CreateBucket with &ldquo;Allow&rdquo; effect to the SCP."
        },
        {
          "label": "B",
          "content": "Remove the account from the OU, and attach the SCP directly to account 1111-1111-1111."
        },
        {
          "label": "C",
          "content": "Instruct the developers to add Amazon S3 permissions to their IAM entities."
        },
        {
          "label": "D",
          "content": "Remove the SCP from account 1111-1111-1111."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Instruct the developers to add Amazon S3 permissions to their IAM entities.</p><p>This is the correct solution because:</p><p>Service Control Policies (SCPs) in AWS Organizations act as permission boundaries or guardrails that limit the maximum permissions available in an account, but they do not grant any permissions themselves.</p><p>Even though the SCP content wasn't provided in the question, we can determine that the issue is not with the SCP blocking S3 bucket creation. If that were the case, adding permissions to IAM entities wouldn't help, as SCPs override IAM permissions when they explicitly deny actions.</p><p>The developers are likely missing the necessary IAM permissions to create S3 buckets. SCPs only define the maximum available permissions, but users still need to have those permissions granted through IAM policies attached to their roles, users, or groups.</p><p>Option A (adding s3:CreateBucket with \"Allow\" effect to the SCP) is incorrect because SCPs don't grant permissions; they only restrict them. Adding an allow statement to an SCP doesn't grant permissions to users.</p><p>Option B (removing the account from the OU and attaching the SCP directly) wouldn't solve the permission issue, as it would just change where the SCP is attached without addressing the underlying IAM permission problem.</p><p>Option D (removing the SCP from the account) is incorrect because the problem isn't that the SCP is blocking the action (if it were, the solution would be to modify the SCP, not remove it entirely).</p><p>The correct approach is to ensure that the developers have the necessary IAM permissions (s3:CreateBucket) attached to their IAM roles, users, or groups, which is what option C suggests.</p><p>Sources</p><p>Enforcing enterprise-wide preventive controls with AWS Organizations | AWS Cloud Operations Blog （https://aws.amazon.com/cn/blogs/mt/enforcing-enterprise-wide-preventive-controls-with-aws-organizations/）</p><p>Service control policies (SCPs) - AWS Organizations （https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html）</p><p>Policies and permissions in AWS Identity and Access Management - AWS Identity and Access Management（https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html） </p><p>When to use AWS Organizations - AWS Account Management （https://docs.aws.amazon.com/accounts/latest/reference/using-orgs.html）</p><p>I cant access all my private repositories in my ECR | AWS re:Post （https://repost.aws/questions/QUtkQFbO_1R4uiXcvDXAFvOg/i-cant-access-all-my-private-repositories-in-my-ecr）</p><p>Troubleshoot SCPs explicit deny errors in AWS Organizations | AWS re:Post（https://repost.aws/knowledge-center/iam-organizations-scp-deny-error） </p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "0704df4a71f944348ad28381667c629c",
      "questionNumber": 58,
      "type": "single",
      "content": "<p>Question #58</p><p>A company has a monolithic application that is critical to the company’s business. The company hosts the application on an Amazon EC2 instance that runs Amazon Linux 2. The company’s application team receives a directive from the legal department to back up the data from the instance’s encrypted Amazon Elastic Block Store (Amazon EBS) volume to an Amazon S3 bucket. The application team does not have the administrative SSH key pair for the instance. The application must continue to serve the users.<br><br>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Attach a role to the instance with permission to write to Amazon S3. Use the AWS Systems Manager Session Manager option to gain access to the instance and run commands to copy data into Amazon S3."
        },
        {
          "label": "B",
          "content": "Create an image of the instance with the reboot option turned on. Launch a new EC2 instance from the image. Attach a role to the new instance with permission to write to Amazon S3. Run a command to copy data into Amazon S3."
        },
        {
          "label": "C",
          "content": "Take a snapshot of the EBS volume by using Amazon Data Lifecycle Manager (Amazon DLM). Copy the data to Amazon S3."
        },
        {
          "label": "D",
          "content": "Create an image of the instance. Launch a new EC2 instance from the image. Attach a role to the new instance with permission to write to Amazon S3. Run a command to copy data into Amazon S3."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p><span style=\"color: rgb(31, 35, 41); background-color: rgb(255, 255, 255); font-size: 16px;\">Core Requirements of the Question Need to back up data from the encrypted Amazon Elastic Block Store (EBS) volume of an EC2 instance to an Amazon S3 bucket, while satisfying two constraints: - No administrative SSH key pair for the instance is available - The application must continue serving users </span></p><p><span style=\"color: rgb(31, 35, 41); background-color: rgb(255, 255, 255); font-size: 16px;\">1. Option A - Logic: Attach an IAM role (with Amazon S3 write permissions) to the EC2 instance. Use AWS Systems Manager Session Manager (which does not require an SSH key, and relies on the SSM agent) to access the instance, then run commands to copy data to Amazon S3. - Reasons for meeting the requirements: - Session Manager enables remote instance access without an SSH key, addressing the \"no administrative SSH key pair\" constraint; - Operations are performed directly on the running instance, so the application service will not be interrupted, satisfying the \"application must continue serving users\" requirement; - After attaching the IAM role, the instance can write data directly to S3, fulfilling the backup objective. </span></p><p><span style=\"color: rgb(31, 35, 41); background-color: rgb(255, 255, 255); font-size: 16px;\">2. Option B - Issue: Enabling the \"reboot option\" when creating the instance image will cause the original instance to reboot, interrupting the application service. This violates the \"application must continue serving users\" requirement. </span></p><p><span style=\"color: rgb(31, 35, 41); background-color: rgb(255, 255, 255); font-size: 16px;\">3. Option C - Issue: Amazon Data Lifecycle Manager (DLM) can only create snapshots of EBS volumes (snapshots are a backup format exclusive to EBS) — it cannot directly copy data from the snapshot to S3. To access snapshot data, the snapshot must first be restored to a new EBS volume and then attached to an EC2 instance. The steps in this option are incomplete, so the backup cannot be completed. </span></p><p><span style=\"color: rgb(31, 35, 41); background-color: rgb(255, 255, 255); font-size: 16px;\">4. Option D - Issue: Creating an instance image and launching a new instance backs up static data as of the image creation time (it cannot sync real-time updated data from the original instance). Additionally, the steps are redundant (requiring an extra instance to be launched), and real-time consistency of the backup data cannot be guaranteed. This option does not meet the requirements.</span></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "3dbb172635a14255bb7602cc308ffa8e",
      "questionNumber": 59,
      "type": "multiple",
      "content": "<p>Question #59</p><p>A solutions architect needs to copy data from an Amazon S3 bucket in one AWS account to a new S3 bucket in a new AWS account. The solutions architect must implement a solution that uses the AWS CLI.<br><br>Which combination of steps will successfully copy the data? (Choose three.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a bucket policy to allow the source bucket to list its contents and to put objects and set object ACLs in the destination bucket. Attach the bucket policy to the destination bucket."
        },
        {
          "label": "B",
          "content": "Create a bucket policy to allow a user in the destination account to list the source bucket&rsquo;s contents and read the source bucket&rsquo;s objects. Attach the bucket policy to the source bucket."
        },
        {
          "label": "C",
          "content": "Create an IAM policy in the source account. Configure the policy to allow a user in the source account to list contents and get objects in the source bucket, and to list contents, put objects, and set object ACLs in the destination bucket. Attach the policy to the user."
        },
        {
          "label": "D",
          "content": "Create an IAM policy in the destination account. Configure the policy to allow a user in the destination account to list contents and get objects in the source bucket, and to list contents, put objects, and set object ACLs in the destination bucket. Attach the policy to the user."
        },
        {
          "label": "E",
          "content": "Run the aws s3 sync command as a user in the source account. Specify the source and destination buckets to copy the data."
        },
        {
          "label": "F",
          "content": "Run the aws s3 sync command as a user in the destination account. Specify the source and destination buckets to copy the data."
        }
      ],
      "correctAnswer": "BDF",
      "explanation": "<p>To successfully copy data from an Amazon S3 bucket in one AWS account to a new S3 bucket in another AWS account using the AWS CLI, the following combination of steps is required:</p><p>Correct Answer: B, D, F &nbsp;</p><p>Explanation: &nbsp;</p><p>1.Step B: &nbsp;</p><p> &nbsp; - Abucket policy must be attached to thesource bucket to allow the destination account's user tolist objects (s3:ListBucket) andread objects (s3:GetObject). &nbsp;</p><p> &nbsp; - This ensures the destination account hasread access to the source bucket. &nbsp;</p><p>2.Step D: &nbsp;</p><p> &nbsp; - AnIAM policy must be created in thedestination account and attached to the user who will run the `aws s3 sync` command. &nbsp;</p><p> &nbsp; - This policy should grant permissions to: &nbsp;</p><p> &nbsp; &nbsp; -Read from the source bucket (`s3:ListBucket`, `s3:GetObject`). &nbsp;</p><p> &nbsp; &nbsp; -Write to the destination bucket (`s3:ListBucket`, `s3:PutObject`, `s3:PutObjectAcl`). &nbsp;</p><p>3.Step F: &nbsp;</p><p> &nbsp; - The `aws s3 sync` command must be runfrom the destination account’s user (the one with the IAM policy from Step D). &nbsp;</p><p> &nbsp; - This ensures the user has the necessary permissions to pull data from the source bucket and write it to the destination bucket. &nbsp;</p><p>Why Not Other Options? &nbsp;</p><p>-A: Incorrect because thebucket policy should be on the source bucket (not destination) to allow cross-account access. &nbsp;</p><p>-C: Incorrect because the IAM policy should be in thedestination account, not the source account. &nbsp;</p><p>-E: Incorrect because the `aws s3 sync` should be run from thedestination account, not the source account. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "897e67b2d21d4141b5a3600e24235fe0",
      "questionNumber": 60,
      "type": "single",
      "content": "<p>Question #60</p><p>A company built an application based on AWS Lambda deployed in an AWS CloudFormation stack. The last production release of the web application introduced an issue that resulted in an outage lasting several minutes. A solutions architect must adjust the deployment process to support a canary release.<br><br>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an alias for every new deployed version of the Lambda function. Use the AWS CLI update-alias command with the routing-conﬁg parameter to distribute the load."
        },
        {
          "label": "B",
          "content": "Deploy the application into a new CloudFormation stack. Use an Amazon Route 53 weighted routing policy to distribute the load."
        },
        {
          "label": "C",
          "content": "Create a version for every new deployed Lambda function. Use the AWS CLI update-function-conﬁguration command with the routing-conﬁg parameter to distribute the load."
        },
        {
          "label": "D",
          "content": "Configure AWS CodeDeploy and use CodeDeployDefault.OneAtATime in the Deployment conﬁguration to distribute the load."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Create an alias for every new deployed version of the Lambda function. Use the AWS CLI update-alias command with the routing-config parameter to distribute the load.</p><p>This solution meets the requirements for implementing a canary release deployment for the Lambda-based application because:</p><p>Lambda aliases with routing configuration provide a built-in way to implement canary deployments for Lambda functions. The routing-config parameter allows you to specify what percentage of traffic should go to the new version versus the old version.</p><p>This approach allows for gradual traffic shifting between Lambda function versions, which is the essence of a canary deployment. You can start by routing a small percentage of traffic to the new version, monitor its performance, and gradually increase the percentage if everything works well.</p><p>If issues are detected with the new version, you can quickly revert all traffic back to the previous stable version by updating the alias routing configuration, minimizing potential downtime.</p><p>This solution doesn't require taking the application offline or creating duplicate infrastructure, ensuring the application continues to serve users during the deployment process.</p><p>The other options are not as suitable:</p><p>Option B (using Route 53 weighted routing with a new CloudFormation stack) would require duplicating the entire infrastructure, which is more complex and resource-intensive than necessary.</p><p>Option C is incorrect because the update-function-configuration command doesn't have a routing-config parameter for traffic distribution.</p><p>Option D (using CodeDeploy with OneAtATime configuration) doesn't implement a true canary deployment pattern. The OneAtATime deployment configuration deploys to one instance at a time, which isn't appropriate for Lambda functions and doesn't provide the gradual traffic shifting needed for canary releases.</p><p>Lambda aliases with routing configuration provide the most straightforward and effective way to implement canary deployments for Lambda functions while minimizing risk and potential downtime.</p><p>Sources</p><p>Deployment approaches - Serverless Applications Lens （https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/deployment-approaches.html）</p><p>Blue/Green deployment | AWS re:Post （https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/deployment-approaches.html）</p><p>Implement Lambda canary deployments using a weighted alias - AWS Lambda （https://docs.aws.amazon.com/lambda/latest/dg/configuring-alias-routing.html）</p><p>AWS CodeDeploy - Overview of Deployment Options on AWS （https://docs.aws.amazon.com/whitepapers/latest/overview-deployment-options/aws-codedeploy.html）</p><p>Iterate rapidly, stay agile - AWS Serverless Multi-Tier Architectures with Amazon API Gateway and AWS Lambda （https://docs.aws.amazon.com/whitepapers/latest/serverless-multi-tier-architectures-api-gateway-lambda/iterate-rapidly-stay-agile.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "091d9177865d41da9b72523dc89fabef",
      "questionNumber": 61,
      "type": "single",
      "content": "<p>Question #61</p><p>A finance company hosts a data lake in Amazon S3. The company receives ﬁnancial data records over SFTP every night from several third parties. The company runs its own SFTP server on an Amazon EC2 instance in a public subnet of a VPC. After the ﬁles are uploaded, they are moved to the data lake by a cron job that runs on the same instance. The SFTP server is reachable on DNS sftp.example.com through the use of Amazon Route 53.<br><br>What should a solutions architect do to improve the reliability and scalability of the SFTP solution?</p>",
      "options": [
        {
          "label": "A",
          "content": "Move the EC2 instance into an Auto Scaling group. Place the EC2 instance behind an Application Load Balancer (ALB). Update the DNS record sftp.example.com in Route 53 to point to the ALB."
        },
        {
          "label": "B",
          "content": "Migrate the SFTP server to AWS Transfer for SFTP. Update the DNS record sftp.example.com in Route 53 to point to the server endpoint hostname."
        },
        {
          "label": "C",
          "content": "Migrate the SFTP server to a ﬁle gateway in AWS Storage Gateway. Update the DNS record sftp.example.com in Route 53 to point to the ﬁle gateway endpoint."
        },
        {
          "label": "D",
          "content": "Place the EC2 instance behind a Network Load Balancer (NLB). Update the DNS record sftp.example.com in Route 53 to point to the NLB."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Migrate the SFTP server to AWS Transfer for SFTP. Update the DNS record sftp.example.com in Route 53 to point to the server endpoint hostname.</p><p>This solution best addresses the requirement to improve reliability and scalability of the SFTP solution because:</p><p>AWS Transfer for SFTP is a fully managed service that automatically handles high availability and scalability without requiring manual intervention. This eliminates the need to manage EC2 instances, patching, and maintenance of the SFTP server software.</p><p>The service automatically scales to accommodate varying loads of file transfers, which directly addresses the scalability requirement mentioned in the question.</p><p>AWS Transfer for SFTP integrates natively with Amazon S3, making it ideal for the company's use case of moving files to their data lake in S3. This eliminates the need for custom cron jobs to transfer the files.</p><p>The service supports custom domain names, allowing the company to continue using their existing sftp.example.com domain by simply updating the DNS record in Route 53 to point to the new Transfer for SFTP endpoint.</p><p>As a managed service, AWS Transfer for SFTP provides better reliability as AWS handles all the underlying infrastructure, redundancy, and availability concerns.</p><p>Option A (using an Auto Scaling group with an Application Load Balancer) is not ideal because:</p><p>SFTP protocol (which runs on TCP port 22) is not natively supported by Application Load Balancers, which are designed for HTTP/HTTPS traffic. ALBs don't support the SFTP protocol.</p><p>This approach would still require the company to manage the SFTP server software, security patches, and configuration across multiple EC2 instances.</p><p>It would require custom scripting to ensure file consistency across multiple instances in the Auto Scaling group.</p><p>The company would still need to maintain the cron jobs for transferring files to S3.</p><p>AWS Transfer for SFTP provides a more comprehensive solution that directly addresses both the reliability and scalability requirements while simplifying the overall architecture.</p><p>Sources</p><p>Transfer data from Snowflake to S3 | AWS re:Post （https://repost.aws/questions/QUAS44xqj_SdKIf0vdRZiNfQ/transfer-data-from-snowflake-to-s3）</p><p>AWS Transfer Family SFTP server vs SFTP connector | AWS re:Post （https://repost.aws/questions/QU_vV7XcunSvmE4TEgWFXSkA/aws-transfer-family-sftp-server-vs-sftp-connector）</p><p>SFTP through AWS LB | AWS re:Post （https://repost.aws/questions/QUbOaWvs92SjWPjTYDKt13dA/sftp-through-aws-lb）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "07b89b0e553f499f8d17bb87d98820d8",
      "questionNumber": 62,
      "type": "single",
      "content": "<p>Question #62</p><p>A company wants to migrate an application to Amazon EC2 from VMware Infrastructure that runs in an on-premises data center. A solutions architect must preserve the software and configuration settings during the migration.<br><br>What should the solutions architect do to meet these requirements? </p>",
      "options": [
        {
          "label": "A",
          "content": "Configure the AWS DataSync agent to start replicating the data store to Amazon FSx for Windows File Server. Use the SMB share to host the VMware data store. Use VM Import/Export to move the VMs to Amazon EC2."
        },
        {
          "label": "B",
          "content": "Use the VMware vSphere client to export the application as an image in Open Virtualization Format (OVF) format. Create an Amazon S3 bucket to store the image in the destination AWS Region. Create and apply an IAM role for VM Import. Use the AWS CLI to run the EC2 import command."
        },
        {
          "label": "C",
          "content": "Configure AWS Storage Gateway for ﬁles service to export a Common Internet File System (CIFS) share. Create a backup copy to the shared folder. Sign in to the AWS Management Console and create an AMI from the backup copy. Launch an EC2 instance that is based on the AMI."
        },
        {
          "label": "D",
          "content": "Create a managed-instance activation for a hybrid environment in AWS Systems Manager. Download and install Systems Manager Agent on the on-premises VM. Register the VM with Systems Manager to be a managed instance. Use AWS Backup to create a snapshot of the VM and create an AMI. Launch an EC2 instance that is based on the AMI."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Use the VMware vSphere client to export the application as an image in Open Virtualization Format (OVF) format. Create an Amazon S3 bucket to store the image in the destination AWS Region. Create and apply an IAM role for VM Import. Use the AWS CLI to run the EC2 import command.</p><p>This solution is the most appropriate for preserving software and configuration settings during migration from VMware to EC2 because:</p><p>VM Import/Export is specifically designed to migrate existing virtual machines from your on-premises environment to Amazon EC2 while preserving the software and configuration settings, which is the key requirement mentioned in the question.</p><p>The process described in option B follows the standard VM Import workflow:</p><p>Exporting the VM as an OVF file from VMware vSphere</p><p>Creating an S3 bucket to store the exported image</p><p>Setting up the necessary IAM permissions for the import process</p><p>Using the EC2 import command to convert the VM to an Amazon EC2 AMI</p><p>This approach ensures that all software installations, configurations, and settings from the original VMware VM are preserved in the resulting EC2 instance.</p><p>Option A is not the correct approach because:</p><p>AWS DataSync is primarily designed for transferring files and objects between storage systems, not for migrating complete virtual machines with their operating systems and configurations.</p><p>While FSx for Windows File Server could store the VMware data, this approach doesn't provide a direct path to convert the VMware VM into an EC2 instance.</p><p>The combination of DataSync and VM Import/Export in this manner doesn't represent the standard workflow for VM migration and would likely introduce unnecessary complexity.</p><p>The VM Import/Export service described in option B is specifically designed for this migration scenario and provides the most direct path to preserve all software and configuration settings when moving from VMware to EC2.</p><p>Sources</p><p>Relocate migration options - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-vmware-aws/relocate-options.html）</p><p>Choosing a migration approach for relocating your VMware applications and workloads to the AWS Cloud - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-vmware-aws/welcome.html）</p><p>What is VM Import/Export? - VM Import/Export （https://docs.aws.amazon.com/vm-import/latest/userguide/what-is-vmimport.html）</p><p>Choosing a migration approach - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-vmware-aws/migration-approach.html）</p><p>Accelerate VMware Migrations to AWS using AWS Migration Hub Journeys | AWS Cloud Operations Blog （https://aws.amazon.com/cn/blogs/mt/accelerate-vmware-migrations-to-aws-using-aws-migration-hub-journeys/）</p><p>VMware ESXI to AWS migration | AWS re:Post （https://repost.aws/questions/QUyuhMhfmQTvGlvy0lI8-f9w/vmware-esxi-to-aws-migration）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "cd1afa126e4947748f6dca9e349c04c3",
      "questionNumber": 63,
      "type": "multiple",
      "content": "<p>Question #63</p><p>A video processing company has an application that downloads images from an Amazon S3 bucket, processes the images, stores a transformed image in a second S3 bucket, and updates metadata about the image in an Amazon DynamoDB table. The application is written in Node.js and runs by using an AWS Lambda function. The Lambda function is invoked when a new image is uploaded to Amazon S3. </p><p><br></p><p>The application ran without incident for a while. However, the size of the images has grown significantly. The Lambda function is now failing frequently with timeout errors. The function timeout is set to its maximum value. A solutions architect needs to refactor the application’s architecture to prevent invocation failures. The company does not want to manage the underlying infrastructure.<br><br>Which combination of steps should the solutions architect take to meet these requirements? (Choose two.) </p>",
      "options": [
        {
          "label": "A",
          "content": "Modify the application deployment by building a Docker image that contains the application code. Publish the image to Amazon Elastic Container Registry (Amazon ECR)."
        },
        {
          "label": "B",
          "content": "Create a new Amazon Elastic Container Service (Amazon ECS) task definition with a compatibility type of AWS Fargate. Configure the task definition to use the new image in Amazon Elastic Container Registry (Amazon ECR). Adjust the Lambda function to invoke an ECS task by using the ECS task definition when a new file arrives in Amazon S3."
        },
        {
          "label": "C",
          "content": "Create an AWS Step Functions state machine with a Parallel state to invoke the Lambda function. Increase the provisioned concurrency of the Lambda function."
        },
        {
          "label": "D",
          "content": "Create a new Amazon Elastic Container Service (Amazon ECS) task definition with a compatibility type of Amazon EC2. Configure the task definition to use the new image in Amazon Elastic Container Registry (Amazon ECR). Adjust the Lambda function to invoke an ECS task by using the ECS task definition when a new file arrives in Amazon S3."
        },
        {
          "label": "E",
          "content": "Modify the application to store images on Amazon Elastic File System (Amazon EFS) and to store metadata on an Amazon RDS DB instance. Adjust the Lambda function to mount the EFS file share."
        }
      ],
      "correctAnswer": "AB",
      "explanation": "<p>The correct answers are A and B. &nbsp;</p><p>Explanation: &nbsp;</p><p>The Lambda function is timing out because the image processing task is exceeding Lambda's maximum timeout limit (15 minutes) and possibly hitting other resource constraints (memory, CPU). Since the company does not want to manage infrastructure,AWS Fargate (serverless ECS) is the best alternative. &nbsp;</p><p>#Steps to Refactor the Architecture: &nbsp;</p><p>1.A. Modify the application deployment by building a Docker image that contains the application code. Publish the image to Amazon ECR. &nbsp;</p><p> &nbsp; - Containerizing the application allows it to run in a more scalable and flexible environment (ECS Fargate). &nbsp;</p><p> &nbsp; - Amazon ECR stores the Docker image for deployment. &nbsp;</p><p>2.B. Create a new Amazon ECS task definition with a compatibility type of AWS Fargate. Configure the task definition to use the new image in Amazon ECR. Adjust the Lambda function to invoke an ECS task when a new file arrives in Amazon S3. &nbsp;</p><p> &nbsp; -AWS Fargate removes the need to manage servers (meets the \"no infrastructure management\" requirement). &nbsp;</p><p> &nbsp; - The Lambda function can now act as a lightweightorchestrator, triggering an ECS task for heavy processing instead of doing the work itself. &nbsp;</p><p>Why Not the Other Options? &nbsp;</p><p>-C: Step Functions with Parallel state and provisioned concurrency does not solve the timeout issue (Lambda still has execution limits). &nbsp;</p><p>-D: UsingEC2-based ECS requires managing infrastructure (violates the requirement). &nbsp;</p><p>-E: Storing images on EFS and metadata on RDS does not address the core issue (Lambda timeout due to long processing). &nbsp;</p><p>Conclusion: &nbsp;</p><p>A + B is the best solution—migrating the heavy processing toECS Fargate while keeping Lambda as the lightweight trigger. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "1646fd4d5bc446e69cf350641b600626",
      "questionNumber": 64,
      "type": "single",
      "content": "<p>Question #64</p><p>A company has an organization in AWS Organizations. The company is using AWS Control Tower to deploy a landing zone for the organization. The company wants to implement governance and policy enforcement. The company must implement a policy that will detect Amazon RDS DB instances that are not encrypted at rest in the company’s production OU.<br><br>Which solution will meet this requirement? </p>",
      "options": [
        {
          "label": "A",
          "content": "Turn on mandatory guardrails in AWS Control Tower. Apply the mandatory guardrails to the production OU."
        },
        {
          "label": "B",
          "content": "Enable the appropriate guardrail from the list of strongly recommended guardrails in AWS Control Tower. Apply the guardrail to the production OU."
        },
        {
          "label": "C",
          "content": "Use AWS Conﬁg to create a new mandatory guardrail. Apply the rule to all accounts in the production OU."
        },
        {
          "label": "D",
          "content": "Create a custom SCP in AWS Control Tower. Apply the SCP tothe production OU."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Enable the appropriate guardrail from the list of strongly recommended guardrails in AWS Control Tower. Apply the guardrail to the production OU.</p><p>This is the correct solution because:</p><p>AWS Control Tower has different types of guardrails: mandatory guardrails and strongly recommended guardrails.</p><p>Mandatory guardrails are automatically enabled when you set up AWS Control Tower and cannot be disabled. They provide foundational security and operational benefits that are essential for maintaining the integrity of your landing zone.</p><p>Strongly recommended guardrails are optional but highly advised based on AWS best practices. They can be enabled or disabled according to your organization's needs.</p><p>The specific guardrail that detects unencrypted Amazon RDS DB instances is called \"Detect Whether Storage Encryption is Enabled for Amazon RDS Database Instances\" and it is classified as a strongly recommended detective guardrail, not a mandatory guardrail.</p><p>This guardrail uses AWS Config rules to monitor RDS instances and check whether storage encryption is enabled, which directly addresses the requirement to detect RDS DB instances that are not encrypted at rest in the production OU.</p><p>Option A is incorrect because mandatory guardrails do not include the specific control for detecting unencrypted RDS instances. Mandatory guardrails focus on foundational security controls that cannot be disabled, while the RDS encryption detection is part of the strongly recommended guardrails that can be selectively applied to specific OUs like the production OU mentioned in the question.</p><p>Therefore, to implement a policy that will detect Amazon RDS DB instances that are not encrypted at rest in the company's production OU, the company should enable the appropriate guardrail from the list of strongly recommended guardrails in AWS Control Tower and apply it to the production OU.</p><p>Sources</p><p>Optional controls - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/designing-control-tower-landing-zone/optional.html）</p><p>Controls - Management and Governance Cloud Environment Guide （https://docs.aws.amazon.com/wellarchitected/latest/management-and-governance-guide/controls.html）</p><p>Strongly recommended controls with detective behavior - AWS Control Tower （https://docs.aws.amazon.com/controltower/latest/controlreference/strongly-recommended-detective-controls.html）</p><p>Preventive vs. Proactive vs. Detective Controls in AWS Control Tower | AWS re:Post (https://repost.aws/articles/ARbi64fKJCQsCpqOskL6vb4g/preventive-vs-proactive-vs-detective-controls-in-aws-control-tower)</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "296b8c8a1a14423d8286fca6e9318417",
      "questionNumber": 65,
      "type": "single",
      "content": "<p>Question #65</p><p>A startup company hosts a fleet of Amazon EC2 instances in private subnets using the latest Amazon Linux 2 AMI. The company’s engineers rely heavily on SSH access to the instances for troubleshooting. </p><p><br></p><p>The company’s existing architecture includes the following: </p><p><br></p><p>• A VPC with private and public subnets, and a NAT gateway. </p><p>• Site-to-Site VPN for connectivity with the on-premises environment. </p><p>• EC2 security groups with direct SSH access from the on-premises environment. </p><p><br></p><p>The company needs to increase security controls around SSH access and provide auditing of commands run by the engineers. </p><p><br></p><p>Which strategy should a solutions architect use? </p>",
      "options": [
        {
          "label": "A",
          "content": "Install and conﬁgure EC2 Instance Connect on the ﬂeet of EC2 instances. Remove all security group rules attached to EC2 instances that allow inbound TCP on port 22. Advise the engineers to remotely access the instances by using the EC2 Instance Connect CLI."
        },
        {
          "label": "B",
          "content": "Update the EC2 security groups to only allow inbound TCP on port 22 to the IP addresses of the engineer&rsquo;s devices. Install the Amazon CloudWatch agent on all EC2 instances and send operating system audit logs to CloudWatch Logs."
        },
        {
          "label": "C",
          "content": "Update the EC2 security groups to only allow inbound TCP on port 22 to the IP addresses of the engineer&rsquo;s devices. Enable AWS Conﬁg for EC2 security group resource changes. Enable AWS Firewall Manager and apply a security group policy that automatically remediates changes to rules."
        },
        {
          "label": "D",
          "content": "Create an IAM role with the AmazonSSMManagedInstanceCore managed policy attached. Attach the IAM role to all the EC2 instances. Remove all security group rules attached to the EC2 instances that allow inbound TCP on port 22. Have the engineers install the AWS Systems Manager Session Manager plugin for their devices and remotely access the instances by using the start-session API call from Systems Manager."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>The correct answer isD. &nbsp;</p><p>Explanation: &nbsp;</p><p>The company needs toincrease security controls around SSH access andprovide auditing of commands run by engineers. The best approach is to useAWS Systems Manager Session Manager, which provides: &nbsp;</p><p>No open inbound ports (eliminates SSH security risks) – Session Manager does not require inbound port 22 access, improving security. &nbsp;</p><p>Auditability – All session activity is logged in AWS CloudTrail and can be stored in S3 or CloudWatch Logs. &nbsp;</p><p>IAM-based access control – Engineers can be granted permissions via IAM policies. &nbsp;</p><p>No need for bastion hosts or NAT gateways – Sessions are established via AWS Systems Manager, reducing complexity. &nbsp;</p><p>Why not the other options? &nbsp;</p><p>-A: EC2 Instance Connect still relies on SSH and temporary SSH keys, which doesn’t fully eliminate SSH exposure. &nbsp;</p><p>-B & C: Both options still allow direct SSH access (port 22), which is less secure than using Session Manager. While logging helps, it doesn’t remove the attack surface. &nbsp;</p><p>Correct Answer: &nbsp;</p><p>D. Create an IAM role with the `AmazonSSMManagedInstanceCore` managed policy, attach it to EC2 instances, remove SSH security group rules, and have engineers use Session Manager. &nbsp;</p><p>This approach provides thehighest security and auditing capabilities while eliminating the need for open SSH ports.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "03e79bc3134a49b389763503fa4bf8b5",
      "questionNumber": 66,
      "type": "multiple",
      "content": "<p>Question #66</p><p>A company that uses AWS Organizations allows developers to experiment on AWS. As part of the landing zone that the company has deployed, developers use their company email address to request an account. The company wants to ensure that developers are not launching costly services or running services unnecessarily. The company must give developers a ﬁxed monthly budget to limit their AWS costs.<br><br>Which combination of steps will meet these requirements? (Choose three.) </p>",
      "options": [
        {
          "label": "A",
          "content": "Create an SCP to set a ﬁxed monthly account usage limit. Apply the SCP to the developer accounts."
        },
        {
          "label": "B",
          "content": "Use AWS Budgets to create a ﬁxed monthly budget for each developer&rsquo;s account as part of the account creation process."
        },
        {
          "label": "C",
          "content": "Create an SCP to deny access to costly services and components. Apply the SCP to the developer accounts."
        },
        {
          "label": "D",
          "content": "Create an IAM policy to deny access to costly services and components. Apply the IAM policy to the developer accounts."
        },
        {
          "label": "E",
          "content": "Create an AWS Budgets alert action to terminate services when the budgeted amount is reached. Configure the action to terminate all services."
        },
        {
          "label": "F",
          "content": "Create an AWS Budgets alert action to send an Amazon Simple Notiﬁcation Service (Amazon SNS) notiﬁcation when the budgeted amount is reached. Invoke an AWS Lambda function to terminate all services."
        }
      ],
      "correctAnswer": "BCF",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Use AWS Budgets to create a fixed monthly budget for each developer's account as part of the account creation process. </p><p>C. Create an SCP to deny access to costly services and components. Apply the SCP to the developer accounts. </p><p>F. Create an AWS Budgets alert action to send an Amazon Simple Notification Service (Amazon SNS) notification when the budgeted amount is reached. Invoke an AWS Lambda function to terminate all services.</p><p>This combination of steps will effectively implement a fixed monthly budget for developers and limit AWS costs:</p><p>Option B establishes the foundation by creating a fixed monthly budget for each developer's account during the account creation process. AWS Budgets allows you to set specific budget amounts and track spending against those budgets.</p><p>Option C provides preventive controls by using Service Control Policies (SCPs) to deny access to costly services and components. This helps prevent developers from launching expensive resources in the first place, addressing the requirement to ensure \"developers are not launching costly services.\"</p><p>Option F implements reactive controls by setting up an automated response when the budget limit is reached. The AWS Budgets alert action sends an SNS notification, which then triggers a Lambda function to terminate all services. This addresses the requirement to limit costs and prevent developers from \"running services unnecessarily\" beyond their fixed monthly budget.</p><p>Together, these three options create a comprehensive cost management strategy:</p><p>Preventive controls (SCPs) to block access to expensive services</p><p>Monitoring controls (AWS Budgets) to track spending against fixed budgets</p><p>Reactive controls (SNS notifications and Lambda functions) to automatically terminate services when budgets are exceeded</p><p>This approach ensures that developers can experiment on AWS while staying within their allocated budget, preventing unexpected or excessive costs for the company.</p><p>Sources</p><p>AWS Budgets & Billing | AWS re:Post (https://repost.aws/questions/QUWxq5rqz5RAaNUQxtd7KnQQ/aws-budgets-billing)</p><p>Budget methods - AWS Cost Management (https://docs.aws.amazon.com/cost-management/latest/userguide/budget-methods.html)</p><p>COST02-BP05 Implement cost controls - AWS Well-Architected Framework (https://docs.aws.amazon.com/wellarchitected/2023-10-03/framework/cost_govern_usage_controls.html)</p><p>COST02-BP05 Implement cost controls - Cost Optimization Pillar (https://docs.aws.amazon.com/wellarchitected/latest/cost-optimization-pillar/cost_govern_usage_controls.html)</p><p>AWS Rekognition | AWS re:Post (https://repost.aws/questions/QU0B7Z5vnzROOEmcB9edL_iQ/aws-rekognition)</p><p>DevOps team wants to automate cost monitoring and receive alerts if AWS costs exceed a certain threshold. What AWS services and automation strategies would you use?. | AWS re:Post (https://repost.aws/questions/QUWml6NyN8TLqU-zfvrIKa2g/devops-team-wants-to-automate-cost-monitoring-and-receive-alerts-if-aws-costs-exceed-a-certain-threshold-what-aws-services-and-automation-strategies-would-you-use)</p><p>Serverless Automated Cost Controls, Part1 | AWS Compute Blog (https://aws.amazon.com/cn/blogs/compute/serverless-automated-cost-controls-part1/)</p><p>COST02-BP05 Implement cost controls - AWS Well-Architected Framework (2023-04-10) (https://docs.aws.amazon.com/wellarchitected/2023-04-10/framework/cost_govern_usage_controls.html)</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "a2f9b071fc924fd99a0109eb50e57415",
      "questionNumber": 67,
      "type": "single",
      "content": "<p>Question #67</p><p>A company has applications in an AWS account that is named Source. The account is in an organization in AWS Organizations. One of the applications uses AWS Lambda functions and stores inventory data in an Amazon Aurora database. The application deploys the Lambda functions by using a deployment package. The company has configured automated backups for Aurora. </p><p><br></p><p>The company wants to migrate the Lambda functions and the Aurora database to a new AWS account that is named Target. The application processes critical data, so the company must minimize downtime.<br><br>Which solution will meet these requirements? </p>",
      "options": [
        {
          "label": "A",
          "content": "Download the Lambda function deployment package from the Source account. Use the deployment package and create new Lambda functions in the Target account. Share the automated Aurora DB cluster snapshot with the Target account."
        },
        {
          "label": "B",
          "content": "Download the Lambda function deployment package from the Source account. Use the deployment package and create new Lambda functions in the Target account. Share the Aurora DB cluster with the Target account by using AWS Resource Access Manager (AWS RAM). Grant the Target account permission to clone the Aurora DB cluster."
        },
        {
          "label": "C",
          "content": "Use AWS Resource Access Manager (AWS RAM) to share the Lambda functions and the Aurora DB cluster with the Target account. Grant the Target account permission to clone the Aurora DB cluster."
        },
        {
          "label": "D",
          "content": "Use AWS Resource Access Manager (AWS RAM) to share the Lambda functions with the Target account. Share the automated Aurora DB cluster snapshot with the Target account."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>The correct answer isB. Here's why:</p><p> Key Requirements:</p><p>1.Migrate Lambda functions and Aurora database to a new AWS account (Target).</p><p>2.Minimize downtime (critical data processing).</p><p>3. The Lambda functions are deployed via adeployment package, and Aurora hasautomated backups.</p><p> Why Option B?</p><p>-Lambda Migration: &nbsp;</p><p> &nbsp;- Since Lambda functions are deployed via a deployment package, you can download the package from theSource account and recreate the functions in theTarget account. AWS RAMcannot be used to share Lambda functions directly (they are not shareable via RAM), so options C and D are incorrect for Lambda migration.</p><p> &nbsp;</p><p>-Aurora Migration: &nbsp;</p><p> &nbsp;- To minimize downtime, sharing thelive Aurora DB cluster via AWS RAM (instead of restoring from a snapshot) allows theTarget account to clone the database, ensuring minimal disruption. &nbsp;</p><p> &nbsp;- Automated snapshots (Option A) require arestore operation, which takes longer and causes more downtime compared to cloning a live cluster. &nbsp;</p><p> &nbsp;- AWS RAM enables cross-account sharing of Aurora clusters, and cloning is faster than snapshot restoration.</p><p> Why Not Other Options?</p><p>-A: Uses a snapshot (slower, more downtime). &nbsp;</p><p>-C: AWS RAM cannot share Lambda functions. &nbsp;</p><p>-D: AWS RAM cannot share Lambda functions, and using a snapshot is slower than cloning.</p><p> Conclusion: &nbsp;</p><p>B is the best solution because it efficiently migrates the Lambda functions (via redeployment) and minimizes Aurora downtime by using AWS RAM to share and clone the live cluster. &nbsp;</p><p>Answer: B</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "a673adb161a94e1c94fef10ea2cc7d4c",
      "questionNumber": 68,
      "type": "single",
      "content": "<p>Question #68</p><p>A company runs a Python script on an Amazon EC2 instance to process data. The script runs every 10 minutes. The script ingests files from an Amazon S3 bucket and processes the files. On average, the script takes approximately 5 minutes to process each file The script will not reprocess a file that the script has already processed. </p><p><br></p><p>The company reviewed Amazon CloudWatch metrics and noticed that the EC2 instance is idle for approximately 40% of the time because of the file processing speed. The company wants to make the workload highly available and scalable. The company also wants to reduce long-term management overhead. </p><p><br></p><p>Which solution will meet these requirements MOST cost-effectively? </p>",
      "options": [
        {
          "label": "A",
          "content": "Migrate the data processing script to an AWS Lambda function. Use an S3 event notification to invoke the Lambda function to process the objects when the company uploads the objects."
        },
        {
          "label": "B",
          "content": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure Amazon S3 to send event notifications to the SQS queue. Create an EC2 Auto Scaling group with a minimum size of one instance. Update the data processing script to poll the SQS queue. Process the S3 objects that the SQS message identifies."
        },
        {
          "label": "C",
          "content": "Migrate the data processing script to a container image. Run the data processing container on an EC2 instance. Configure the container to poll the S3 bucket for new objects and to process the resulting objects."
        },
        {
          "label": "D",
          "content": "Migrate the data processing script to a container image that runs on Amazon Elastic Container Service (Amazon ECS) on AWS Fargate. Create an AWS Lambda function that calls the Fargate RunTaskAPI operation when the container processes the file. Use an S3 event notification to invoke the Lambda function."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Themost cost-effective solution that meets the requirements ofhigh availability, scalability, and reduced management overhead is:</p><p>Option A &nbsp;</p><p>Migrate the data processing script to an AWS Lambda function. Use an S3 event notification to invoke the Lambda function to process the objects when the company uploads the objects.</p><p>Why Option A? &nbsp;</p><p>1.Serverless & Cost-Effective: &nbsp;</p><p> &nbsp; - Lambda runs only when triggered (by S3 events), eliminating idle time. &nbsp;</p><p> &nbsp; - No need to manage EC2 instances or pay for unused compute time. &nbsp;</p><p>2.Highly Available & Scalable: &nbsp;</p><p> &nbsp; - Lambda automatically scales with the number of file uploads. &nbsp;</p><p> &nbsp; - No need to manage scaling policies (unlike EC2 Auto Scaling in Option B). &nbsp;</p><p>3.Reduced Management Overhead: &nbsp;</p><p> &nbsp; - No EC2 instances, containers, or clusters to manage (unlike Options B, C, and D). &nbsp;</p><p> &nbsp; - S3 event notifications directly trigger Lambda, simplifying the architecture. &nbsp;</p><p>Why Not Other Options? &nbsp;</p><p>-Option B: Uses EC2 Auto Scaling, which still requires managing instances and incurs costs during idle time. &nbsp;</p><p>-Option C: Running a container on EC2 still involves managing infrastructure and idle costs. &nbsp;</p><p>-Option D: Using Fargate + Lambda adds complexity and higher costs compared to a pure Lambda solution. &nbsp;</p><p>Conclusion &nbsp;</p><p>Option A is themost cost-effective, scalable, and low-management solution for this workload. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "adc8503155b74b47afc5211bb3d6bd3e",
      "questionNumber": 69,
      "type": "single",
      "content": "<p>Question #69</p><p>A financial services company in North America plans to release a new online web application to its customers on AWS. The company will launch the application in the us-east-1 Region on Amazon EC2 instances. The application must be highly available and must dynamically scale to meet user traffic. The company also wants to implement a disaster recovery environment for the application in the us-west-1 Region by using active-passive failover.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a VPC in us-east-1 and a VPC in us-west-1. Configure VPC peering. In the us-east-1 VPC, create an Application Load Balancer (ALB) that extends across multiple Availability Zones in both VPCs. Create an Auto Scaling group that deploys the EC2 instances across the multiple Availability Zones in both VPCs. Place the Auto Scaling group behind the ALB."
        },
        {
          "label": "B",
          "content": "Create a VPC in us-east-1 and a VPC in us-west-1. In the us-east-1 VPC, create an Application Load Balancer (ALB) that extends across multiple Availability Zones in that VPC. Create an Auto Scaling group that deploys the EC2 instances across the multiple Availability Zones in the us-east-1 VPC. Place the Auto Scaling group behind the ALB. Set up the same configuration in the us-west-1 VPC. Create an Amazon Route 53 hosted zone. Create separate records for each ALB. Enable health checks to ensure high availability between Regions."
        },
        {
          "label": "C",
          "content": "Create a VPC in us-east-1 and a VPC in us-west-1. In the us-east-1 VPC, create an Application Load Balancer (ALB) that extends across multiple Availability Zones in that VPC. Create an Auto Scaling group that deploys the EC2 instances across the multiple Availability Zones in the us-east-1 VPC. Place the Auto Scaling group behind the ALB. Set up the same configuration in the us-west-1 VPC. Create an Amazon Route 53 hosted zone. Create separate records for each ALB. Enable health checks and configure a failover routing policy for each record."
        },
        {
          "label": "D",
          "content": "Create a VPC in us-east-1 and a VPC in us-west-1. Configure VPC peering. In the us-east-1 VPC, create an Application Load Balancer (ALB) that extends across multiple Availability Zones in both VPCs. Create an Auto Scaling group that deploys the EC2 instances across the multiple Availability Zones in both VPCs. Place the Auto Scaling group behind the ALB. Create an Amazon Route 53 hosted zone. Create a record for the ALB."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>The correct answer isC. &nbsp;</p><p> Explanation:</p><p>The requirements are:</p><p>1.High availability in the primary Region (us-east-1) with dynamic scaling.</p><p>2.Disaster recovery (DR) in us-west-1 using anactive-passive failover model.</p><p>3. The solution must useAmazon Route 53 for DNS-based failover.</p><p>Let’s analyze the options:</p><p>#Option A ❌</p><p>-VPC peering is not required for a multi-Region failover setup (and VPC peering does not work across Regions).</p><p>- The ALBcannot span multiple VPCs or Regions, so this setup is invalid.</p><p>- No mention ofRoute 53 failover routing, which is required for active-passive DR.</p><p>#Option B ❌</p><p>- Correctly sets upALB + Auto Scaling in both Regions.</p><p>- However, itlacks a failover routing policy in Route 53, which is necessary for active-passive failover.</p><p>- Simply enabling health checks is not enough—you need to configurefailover routing.</p><p>#Option C ✅</p><p>-Correctly sets up ALB + Auto Scaling in both Regions (us-east-1 as active, us-west-1 as passive).</p><p>-Route 53 failover routing policy is configured with health checks, ensuring automatic failover if the primary Region fails.</p><p>- This matches theactive-passive disaster recovery requirement.</p><p>#Option D ❌</p><p>- Similar to Option A,VPC peering across Regions is not possible.</p><p>- The ALBcannot span multiple Regions.</p><p>- Nofailover routing policy is mentioned, only a single record.</p><p>Why C is the Best Answer:</p><p>-High Availability in Primary Region: ALB + Auto Scaling across multiple AZs in us-east-1.</p><p>-Disaster Recovery in Secondary Region: Passive deployment in us-west-1.</p><p>-Active-Passive Failover: Route 53 health checks and failover routing ensure automatic switch to us-west-1 if us-east-1 fails.</p><p>Thus,Option C is the correct solution. </p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "2012511832094d1cbf41c76f7e2543ad",
      "questionNumber": 70,
      "type": "single",
      "content": "Question #70<br><br><br><p>A company has an environment that has a single AWS account. A solutions architect is reviewing the environment to recommend what the company could improve specifi cally in terms of access to the AWS Management Console. The company’s IT support workers currently access the console for administrative tasks, authenticating with named IAM users that have been mapped to their job role. </p><p><br></p><p>The IT support workers no longer want to maintain both their Active Directory and IAM user accounts. They want to be able to access the console by using their existing Active Directory credentials. The solutions architect is using AWS IAM Identity Center (AWS Single Sign-On) to implement this functionality. </p><p><br></p><p>Which solution will meet these requirements MOST cost-effectively?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an organization in AWS Organizations. Turn on the IAM Identity Center feature in Organizations. Create and confi gure a directory in AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD) with a two-way trust to the company&rsquo;s on-premises Active Directory. Confi gure IAM Identity Center and set the AWS Managed Microsoft AD directory as the identity source. Create permission sets and map them to the existing groups within the AWS Managed Microsoft AD directory.&nbsp;"
        },
        {
          "label": "B",
          "content": "Create an organization in AWS Organizations. Turn on the IAM Identity Center feature in Organizations. Create and confi gure an AD Connector to connect to the company&rsquo;s on-premises Active Directory. Confi gure IAM Identity Center and select the AD Connector as the identity source. Create permission sets and map them to the existing groups within the company&rsquo;s Active Directory.&nbsp;"
        },
        {
          "label": "C",
          "content": "Create an organization in AWS Organizations. Turn on all features for the organization. Create and confi gure a directory in AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD) with a two-way trust to the company&rsquo;s on-premises Active Directory. Confi gure IAM Identity Center and select the AWS Managed Microsoft AD directory as the identity source. Create permission sets and map them to the existing groups within the AWS Managed Microsoft AD directory.&nbsp;"
        },
        {
          "label": "D",
          "content": "&nbsp;Create an organization in AWS Organizations. Turn on all features for the organization. Create and confi gure an AD Connector to connect to the company&rsquo;s on-premises Active Directory. Confi gure IAM Identity Center and set the AD Connector as the identity source. Create permission sets and map them to the existing groups within the company&rsquo;s Active Directory.&nbsp;"
        }
      ],
      "correctAnswer": "D",
      "explanation": "https://www.examtopics.com/discussions/amazon/view/69172-exam-aws-certified-solutions-architect-professional-topic-1/ You are correct, I apologize for the oversight. To meet the requirements of the IT support workers, option D would be the correct solution: <p>This option will first enable all features in AWS Organizations, then create and configure an AD Connector to connect to the company's one premises Active Directory. Then, it will configure IAM Identity Center (AWS SSO) and set the AD Connector as the identity source, allowing the IT support workers to access the console using their existing Active Directory credentials. Finally, it will create permission sets and map them to the existing groups within the company's Active Directory. This solution will also be cost-effective as it does not involve creating a new directory in AWS Directory Service. </p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "4530b82a9c154b05bcfe2069fa88c526",
      "questionNumber": 71,
      "type": "multiple",
      "content": "<p>Question #71</p><p>A video streaming company recently launched a mobile app for video sharing. The app uploads various files to an Amazon S3 bucket in the us-east-1 Region. The files range in size from 1 GB to 10 GB.</p><p><br></p><p>Users who access the app from Australia have experienced uploads that take long periods of time. Sometimes the files fail to completely upload for these users. A solutions architect must improve the app’s performance for these uploads.</p><p><br></p><p>Which solutions will meet these requirements? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Enable S3 Transfer Acceleration on the S3 bucket. Configure the app to use the Transfer Acceleration endpoint for uploads."
        },
        {
          "label": "B",
          "content": "Configure an S3 bucket in each Region to receive the uploads. Use S3 Cross-Region Replication to copy the files to the distribution S3 bucket."
        },
        {
          "label": "C",
          "content": "Set up Amazon Route 53 with latency-based routing to route the uploads to the nearest S3 bucket Region."
        },
        {
          "label": "D",
          "content": "Configure the app to break the video files into chunks. Use a multipart upload to transfer files to Amazon S3."
        },
        {
          "label": "E",
          "content": "Modify the app to add random prefixes to the files before uploading."
        }
      ],
      "correctAnswer": "AD",
      "explanation": "<p>Based on the information provided, the correct answers are:</p><p>A. Enable S3 Transfer Acceleration on the S3 bucket. Configure the app to use the Transfer Acceleration endpoint for uploads.</p><p>D. Configure the app to break the video files into chunks. Use a multipart upload to transfer files to Amazon S3.</p><p>These solutions will improve the performance of large file uploads from Australia to the S3 bucket in us-east-1 for the following reasons:</p><p>S3 Transfer Acceleration (Option A):</p><p>This feature is specifically designed to optimize long-distance transfers across different geographic regions.</p><p>It uses Amazon CloudFront's globally distributed edge locations to route data through an optimized network path.</p><p>For users in Australia uploading to us-east-1, Transfer Acceleration can significantly improve transfer speeds (by 50-500% for cross-continental transfers).</p><p>It addresses the long upload times experienced by Australian users by providing a more efficient network path to the destination bucket.</p><p>Multipart Uploads (Option D):</p><p>This approach is particularly effective for large files (1-10 GB in this case).</p><p>Breaking files into smaller chunks allows for parallel uploads, increasing throughput.</p><p>If a part fails during upload, only that specific part needs to be retried rather than the entire file.</p><p>This directly addresses the issue of files failing to completely upload by providing resilience against network interruptions.</p><p>AWS recommends multipart uploads for files larger than 100MB, which applies to all files in this scenario (1-10 GB).</p><p>Option E (adding random prefixes) is incorrect because:</p><p>While prefix strategies can help with S3 performance for high-throughput scenarios with many small files, they don't significantly improve upload speeds for individual large files.</p><p>Random prefixes help distribute objects across multiple partitions to avoid throttling when you have extremely high request rates, but this doesn't address the network latency and reliability issues experienced by Australian users.</p><p>The combination of S3 Transfer Acceleration and multipart uploads provides the most effective solution for improving upload performance and reliability for large files being transferred from distant geographic locations like Australia to us-east-1.</p><p>Sources</p><p>Troubleshoot cross-region transfer issues - AWS Transfer Family （https://docs.aws.amazon.com/transfer/latest/userguide/cross-region-transfer-issues.html）</p><p>Using Amazon S3 Transfer Acceleration to Accelerate Geographically Disparate Data Transfers - Best Practices Design Patterns: Optimizing Amazon S3 Performance （https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html）</p><p>Performance guidelines for Amazon S3 - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance-guidelines.html）</p><p>Selection - Streaming Media Lens （https://docs.aws.amazon.com/wellarchitected/latest/streaming-media-lens/selection.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "674f121c69e649b68f6a43b4984bbfc9",
      "questionNumber": 72,
      "type": "single",
      "content": "<p>Question #72</p><p>An application is using an Amazon RDS for MySQL Multi-AZ DB instance in the us-east-1 Region. After a failover test, the application lost the connections to the database and could not re-establish the connections. After a restart of the application, the application re-established the connections.</p><p><br></p><p>A solutions architect must implement a solution so that the application can re-establish connections to the database without requiring a restart.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an Amazon Aurora MySQL Serverless v1 DB instance. Migrate the RDS DB instance to the Aurora Serverless v1 DB instance. Update the connection settings in the application to point to the Aurora reader endpoint."
        },
        {
          "label": "B",
          "content": "Create an RDS proxy. Configure the existing RDS endpoint as a target. Update the connection settings in the application to point to the RDS proxy endpoint."
        },
        {
          "label": "C",
          "content": "Create a two-node Amazon Aurora MySQL DB cluster. Migrate the RDS DB instance to the Aurora DB cluster. Create an RDS proxy. Configure the existing RDS endpoint as a target. Update the connection settings in the application to point to the RDS proxy endpoint."
        },
        {
          "label": "D",
          "content": "Create an Amazon S3 bucket. Export the database to Amazon S3 by using AWS Database Migration Service (AWS DMS). Configure Amazon Athena to use the S3 bucket as a data store. Install the latest Open Database Connectivity (ODBC) driver for the application. Update the connection settings in the application to point to the Athena endpoint."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Create an RDS proxy. Configure the existing RDS endpoint as a target. Update the connection settings in the application to point to the RDS proxy endpoint.</p><p>This solution will meet the requirements because:</p><p>The core issue described in the scenario is that the application loses connections to the database during a failover event and cannot re-establish them without a restart. This is a common problem with direct database connections during failover events.</p><p>Amazon RDS Proxy is specifically designed to address connection management issues during failover events. It maintains a pool of database connections and actively manages them during failovers.</p><p>When a failover occurs with RDS Proxy in place:</p><p>The proxy is aware of the failover event</p><p>It preserves most existing connections, only canceling those in the middle of a transaction</p><p>It automatically redirects connections to the new primary instance without relying on DNS propagation</p><p>It queues incoming requests until new connections can be established</p><p>By pointing the application to the RDS Proxy endpoint instead of directly to the database, the application can maintain its connection to the proxy even when the underlying database fails over to the standby instance.</p><p>This solution doesn't require changing the database engine (staying with RDS for MySQL) and works with the existing Multi-AZ setup.</p><p>Option A (migrating to Aurora Serverless v1 and using the reader endpoint) is incorrect because:</p><p>Aurora reader endpoints are primarily designed for load balancing read traffic across multiple read replicas</p><p>Reader endpoints still rely on DNS propagation during failover events, which can cause connection issues</p><p>Reader endpoints don't provide the connection pooling and active connection management that RDS Proxy offers</p><p>Changing from RDS MySQL to Aurora Serverless is a more complex solution than needed to solve this specific connection management issue</p><p>RDS Proxy is the most appropriate solution for this scenario as it directly addresses the connection management problem during failover events without requiring application restarts.</p><p>Sources</p><p>Improving application availability with Amazon RDS Proxy | AWS Database Blog （https://aws.amazon.com/cn/blogs/database/improving-application-availability-with-amazon-rds-proxy/）</p><p>Targeted business outcomes - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/amazon-rds-proxy/targeted-business-outcomes.html）</p><p>Working with Amazon RDS Proxy endpoints - Amazon Aurora（https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-proxy-endpoints.html） </p><p>RDS Proxy concepts and terminology - Amazon Relational Database Service （https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-proxy.howitworks.html）</p><p>Improve application availability on Amazon Aurora | AWS Database Blog （https://aws.amazon.com/cn/blogs/database/improve-application-availability-on-amazon-aurora/）</p><p>RDS Proxy concepts and terminology - Amazon Aurora （https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-proxy.howitworks.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "13077eb26cbe4b75bacc5901dc7c6414",
      "questionNumber": 73,
      "type": "single",
      "content": "<p>Question #73</p><p>A company is building a solution in the AWS Cloud. Thousands of devices will connect to the solution and send data. Each device needs to be able to send and receive data in real time over the MQTT protocol. Each device must authenticate by using a unique X.509 certificate.</p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Set up AWS IoT Core. For each device, create a corresponding Amazon MQ queue and provision a certificate. Connect each device to Amazon MQ."
        },
        {
          "label": "B",
          "content": "Create a Network Load Balancer (NLB) and configure it with an AWS Lambda authorizer. Run an MQTT broker on Amazon EC2 instances in an Auto Scaling group. Set the Auto Scaling group as the target for the NLB. Connect each device to the NLB."
        },
        {
          "label": "C",
          "content": "Set up AWS IoT Core. For each device, create a corresponding AWS IoT thing and provision a certificate. Connect each device to AWS IoT Core."
        },
        {
          "label": "D",
          "content": "Set up an Amazon API Gateway HTTP API and a Network Load Balancer (NLB). Create integration between API Gateway and the NLB. Configure a mutual TLS certificate authorizer on the HTTP API. Run an MQTT broker on an Amazon EC2 instance that the NLB targets. Connect each device to the NLB."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Set up AWS IoT Core. For each device, create a corresponding AWS IoT thing and provision a certificate. Connect each device to AWS IoT Core.</p><p>This solution will meet the requirements with the least operational overhead because:</p><p>AWS IoT Core is specifically designed for IoT scenarios where thousands of devices need to connect using the MQTT protocol. It's a fully managed service that handles the complexity of scaling to support thousands of concurrent device connections.</p><p>AWS IoT Core has built-in support for X.509 certificate management, making it easy to provision, manage, and rotate unique certificates for each device. This addresses the requirement for each device to authenticate using a unique X.509 certificate.</p><p>The \"thing\" concept in AWS IoT Core is designed to represent each physical device, providing a logical representation that can be associated with certificates, policies, and device shadows. This makes it straightforward to manage device identity and permissions.</p><p>As a fully managed service, AWS IoT Core eliminates the operational overhead of provisioning, managing, and scaling infrastructure components like MQTT brokers, load balancers, and EC2 instances.</p><p>Option A (using Amazon MQ) would involve higher operational overhead because:</p><p>Amazon MQ is not specifically optimized for IoT scenarios with thousands of devices</p><p>Creating and managing individual queues for each device would be cumbersome</p><p>Certificate management would be more manual compared to IoT Core</p><p>Option B (running MQTT brokers on EC2 with an NLB) would involve significantly higher operational overhead because:</p><p>You would need to manage EC2 instances, Auto Scaling groups, and a Network Load Balancer</p><p>You would need to implement and manage your own certificate authority and authentication system</p><p>You would be responsible for scaling, patching, and maintaining the MQTT broker software</p><p>AWS IoT Core provides the most streamlined solution with the least operational overhead for the specific requirements of connecting thousands of devices using MQTT with X.509 certificate authentication.</p><p>Sources</p><p>Connecting client devices to an AWS IoT Greengrass Core device with an MQTT broker - AWS IoT Greengrass （https://docs.aws.amazon.com/greengrass/v2/developerguide/connecting-to-mqtt.html）</p><p>Seamless migration: Securely transitioning large IoT fleets to AWS | The Internet of Things on AWS – Official Blog （https://aws.amazon.com/cn/blogs/iot/seamless-migration-securely-transitioning-large-iot-fleets-to-aws/）</p><p>Custom authentication with X.509 client certificates - AWS IoT Core （https://docs.aws.amazon.com/iot/latest/developerguide/custom-auth-509cert.html）</p><p>Choosing an AWS IoT service - Choosing an AWS IoT service （https://docs.aws.amazon.com/decision-guides/latest/iot-on-aws-how-to-choose/iot.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "ce7b6e02ddc944faa6ec37c0a785445c",
      "questionNumber": 74,
      "type": "single",
      "content": "<p>Question #74</p><p>A company is running several workloads in a single AWS account. A new company policy states that engineers can provision only approved resources and that engineers must use AWS CloudFormation to provision these resources. A solutions architect needs to create a solution to enforce the new restriction on the IAM role that the engineers use for access.</p><p><br></p><p>What should the solutions architect do to create the solution?</p>",
      "options": [
        {
          "label": "A",
          "content": "Upload AWS CloudFormation templates that contain approved resources to an Amazon S3 bucket. Update the IAM policy for the engineers&rsquo; IAM role to only allow access to Amazon S3 and AWS CloudFormation. Use AWS CloudFormation templates to provision resources."
        },
        {
          "label": "B",
          "content": "Update the IAM policy for the engineers&rsquo; IAM role with permissions to only allow provisioning of approved resources and AWS CloudFormation. Use AWS CloudFormation templates to create stacks with approved resources."
        },
        {
          "label": "C",
          "content": "Update the IAM policy for the engineers&rsquo; IAM role with permissions to only allow AWS CloudFormation actions. Create a new IAM policy with permission to provision approved resources, and assign the policy to a new IAM service role. Assign the IAM service role to AWS CloudFormation during stack creation."
        },
        {
          "label": "D",
          "content": "Provision resources in AWS CloudFormation stacks. Update the IAM policy for the engineers&rsquo; IAM role to only allow access to their own AWS CloudFormation stack."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Update the IAM policy for the engineers' IAM role with permissions to only allow AWS CloudFormation actions. Create a new IAM policy with permission to provision approved resources, and assign the policy to a new IAM service role. Assign the IAM service role to AWS CloudFormation during stack creation.</p><p>This solution is the most appropriate because:</p><p>It follows the principle of least privilege by restricting the engineers' IAM role to only perform CloudFormation actions. This ensures engineers can only provision resources through CloudFormation as required by the new company policy.</p><p>By creating a separate IAM service role with permissions to provision only approved resources, the solution provides a clear separation of concerns. The engineers don't have direct permissions to create resources - they can only do so through CloudFormation using the service role.</p><p>When CloudFormation creates resources, it assumes the service role that has been carefully configured with permissions for only approved resources. This creates an additional layer of control and security.</p><p>This approach makes it easier to audit and track resource creation, as all changes must go through CloudFormation using the defined service role.</p><p>Option B is less secure because:</p><p>It gives engineers direct permissions to provision approved resources outside of CloudFormation</p><p>This could potentially allow engineers to bypass the requirement to use CloudFormation for resource provisioning</p><p>It doesn't provide the same level of separation between the ability to use CloudFormation and the ability to create resources</p><p>The approach in Option C is more aligned with AWS security best practices for CloudFormation deployments, as it ensures that:</p><p>Engineers can only use CloudFormation (enforcing the \"must use CloudFormation\" policy)</p><p>Only approved resources can be provisioned (enforcing the \"only approved resources\" policy)</p><p>The service role provides a controlled and auditable mechanism for resource creation</p><p>This implementation provides better security controls and more clearly enforces the new company policy.</p><p>Sources</p><p>Configuring least-privilege permissions to use CloudFormation - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/least-privilege-cloudformation/permissions-use-cloudformation.html）</p><p>Best practices for configuring identity-based policies for least-privilege CloudFormation access - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/least-privilege-cloudformation/best-practices-identity-based-policies.html）</p><p>Managing permissions for individuals - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/transitioning-to-multiple-aws-accounts/managing-permissions-for-individuals.html）</p><p>Service roles for CloudFormation - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/least-privilege-cloudformation/service-roles-for-cloudformation.html）</p><p>Implementing policies for least-privilege permissions for AWS CloudFormation - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/least-privilege-cloudformation/introduction.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "7170de081a184a50b4701a69ba1b11e5",
      "questionNumber": 75,
      "type": "single",
      "content": "<p>Question #75</p><p>A solutions architect is designing the data storage and retrieval architecture for a new application that a company will be launching soon. The application is designed to ingest millions of small records per minute from devices all around the world. Each record is less than 4 KB in size and needs to be stored in a durable location where it can be retrieved with low latency. The data is ephemeral and the company is required to store the data for 120 days only, after which the data can be deleted.</p><p><br></p><p>The solutions architect calculates that, during the course of a year, the storage requirements would be about 10-15 TB.</p><p><br>Which storage strategy is the MOST cost-effective and meets the design requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Design the application to store each incoming record as a single .csv file in an Amazon S3 bucket to allow for indexed retrieval. Configure a lifecycle policy to delete data older than 120 days."
        },
        {
          "label": "B",
          "content": "Design the application to store each incoming record in an Amazon DynamoDB table properly configured for the scale. Configure the DynamoDB Time to Live (TTL) feature to delete records older than 120 days."
        },
        {
          "label": "C",
          "content": "Design the application to store each incoming record in a single table in an Amazon RDS MySQL database. Run a nightly cron job that runs a query to delete any records older than 120 days."
        },
        {
          "label": "D",
          "content": "Design the application to batch incoming records before writing them to an Amazon S3 bucket. Update the metadata for the object to contain the list of records in the batch and use the Amazon S3 metadata search feature to retrieve the data. Configure a lifecycle policy to delete the data after 120 days."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Themost cost-effective and suitable storage strategy for this scenario is Option B: &nbsp;</p><p>Correct Answer: B &nbsp;</p><p>Design the application to store each incoming record in an Amazon DynamoDB table properly configured for the scale. Configure the DynamoDB Time to Live (TTL) feature to delete records older than 120 days. &nbsp;</p><p>Why? &nbsp;</p><p>1.High Write Throughput & Low Latency Retrieval: &nbsp;</p><p> &nbsp; - DynamoDB is optimized forhigh-velocity, small-record ingestion (millions of records per minute) withsingle-digit millisecond latency for retrieval. &nbsp;</p><p> &nbsp; - It automatically scales to handle the workload without manual intervention. &nbsp;</p><p>2.Automatic Data Expiration with TTL: &nbsp;</p><p> &nbsp; - DynamoDB’sTime to Live (TTL) feature automatically deletes records older than 120 days, eliminating the need for manual cleanup. &nbsp;</p><p>3.Cost-Effectiveness: &nbsp;</p><p> &nbsp; - Forephemeral data (120-day retention), DynamoDB’spay-per-request pricing (or provisioned capacity, if predictable) is more cost-efficient than S3 for frequent small writes and indexed retrievals. &nbsp;</p><p> &nbsp; - S3 is cheaper for large batches but inefficient for millions of small individual writes per minute. &nbsp;</p><p>Why Not the Other Options? &nbsp;</p><p>-A (S3 per-record .csv files): &nbsp;</p><p> &nbsp;-Inefficient for millions of small writes per minute (S3 hashigh PUT request costs at scale). &nbsp;</p><p> &nbsp;-No built-in indexing for retrieval (you’d need Athena/S3 Select, adding complexity). &nbsp;</p><p>-C (RDS MySQL): &nbsp;</p><p> &nbsp;-Not scalable for millions of writes per minute (RDS hashigher operational overhead and scaling limits). &nbsp;</p><p> &nbsp;-Nightly cron jobs are less reliable than DynamoDB TTL for expiration. &nbsp;</p><p>-D (S3 batched writes with metadata search): &nbsp;</p><p> &nbsp;-Batching helps with S3 costs, butmetadata search is limited (not designed for querying record contents). &nbsp;</p><p> &nbsp;-Slower retrieval compared to DynamoDB. &nbsp;</p><p>Conclusion: &nbsp;</p><p>DynamoDB is thebest fit forhigh-speed ingestion, low-latency retrieval, automatic TTL cleanup, and cost efficiency at this scale. &nbsp;</p><p>Final Answer: B</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "33469f0ccb414b76a263ac5b4d734243",
      "questionNumber": 76,
      "type": "single",
      "content": "<p>Question #76</p><p>A retail company is hosting an ecommerce website on AWS across multiple AWS Regions. The company wants the website to be operational at all times for online purchases. The website stores data in an Amazon RDS for MySQL DB instance.</p><p><br></p><p>Which solution will provide the HIGHEST availability for the database?</p>",
      "options": [
        {
          "label": "A",
          "content": "Configure automated backups on Amazon RDS. In the case of disruption, promote an automated backup to be a standalone DB instance. Direct database traffic to the promoted DB instance. Create a replacement read replica that has the promoted DB instance as its source."
        },
        {
          "label": "B",
          "content": "Configure global tables and read replicas on Amazon RDS. Activate the cross-Region scope. In the case of disruption, use AWS Lambda to copy the read replicas from one Region to another Region."
        },
        {
          "label": "C",
          "content": "Configure global tables and automated backups on Amazon RDS. In the case of disruption, use AWS Lambda to copy the read replicas from one Region to another Region."
        },
        {
          "label": "D",
          "content": "Configure read replicas on Amazon RDS. In the case of disruption, promote a cross-Region and read replica to be a standalone DB instance. Direct database traffic to the promoted DB instance. Create a replacement read replica that has the promoted DB instance as its source."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. Configure read replicas on Amazon RDS. In the case of disruption, promote a cross-Region and read replica to be a standalone DB instance. Direct database traffic to the promoted DB instance. Create a replacement read replica that has the promoted DB instance as its source.</p><p>This solution provides the highest availability for the database because:</p><p>Cross-Region read replicas in Amazon RDS for MySQL provide a disaster recovery solution that allows the database to remain operational even if an entire AWS Region experiences an outage.</p><p>The promotion process allows a read replica in another Region to be quickly converted to a standalone database instance that can accept write operations, minimizing downtime during a regional failure.</p><p>After promoting the read replica and redirecting traffic to it, creating a replacement read replica with the promoted instance as its source maintains the high availability architecture for future resilience.</p><p>This approach provides a practical and proven method for cross-Region failover with RDS for MySQL databases.</p><p>Options B and C are incorrect because:</p><p>Amazon RDS for MySQL does not support \"global tables\" - this is a feature of DynamoDB and Aurora, not standard RDS MySQL</p><p>Using AWS Lambda to \"copy\" read replicas between Regions is not a standard or efficient approach for database failover</p><p>These solutions describe architectures that don't align with RDS MySQL's actual capabilities</p><p>The solution in option D represents the standard approach for achieving cross-Region high availability with Amazon RDS for MySQL. It leverages the built-in read replica promotion capability, which is specifically designed for this type of failover scenario. This approach provides the highest availability for the retail company's ecommerce website by ensuring the database can remain operational even during a regional outage.</p><p>Sources</p><p>High availability for Amazon Aurora - Amazon Aurora （https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html）</p><p>Scaling and high availability in Amazon RDS - Amazon Relational Database Service （https://docs.aws.amazon.com/AmazonRDS/latest/gettingstartedguide/scaling-ha.html）</p><p>Resilience in Amazon RDS - Amazon Relational Database Service （https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/disaster-recovery-resiliency.html）</p><p>Comparing Amazon Aurora replication solutions - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/aurora-replication-options/compare-solutions.html）</p><p>Replication with Amazon Aurora - Amazon Aurora （https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html）</p><p>Replication with Amazon Aurora MySQL - Amazon Aurora （https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Replication.html）</p><p>How to choose the best disaster recovery option for your Amazon Aurora MySQL cluster | AWS Database Blog （https://aws.amazon.com/cn/blogs/database/how-to-choose-the-best-disaster-recovery-option-for-your-amazon-aurora-mysql-cluster/）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "6118d056b4d5461aa20c06d1eff76a04",
      "questionNumber": 77,
      "type": "single",
      "content": "<p>Question #77</p><p>Example Corp. has an on-premises data center and a VPC named VPC A in the Example Corp. AWS account. The on-premises network connects to VPC A through an AWS Site-To-Site VPN. The on-premises servers can properly access VPC A. Example Corp. just acquired AnyCompany, which has a VPC named VPC B. There is no IP address overlap among these networks. Example Corp. has peered VPC A and VPC B.</p><p><br></p><p>Example Corp. wants to connect from its on-premise servers to VPC B. Example Corp. has properly set up the network ACL and security groups.</p><p><br></p><p>Which solution will meet this requirement with the LEAST operational effort?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a transit gateway. Attach the Site-to-Site VPN, VPC A, and VPC B to the transit gateway. Update the transit gateway route tables for all networks to add IP range routes for all other networks."
        },
        {
          "label": "B",
          "content": "Create a transit gateway. Create a Site-to-Site VPN connection between the on-premises network and VPC B, and connect the VPN connection to the transit gateway. Add a route to direct traffic to the peered VPCs, and add an authorization rule to give clients access to the VPCs A and B."
        },
        {
          "label": "C",
          "content": "Update the route tables for the Site-to-Site VPN and both VPCs for all three networks. Configure BGP propagation for all three networks. Wait for up to 5 minutes for BGP propagation to finish."
        },
        {
          "label": "D",
          "content": "<p>Modify the Site-to-Site VPN’s virtual private gateway definition to include VPC A and VPC B. Split the two routers of the virtual private gateway between the two VPCs.</p>"
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>The correct answer isA. Here's why:</p><p>Scenario Requirements:</p><p>-On-premises servers need to accessVPC B (in addition to VPC A, which already works).</p><p>-VPC A andVPC B are already peered.</p><p>- No IP overlap exists.</p><p>- Network ACLs and security groups are properly configured.</p><p>Problem:</p><p>- The existingSite-to-Site VPN only connectson-premises toVPC A.</p><p>-VPC peering does NOT support transitive routing, so traffic from on-premises to VPC B won’t work by default.</p><p>Solution:</p><p>-Transit Gateway (TGW) allows transitive routing between:</p><p> &nbsp;- TheSite-to-Site VPN (on-premises)</p><p> &nbsp;-VPC A</p><p> &nbsp;-VPC B</p><p>- By attaching all three to the TGW and updating theroute tables, traffic can flow between all networks.</p><p>Why Option A?</p><p>-Least operational effort: </p><p> &nbsp;- Only requirescreating a TGW, attaching the VPN, VPC A, and VPC B, and updating routes.</p><p> &nbsp;- No need for additional VPNs (unlike Option B) or complex BGP configurations (unlike Option C).</p><p> &nbsp;- Option D is invalid because avirtual private gateway cannot be split between VPCs.</p><p>Why Not Other Options?</p><p>-B: Unnecessary to create asecond VPN connection (extra effort).</p><p>-C:BGP propagation is unnecessary and adds complexity (VPC peering doesn’t support transitive routing).</p><p>-D:Virtual private gateway cannot be shared/modified this way (incorrect approach).</p><p>Final Answer:</p><p>✅A. Create a transit gateway. Attach the Site-to-Site VPN, VPC A, and VPC B to the transit gateway. Update the transit gateway route tables for all networks to add IP range routes for all other networks.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "3f2a55423f544494ace6c425ed7fee60",
      "questionNumber": 78,
      "type": "single",
      "content": "<p>Question #78</p><p>A company recently completed the migration from an on-premises data center to the AWS Cloud by using a replatforming strategy. One of the migrated servers is running a legacy Simple Mail Transfer Protocol (SMTP) service that a critical application relies upon. The application sends outbound email messages to the company’s customers. The legacy SMTP server does not support TLS encryption and uses TCP port 25. The application can use SMTP only.</p><p><br></p><p>The company decides to use Amazon Simple Email Service (Amazon SES) and to decommission the legacy SMTP server. The company has created and validated the SES domain. The company has lifted the SES limits.</p><p><br></p><p>What should the company do to modify the application to send email messages from Amazon SES?</p>",
      "options": [
        {
          "label": "A",
          "content": "Configure the application to connect to Amazon SES by using TLS Wrapper. Create an IAM role that has ses:SendEmail and ses:SendRawEmail permissions. Attach the IAM role to an Amazon EC2 instance."
        },
        {
          "label": "B",
          "content": "Configure the application to connect to Amazon SES by using STARTTLS. Obtain Amazon SES SMTP credentials. Use the credentials to authenticate with Amazon SES."
        },
        {
          "label": "C",
          "content": "Configure the application to use the SES API to send email messages. Create an IAM role that has ses:SendEmail and ses:SendRawEmail permissions. Use the IAM role as a service role for Amazon SES."
        },
        {
          "label": "D",
          "content": "Configure the application to use AWS SDKs to send email messages. Create an IAM user for Amazon SES. Generate API access keys. Use the access keys to authenticate with Amazon SES."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>The correct answer isB. Configure the application to connect to Amazon SES by using STARTTLS. Obtain Amazon SES SMTP credentials. Use the credentials to authenticate with Amazon SES.</p><p> Explanation:</p><p>1.Legacy SMTP Constraints: The application can only use SMTP (not APIs or SDKs), and the legacy SMTP server does not support TLS encryption, using only TCP port 25. &nbsp;</p><p>2.Amazon SES Requirements: &nbsp;</p><p> &nbsp; - SES requiresTLS encryption (either viaSTARTTLS orTLS Wrapper) for SMTP connections. &nbsp;</p><p> &nbsp; - Since the application cannot be modified to use APIs/SDKs, using SES SMTP credentials is the best approach. &nbsp;</p><p>3.STARTTLS vs. TLS Wrapper: &nbsp;</p><p> &nbsp; -STARTTLS (Option B) is the correct choice because it upgrades an existing plaintext SMTP connection to an encrypted one (on port 25 or 587). &nbsp;</p><p> &nbsp; -TLS Wrapper (Option A) is incorrect because it requires a different port (typically 465) and may not be supported by the legacy application. &nbsp;</p><p>4.Authentication: &nbsp;</p><p> &nbsp; - SES SMTP credentials (username/password) must be used for authentication (not IAM roles or API keys, as SMTP does not support IAM roles directly). &nbsp;</p><p>5.Why Not Other Options? &nbsp;</p><p> &nbsp; -A: Incorrect because TLS Wrapper is not the standard method for port 25, and IAM roles cannot be used for SMTP authentication. &nbsp;</p><p> &nbsp; -C & D: Incorrect because the applicationcannot use SES APIs or SDKs—it must use SMTP. &nbsp;</p><p> Correct Steps (Option B):</p><p>1.Configure the application to connect to theAmazon SES SMTP endpoint (e.g., `email-smtp.us-east-1.amazonaws.com`). &nbsp;</p><p>2.Enable STARTTLS to encrypt the connection (port 25 or 587). &nbsp;</p><p>3.Generate SES SMTP credentials (IAM user with `ses:SendRawEmail` permissions). &nbsp;</p><p>4.Authenticate with SES using the SMTP credentials. &nbsp;</p><p>This approach ensures the legacy application works with SES while meeting security requirements. &nbsp;</p><p>Answer: B</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "5116a9c8d294461d816658ccccc8a3ad",
      "questionNumber": 79,
      "type": "single",
      "content": "<p>Question #79</p><p>A company recently acquired several other companies. Each company has a separate AWS account with a different billing and reporting method. The acquiring company has consolidated all the accounts into one organization in AWS Organizations. However, the acquiring company has found it difficult to generate a cost report that contains meaningful groups for all the teams.</p><p><br></p><p>The acquiring company’s finance team needs a solution to report on costs for all the companies through a self-managed application.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an AWS Cost and Usage Report for the organization. Define tags and cost categories in the report. Create a table in Amazon Athena. Create an Amazon QuickSight dataset based on the Athena table. Share the dataset with the finance team."
        },
        {
          "label": "B",
          "content": "Create an AWS Cost and Usage Report for the organization. Define tags and cost categories in the report. Create a specialized template in AWS Cost Explorer that the finance department will use to build reports."
        },
        {
          "label": "C",
          "content": "Create an Amazon QuickSight dataset that receives spending information from the AWS Price List Query API. Share the dataset with the finance team."
        },
        {
          "label": "D",
          "content": "Use the AWS Price List Query API to collect account spending information. Create a specialized template in AWS Cost Explorer that the finance department will use to build reports."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>The correct answer is: &nbsp;</p><p>A. Create an AWS Cost and Usage Report for the organization. Define tags and cost categories in the report. Create a table in Amazon Athena. Create an Amazon QuickSight dataset based on the Athena table. Share the dataset with the finance team. &nbsp;</p><p> Explanation: &nbsp;</p><p>1.AWS Cost and Usage Report (CUR) provides comprehensive cost and usage data for all accounts in the organization, which is necessary for consolidated reporting. &nbsp;</p><p>2.Tags and Cost Categories allow meaningful grouping of costs by teams, departments, or business units. &nbsp;</p><p>3.Amazon Athena can query the CUR data stored in Amazon S3, enabling SQL-based analysis. &nbsp;</p><p>4.Amazon QuickSight can visualize the data from Athena, and the dataset can be shared with the finance team for self-service reporting. &nbsp;</p><p> Why not the other options? &nbsp;</p><p>-B: AWS Cost Explorer does not support sharing templates, and it lacks the flexibility of a self-managed application. &nbsp;</p><p>-C: The AWS Price List Query API provides pricing information, not actual spending data. &nbsp;</p><p>-D: Again, the Price List Query API does not provide actual spending data, and Cost Explorer lacks self-service capabilities for a finance application. &nbsp;</p><p>Thus,Option A is the best solution for generating a consolidated, customizable cost report.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "5c5a2feda09d408fba173802187fbce1",
      "questionNumber": 80,
      "type": "multiple",
      "content": "<p>Question #80</p><p>A company runs an IoT platform on AWS. IoT sensors in various locations send data to the company’s Node.js API servers on Amazon EC2 instances running behind an Application Load Balancer. The data is stored in an Amazon RDS MySQL DB instance that uses a 4 TB General Purpose SSD volume.</p><p><br></p><p>The number of sensors the company has deployed in the field has increased over time, and is expected to grow significantly. The API servers are consistently overloaded and RDS metrics show high write latency.</p><p><br></p><p>Which of the following steps together will resolve the issues permanently and enable growth as new sensors are provisioned, while keeping this platform cost-efficient? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Resize the MySQL General Purpose SSD storage to 6 TB to improve the volume&rsquo;s IOPS."
        },
        {
          "label": "B",
          "content": "Re-architect the database tier to use Amazon Aurora instead of an RDS MySQL DB instance and add read replicas."
        },
        {
          "label": "C",
          "content": "Leverage Amazon Kinesis Data Streams and AWS Lambda to ingest and process the raw data."
        },
        {
          "label": "D",
          "content": "Use AWS X-Ray to analyze and debug application issues and add more API servers to match the load."
        },
        {
          "label": "E",
          "content": "Re-architect the database tier to use Amazon DynamoDB instead of an RDS MySQL DB instance."
        }
      ],
      "correctAnswer": "CE",
      "explanation": "<p>Based on the information provided, the correct answers are:</p><p>C. Leverage Amazon Kinesis Data Streams and AWS Lambda to ingest and process the raw data. </p><p>E. Re-architect the database tier to use Amazon DynamoDB instead of an RDS MySQL DB instance.</p><p>These two solutions together will resolve the issues permanently and enable growth while keeping the platform cost-efficient for the following reasons:</p><p>Leveraging Amazon Kinesis Data Streams with AWS Lambda (Option C):</p><p>This approach decouples the data ingestion process from the API servers, addressing the issue of consistently overloaded EC2 instances.</p><p>Kinesis Data Streams can handle millions of records per second, providing massive scalability for the growing number of IoT sensors.</p><p>Lambda functions can process the incoming data in parallel, eliminating bottlenecks in the current architecture.</p><p>This serverless approach scales automatically with the incoming data volume, making it ideal for the company's expected significant growth.</p><p>The pay-per-use model is cost-efficient for IoT workloads with variable traffic patterns.</p><p>Re-architecting to use Amazon DynamoDB (Option E):</p><p>DynamoDB is specifically designed for high-scale, low-latency workloads, addressing the high write latency issues currently experienced with RDS MySQL.</p><p>It provides single-digit millisecond response times regardless of scale, which is critical for IoT applications.</p><p>DynamoDB scales automatically without the need to provision or manage database infrastructure.</p><p>It eliminates the storage constraints and IOPS limitations of RDS volumes.</p><p>The on-demand capacity mode makes it cost-efficient for IoT workloads with unpredictable traffic patterns.</p><p>Option A (resizing the MySQL General Purpose SSD storage to 6 TB) is not a good long-term solution because:</p><p>It only provides a temporary fix by increasing IOPS (which are tied to volume size for GP2 volumes)</p><p>It doesn't address the fundamental architectural issues</p><p>It's not cost-efficient as the company would be paying for storage they don't need just to get more IOPS</p><p>There's still an upper limit to the IOPS available (16,000 for GP2), which will eventually be reached as the sensor count grows significantly</p><p>The combination of Kinesis Data Streams with Lambda for ingestion and processing, along with DynamoDB for storage, creates a serverless, highly scalable architecture that can handle the company's growing IoT platform needs while remaining cost-efficient through pay-per-use pricing models.</p><p>Sources</p><p>Recommendations from Amazon RDS reference - Amazon Relational Database Service （https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USERRecommendationsManage.RecommendationReference.html）</p><p>Key AWS services - IoT Lens （https://docs.aws.amazon.com/wellarchitected/latest/iot-lens/key-aws-services-perf.html）</p><p>Conclusion - Optimizing MySQL Running on Amazon EC2 Using Amazon EBS （https://docs.aws.amazon.com/whitepapers/latest/optimizing-mysql-on-ec2-using-amazon-ebs/conclusion.html）</p><p>Lambda vs Kinesis for uploading AWS IoT data to a database. | AWS re:Post （https://repost.aws/questions/QU20mw9eVYQd2UKKBUSRDLxw/lambda-vs-kinesis-for-uploading-aws-iot-data-to-a-database）</p><p>Amazon RDS DB instance storage - Amazon Relational Database Service （https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Storage.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "5de70b7724254dd4bd59a0a225d3b04c",
      "questionNumber": 81,
      "type": "multiple",
      "content": "<p>Question #81</p><p>A company is building an electronic document management system in which users upload their documents. The application stack is entirely serverless and runs on AWS in the eu-central-1 Region. The system includes a web application that uses an Amazon CloudFront distribution for delivery with Amazon S3 as the origin. The web application communicates with Amazon API Gateway Regional endpoints. The API Gateway APIs call AWS Lambda functions that store metadata in an Amazon Aurora Serverless database and put the documents into an S3 bucket.</p><p><br></p><p>The company is growing steadily and has completed a proof of concept with its largest customer. The company must improve latency outside of Europe.</p><p><br></p><p>Which combination of actions will meet these requirements? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Enable S3 Transfer Acceleration on the S3 bucket. Ensure that the web application uses the Transfer Acceleration signed URLs."
        },
        {
          "label": "B",
          "content": "Create an accelerator in AWS Global Accelerator. Attach the accelerator to the CloudFront distribution."
        },
        {
          "label": "C",
          "content": "Change the API Gateway Regional endpoints to edge-optimized endpoints."
        },
        {
          "label": "D",
          "content": "Provision the entire stack in two other locations that are spread across the world. Use global databases on the Aurora Serverless cluster."
        },
        {
          "label": "E",
          "content": "Add an Amazon RDS proxy between the Lambda functions and the Aurora Serverless database."
        }
      ],
      "correctAnswer": "AC",
      "explanation": "<p>Based on the information provided, the correct answers are:</p><p>A. Enable S3 Transfer Acceleration on the S3 bucket. Ensure that the web application uses the Transfer Acceleration signed URLs. </p><p>C. Change the API Gateway Regional endpoints to edge-optimized endpoints.</p><p>These two solutions together will effectively improve latency for users outside of Europe for the following reasons:</p><p>Enable S3 Transfer Acceleration (Option A):</p><p>S3 Transfer Acceleration is specifically designed to optimize the upload of data to S3 buckets over long distances.</p><p>It uses Amazon's global network of edge locations to route data through optimized network paths.</p><p>For users outside of Europe uploading documents to the eu-central-1 bucket, this will significantly improve upload speeds.</p><p>The document management system requires users to upload documents, making this feature particularly relevant.</p><p>Using Transfer Acceleration signed URLs ensures that the web application properly leverages this optimization.</p><p>Change API Gateway Regional endpoints to edge-optimized endpoints (Option C):</p><p>Edge-optimized endpoints are specifically designed for geographically dispersed clients accessing APIs from various locations around the world.</p><p>When using edge-optimized endpoints, API requests are routed through CloudFront edge locations closest to the users, reducing latency.</p><p>This is particularly beneficial for users outside of Europe who would otherwise need to connect directly to the eu-central-1 Region.</p><p>The API Gateway is a critical component of the application architecture, handling communication between the web application and Lambda functions.</p><p>Optimizing this component will improve overall application responsiveness for global users.</p><p>Together, these two changes address both the upload path (S3 Transfer Acceleration) and the API request path (edge-optimized endpoints) of the application, providing comprehensive latency improvements for users outside of Europe.</p><p>These solutions are also aligned with the serverless architecture of the application, maintaining its scalability and operational efficiency while enhancing global performance. They don't require significant architectural changes or the complexity of multi-region deployments, making them relatively straightforward to implement while still providing substantial latency improvements.</p><p>Sources</p><p>Multi-region API Gateway with CloudFront - Multi-region API Gateway with CloudFront （https://docs.aws.amazon.com/architecture-diagrams/latest/multi-region-api-gateway-with-cloudfront/multi-region-api-gateway-with-cloudfront.html）</p><p>PERF04-BP02 Evaluate available networking features - AWS Well-Architected Framework （https://docs.aws.amazon.com/wellarchitected/2024-06-27/framework/perf_networking_evaluate_networking_features.html）</p><p>Amazon API Gateway - Serverless Applications Lens （https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/amazon-api-gateway.html）</p><p>Selection - Serverless Applications Lens （https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/selection.html）</p><p>PERF04-BP02 Evaluate available networking features - AWS Well-Architected Framework （https://docs.aws.amazon.com/wellarchitected/2023-10-03/framework/perf_networking_evaluate_networking_features.html）</p><p>Using Amazon S3 Transfer Acceleration to Accelerate Geographically Disparate Data Transfers - Best Practices Design Patterns: Optimizing Amazon S3 Performance （https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "92885eb22cee4354951baba815cef565",
      "questionNumber": 82,
      "type": "single",
      "content": "<p>Question #82</p><p>An adventure company has launched a new feature on its mobile app. Users can use the feature to upload their hiking and rafting photos and videos anytime. The photos and videos are stored in Amazon S3 Standard storage in an S3 bucket and are served through Amazon CloudFront.</p><p><br></p><p>The company needs to optimize the cost of the storage. A solutions architect discovers that most of the uploaded photos and videos are accessed infrequently after 30 days. However, some of the uploaded photos and videos are accessed frequently after 30 days. The solutions architect needs to implement a solution that maintains millisecond retrieval availability of the photos and videos at the lowest possible cost.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Configure S3 Intelligent-Tiering on the S3 bucket."
        },
        {
          "label": "B",
          "content": "Configure an S3 Lifecycle policy to transition image objects and video objects from S3 Standard to S3 Glacier Deep Archive after 30 days."
        },
        {
          "label": "C",
          "content": "Replace Amazon S3 with an Amazon Elastic File System (Amazon EFS) file system that is mounted on Amazon EC2 instances."
        },
        {
          "label": "D",
          "content": "Add a Cache-Control: max-age header to the S3 image objects and S3 video objects. Set the header to 30 days."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Configure S3 Intelligent-Tiering on the S3 bucket.</p><p>This solution will meet the requirements for the following reasons:</p><p>Mixed Access Patterns: The scenario states that \"most of the uploaded photos and videos are accessed infrequently after 30 days. However, some of the uploaded photos and videos are accessed frequently after 30 days.\" S3 Intelligent-Tiering is specifically designed for this type of mixed or unpredictable access pattern.</p><p>Automatic Optimization: S3 Intelligent-Tiering automatically moves objects between two access tiers based on changing access patterns:</p><p>Frequent Access tier for objects that are accessed regularly</p><p>Infrequent Access tier for objects that haven't been accessed for 30 consecutive days</p><p>Millisecond Retrieval: Both tiers in S3 Intelligent-Tiering provide millisecond retrieval times, meeting the requirement to maintain \"millisecond retrieval availability.\"</p><p>Cost Optimization: S3 Intelligent-Tiering can save up to 40% on storage costs for objects that become infrequently accessed, while automatically moving objects back to the Frequent Access tier if they're accessed again, with no retrieval fees.</p><p>No Performance Impact: There is no performance degradation or change in latency when objects move between tiers, ensuring consistent user experience regardless of how frequently the photos and videos are accessed.</p><p>Option D (adding a Cache-Control: max-age header set to 30 days) would not effectively address the storage cost optimization requirement because:</p><p>Cache-Control headers primarily affect how content is cached by browsers and CDNs like CloudFront, not how the data is stored in S3.</p><p>While caching can reduce the number of requests to S3 (potentially reducing request costs), it doesn't change the underlying storage costs in S3.</p><p>Cache-Control headers don't move objects between storage tiers based on access patterns.</p><p>This approach wouldn't distinguish between frequently and infrequently accessed content after 30 days, which is a key requirement in this scenario.</p><p>S3 Intelligent-Tiering is the most appropriate solution as it automatically optimizes storage costs based on actual access patterns while maintaining the required millisecond retrieval availability for all objects.</p><p>Sources</p><p>Managing storage costs with Amazon S3 Intelligent-Tiering - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/intelligent-tiering.html）</p><p>Amazon S3 Intelligent-Tiering Storage Class | AWS （https://aws.amazon.com/cn/s3/storage-classes/intelligent-tiering/）</p><p>Understanding and managing Amazon S3 storage classes - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html）</p><p>How to Optimize AWS S3 Storage Costs? | AWS re:Post （https://repost.aws/questions/QU97-ATukcRz-XashRXJSkHA/how-to-optimize-aws-s3-storage-costs）</p><p>How Rivian optimized cloud storage for its ADAS and Product Engineering workloads on AWS | AWS for Industries （https://aws.amazon.com/cn/blogs/industries/how-rivian-optimized-cloud-storage-for-its-adas-and-product-engineering-workloads-on-aws/）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "9d69e908a85643938b227e1bdb9bb29c",
      "questionNumber": 83,
      "type": "single",
      "content": "<p>Question #83</p><p>A company uses Amazon S3 to store files and images in a variety of storage classes. The company's S3 costs have increased substantially during the past year. </p><p><br></p><p>A solutions architect needs to review data trends for the past 12 months and identify the appropriate storage class for the objects. </p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Download AWS Cost and Usage Reports for the last 12 months of S3 usage. Review AWS Trusted Advisor recommendations for cost savings."
        },
        {
          "label": "B",
          "content": "Use S3 storage class analysis. Import data trends into an Amazon QuickSight dashboard to analyze storage trends."
        },
        {
          "label": "C",
          "content": "Use Amazon S3 Storage Lens. Upgrade the default dashboard to include advanced metrics for storage trends."
        },
        {
          "label": "D",
          "content": "Use Access Analyzer for S3. Download the Access Analyzer for S3 report for the last 12 months. Import the .csv file to an Amazon QuickSight dashboard."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Use Amazon S3 Storage Lens. Upgrade the default dashboard to include advanced metrics for storage trends.</p><p>This solution will meet the requirements for the following reasons:</p><p>Amazon S3 Storage Lens is specifically designed for analyzing S3 storage usage, activity trends, and providing optimization recommendations. It's purpose-built for exactly the kind of analysis the company needs to perform.</p><p>By upgrading to the advanced metrics in S3 Storage Lens, the company gains access to 15 months of historical data, which exceeds the required 12-month analysis period mentioned in the scenario.</p><p>S3 Storage Lens provides contextual recommendations for cost optimization, including suggestions for appropriate storage classes based on actual usage patterns. This directly addresses the requirement to \"identify the appropriate storage class for the objects.\"</p><p>The solution includes built-in interactive dashboards in the S3 console, making it easy for the solutions architect to review and analyze the data without requiring additional setup or integration with other services.</p><p>S3 Storage Lens allows for detailed analysis at the bucket, prefix, and account levels, providing comprehensive visibility into storage patterns across the company's entire S3 environment.</p><p>Option A (AWS Cost and Usage Reports with Trusted Advisor) would provide cost data but lacks the specific storage pattern analysis needed to make informed storage class decisions. While it shows what you're spending, it doesn't provide the same level of insight into object access patterns that would inform storage class decisions.</p><p>Option B (S3 storage class analysis with QuickSight) would require additional setup and integration with QuickSight. While it can provide useful insights, it's not as comprehensive as S3 Storage Lens for organization-wide storage analysis and doesn't include the built-in recommendations for storage class optimization.</p><p>Amazon S3 Storage Lens with advanced metrics is the most direct and comprehensive solution for analyzing 12 months of S3 storage trends and identifying appropriate storage classes to optimize costs.</p><p>Sources</p><p>Analyze access patterns and use the most cost-effective Amazon S3 storage class | AWS Storage Blog（https://aws.amazon.com/cn/blogs/storage/analyze-access-patterns-and-use-the-most-cost-effective-amazon-s3-storage-class/） </p><p>AWS Tools for Reporting and Cost Optimization - Laying the Foundation: Setting Up Your Environment for Cost Optimization （https://aws.amazon.com/cn/aws-cost-management/cost-optimization/）</p><p>S3 Storage Analytics and Insights – Amazon S3 （https://aws.amazon.com/cn/s3/storage-analytics-insights/）</p><p>Assessing your storage activity and usage with Amazon S3 Storage Lens - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage_lens.html）</p><p>Achieve cost effective cloud operations with AWS Managed Services | AWS Cloud Operations Blog （https://aws.amazon.com/cn/blogs/mt/achieve-cost-effective-cloud-operations-with-aws-managed-services/）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "9c336dddcac7447582c83e7ceca2d7b6",
      "questionNumber": 84,
      "type": "single",
      "content": "<p>Question #84</p><p>A company has its cloud infrastructure on AWS. A solutions architect needs to define the infrastructure as code. The infrastructure is currently deployed in one AWS Region. The company’s business expansion plan includes deployments in multiple Regions across multiple AWS accounts. </p><p><br></p><p>What should the solutions architect do to meet these requirements？</p>",
      "options": [
        {
          "label": "A",
          "content": "Use AWS CloudFormation templates. Add IAM policies to control the various accounts, Deploy the templates across the multiple Regions."
        },
        {
          "label": "B",
          "content": "Use AWS Organizations. Deploy AWS CloudFormation templates from the management account. Use AWS Control Tower to manage deployments across accounts."
        },
        {
          "label": "C",
          "content": "Use AWS Organizations and AWS CloudFormation StackSets. Deploy a CloudFormation template from an account that has the necessary IAM permissions."
        },
        {
          "label": "D",
          "content": "Use nested stacks with AWS CloudFormation templates. Change the Region by using nested stacks."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Use AWS Organizations and AWS CloudFormation StackSets. Deploy a CloudFormation template from an account that has the necessary IAM permissions.</p><p>This solution is the most appropriate for defining infrastructure as code that can be deployed across multiple regions and AWS accounts because:</p><p>AWS Organizations provides a way to centrally manage and govern multiple AWS accounts. It allows the company to organize accounts into organizational units (OUs) and apply policies across them, which is essential for managing a multi-account environment.</p><p>AWS CloudFormation StackSets extends the capability of CloudFormation by enabling you to create, update, or delete stacks across multiple accounts and regions with a single operation. This is exactly what's needed for the company's business expansion plan that includes deployments in multiple regions across multiple AWS accounts.</p><p>Deploying from an account with the necessary IAM permissions follows the principle of least privilege and establishes proper security controls. This approach allows for centralized management while maintaining appropriate access controls.</p><p>This combination provides the best scalability for multi-account, multi-region deployments with the lowest management complexity compared to the other options.</p><p>Option A (CloudFormation templates with IAM policies) would require managing separate deployments for each region and account, leading to higher management complexity and potential inconsistencies.</p><p>Option B (Organizations with Control Tower) is more focused on account governance and management rather than efficient multi-region deployments. While Control Tower is useful for establishing baseline configurations, it's not the most efficient tool for deploying the same infrastructure across multiple regions and accounts.</p><p>Option D (Nested stacks) is primarily designed for organizing complex CloudFormation templates within a single deployment, not for managing deployments across multiple accounts and regions. Changing regions using nested stacks is not a standard or recommended approach.</p><p>AWS Organizations combined with CloudFormation StackSets is the AWS-recommended solution for deploying infrastructure as code across multiple accounts and regions, making it the most suitable choice for the company's expansion plans.</p><p>Sources</p><p>Using AWS CloudFormation as an IaC tool - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/choose-iac-tool/cloudformation.html）</p><p>Stack vs StackSets deployment - Automated Security Response on AWS （https://docs.aws.amazon.com/solutions/latest/automated-security-response-on-aws/stack-vs-stacksets-deployment.html）</p><p>Managing stacks across accounts and Regions with StackSets - AWS CloudFormation （https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html）</p><p>Infrastructure as Code - Developing and Deploying .NET Applications on AWS （https://docs.aws.amazon.com/whitepapers/latest/develop-deploy-dotnet-apps-on-aws/infrastructure-as-code.html）</p><p>AWS CloudFormation StackSets and AWS Organizations - AWS Organizations（https://docs.aws.amazon.com/organizations/latest/userguide/services-that-can-integrate-cloudformation.html） </p><p>How SMBs can deploy a multi-account environment quickly using AWS Organizations and AWS CloudFormation StackSets | AWS Cloud Operations Blog （https://aws.amazon.com/cn/blogs/mt/how-smbs-can-deploy-a-multi-account-environment-quickly-using-aws-organizations-and-aws-cloudformation-stacksets/）</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "78c539af2f1749c899ed719eb7ff6704",
      "questionNumber": 85,
      "type": "single",
      "content": "<p>Question #85</p><p>A company has its cloud infrastructure on AWS. A solutions architect needs to define the infrastructure as code. The infrastructure is currently deployed in one AWS Region. The company’s business expansion plan includes deployments in multiple Regions across multiple AWS accounts. <br><br>What should the solutions architect do to meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use AWS CloudFormation templates. Add IAM policies to control the various accounts, Deploy the templates across the multiple Regions."
        },
        {
          "label": "B",
          "content": "Use AWS Organizations. Deploy AWS CloudFormation templates from the management account. Use AWS Control Tower to manage deployments across accounts."
        },
        {
          "label": "C",
          "content": "Use AWS Organizations and AWS CloudFormation StackSets. Deploy a CloudFormation template from an account that has the necessary IAM permissions."
        },
        {
          "label": "D",
          "content": "Use nested stacks with AWS CloudFormation templates. Change the Region by using nested stacks."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the information available, the correct answer is:</p><p>C. Use AWS Organizations and AWS CloudFormation StackSets. Deploy a CloudFormation template from an account that has the necessary IAM permissions.</p><p>This solution is the most appropriate for defining infrastructure as code that can be deployed across multiple regions and AWS accounts because:</p><p>AWS Organizations provides a way to centrally manage and govern multiple AWS accounts. It allows the company to organize accounts into organizational units (OUs) and apply policies across them, which is essential for managing a multi-account environment.</p><p>AWS CloudFormation StackSets extends the capability of CloudFormation by enabling you to create, update, or delete stacks across multiple accounts and regions with a single operation. This is exactly what's needed for the company's business expansion plan that includes deployments in multiple regions across multiple AWS accounts.</p><p>Key advantages of using CloudFormation StackSets include:</p><p>Centralized management of resources across multiple accounts and regions</p><p>Ability to deploy the same infrastructure consistently across the organization</p><p>Simplified updates that propagate changes across all accounts and regions</p><p>Integration with AWS Organizations for automated deployment to member accounts</p><p>Deploying from an account with the necessary IAM permissions follows the principle of least privilege and establishes proper security controls.</p><p>Option A (CloudFormation templates with IAM policies) would require managing separate deployments for each region and account, leading to higher management complexity and potential inconsistencies.</p><p>Option B (Organizations with Control Tower) is more focused on account governance and management rather than efficient multi-region deployments. While Control Tower is useful for establishing baseline configurations, it's not the most efficient tool for deploying the same infrastructure across multiple regions and accounts.</p><p>Option D (Nested stacks) is primarily designed for organizing complex CloudFormation templates within a single deployment, not for managing deployments across multiple accounts and regions. Changing regions using nested stacks is not a standard or recommended approach.</p><p>AWS Organizations combined with CloudFormation StackSets is the AWS-recommended solution for deploying infrastructure as code across multiple accounts and regions, making it the most suitable choice for the company's expansion plans.</p><p>Sources</p><p>Deploy CloudFormation Hooks to an Organization with service-managed StackSets | AWS DevOps Blog （https://aws.amazon.com/cn/blogs/devops/deploy-cloudformation-hooks-to-an-organization-with-service-managed-stacksets/）</p><p>Managing stacks across accounts and Regions with StackSets - AWS CloudFormation （https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html）</p><p>Stack vs StackSets deployment - Automated Security Response on AWS （https://docs.aws.amazon.com/solutions/latest/automated-security-response-on-aws/stack-vs-stacksets-deployment.html）</p><p>Using AWS CloudFormation StackSets - AWS Service Catalog （https://docs.aws.amazon.com/servicecatalog/latest/adminguide/using-stacksets.html）</p><p>AWS CloudFormation StackSets and AWS Organizations - AWS Organizations （https://docs.aws.amazon.com/organizations/latest/userguide/services-that-can-integrate-cloudformation.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "ac7b21481812481b955b93a95abd2460",
      "questionNumber": 86,
      "type": "single",
      "content": "<p>Question #86</p><p>A company plans to refactor a monolithic application into a modern application design deployed on AWS. The CI/CD pipeline needs to be upgraded to support the modern design for the application with the following requirements:</p><p><br></p><p>• It should allow changes to be released several times every hour.</p><p>• It should be able to roll back the changes as quickly as possible.</p><p><br></p><p>Which design will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Deploy a CI/CD pipeline that incorporates AMIs to contain the application and their configurations. Deploy the application by replacing Amazon EC2 instances."
        },
        {
          "label": "B",
          "content": "Specify AWS Elastic Beanstalk to stage in a secondary environment as the deployment target for the CI/CD pipeline of the application. To deploy, swap the staging and production environment URLs."
        },
        {
          "label": "C",
          "content": "Use AWS Systems Manager to re-provision the infrastructure for each deployment. Update the Amazon EC2 user data to pull the latest code artifact from Amazon S3 and use Amazon Route 53 weighted routing to point to the new environment."
        },
        {
          "label": "D",
          "content": "Roll out the application updates as part of an Auto Scaling event using prebuilt AMIs. Use new versions of the AMIs to add instances and phase out all instances that use the previous AMI version with the configured termination policy during a deployment event."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the requirements for a CI/CD pipeline that supports releasing changes several times every hour and rolling back changes as quickly as possible, the answer is:</p><p>B. Specify AWS Elastic Beanstalk to stage in a secondary environment as the deployment target for the CI/CD pipeline of the application. To deploy, swap the staging and production environment URLs.</p><p>This is the correct choice because:</p><p>Elastic Beanstalk with environment URL swapping enables faster deployments, which is essential for releasing changes several times per hour.</p><p>The URL swapping approach provides an extremely quick rollback mechanism - if an issue is detected, you can simply swap the URLs back to the previous environment, which happens almost instantly.</p><p>This blue/green deployment approach with Elastic Beanstalk allows for zero-downtime deployments, as the new version is fully deployed and tested in the staging environment before becoming production.</p><p>The process is highly automated, reducing the potential for human error during frequent releases.</p><p>Option A (using AMIs with EC2 instance replacement) would be slower for both deployments and rollbacks because:</p><p>Creating and distributing AMIs is more time-consuming</p><p>Rolling back requires launching new EC2 instances from previous AMIs, which takes longer</p><p>This approach wouldn't support the requirement of releasing changes several times per hour as efficiently</p><p>Sources</p><p>Build - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/strategy-devops-aws-mainframe-modernization/build.html）</p><p>Deploy to Elastic Beanstalk with GitHub Actions | .NET on AWS Blog （https://aws.amazon.com/cn/blogs/dotnet/deploy-to-elastic-beanstalk-environment-with-github-actions/）</p><p>AWS Elastic Beanstalk - Developing and Deploying .NET Applications on AWS（https://docs.aws.amazon.com/whitepapers/latest/develop-deploy-dotnet-apps-on-aws/aws-elastic-beanstalk.html） </p><p>Deployment policies and settings - AWS Elastic Beanstalk （https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html）</p><p>Stage 2: Design and implement - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/startup-resiliency-baseline/stage-2.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "f26491e8877f4a40b59dad4fa453c3a3",
      "questionNumber": 87,
      "type": "multiple",
      "content": "<p>Question #87</p><p>A company has an application that runs on Amazon EC2 instances. A solutions architect is designing VPC infrastructure in an AWS Region where the application needs to access an Amazon Aurora DB Cluster. The EC2 instances are all associated with the same security group. The DB cluster is associated with its own security group.</p><p>The solutions architect needs to add rules to the security groups to provide the application with least privilege access to the DB Cluster.</p><p>Which combination of steps will meet these requirements (Choose two.)?</p>",
      "options": [
        {
          "label": "A",
          "content": "Add an inbound rule to the EC2 instances&#39; security group. Specify the DB cluster&#39;s security group as the source over the default Aurora port."
        },
        {
          "label": "B",
          "content": "Add an outbound rule to the EC2 instances&#39; security group. Specify the DB cluster&#39;s security group as the destination over the default Aurora port."
        },
        {
          "label": "C",
          "content": "Add an inbound rule to the DB cluster&#39;s security group. Specify the EC2 instances&#39; security group as the source over the default Aurora port."
        },
        {
          "label": "D",
          "content": "Add an outbound rule to the DB cluster&#39;s security group. Specify the EC2 instances&#39; security group as the destination over the default Aurora port."
        },
        {
          "label": "E",
          "content": "Add an outbound rule to the DB cluster&#39;s security group. Specify the EC2 instances&#39; security group as the destination over the ephemeral ports."
        }
      ],
      "correctAnswer": "BC",
      "explanation": "<p>Based on the requirements for configuring security groups to provide least privilege access from EC2 instances to an Aurora DB cluster, the correct answers are:</p><p>B. Add an outbound rule to the EC2 instances' security group. Specify the DB cluster's security group as the destination over the default Aurora port.</p><p>C. Add an inbound rule to the DB cluster's security group. Specify the EC2 instances' security group as the source over the default Aurora port.</p><p>These two steps work together to create a secure connection between the EC2 instances and the Aurora DB cluster:</p><p>The outbound rule in the EC2 security group (option B) allows traffic to leave the EC2 instances and reach the DB cluster, but only on the specific Aurora port (typically 3306 for MySQL or 5432 for PostgreSQL).</p><p>The inbound rule in the DB cluster's security group (option C) allows traffic to enter the DB cluster, but only if it's coming from the EC2 instances' security group and only on the specific Aurora port.</p><p>This configuration follows the principle of least privilege by:</p><p>Restricting communication to only the necessary port</p><p>Using security group references instead of broad IP ranges</p><p>Creating a precise, bidirectional security group relationship that only permits the required traffic flow</p><p>Ensuring that only the EC2 instances in the specified security group can access the DB cluster</p><p>This approach is more secure than using CIDR blocks or allowing all traffic, as it dynamically updates when instances are added or removed from the security groups.</p><p>Sources</p><p>Automatically connecting an EC2 instance and an Aurora DB cluster - Amazon Aurora （https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/ec2-rds-connect.html）</p><p>Scenarios for accessing a DB cluster in a VPC - Amazon Aurora （https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/USER_VPC.Scenarios.html）</p><p>Controlling access with security groups - Amazon Relational Database Service （https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.RDSSecurityGroups.html）</p><p>Creating an Amazon Aurora DB cluster - Amazon Aurora （https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.CreateInstance.html）</p><p>Best Practice- Security Group architecture | AWS re:Post （https://repost.aws/questions/QUBp50iDPqSju0TINNsO8-fw/best-practice-security-group-architecture）</p><p>How to restrict Aurora db access to EC2 instances only | AWS re:Post （https://repost.aws/questions/QU9gGOwqiiSu-8H3zBRlpcbA/how-to-restrict-aurora-db-access-to-ec2-instances-only）</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "cc751ac2600d4d5997c1e6467a06ed4e",
      "questionNumber": 88,
      "type": "single",
      "content": "<p>Question #88</p><p>A company wants to change its internal cloud billing strategy for each of its business units. Currently, the cloud governance team shares reports for overall cloud spending with the head of each business unit. The company uses AWS Organizations to manage the separate AWS accounts for each business unit. The existing tagging standard in Organizations includes the application, environment, and owner. The cloud governance team wants a centralized solution so each business unit receives monthly reports on its cloud spending. The solution should also send notifications for any cloud spending that exceeds a set threshold.</p><p>Which solution is the MOST cost-effective way to meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Configure AWS Budgets in each account and configure budget alerts that are grouped by application, environment, and owner. Add each business unit to an Amazon SNS topic for each alert. Use Cost Explorer in each account to create monthly reports for each business unit."
        },
        {
          "label": "B",
          "content": "Configure AWS Budgets in the organization&#39;s management account and configure budget alerts that are grouped by application, environment, and owner. Add each business unit to an Amazon SNS topic for each alert. Use Cost Explorer in the organization&#39;s management account to create monthly reports for each business unit."
        },
        {
          "label": "C",
          "content": "Configure AWS Budgets in each account and configure budget alerts that are grouped by application, environment, and owner. Add each business unit to an Amazon SNS topic for each alert. Use the AWS Billing and Cost Management dashboard in each account to create monthly reports for each business unit."
        },
        {
          "label": "D",
          "content": "Enable AWS Cost and Usage Reports in the organization&#39;s management account and configure reports grouped by application, environment, and owner. Create an AWS Lambda function that processes AWS Cost and Usage Reports, sends budget alerts, and sends monthly reports to each business unit&#39;s email list."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>TheMOST cost-effective solution that meets the requirements is: &nbsp;</p><p>Answer: B &nbsp;</p><p>Configure AWS Budgets in the organization's management account and configure budget alerts that are grouped by application, environment, and owner. Add each business unit to an Amazon SNS topic for each alert. Use Cost Explorer in the organization's management account to create monthly reports for each business unit. &nbsp;</p><p>Explanation: &nbsp;</p><p>1.Centralized AWS Budgets in the management account allows monitoring and alerting across all linked accounts in AWS Organizations, avoiding duplication of budgets in each account. &nbsp;</p><p>2.Grouping by tags (application, environment, owner) ensures spending is tracked per business unit. &nbsp;</p><p>3.Amazon SNS notifications can be set up for budget threshold alerts. &nbsp;</p><p>4.Cost Explorer in the management account provides consolidated cost reporting with filtering by tags, eliminating the need for manual report generation in each account. &nbsp;</p><p>Why Not Other Options? &nbsp;</p><p>-A & C: Require configuring AWS Budgets ineach account, leading to duplication and higher management overhead. &nbsp;</p><p>-D: While possible, it introduces unnecessary complexity with Lambda processing instead of using native AWS Budgets and Cost Explorer features. &nbsp;</p><p>Thus,Option B is the most cost-effective and efficient solution.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "c5f616fb09a6417799739d34d0181a93",
      "questionNumber": 89,
      "type": "single",
      "content": "<p>Question #89</p><p>A company is using AWS CloudFormation to deploy its infrastructure. The company is concerned that, if a production CloudFormation stack is deleted, important data stored in Amazon RDS databases or Amazon EBS volumes might also be deleted.</p><p>How can the company prevent users from accidentally deleting data in this way?</p>",
      "options": [
        {
          "label": "A",
          "content": "Modify the CloudFormation templates to add a DeletionPolicy attribute to RDS and EBS resources."
        },
        {
          "label": "B",
          "content": "Configure a stack policy that disallows the deletion of RDS and EBS resources."
        },
        {
          "label": "C",
          "content": "Modify IAM policies to deny deleting RDS and EBS resources that are tagged with an &quot;aws:cloudformation:stack-name&quot; tag."
        },
        {
          "label": "D",
          "content": "Use AWS Config rules to prevent deleting RDS and EBS resources."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Based on the requirements to prevent accidental deletion of important data in RDS databases or EBS volumes when a CloudFormation stack is deleted, the answer is:</p><p>A. Modify the CloudFormation templates to add a DeletionPolicy attribute to RDS and EBS resources.</p><p>This is the correct choice because:</p><p>The DeletionPolicy attribute is specifically designed to control what happens to resources when they are removed from a CloudFormation stack or when the stack is deleted.</p><p>By setting DeletionPolicy to \"Retain\" for critical resources like RDS databases and EBS volumes, these resources will remain intact even if the CloudFormation stack is deleted.</p><p>Alternatively, setting DeletionPolicy to \"Snapshot\" would create a final snapshot before deletion, preserving the data.</p><p>This approach directly addresses the company's concern about data loss during stack deletion.</p><p>The other options are less suitable because:</p><p>Option B (stack policy): Stack policies control what update actions are allowed on resources within a stack, but they don't prevent the entire stack from being deleted.</p><p>Option C (IAM policies): While IAM policies can restrict deletion actions, they would need to be carefully crafted and maintained, and might not provide comprehensive protection against stack deletions.</p><p>Option D (AWS Config rules): AWS Config rules can monitor compliance but don't directly prevent deletion actions during CloudFormation stack operations.</p><p>Sources</p><p>AWS::RDS::DBInstance - AWS CloudFormation （https://docs.aws.amazon.com/AWSCloudFormation/latest/TemplateReference/aws-resource-rds-dbinstance.html）</p><p>DeletionPolicy attribute - AWS CloudFormation （https://docs.aws.amazon.com/AWSCloudFormation/latest/TemplateReference/aws-attribute-deletionpolicy.html）</p><p>Protect CloudFormation stacks from being deleted - AWS CloudFormation （https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-protect-stacks.html）</p><p>UpdateReplacePolicy attribute - AWS CloudFormation （https://docs.aws.amazon.com/AWSCloudFormation/latest/TemplateReference/aws-attribute-updatereplacepolicy.html）</p><p>Amazon Relational Database Service (Amazon RDS) controls - AWS Control Tower （https://docs.aws.amazon.com/controltower/latest/controlreference/rds-rules.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "68695f039209404293454ae264136447",
      "questionNumber": 90,
      "type": "single",
      "content": "<p>Question #90</p><p>A company has VPC flow logs enabled for its NAT gateway. The company is seeing Action = ACCEPT for inbound traffic that comes from a public IP address 198.51.100.2 destined for a private Amazon EC2 instance.</p><p>The company wants to determine whether the traffic represents unsolicited inbound connections from the internet. The first two octets of the VPC CIDR block are 203.0.</p><p><br></p><p>Which set of steps should the solutions architect take to meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Open the AWS CloudTrail console. Select the log group that contains the NAT gateway&#39;s elastic network interface and the private instance&#39;s elastic network interface. Run a query to filter with the destination address set as &quot;like 203.0&quot; and the source address set as &quot;like 198.51.100.2&quot;. Run the stats command to filter the sum of bytes transferred by the source address and the destination address."
        },
        {
          "label": "B",
          "content": "Open the Amazon CloudWatch console. Select the log group that contains the NAT gateway&#39;s elastic network interface and the private instance&#39;s elastic network interface. Run a query to filter with the destination address set as &quot;like 203.0&quot; and the source address set as &quot;like 198.51.100.2&quot;. Run the stats command to filter the sum of bytes transferred by the source address and the destination address."
        },
        {
          "label": "C",
          "content": "Open the AWS CloudTrail console. Select the log group that contains the NAT gateway&#39;s elastic network interface and the private instance&rsquo;s elastic network interface. Run a query to filter with the destination address set as &quot;like 198.51.100.2&quot; and the source address set as &quot;like 203.0&quot;. Run the stats command to filter the sum of bytes transferred by the source address and the destination address."
        },
        {
          "label": "D",
          "content": "Open the Amazon CloudWatch console. Select the log group that contains the NAT gateway&#39;s elastic network interface and the private instance&#39;s elastic network interface. Run a query to filter with the destination address set as &quot;like 198.51.100.2&quot; and the source address set as &quot;like 203.0&quot;. Run the stats command to filter the sum of bytes transferred by the source address and the destination address."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>The correct answer isB. &nbsp;</p><p> Explanation: &nbsp;</p><p>The question involves analyzingVPC Flow Logs to determine if inbound traffic from a public IP (`198.51.100.2`) to a private EC2 instance (with a CIDR starting with `203.0`) represents unsolicited connections. &nbsp;</p><p>1.VPC Flow Logs are stored in Amazon CloudWatch Logs, not AWS CloudTrail (which is for API activity logging). So, options A and C (which suggest using CloudTrail) are incorrect. &nbsp;</p><p>2. The goal is to check if the private instance (`203.0.x.x`) initiated outbound traffic to `198.51.100.2` before receiving inbound traffic (indicating a response rather than unsolicited traffic). &nbsp;</p><p> &nbsp; - The correct filter should look for: &nbsp;</p><p> &nbsp; &nbsp; -Source IP (`srcaddr`) like `203.0` (outbound traffic from the private instance). &nbsp;</p><p> &nbsp; &nbsp; -Destination IP (`dstaddr`) like `198.51.100.2` (the public IP). &nbsp;</p><p> &nbsp; - If no such traffic exists, the inbound `ACCEPT` traffic from `198.51.100.2` is likely unsolicited. &nbsp;</p><p>3. OptionB correctly usesCloudWatch Logs and filters for: &nbsp;</p><p> &nbsp; - `dstaddr` (destination) like `203.0` (private instance). &nbsp;</p><p> &nbsp; - `srcaddr` (source) like `198.51.100.2` (public IP). &nbsp;</p><p> &nbsp; - This helps identify if the private instance sent traffic to the public IP first. &nbsp;</p><p>Option D is incorrect because it reverses the source and destination filters, looking for traffic where `198.51.100.2` is the destination (which is not relevant here). &nbsp;</p><p>Correct Answer: B</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "b4404f1630f545698f430f988d866cd9",
      "questionNumber": 91,
      "type": "single",
      "content": "<p>Question #91</p><p>A company consists of two separate business units. Each business unit has its own AWS account within a single organization in AWS Organizations. The business units regularly share sensitive documents with each other. To facilitate sharing, the company created an Amazon S3 bucket in each account and configured low-way replication between the S3 buckets. The S3 buckets have millions of objects.</p><p><br></p><p>Recently, a security audit identified that neither S3 bucket has encryption at rest enabled. Company policy requires that all documents must be stored with encryption at rest. The company wants to implement server-side encryption with Amazon S3 managed encryption keys (SSE-S3).</p><p><br></p><p>What is the MOST operationally efficient solution that meets these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Turn on SSE-S3 on both S3 buckets. Use S3 Batch Operations to copy and encrypt the objects in the same location."
        },
        {
          "label": "B",
          "content": "Create an AWS Key Management Service (AWS KMS) key in each account. Turn on server-side encryption with AWS KMS keys (SSE-KMS) on each S3 bucket by using the corresponding KMS key in that AWS account. Encrypt the existing objects by using an S3 copy command in the AWS CLI."
        },
        {
          "label": "C",
          "content": "Turn on SSE-S3 on both S3 buckets. Encrypt the existing objects by using an S3 copy command in the AWS CLI."
        },
        {
          "label": "D",
          "content": "Create an AWS Key Management Service (AWS KMS) key in each account. Turn on server-side encryption with AWS KMS keys (SSE-KMS) on each S3 bucket by using the corresponding KMS key in that AWS account. Use S3 Batch Operations to copy the objects into the same location."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>TheMOST operationally efficient solution that meets the company's requirements is:</p><p>A. Turn on SSE-S3 on both S3 buckets. Use S3 Batch Operations to copy and encrypt the objects in the same location.</p><p>1.SSE-S3 is required by company policy, and this option directly enables it.</p><p>2.S3 Batch Operations is the most efficient way to encrypt millions of objects at scale, as it automates the copying process without manual scripting.</p><p>3.No need for AWS KMS (as in options B and D) since the requirement is SSE-S3, not SSE-KMS.</p><p>4.Operationally simpler than manually running CLI commands (as in option C), especially for millions of objects.</p><p>#Why not the other options?</p><p>-B & D: These suggest usingSSE-KMS, which is unnecessary since the requirement isSSE-S3.</p><p>-C: While correct in enabling SSE-S3, manually running anS3 copy command via CLI is inefficient for millions of objects compared toS3 Batch Operations.</p><p>Thus,Option A is the best choice. &nbsp;</p><p>Final Answer: A </p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "2581cfe62f7d4fbfa1974112021d5f3b",
      "questionNumber": 92,
      "type": "single",
      "content": "<p>Question #92</p><p>A company is running an application in the AWS Cloud. The application collects and stores a large amount of unstructured data in an Amazon S3 bucket. The S3 bucket contains several terabytes of data and uses the S3 Standard storage class. The data increases in size by several gigabytes every day.</p><p><br></p><p>The company needs to query and analyze the data. The company does not access data that is more than 1 year old. However, the company must retain all the data indefinitely for compliance reasons.</p><p><br></p><p>Which solution will meet these requirements MOST cost-effectively?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use S3 Select to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Glacier Deep Archive."
        },
        {
          "label": "B",
          "content": "Use Amazon Redshift Spectrum to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Glacier."
        },
        {
          "label": "C",
          "content": "Use an AWS Glue Data Catalog and Amazon Athena to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Glacier Deep Archive."
        },
        {
          "label": "D",
          "content": "Use Amazon Redshift Spectrum to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Intelligent-Tiering."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the requirements for a cost-effective solution to query and analyze several terabytes of unstructured data in an S3 bucket, where data increases by gigabytes daily, data older than 1 year is not accessed but must be retained indefinitely, the answer is:</p><p>C. Use an AWS Glue Data Catalog and Amazon Athena to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Glacier Deep Archive.</p><p>This is the correct choice because:</p><p>AWS Glue Data Catalog and Amazon Athena provide a comprehensive solution for querying and analyzing large volumes of unstructured data directly from S3. This combination is specifically designed for data lake analytics at scale.</p><p>Athena allows for complex SQL queries across the entire dataset, which is important for analyzing several terabytes of data that grows daily.</p><p>AWS Glue can automatically discover and catalog the schema of the unstructured data, making it easier to query with Athena.</p><p>S3 Glacier Deep Archive is the most cost-effective storage class for data that must be retained indefinitely but is rarely or never accessed (in this case, data older than 1 year).</p><p>The S3 Lifecycle policy automates the transition of older data to the cheaper storage tier, optimizing costs without manual intervention.</p><p>Option A (using S3 Select) is less suitable because:</p><p>S3 Select is designed for simpler queries on individual objects</p><p>It doesn't provide the comprehensive analytics capabilities needed for terabytes of unstructured data</p><p>It's not as efficient for complex queries across multiple files or large datasets</p><p>The combination of AWS Glue, Athena, and S3 Glacier Deep Archive provides the most cost-effective solution that meets all the stated requirements.</p><p>Sources</p><p>Query data from multiple sources in S3 on Athena? | AWS re:Post （https://repost.aws/questions/QUeZq3d77YQ8-9EPtDDBe6RQ/query-data-from-multiple-sources-in-s3-on-athena）</p><p>Migrate and Archive data for ADAS workloads on AWS | AWS for Industries （https://aws.amazon.com/cn/blogs/industries/migrate-and-archive-data-for-adas-workloads-on-aws/）</p><p>Community | Unleashing Data Analytics on S3 Data lake with AWS Glue Crawler and Amazon Athena （https://community.aws/content/2d0KCVyijMpSQoN4xtFbJmN0tob/analytics-on-s3-data-lake-using-aws-glue-crawler-and-amazon-athena）</p><p>Amazon EMR streamlines big data processing with simplified Amazon S3 Glacier access | AWS Big Data Blog （https://aws.amazon.com/cn/blogs/big-data/amazon-emr-streamlines-big-data-processing-with-simplified-amazon-s3-glacier-access/）</p><p>Understanding S3 Glacier storage classes for long-term data storage - Amazon Simple Storage Service（https://docs.aws.amazon.com/AmazonS3/latest/userguide/glacier-storage-classes.html） </p><p>Secure archive storage – Amazon S3 Glacier storage classes – AWS （https://aws.amazon.com/cn/s3/storage-classes/glacier/?ams%23interactive-card-vertical%23pattern-data.filter=%257B%2522filters%2522%253A%255B%255D%257D）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "dd8a8ccb991f41e98c11eeb01da8b8be",
      "questionNumber": 93,
      "type": "single",
      "content": "<p>Question #93</p><p>A video processing company wants to build a machine learning (ML) model by using 600 TB of compressed data that is stored as thousands of files in the company's on-premises network attached storage system. The company does not have the necessary compute resources on premises for ML experiments and wants to use AWS.</p><p><br></p><p>The company needs to complete the data transfer to AWS within 3 weeks. The data transfer will be a one-time transfer. The data must be encrypted in transit. The measured upload speed of the company's internet connection is 100 Mbps. and multiple departments share the connection.</p><p><br></p><p>Which solution will meet these requirements MOST cost-effectively?</p>",
      "options": [
        {
          "label": "A",
          "content": "Order several AWS Snowball Edge Storage Optimized devices by using the AWS Management Console. Configure the devices with a destination S3 bucket. Copy the data to the devices. Ship the devices back to AWS."
        },
        {
          "label": "B",
          "content": "Set up a 10 Gbps AWS Direct Connect connection between the company location and the nearest AWS Region. Transfer the data over a VPN connection into the Region to store the data in Amazon S3."
        },
        {
          "label": "C",
          "content": "&nbsp;Create a VPN connection between the on-premises network attached storage and the nearest AWS Region. Transfer the data over the VPN connection."
        },
        {
          "label": "D",
          "content": "Deploy an AWS Storage Gateway file gateway on premises. Configure the file gateway with a destination S3 bucket. Copy the data to the file gateway."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Themost cost-effective solution that meets the requirements is: &nbsp;</p><p>A. Order several AWS Snowball Edge Storage Optimized devices by using the AWS Management Console. Configure the devices with a destination S3 bucket. Copy the data to the devices. Ship the devices back to AWS. &nbsp;</p><p>Why? &nbsp;</p><p>1.Data Size & Time Constraint (600 TB in 3 weeks): &nbsp;</p><p> &nbsp; - The company's internet upload speed is100 Mbps (12.5 MB/s). &nbsp;</p><p> &nbsp; - Transferring600 TB (600,000 GB) over the internet would take: &nbsp;</p><p> &nbsp; &nbsp; \\[</p><p> &nbsp; &nbsp; \\frac{600,000 \\text{ GB} \\times 1024 \\text{ MB/GB}}{12.5 \\text{ MB/s}} \\approx 5,529,600 \\text{ seconds} \\approx 64 \\text{ days}</p><p> &nbsp; &nbsp; \\] &nbsp;</p><p> &nbsp; &nbsp; This isfar beyond the 3-week deadline. &nbsp;</p><p>2.Snowball Edge Storage Optimized: &nbsp;</p><p> &nbsp; - Each Snowball Edge Storage Optimized device can hold80 TB. &nbsp;</p><p> &nbsp; - For600 TB, the company would need8 devices (since 600/80 = 7.5 → round up to 8). &nbsp;</p><p> &nbsp; - AWS Snowball is designed forlarge-scale, one-time transfers and ismuch faster than internet transfer. &nbsp;</p><p> &nbsp; - Data isencrypted in transit and at rest. &nbsp;</p><p>3.Cost-Effectiveness: &nbsp;</p><p> &nbsp; -Direct Connect (Option B) is expensive (requires 10 Gbps setup, ongoing costs) andoverkill for a one-time transfer. &nbsp;</p><p> &nbsp; -VPN Transfer (Option C) is too slow (as calculated above). &nbsp;</p><p> &nbsp; -Storage Gateway (Option D) is meant forongoing hybrid storage, not a one-time bulk transfer. &nbsp;</p><p>Conclusion: &nbsp;</p><p>AWS Snowball Edge is thefastest, most secure, and cost-effective solution for transferring600 TB within3 weeks. &nbsp;</p><p>Answer: A</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "6bca25f869d64c218f9a48dca7c8e545",
      "questionNumber": 94,
      "type": "single",
      "content": "<p>Question #94</p><p>A company has migrated Its forms-processing application to AWS. When users interact with the application, they upload scanned forms as files through a web application. A database stores user metadata and references to files that are stored in Amazon S3. The web application runs on Amazon EC2 instances and an Amazon RDS for PostgreSQL database.</p><p><br></p><p>When forms are uploaded, the application sends notifications to a team through Amazon Simple Notification Service (Amazon SNS). A team member then logs in and processes each form. The team member performs data validation on the form and extracts relevant data before entering the information into another system that uses an API.</p><p><br></p><p>A solutions architect needs to automate the manual processing of the forms. The solution must provide accurate form extraction, minimize time to market, and minimize long-term operational overhead.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Develop custom libraries to perform optical character recognition (OCR) on the forms. Deploy the libraries to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster as an application tier. Use this tier to process the forms when forms are uploaded. Store the output in Amazon S3. Parse this output by extracting the data into an Amazon DynamoDB table. Submit the data to the target system&#39;s APL. Host the new application tier on EC2 instances.&nbsp;"
        },
        {
          "label": "B",
          "content": "Extend the system with an application tier that uses AWS Step Functions and AWS Lambda. Configure this tier to use artificial intelligence and machine learning (AI/ML) models that are trained and hosted on an EC2 instance to perform OCR on the forms when forms are uploaded. Store the output in Amazon S3. Parse this output by extracting the data that is required within the application tier. Submit the data to the target system&#39;s API."
        },
        {
          "label": "C",
          "content": "Host a new application tier on EC2 instances. Use this tier to call endpoints that host artificial intelligence and machine learning (AI/ML) models that are trained and hosted in Amazon SageMaker to perform OCR on the forms. Store the output in Amazon ElastiCache. Parse this output by extracting the data that is required within the application tier. Submit the data to the target system&#39;s API."
        },
        {
          "label": "D",
          "content": "Extend the system with an application tier that uses AWS Step Functions and AWS Lambda. Configure this tier to use Amazon Textract and Amazon Comprehend to perform OCR on the forms when forms are uploaded. Store the output in Amazon S3. Parse this output by extracting the data that is required within the application tier. Submit the data to the target system&#39;s API."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>The correct answer isD. Here's why:</p><p> Key Requirements:</p><p>1.Accurate form extraction: The solution must use reliable Optical Character Recognition (OCR) and data extraction.</p><p>2.Minimize time to market: Avoid building custom ML models or complex infrastructure.</p><p>3.Minimize long-term operational overhead: Prefer serverless and managed services over self-hosted solutions.</p><p> Why Option D is Best:</p><p>-Amazon Textract: A fully managed AWS AI service for OCR and form data extraction (no need to train custom ML models).</p><p>-Amazon Comprehend: Can be used for additional NLP tasks if needed (e.g., understanding unstructured text).</p><p>-AWS Step Functions & Lambda: Serverless orchestration and compute, reducing operational overhead.</p><p>-Amazon S3: Stores output reliably.</p><p>-No custom ML models or EC2 hosting: Eliminates maintenance of SageMaker/EC2 instances (unlike B and C).</p><p>-Faster deployment: Uses pre-trained AWS AI services instead of developing custom libraries (unlike A).</p><p> Why Other Options Are Inferior:</p><p>-A: Custom OCR libraries and EC2 hosting increase development time and operational overhead.</p><p>-B: Requires training/hosting custom ML models on EC2, which adds complexity and maintenance.</p><p>-C: Uses SageMaker (overkill for OCR) and ElastiCache (not needed for this workflow), increasing cost and overhead.</p><p> Conclusion:</p><p>D leverages AWS managed AI services (Textract/Comprehend) and serverless components (Step Functions/Lambda) to meet all requirements efficiently. </p><p>Answer: D</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "16c1f76e217b479e9cb582d3ad233609",
      "questionNumber": 95,
      "type": "single",
      "content": "<p>Question #95</p><p>A company is refactoring its on-premises order-processing platform in the AWS Cloud. The platform includes a web front end that is hosted on a fleet of VMs, RabbitMQ to connect the front end to the backend, and a Kubernetes cluster to run a containerized backend system to process the orders. The company does not want to make any major changes to the application.<br><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an AMI of the web server VM. Create an Amazon EC2 Auto Scaling group that uses the AMI and an Application Load Balancer. Set up Amazon MQ to replace the on-premises messaging queue. Conﬁgure Amazon Elastic Kubernetes Service (Amazon EKS) to host the order-processing backend."
        },
        {
          "label": "B",
          "content": "Create a custom AWS Lambda runtime to mimic the web server environment. Create an Amazon API Gateway API to replace the front-end web servers. Set up Amazon MQ to replace the on-premises messaging queue. Conﬁgure Amazon Elastic Kubernetes Service (Amazon EKS) to host the order-processing backend."
        },
        {
          "label": "C",
          "content": "Create an AMI of the web server VM. Create an Amazon EC2 Auto Scaling group that uses the AMI and an Application Load Balancer. Set up Amazon MQ to replace the on-premises messaging queue. Install Kubernetes on a fleet of different EC2 instances to host the order-processing backend."
        },
        {
          "label": "D",
          "content": "Create an AMI of the web server VM. Create an Amazon EC2 Auto Scaling group that uses the AMI and an Application Load Balancer. Set up an Amazon Simple Queue Service (Amazon SQS) queue to replace the on-premises messaging queue. Conﬁgure Amazon Elastic Kubernetes Service (Amazon EKS) to host the order-processing backend."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>The correct answer isA. &nbsp;</p><p> Explanation: &nbsp;</p><p>The question asks for a solution that refactors the on-premises order-processing platform in AWSwith the least operational overhead andwithout major changes to the application. &nbsp;</p><p>-Option A is correct because: &nbsp;</p><p> &nbsp;- It uses anAMI of the existing web server VM, allowing the front end to run onEC2 Auto Scaling with minimal changes. &nbsp;</p><p> &nbsp;- It replaces RabbitMQ withAmazon MQ (a managed RabbitMQ or ActiveMQ service), which is a direct replacement with minimal configuration. &nbsp;</p><p> &nbsp;- It usesAmazon EKS (managed Kubernetes) for the backend, reducing operational overhead compared to self-managed Kubernetes. &nbsp;</p><p> &nbsp;- This approach requires the least modification to the existing application. &nbsp;</p><p>-Option B is incorrect because: &nbsp;</p><p> &nbsp;- Replacing the web front end withLambda and API Gateway would require significant code changes (serverless vs. VM-based). &nbsp;</p><p> &nbsp;- A custom Lambda runtime adds unnecessary complexity. &nbsp;</p><p>-Option C is incorrect because: &nbsp;</p><p> &nbsp;- It suggests installingself-managed Kubernetes on EC2, which introduces high operational overhead compared toAmazon EKS. &nbsp;</p><p>-Option D is incorrect because: &nbsp;</p><p> &nbsp;-Amazon SQS is not a direct replacement forRabbitMQ (which is a message broker supporting advanced protocols like AMQP).Amazon MQ is the correct managed alternative. &nbsp;</p><p> Conclusion: &nbsp;</p><p>Option A provides the most seamless migration with the least operational overhead while keeping the application mostly unchanged. &nbsp;</p><p>Answer: A</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "42dc7d88ea6e422ca36b885749176fcb",
      "questionNumber": 96,
      "type": "single",
      "content": "<p>Question #96</p><p>A solutions architect needs to implement a client-side encryption mechanism for objects that will be stored in a new Amazon S3 bucket. The solutions architect created a CMK that is stored in AWS Key Management Service (AWS KMS) for this purpose.</p><p>The solutions architect created the following IAM policy and attached it to an IAM role:</p><p><img src=\"images/sap/54feb655939161fcb125a0ac4c163ccce2cba289.png\" alt=\"b3df9271dd7e405314012b4226b7562d.png\" data-href=\"\" style=\"\"/></p><p>During tests, the solutions architect was able to successfully get existing test objects in the S3 bucket. However, attempts to upload a new object resulted in an error message. The error message stated that the action was forbidden.</p><p><br></p><p>Which action must the solutions architect add to the IAM policy to meet all the requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "kms:GenerateDataKey"
        },
        {
          "label": "B",
          "content": "kms:GetKeyPolicy"
        },
        {
          "label": "C",
          "content": "kms:GetPublicKey"
        },
        {
          "label": "D",
          "content": "kms:Sign"
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Based on the scenario where a solutions architect needs to implement client-side encryption for S3 objects using a CMK stored in AWS KMS, and is able to get existing objects but receives a \"forbidden\" error when trying to upload new objects, the answer is:</p><p>A. kms:GenerateDataKey</p><p>This is the correct choice because:</p><p>For client-side encryption of S3 objects using AWS KMS, the client application needs to generate a data key from the CMK.</p><p>The kms:GenerateDataKey permission is specifically required to request a unique data key from KMS, which is then used to encrypt the object locally before uploading to S3.</p><p>Without this permission, the client cannot obtain the necessary data key to perform the encryption, resulting in the \"forbidden\" error message when attempting to upload new objects.</p><p>Since the solutions architect can already get existing objects, the IAM policy likely already includes permissions for operations like kms:Decrypt, but is missing the crucial permission needed for the encryption of new objects.</p><p>The other options are not correct for this specific scenario:</p><p>kms:GetKeyPolicy (option B) is used to view a key policy, not for encryption operations</p><p>kms:GetPublicKey (option C) is used for asymmetric keys to retrieve the public key, not for generating data keys</p><p>kms:Sign (option D) is used for digital signatures, not for encryption of S3 objects</p><p>Sources</p><p>Specifying dual-layer server-side encryption with AWS KMS keys (DSSE-KMS) - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/specifying-dsse-encryption.html）</p><p>Using server-side encryption with AWS KMS keys (SSE-KMS) in directory buckets - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-express-UsingKMSEncryption.html）</p><p>Setting default server-side encryption behavior for Amazon S3 buckets - Amazon Simple Storage Service（https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-encryption.html） </p><p>Amazon S3 Express One Zone now supports AWS KMS with customer managed keys | AWS News Blog（https://aws.amazon.com/cn/blogs/aws/amazon-s3-express-one-zone-now-supports-aws-kms-with-customer-managed-keys/） </p><p>Understanding Amazon S3 client-side encryption options | AWS Storage Blog （https://aws.amazon.com/cn/blogs/storage/understanding-amazon-s3-client-side-encryption-options/）</p><p>How to encrypt your object in S3 | AWS re:Post （https://repost.aws/questions/QUa2uGgWvVTeCAkp6vKHBmdw/how-to-encrypt-your-object-in-s3）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "47b6938cd9384bb89b0bff7d96819c2c",
      "questionNumber": 97,
      "type": "single",
      "content": "<p>Question #97</p><p>A company has developed a web application. The company is hosting the application on a group of Amazon EC2 instances behind an Application Load Balancer. The company wants to improve the security posture of the application and plans to use AWS WAF web ACLs. The solution must not adversely affect legitimate traffic to the application.</p><p><br></p><p>How should a solutions architect configure the web ACLs to meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Set the action of the web ACL rules to Count. Enable AWS WAF logging. Analyze the requests for false positives. Modify the rules to avoid any false positive. Over time, change the action of the web ACL rules from Count to Block."
        },
        {
          "label": "B",
          "content": "Use only rate-based rules in the web ACLs, and set the throttle limit as high as possible. Temporarily block all requests that exceed the limit. Define nested rules to narrow the scope of the rate tracking."
        },
        {
          "label": "C",
          "content": "Set the action of the web ACL rules to Block. Use only AWS managed rule groups in the web ACLs. Evaluate the rule groups by using Amazon CloudWatch metrics with AWS WAF sampled requests or AWS WAF logs."
        },
        {
          "label": "D",
          "content": "Use only custom rule groups in the web ACLs, and set the action to Allow. Enable AWS WAF logging. Analyze the requests for false positives. Modify the rules to avoid any false positive. Over time, change the action of the web ACL rules from Allow to Block."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Based on the requirements to improve the security posture of a web application using AWS WAF web ACLs without adversely affecting legitimate traffic, the answer is:</p><p>A. Set the action of the web ACL rules to Count. Enable AWS WAF logging. Analyze the requests for false positives. Modify the rules to avoid any false positive. Over time, change the action of the web ACL rules from Count to Block.</p><p>This is the correct choice because:</p><p>Starting with rules in \"Count\" mode allows you to monitor traffic patterns without blocking any legitimate requests. This is a non-disruptive approach that meets the requirement of not adversely affecting legitimate traffic.</p><p>Enabling AWS WAF logging provides visibility into which requests would be blocked by your rules, allowing you to identify potential false positives before they impact users.</p><p>Analyzing the logs and modifying rules to avoid false positives ensures that when you do switch to \"Block\" mode, you'll only be blocking malicious traffic.</p><p>The gradual approach of changing from \"Count\" to \"Block\" over time allows for careful testing and validation, minimizing the risk to the application.</p><p>Option B (using only rate-based rules with high throttle limits) is less suitable because:</p><p>It focuses solely on rate limiting, which only addresses volumetric attacks</p><p>It doesn't provide the comprehensive protection that a full set of WAF rules would offer</p><p>Even with nested rules, this approach could still block legitimate traffic during usage spikes</p><p>It doesn't include the critical testing phase to identify and address false positives</p><p>The approach in option A represents a best practice for implementing AWS WAF in a way that balances improved security with minimal disruption to legitimate users.</p><p>Sources</p><p>Restrict access based on IP address or geolocation by using AWS WAF - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/aws-waf-restrict-access-geolocation.html）</p><p>Testing and tuning - Guidelines for Implementing AWS WAF （https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html）</p><p>Is it possible to \"dry run\" WAF rate limiting rules? | AWS re:Post （https://repost.aws/questions/QUibVWsVgbSaqRoOf2_7dR9g/is-it-possible-to-dry-run-waf-rate-limiting-rules）</p><p>Preparing for testing your AWS WAF protections - AWS WAF, AWS Firewall Manager, AWS Shield Advanced, and AWS Shield network security director (https://docs.aws.amazon.com/waf/latest/developerguide/web-acl-testing-prep.html)</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "db0eef6fd2c541b79d6d1380c40274a9",
      "questionNumber": 98,
      "type": "single",
      "content": "<p>Question #98</p><p>A company has an organization that has many AWS accounts in AWS Organizations. A solutions architect must improve how the company manages common security group rules for the AWS accounts in the organization.</p><p><br></p><p>The company has a common set of IP CIDR ranges in an allow list in each AWS account to allow access to and from the company’s on-premises network. Developers within each account are responsible for adding new IP CIDR ranges to their security groups. The security team has its own AWS account. Currently, the security team notifies the owners of the other AWS accounts when changes are made to the allow list.</p><p><br></p><p>The solutions architect must design a solution that distributes the common set of CIDR ranges across all accounts.</p><p><br></p><p>Which solution meets these requirements with the LEAST amount of operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Set up an Amazon Simple Notification Service (Amazon SNS) topic in the security team&rsquo;s AWS account. Deploy an AWS Lambda function in each AWS account. Configure the Lambda function to run every time an SNS topic receives a message. Configure the Lambda function to take an IP address as input and add it to a list of security groups in the account. Instruct the security team to distribute changes by publishing messages to its SNS topic."
        },
        {
          "label": "B",
          "content": "Create new customer-managed prefix lists in each AWS account within the organization. Populate the prefix lists in each account with all internal CIDR ranges. Notify the owner of each AWS account to allow the new customer-managed prefix list IDs in their security groups.Instruct the security team to share updates with each AWS account owner.&nbsp;"
        },
        {
          "label": "C",
          "content": "Create a new customer-managed prefix list in the security team&rsquo;s AWS account. Populate the customer-managed prefix list with all internal CIDR ranges. Share the customer-managed prefix list with the organization by using AWS Resource Access Manager. Notify the owner of each AWS account to allow the new customer-managed prefix list ID in their security groups."
        },
        {
          "label": "D",
          "content": "Create an IAM role in each account in the organization. Grant permissions to update security groups. Deploy an AWS Lambda function in the security team&rsquo;s AWS account. Configure the Lambda function to take a list of internal IP addresses as input, assume a role in each organization account, and add the list of IP addresses to the security groups in each account."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>The correct answer isC. &nbsp;</p><p>Explanation: &nbsp;</p><p>The requirement is to distribute common CIDR ranges across all AWS accounts with theleast operational overhead. Here’s whyOption C is the best solution: &nbsp;</p><p>1.Centralized Management: &nbsp;</p><p> &nbsp; - Acustomer-managed prefix list is created in thesecurity team’s AWS account, which acts as a single source of truth for CIDR ranges. &nbsp;</p><p> &nbsp; - Instead of manually updating each account, the security team only needs to update thisone prefix list. &nbsp;</p><p>2.Shared Across the Organization: &nbsp;</p><p> &nbsp; - UsingAWS Resource Access Manager (RAM), the prefix list isshared with all accounts in the organization. &nbsp;</p><p> &nbsp; - This avoids duplication and ensures consistency. &nbsp;</p><p>3.Reduced Operational Overhead: &nbsp;</p><p> &nbsp; - Developers in each account only need toreference the shared prefix list in their security groups (instead of manually updating IPs). &nbsp;</p><p> &nbsp; - The security teamdoes not need to notify each account owner for every change—they just update the prefix list, and all accounts automatically get the latest CIDRs. &nbsp;</p><p>Why Other Options Are Less Optimal: &nbsp;</p><p>-Option A: Requires deploying Lambda functions in every account and manual SNS notifications. This increases operational overhead. &nbsp;</p><p>-Option B: Requires creating and maintaining separate prefix lists ineach account, which is redundant and harder to manage. &nbsp;</p><p>-Option D: Requires Lambda to assume roles in every account and update security groups directly, which is complex and increases operational risk. &nbsp;</p><p>Conclusion: &nbsp;</p><p>Option C is themost scalable and least operationally intensive solution, as it centralizes CIDR management and leverages AWS RAM for easy sharing. &nbsp;</p><p>Answer: C</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "789171fc8f7f43dab544e33afeec16ba",
      "questionNumber": 99,
      "type": "single",
      "content": "<p>Question #99</p><p>A company has introduced a new policy that allows employees to work remotely from their homes if they connect by using a VPN. The company is hosting internal applications with VPCs in multiple AWS accounts. Currently, the applications are accessible from the company's on-premises office network through an AWS Site-to-Site VPN connection. The VPC in the company's main AWS account has peering connections established with VPCs in other AWS accounts.<br><br></p><p>A solutions architect must design a scalable AWS Client VPN solution for employees to use while they work from home. </p><p><br></p><p>What is the MOST cost-effective solution that meets these requirements? </p>",
      "options": [
        {
          "label": "A",
          "content": "Create a Client VPN endpoint in each AWS account. Configure required routing that allows access to internal applications."
        },
        {
          "label": "B",
          "content": "Create a Client VPN endpoint in the main AWS account. Configure required routing that allows access to internal applications."
        },
        {
          "label": "C",
          "content": "Create a Client VPN endpoint in the main AWS account. Provision a transit gateway that is connected to each AWS account. Configure required routing that allows access to internal applications."
        },
        {
          "label": "D",
          "content": "Create a Client VPN endpoint in the main AWS account. Establish connectivity between the Client VPN endpoint and the AWS Site-to-Site VPN."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the requirements for a cost-effective AWS Client VPN solution that allows remote employees to access internal applications hosted in VPCs across multiple AWS accounts, the answer is:</p><p>B. Create a Client VPN endpoint in the main AWS account. Configure required routing that allows access to internal applications.</p><p>This is the correct choice because:</p><p>Creating a single Client VPN endpoint in the main AWS account is more cost-effective than deploying multiple endpoints across different accounts. AWS Client VPN pricing is based on the number of endpoints and connections, so consolidating to one endpoint reduces costs.</p><p>Since the VPC in the main AWS account already has peering connections established with VPCs in other AWS accounts, you can leverage this existing network architecture. The Client VPN endpoint can route traffic through these peering connections to reach resources in all connected VPCs.</p><p>This approach simplifies management and configuration by centralizing the VPN solution in one account rather than managing multiple endpoints across different accounts.</p><p>The required routing can be configured to allow access to all internal applications across the peered VPCs, meeting the requirement for employees to access applications in multiple AWS accounts.</p><p>Option A (creating a Client VPN endpoint in each AWS account) would be more complex to manage and significantly more expensive due to the additional endpoints, making it less cost-effective than the centralized approach in option B.</p><p>Sources</p><p>Client VPN with VPC peering vs Client VPN withTransit gateway | AWS re:Post (https://repost.aws/questions/QUUotXjy2mTV6noMA-J86AXQ/client-vpn-with-vpc-peering-vs-client-vpn-withtransit-gateway)</p><p>VPN &nbsp;- Building a Scalable and Secure Multi-VPC AWS Network Infrastructure (https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/vpn.html)</p><p>Software remote access-to-Amazon VPC connectivity options - Amazon Virtual Private Cloud Connectivity Options (https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/software-remote-access-to-amazon-vpc-connectivity-options.html)</p><p>How AWS Client VPN works - AWS Client VPN (https://docs.aws.amazon.com/vpn/latest/clientvpn-admin/how-it-works.html)</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "ed0d55da9e0742939157e1ba5eacf3f4",
      "questionNumber": 100,
      "type": "single",
      "content": "<p>Question #100</p><p>A company is running an application in the AWS Cloud. Recent application metrics show inconsistent response times and a significant increase in error rates. Calls to third-party services are causing the delays. Currently, the application calls third-party services synchronously by directly invoking an AWS Lambda function.<br><br></p><p>A solutions architect needs to decouple the third-party service calls and ensure that all the calls are eventually completed. </p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use an Amazon Simple Queue Service (Amazon SQS) queue to store events and invoke the Lambda function."
        },
        {
          "label": "B",
          "content": "Use an AWS Step Functions state machine to pass events to the Lambda function."
        },
        {
          "label": "C",
          "content": "Use an Amazon EventBridge rule to pass events to the Lambda function."
        },
        {
          "label": "D",
          "content": "Use an Amazon Simple Notification Service (Amazon SNS) topic to store events and invoke the Lambda function."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Based on the requirements to decouple third-party service calls and ensure all calls are eventually completed, the answer is:</p><p>A. Use an Amazon Simple Queue Service (Amazon SQS) queue to store events and invoke the Lambda function.</p><p>This is the correct choice because:</p><p>Amazon SQS provides a reliable message queuing service that effectively decouples components of a distributed application, which directly addresses the requirement to decouple the third-party service calls.</p><p>SQS ensures that all messages (service calls) are eventually processed, even if there are temporary failures or throttling from the third-party services, meeting the requirement that \"all calls are eventually completed.\"</p><p>SQS acts as a buffer that can absorb traffic spikes and allows the Lambda function to process messages at a consistent rate, which helps address the inconsistent response times issue.</p><p>SQS offers features like visibility timeout and dead-letter queues that provide robust error handling and retry mechanisms for failed processing attempts.</p><p>The other options are less suitable for this specific scenario:</p><p>Option B (AWS Step Functions): While good for orchestrating complex workflows, it's more suited for coordinating multiple steps rather than simple decoupling of service calls.</p><p>Option C (Amazon EventBridge): Better for event routing based on patterns rather than ensuring eventual completion of all calls.</p><p>Option D (Amazon SNS): Primarily designed for pub/sub messaging and fanout patterns, SNS doesn't store messages for later processing if the consumer is unavailable, making it less suitable for ensuring all calls are eventually completed.</p><p>Sources</p><p>Capturing records of Lambda asynchronous invocations - AWS Lambda （https://docs.aws.amazon.com/lambda/latest/dg/invocation-async-retain-records.html）</p><p>Transforming Order Fulfillment with Event-Driven Architecture: A Rapid7 Success Story | AWS Partner Network (APN) Blog （https://aws.amazon.com/cn/blogs/apn/transforming-order-fulfillment-with-event-driven-architecture-a-rapid7-success-story/）</p><p>Creating event-driven architectures with Lambda - AWS Lambda （https://docs.aws.amazon.com/lambda/latest/dg/concepts-event-driven-architectures.html）</p><p>Application integration services - Amazon Simple Notification Service （https://docs.aws.amazon.com/sns/latest/dg/sns-event-sources-application-integration.html）</p><p>Field Notes: Orchestrating and Monitoring Complex, Long-running Workflows Using AWS Step Functions | AWS Architecture Blog （https://aws.amazon.com/cn/blogs/architecture/field-notes-orchestrating-and-monitoring-complex-long-running-workflows-using-aws-step-functions/）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "1917d5e4d19d40d39b15c944b2375690",
      "questionNumber": 101,
      "type": "single",
      "content": "Question #101<p>A company is running applications on AWS in a multi-account environment. The company's sales team and marketing team use separate AWS accounts in AWS Organizations.<br><br></p><p>The sales team stores petabytes of data in an Amazon S3 bucket. The marketing team uses Amazon QuickSight for data visualizations. The marketing team needs access to data that the sates team stores in the S3 bucket. The company has encrypted the S3 bucket with an AWS Key Management Service (AWS KMS) key. The marketing team has already created the IAM service role for QuickSight to provide QuickSight access in the marketing AWS account. The company needs a solution that will provide secure access to the data in the S3 bucket across AWS accounts. </p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead? </p>",
      "options": [
        {
          "label": "A",
          "content": "Create a new S3 bucket in the marketing account. Create an S3 replication rule in the sales account to copy the objects to the new S3 bucket in the marketing account. Update the QuickSight permissions in the marketing account to grant access to the new S3 bucket."
        },
        {
          "label": "B",
          "content": "Create an SCP to grant access to the S3 bucket to the marketing account. Use AWS Resource Access Manager (AWS RAM) to share the KMS key from the sales account with the marketing account. Update the QuickSight permissions in the marketing account to grant access to the S3 bucket."
        },
        {
          "label": "C",
          "content": "Update the S3 bucket policy in the marketing account to grant access to the QuickSight role. Create a KMS grant for the encryption key that is used in the S3 bucket. Grant decrypt access to the QuickSight role. Update the QuickSight permissions in the marketing account to grant access to the S3 bucket.&nbsp;"
        },
        {
          "label": "D",
          "content": "Create an IAM role in the sales account and grant access to the S3 bucket. From the marketing account, assume the IAM role in the sales account to access the S3 bucket. Update the QuickSight role to create a trust relationship with the new IAM role in the sales account."
        }
      ],
      "correctAnswer": "D",
      "explanation": "To provide secure access to the S3 bucket across AWS accounts with the least operational overhead, I recommend the following solution:<p>1. Create a cross-account IAM role in the sales team's AWS account:</p><p>   - This role should grant access to the S3 bucket and the KMS key used for encryption.</p><p>   - The trust policy should allow the marketing team's AWS account to assume this role.</p><p>2. Update the existing IAM service role for QuickSight in the marketing team's account:</p><p>   - Add permissions to assume the cross-account role created in step 1.</p><p>3. Configure QuickSight in the marketing team's account:</p><p>   - Set up a new data source using the cross-account role to access the S3 bucket.</p><p>4. (Optional) Use AWS Lake Formation for fine-grained access control:</p><p>   - If more granular control over data access is needed, consider implementing Lake Formation.</p><p>This solution provides several benefits:</p><p>- Minimal operational overhead: Leverages existing QuickSight setup and requires minimal new resources.</p><p>- Secure: Maintains the principle of least privilege by using cross-account roles.</p><p>- Scalable: Can easily accommodate future needs for data sharing between accounts.</p><p>Best practices to consider:</p><p>- Regularly review and audit the permissions granted to ensure they remain appropriate.</p><p>- Use condition keys in the IAM policies to further restrict access (e.g., by IP range or time of day).</p><p>- Enable AWS CloudTrail to monitor and log access to the S3 bucket and KMS key.</p><p>- Implement a process for regular rotation of the KMS key used for S3 bucket encryption.</p><p>Remember to test this setup in a non-production environment before implementing it in production. Also, consult the AWS documentation for the most up-to-date information on cross-account access and QuickSight configuration.</p><p>Sources</p><p>[1] [Example 4 - Bucket owner granting cross-account permission to objects it does not own - Amazon Simple Storage Service] (https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-walkthroughs-managing-access-example4.html)</p><p>[3] [Community | AWS Cross Account S3 Access Through Lambda Functions] (https://community.aws/content/2cCcsg05ZX2PoMHwMcWhv2BMlPL/aws-cross-account-s3-access-through-lambda-functions)</p><p>[6] [Configure IAM roles - AWS Transfer Family] (https://docs.aws.amazon.com/transfer/latest/userguide/webapp-roles.html)</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "5d495919b12040368f019c2924f702c1",
      "questionNumber": 102,
      "type": "single",
      "content": "<p>Question #102</p><p>A company is planning to migrate its business-critical applications from an on-premises data center to AWS. The company has an on-premises installation of a Microsoft SQL Server Always On cluster. The company wants to migrate to an AWS managed database service.A solutions architect must design a heterogeneous database migration on AWS. </p><p><br></p><p>Which solution will meet these requirements? </p>",
      "options": [
        {
          "label": "A",
          "content": "Migrate the SQL Server databases to Amazon RDS for MySQL by using backup and restore utilities."
        },
        {
          "label": "B",
          "content": "Use an AWS Snowball Edge Storage Optimized device to transfer data to Amazon S3. Set up Amazon RDS for MySQL. Use S3 integration with SQL Server features, such as BULK INSERT."
        },
        {
          "label": "C",
          "content": "Use the AWS Schema Conversion Tool to translate the database schema to Amazon RDS for MySQL. Then use AWS Database Migration Service (AWS DMS) to migrate the data from on-premises databases to Amazon RDS."
        },
        {
          "label": "D",
          "content": "<p>Use AWS DataSync to migrate data over the network between on-premises storage and Amazon S3. Set up Amazon RDS for MySQL. Use S3 integration with SQL Server features, such as BULK INSERT.</p>"
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Use the AWS Schema Conversion Tool to translate the database schema to Amazon RDS for MySQL. Then use AWS Database Migration Service (AWS DMS) to migrate the data from on-premises databases to Amazon RDS.</p><p>This is the appropriate solution because:</p><p>The company needs to perform a heterogeneous database migration (moving from Microsoft SQL Server to a different database engine).</p><p>AWS Schema Conversion Tool (SCT) is specifically designed to convert database schemas from one database engine to another, handling the differences in data types and SQL syntax.</p><p>AWS Database Migration Service (DMS) is the recommended service for migrating data between different database platforms with minimal downtime.</p><p>This approach provides a comprehensive migration path that addresses both schema conversion and data migration requirements.</p><p>Options A and D don't provide proper schema conversion capabilities needed for heterogeneous migrations, and they would likely result in compatibility issues when moving from SQL Server to MySQL.</p><p>Sources</p><p>Migrating a SQL Server Always On Database to Amazon Web Services - Database Migration Guide （https://docs.aws.amazon.com/dms/latest/sbs/chap-manageddatabases.sqlserveralwayson.html）</p><p>SQL Server database migration strategies - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-sql-server/strategies.html）</p><p>Migrate legacy database | AWS re:Post （https://repost.aws/questions/QUTC4STLNPTNeCYjbQMSErcA/migrate-legacy-database）</p><p>Tools for heterogeneous database migrations - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-sql-server/heterogeneous-migration-tools.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "b103d915bc0d4dbbb1099acab27a8987",
      "questionNumber": 103,
      "type": "multiple",
      "content": "<p>Question #103</p><p>A publishing company's design team updates the icons and other static assets that an ecommerce web application uses. The company serves the icons and assets from an Amazon S3 bucket that is hosted in the company's production account. The company also uses a development account that members of the design team can access.<br><br><br></p><p>After the design team tests the static assets in the development account, the design team needs to load the assets into the S3 bucket in the production account. A solutions architect must provide the design team with access to the production account without exposing other parts of the web application to the risk of unwanted changes. </p><p><br></p><p>Which combination of steps will meet these requirements? (Choose three.) </p>",
      "options": [
        {
          "label": "A",
          "content": "In the production account, create a new IAM policy that allows read and write access to the S3 bucket."
        },
        {
          "label": "B",
          "content": "In the development account, create a new IAM policy that allows read and write access to the S3 bucket."
        },
        {
          "label": "C",
          "content": "In the production account, create a role. Attach the new policy to the role. Define the development account as a trusted entity."
        },
        {
          "label": "D",
          "content": "In the development account, create a role. Attach the new policy to the role. Define the production account as a trusted entity."
        },
        {
          "label": "E",
          "content": "<p>In the development account, create a group that contains all the IAM users of the design team. Attach a different IAM policy to the group to allow the sts:AssumeRole action on the role In the production account.</p>"
        },
        {
          "label": "F",
          "content": "<p><span style=\"color: rgb(80, 80, 80);\">In the development account, create a group that contains all the IAM users of the design team Attach a different IAM </span></p><p><span style=\"color: rgb(80, 80, 80);\">policy to the group to allow the sts:AssumeRole action on the role in the development account.</span></p>"
        }
      ],
      "correctAnswer": "ACE",
      "explanation": "<p>Answer: &nbsp;</p><p>A, C, E &nbsp;</p><p> Explanation: &nbsp;Key Requirements: &nbsp;</p><p>1. Design Team Access: &nbsp;</p><p> &nbsp; - The design team (in the development account) needs read/write access to a specific S3 bucket in the production account. &nbsp;</p><p>2. Least Privilege: &nbsp;</p><p> &nbsp; - Restrict access only to the S3 bucket (no other production resources). &nbsp;</p><p>3. Security: &nbsp;</p><p> &nbsp; - Avoid sharing production account credentials. &nbsp;</p><p> Correct Steps (A, C, E): &nbsp;</p><p>1. A (Production Account - IAM Policy for S3): &nbsp;</p><p> &nbsp; - Create a policy in the production account granting `s3:GetObject`, `s3:PutObject`, etc., only for the target bucket. &nbsp;</p><p> &nbsp; ```json</p><p> &nbsp; {</p><p> &nbsp; &nbsp; \"Version\": \"2012-10-17\",</p><p> &nbsp; &nbsp; \"Statement\": [{</p><p> &nbsp; &nbsp; &nbsp; \"Effect\": \"Allow\",</p><p> &nbsp; &nbsp; &nbsp; \"Action\": [\"s3:GetObject\", \"s3:PutObject\"],</p><p> &nbsp; &nbsp; &nbsp; \"Resource\": \"arn:aws:s3:::production-bucket/*\"</p><p> &nbsp; &nbsp; }]</p><p> &nbsp; }</p><p> &nbsp; ``` &nbsp;</p><p>2. C (Production Account - Role with Trust): &nbsp;</p><p> &nbsp; - Create an IAM role in the production account, attach the policy from A, and trust the development account. &nbsp;</p><p> &nbsp; ```json</p><p> &nbsp; {</p><p> &nbsp; &nbsp; \"Version\": \"2012-10-17\",</p><p> &nbsp; &nbsp; \"Statement\": [{</p><p> &nbsp; &nbsp; &nbsp; \"Effect\": \"Allow\",</p><p> &nbsp; &nbsp; &nbsp; \"Principal\": { \"AWS\": \"arn:aws:iam::DevelopmentAccountID:root\" },</p><p> &nbsp; &nbsp; &nbsp; \"Action\": \"sts:AssumeRole\"</p><p> &nbsp; &nbsp; }]</p><p> &nbsp; }</p><p> &nbsp; ``` &nbsp;</p><p>3. E (Development Account - Group with AssumeRole): &nbsp;</p><p> &nbsp; - Create a group for the design team in the development account and attach a policy allowing `sts:AssumeRole` on the production account’s role. &nbsp;</p><p> &nbsp; ```json</p><p> &nbsp; {</p><p> &nbsp; &nbsp; \"Version\": \"2012-10-17\",</p><p> &nbsp; &nbsp; \"Statement\": [{</p><p> &nbsp; &nbsp; &nbsp; \"Effect\": \"Allow\",</p><p> &nbsp; &nbsp; &nbsp; \"Action\": \"sts:AssumeRole\",</p><p> &nbsp; &nbsp; &nbsp; \"Resource\": \"arn:aws:iam::ProductionAccountID:role/DesignTeamRole\"</p><p> &nbsp; &nbsp; }]</p><p> &nbsp; }</p><p> &nbsp; ``` &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- B (Policy in Dev Account): &nbsp;</p><p> &nbsp;- Policies in the dev account can’t grant access to production resources. &nbsp;</p><p>- D (Role in Dev Account): &nbsp;</p><p> &nbsp;- Roles in the dev account can’t access production S3 directly. &nbsp;</p><p>- F (AssumeRole on Dev Role): &nbsp;</p><p> &nbsp;- Doesn’t solve cross-account access. &nbsp;</p><p> Workflow: &nbsp;</p><p>1. Design team members (in dev account) assume the production role. &nbsp;</p><p>2. AWS grants temporary credentials to upload/download S3 assets. &nbsp;</p><p> Conclusion: &nbsp;</p><p>A, C, E is the secure, least-privilege solution for cross-account S3 access. &nbsp;</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "18f660ab74704fee970bf2e4d6689ac0",
      "questionNumber": 104,
      "type": "single",
      "content": "<p>Question #104</p><p>A company developed a pilot application by using AWS Elastic Beanstalk and Java. To save costs during development, the company's development team deployed the application into a single-instance environment. Recent tests indicate that the application consumes more CPU than expected. CPU utilization is regularly greater than 85%, which causes some performance bottlenecks.<br><br></p><p>A solutions architect must mitigate the performance issues before the company launches the application to production. <br><br></p><p>Which solution will meet these requirements with the LEAST operational overhead? </p>",
      "options": [
        {
          "label": "A",
          "content": "Create a new Elastic Beanstalk application. Select a load-balanced environment type. Select all Availability Zones. Add a scale-out rule that will run if the maximum CPU utilization is over 85% for 5 minutes."
        },
        {
          "label": "B",
          "content": "Create a second Elastic Beanstalk environment. Apply the traffic-splitting deployment policy. Specify a percentage of incoming traffic to direct to the new environment in the average CPU utilization is over 85% for 5 minutes."
        },
        {
          "label": "C",
          "content": "Modify the existing environment&rsquo;s capacity configuration to use a load-balanced environment type. Select all Availability Zones. Add a scale-out rule that will run if the average CPU utilization is over 85% for 5 minutes."
        },
        {
          "label": "D",
          "content": "Select the Rebuild environment action with the load balancing option. Select an Availability Zones. Add a scale-out rule that will run if the sum CPU utilization is over 85% for 5 minutes."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Answer: &nbsp;</p><p>C. Modify the existing environment’s capacity configuration to use a load-balanced environment type. Select all Availability Zones. Add a scale-out rule that will run if the average CPU utilization is over 85% for 5 minutes. &nbsp;</p><p> Explanation: &nbsp;</p><p> Key Requirements: &nbsp;</p><p>1. Performance Bottleneck: &nbsp;</p><p> &nbsp; - High CPU utilization (&gt;85%) in the single-instance Elastic Beanstalk environment. &nbsp;</p><p>2. Production Readiness: &nbsp;</p><p> &nbsp; - Need scalability (auto-scaling) and high availability (multi-AZ). &nbsp;</p><p>3. Least Operational Overhead: &nbsp;</p><p> &nbsp; - Avoid recreating the application or managing multiple environments. &nbsp;</p><p> Why Option C is Best? &nbsp;</p><p>- Convert to Load-Balanced Environment: &nbsp;</p><p> &nbsp;- Upgrades the existing single-instance environment to: &nbsp;</p><p> &nbsp; &nbsp;- Auto-scaling group (handles CPU spikes). &nbsp;</p><p> &nbsp; &nbsp;- Multi-AZ deployment (improves availability). &nbsp;</p><p>- Auto-Scaling Rule: &nbsp;</p><p> &nbsp;- Scales out when CPU &gt;85% for 5 minutes (matches observed bottleneck). &nbsp;</p><p>- Minimal Changes: &nbsp;</p><p> &nbsp;- No need to: &nbsp;</p><p> &nbsp; &nbsp;- Rebuild the app (unlike Option D). &nbsp;</p><p> &nbsp; &nbsp;- Split traffic manually (unlike Option B). &nbsp;</p><p> &nbsp; &nbsp;- Create a new app (unlike Option A). &nbsp;</p><p> Why Other Options Fail? &nbsp;</p><p>- A (New Application): &nbsp;</p><p> &nbsp;- Redundant: Requires redeploying the app to a new environment. &nbsp;</p><p>- B (Traffic Splitting): &nbsp;</p><p> &nbsp;- Complex: Manages two environments and splits traffic (not true auto-scaling). &nbsp;</p><p>- D (Rebuild Environment): &nbsp;</p><p> &nbsp;- Risky: \"Rebuild\" may cause downtime; also only selects one AZ (not highly available). &nbsp;</p><p> Implementation Steps: &nbsp;</p><p>1. In Elastic Beanstalk console: &nbsp;</p><p> &nbsp; - Go to Configuration → Capacity. &nbsp;</p><p> &nbsp; - Change Environment type to \"Load balanced\". &nbsp;</p><p> &nbsp; - Enable all Availability Zones. &nbsp;</p><p>2. Add auto-scaling rule: &nbsp;</p><p> &nbsp; - Metric: CPUUtilization. &nbsp;</p><p> &nbsp; - Threshold: 85% for 5 minutes. &nbsp;</p><p> &nbsp; - Action: Add instances. &nbsp;</p><p> Cost Consideration: &nbsp;</p><p>- Load balancer + multi-AZ adds cost but is necessary for production. &nbsp;</p><p> Conclusion: &nbsp;</p><p>C provides the simplest, most reliable path to production readiness with auto-scaling. &nbsp;</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "14fb1eb99ccf4a719e8fb11302e3d074",
      "questionNumber": 105,
      "type": "single",
      "content": "<p>Question #105</p><p>A finance company is running its business-critical application on current-generation Linux EC2 instances. The application includes a self-managed MySQL database performing heavy I/O operations. The application is working fine to handle a moderate amount of traffic during the month. However, it slows down during the final three days of each month due to month-end reporting, even though the company is using Elastic Load Balancers and Auto Scaling within its infrastructure to meet the increased demand.<br><br>Which of the following actions would allow the database to handle the month-end load with the LEAST impact on performance?</p>",
      "options": [
        {
          "label": "A",
          "content": "Pre-warming Elastic Load Balancers, using a bigger instance type, changing all Amazon EBS volumes to GP2 volumes."
        },
        {
          "label": "B",
          "content": "Performing a one-time migration of the database cluster to Amazon RDS, and creating several additional read replicas to handle the load during end of month."
        },
        {
          "label": "C",
          "content": "Using Amazon CloudWatch with AWS Lambda to change the type, size, or IOPS of Amazon EBS volumes in the cluster based on a specific CloudWatch metric."
        },
        {
          "label": "D",
          "content": "Replacing all existing Amazon EBS volumes with new PIOPS volumes that have the maximum available storage size and I/O per second by taking snapshots before the end of the month and reverting back afterwards."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Performing a one-time migration of the database cluster to Amazon RDS, and creating several additional read replicas to handle the load during end of month.</p><p>This solution addresses the core issue in the most effective way because:</p><p>Migrating to Amazon RDS removes the operational overhead of managing the MySQL database, allowing the company to focus on their application rather than database administration.</p><p>Creating additional read replicas specifically for the month-end reporting would distribute the read-heavy workload across multiple database instances, significantly improving performance during these peak periods.</p><p>Read replicas can be automatically scaled up for the month-end reporting period and scaled down afterward, providing cost-effective performance optimization.</p><p>Amazon RDS is designed to handle heavy I/O operations and provides optimized performance for database workloads.</p><p>Option A (pre-warming ELBs, using bigger instances, and changing to GP2 volumes) would provide some improvement but doesn't address the fundamental issue as effectively as distributing the read workload across multiple database instances.</p><p>Sources</p><p>Conclusion - Optimizing MySQL Running on Amazon EC2 Using Amazon EBS （https://docs.aws.amazon.com/whitepapers/latest/optimizing-mysql-on-ec2-using-amazon-ebs/conclusion.html）</p><p>Ola Money achieved operational excellence, disaster recovery site in Asia Pacific (Hyderabad) Region, and up to 60% cost savings using Amazon Aurora | AWS Database Blog （https://aws.amazon.com/blogs/database/ola-money-achieved-operational-excellence-disaster-recovery-site-in-asia-pacific-hyderabad-region-and-up-to-60-cost-savings-using-amazon-aurora/）</p><p>Strategies for Optimizing Amazon RDS Performance for High-Read Workloads | AWS re:Post （https://repost.aws/questions/QUVtqJgk3PSfe2SRFypOKOGw/strategies-for-optimizing-amazon-rds-performance-for-high-read-workloads）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "4080cab9692244e684c3158e5265138d",
      "questionNumber": 106,
      "type": "single",
      "content": "<p>Question #106</p><p>A company runs a Java application that has complex dependencies on VMs that are in the company's data center. The application is stable but the company wants to modernize the technology stack. The company wants to migrate the application to AWS and minimize the administrative overhead to maintain the servers.<br><br>Which solution will meet these requirements with the LEAST code changes?</p>",
      "options": [
        {
          "label": "A",
          "content": "Migrate the application to Amazon Elastic Container Service (Amazon ECS) on AWS Fargate by using AWS App2Container. Store container images in Amazon Elastic Container Registry (Amazon ECR). Grant the ECS task execution role permission to access the ECR image repository. Configure Amazon ECS to use an Application Load Balancer (ALB). Use the ALB to interact with the application."
        },
        {
          "label": "B",
          "content": "Migrate the application code to a container that runs in AWS Lambda. Build an Amazon API Gateway REST API with Lambda integration. Use API Gateway to interact with the application."
        },
        {
          "label": "C",
          "content": "Migrate the application to Amazon Elastic Kubernetes Service (Amazon EKS) on EKS managed node groups by using AWS App2Container. Store container images in Amazon Elastic Container Registry (Amazon ECR). Give the EKS nodes permission to access the ECR image repository. Use Amazon API Gateway to interact with the application."
        },
        {
          "label": "D",
          "content": "Migrate the application code to a container that runs in AWS Lambda. Configure Lambda to use an Application Load Balancer (ALB). Use the ALB to interact with the application."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Migrate the application to Amazon Elastic Container Service (Amazon ECS) on AWS Fargate by using AWS App2Container. Store container images in Amazon Elastic Container Registry (Amazon ECR). Grant the ECS task execution role permission 10 access the ECR image repository. Configure Amazon ECS to use an Application Load Balancer (ALB). Use the ALB to interact with the application.</p><p>This solution is the best choice because:</p><p>AWS App2Container is specifically designed to containerize existing applications with minimal code changes, which directly addresses the requirement to minimize code changes.</p><p>Using Amazon ECS on AWS Fargate provides a fully managed container orchestration service that eliminates the need to provision and maintain servers, reducing administrative overhead.</p><p>This approach preserves the application's existing architecture and dependencies while modernizing the infrastructure.</p><p>The containerization approach allows the complex dependencies to be packaged together with the application, ensuring compatibility.</p><p>Option B (migrating to AWS Lambda with API Gateway) would require significant refactoring of the Java application to fit the serverless model, especially considering the complex dependencies. Lambda has limitations on execution time and package size that might not be suitable for a complex Java application, and would likely require substantial code changes.</p><p>Sources</p><p>Migrate on-premises Java applications to AWS using AWS App2Container - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-on-premises-java-applications-to-aws-using-aws-app2container.html）</p><p>Containerizing traditional Java EE applications for the AWS Cloud - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/modernization-containerize-javaee/welcome.html）</p><p>Containerize and migrate - AWS Prescriptive Guidance (https://docs.aws.amazon.com/prescriptive-guidance/latest/containerize-java-a2c/containerize.html)</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "b78a5cc4c42a443abe3feeddd5a20e16",
      "questionNumber": 107,
      "type": "single",
      "content": "<p>Question #107</p><p>A company has an asynchronous HTTP application that is hosted as an AWS Lambda function. A public Amazon API Gateway endpoint invokes the Lambda function. The Lambda function and the API Gateway endpoint reside in the us-east-1 Region. A solutions architect needs to redesign the application to support failover to another AWS Region.<br><br>Which solution will meet these requirements? </p>",
      "options": [
        {
          "label": "A",
          "content": "Create an API Gateway endpoint in the us-west-2 Region to direct traffic to the Lambda function in us-east-1. Configure Amazon Route 53 to use a failover routing policy to route traffic for the two API Gateway endpoints."
        },
        {
          "label": "B",
          "content": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure API Gateway to direct traffic to the SQS queue instead of to the Lambda function. Configure the Lambda function to pull messages from the queue for processing."
        },
        {
          "label": "C",
          "content": "Deploy the Lambda function to the us-west-2 Region. Create an API Gateway endpoint in us-west-2 to direct traffic to the Lambda function in us-west-2. Configure AWS Global Accelerator and an Application Load Balancer to manage traffic across the two API Gateway endpoints."
        },
        {
          "label": "D",
          "content": "Deploy the Lambda function and an API Gateway endpoint to the us-west-2 Region. Configure Amazon Route 53 to use a failover routing policy to route traffic for the two API Gateway endpoints."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. Deploy the Lambda function and an API Gateway endpoint to the us-west-2 Region. Configure Amazon Route 53 to use a failover routing policy to route traffic for the two API Gateway endpoints.</p><p>This solution is the best choice because:</p><p>It provides true regional redundancy by having both the Lambda function and API Gateway deployed in two separate regions (us-east-1 and us-west-2).</p><p>Using Route 53 with a failover routing policy allows automatic traffic redirection to the healthy region if one region experiences an outage.</p><p>This approach creates a complete, independent stack in each region, eliminating cross-region dependencies that could become points of failure.</p><p>Having both components (Lambda and API Gateway) in the same region reduces latency and improves performance.</p><p>Option A (creating only an API Gateway in us-west-2 that points to the Lambda function in us-east-1) would not provide true regional failover capability because the Lambda function would still be a single point of failure in us-east-1. If that region experienced an outage, the application would still fail despite having an API Gateway endpoint in another region.</p><p>Sources</p><p>Implementing multi-Region failover for Amazon API Gateway | AWS Compute Blog （https://aws.amazon.com/cn/blogs/compute/implementing-multi-region-failover-for-amazon-api-gateway/）</p><p>Does Route 53 latency-based routing perform failover when one region performance degrades? | AWS re:Post （https://repost.aws/questions/QUGaNY82V0QlCZyjpgRvFwIQ/does-route-53-latency-based-routing-perform-failover-when-one-region-performance-degrades）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": true,
      "noteContent": "API Gateway的区域性限制"
    },
    {
      "id": "dd48837d95944956a3ae0eeccd78ba35",
      "questionNumber": 108,
      "type": "single",
      "content": "<p>Question #108</p><p>A retail company has structured its AWS accounts to be part of an organization in AWS Organizations. The company has set up consolidated billing and has mapped its departments to the following OUs: Finance, Sales, Human Resources (HR), Marketing, and Operations. Each OU has multiple AWS accounts, one for each environment within a department. These environments are development, test, pre-production, and production.<br><br>The HR department is releasing a new system that will launch in 3 months. In preparation, the HR department has purchased several Reserved Instances (RIs) in its production AWS account. The HR department wants to make sure that other departments cannot share the RI discounts.<br><br>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "In the AWS Billing and Cost Management console for the HR department&#39;s production account turn off RI sharing."
        },
        {
          "label": "B",
          "content": "Remove the HR department&#39;s production AWS account from the organization. Add the account to the consolidating billing configuration only."
        },
        {
          "label": "C",
          "content": "In the AWS Billing and Cost Management console. use the organization&rsquo;s management account to turn off RI Sharing for the HR departments production AWS account."
        },
        {
          "label": "D",
          "content": "Create an SCP in the organization to restrict access to the RIs. Apply the SCP to the OUs of the other departments."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Answer: &nbsp;</p><p>C. In the AWS Billing and Cost Management console, use the organization’s management account to turn off RI Sharing for the HR department’s production AWS account. &nbsp;</p><p> Explanation: &nbsp;</p><p> Key Requirements: &nbsp;</p><p>1. Restrict RI Discount Sharing: &nbsp;</p><p> &nbsp; - HR’s Reserved Instances (RIs) should only apply to their production account. &nbsp;</p><p> &nbsp; - Prevent other departments (Finance, Sales, etc.) from benefiting from the discounts. &nbsp;</p><p>2. AWS Organizations Context: &nbsp;</p><p> &nbsp; - The company uses consolidated billing with multiple OUs/departments. &nbsp;</p><p> &nbsp; - By default, RIs are shared across all accounts in an organization. &nbsp;</p><p> Why Option C is Correct? &nbsp;</p><p>- RI Sharing Settings: &nbsp;</p><p> &nbsp;- Only the management account of an AWS Organization can disable RI sharing. &nbsp;</p><p> &nbsp;- Navigate to: &nbsp;</p><p> &nbsp; &nbsp;AWS Billing Console → Preferences → Disable \"RI Discount Sharing\" for the HR production account. &nbsp;</p><p>- Effect: &nbsp;</p><p> &nbsp;- HR’s RIs will only discount their production account’s usage. &nbsp;</p><p> &nbsp;- Other departments’ accounts cannot use the discounts. &nbsp;</p><p> Why Other Options Fail? &nbsp;</p><p>- A (HR Account Console): &nbsp;</p><p> &nbsp;- Individual accounts cannot modify RI sharing—only the management account can. &nbsp;</p><p>- B (Remove Account from Org): &nbsp;</p><p> &nbsp;- Overkill: Loses OU benefits (e.g., SCPs, centralized logging). &nbsp;</p><p> &nbsp;- Doesn’t block sharing: RIs are still account-specific unless sharing is disabled. &nbsp;</p><p>- D (SCP to Block RI Access): &nbsp;</p><p> &nbsp;- SCPs can’t control RI sharing—they only deny permissions, not billing behaviors. &nbsp;</p><p> How RI Sharing Works: &nbsp;</p><p>| Setting &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| Default (Enabled) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | Disabled (Option C) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | &nbsp;</p><p>|----------------------------------|---------------------------------|-----------------------------------| &nbsp;</p><p>| RI Discount Scope &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| All linked accounts in the Org &nbsp;| Only the purchasing account &nbsp; &nbsp; &nbsp; | &nbsp;</p><p>| Management Required? &nbsp; &nbsp; &nbsp; &nbsp; | No &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| Yes (management account only) &nbsp; &nbsp; | &nbsp;</p><p> Conclusion: &nbsp;</p><p>To ensure HR’s RIs aren’t shared, C is the only valid solution via the management account’s billing console. &nbsp;</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "2a7d86fe4c674954a057e676c82e9d3f",
      "questionNumber": 109,
      "type": "single",
      "content": "<p>Question #109</p><p>A large company is running a popular web application. The application runs on several Amazon EC2 Linux instances in an Auto Scaling group in a private subnet. An Application Load Balancer is targeting the instances in the Auto Scaling group in the private subnet. AWS Systems Manager Session Manager is confi gured, and AWS Systems Manager Agent is running on all the EC2 instances. </p><p><br></p><p>The company recently released a new version of the application. Some EC2 instances are now being marked as unhealthy and are being terminated. As a result, the application is running at reduced capacity. A solutions architect tries to determine the root cause by analyzing Amazon CloudWatch logs that are collected from the application, but the logs are inconclusive. </p><p><br></p><p>How should the solutions architect gain access to an EC2 instance to troubleshoot the issue?</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Suspend the Auto Scaling group&rsquo;s HealthCheck scaling process. Use Session Manager to log in to an instance that is marked as unhealthy. "
        },
        {
          "label": "B",
          "content": "Enable EC2 instance termination protection. Use Session Manager to log in to an instance that is marked as unhealthy"
        },
        {
          "label": "C",
          "content": "&nbsp;Set the termination policy to OldestInstance on the Auto Scaling group. Use Session Manager to log in to an instance that is marked an unhealthy."
        },
        {
          "label": "D",
          "content": "Suspend the Auto Scaling group&rsquo;s Terminate process. Use Session Manager to log in to an instance that is marked as unhealthy. "
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Answer: &nbsp;</p><p>D. Suspend the Auto Scaling group’s Terminate process. Use Session Manager to log in to an instance that is marked as unhealthy. &nbsp;</p><p> Explanation: &nbsp;</p><p> Key Requirements: &nbsp;</p><p>1. Troubleshoot Unhealthy Instances: &nbsp;</p><p> &nbsp; - EC2 instances are being marked unhealthy and terminated before investigation. &nbsp;</p><p> &nbsp; - Need to preserve an unhealthy instance for debugging. &nbsp;</p><p>2. Access Method: &nbsp;</p><p> &nbsp; - Session Manager is already configured (SSH/RDP not required). &nbsp;</p><p>3. Minimal Disruption: &nbsp;</p><p> &nbsp; - Avoid affecting healthy instances or application availability. &nbsp;</p><p> Why Option D is Correct? &nbsp;</p><p>- Suspend Terminate Process: &nbsp;</p><p> &nbsp;- Temporarily stops Auto Scaling from terminating unhealthy instances. &nbsp;</p><p> &nbsp;- Allows time to: &nbsp;</p><p> &nbsp; &nbsp;1. Use Session Manager to access the instance. &nbsp;</p><p> &nbsp; &nbsp;2. Check logs/processes (`/var/log/`, `systemctl status`). &nbsp;</p><p> &nbsp; &nbsp;3. Identify root cause (e.g., failed health checks, app crashes). &nbsp;</p><p>- Safe and Reversible: &nbsp;</p><p> &nbsp;- Resume termination after debugging. &nbsp;</p><p> Why Other Options Fail? &nbsp;</p><p>- A (Suspend HealthCheck): &nbsp;</p><p> &nbsp;- Doesn’t prevent termination—unhealthy instances will still be replaced. &nbsp;</p><p>- B (Termination Protection): &nbsp;</p><p> &nbsp;- EC2 termination protection doesn’t block Auto Scaling terminations. &nbsp;</p><p>- C (OldestInstance Policy): &nbsp;</p><p> &nbsp;- Doesn’t preserve unhealthy instances—only affects which instance is terminated first. &nbsp;</p><p> Steps to Implement: &nbsp;</p><p>1. Suspend Terminate Process: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws autoscaling suspend-processes --auto-scaling-group-name my-asg --scaling-processes Terminate</p><p> &nbsp; ``` &nbsp;</p><p>2. Access Instance via Session Manager: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws ssm start-session --target i-1234567890abcdef0</p><p> &nbsp; ``` &nbsp;</p><p>3. Resume After Debugging: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws autoscaling resume-processes --auto-scaling-group-name my-asg --scaling-processes Terminate</p><p> &nbsp; ``` &nbsp;</p><p> Common Root Causes for Unhealthy Instances: &nbsp;</p><p>- Application crashes (check `journalctl -u &lt;service&gt;`). &nbsp;</p><p>- Failed ALB health checks (check `/var/log/nginx/error.log`). &nbsp;</p><p>- Resource exhaustion (`top`, `df -h`). &nbsp;</p><p> Conclusion: &nbsp;</p><p>D is the only option that lets you inspect unhealthy instances before termination, minimizing downtime. &nbsp;</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "24ce1b4c908c4d1eb25474b123f0a3a1",
      "questionNumber": 110,
      "type": "single",
      "content": "<p>Question #110</p><p>A company wants to deploy an AWS WAF solution to manage AWS WAF rules across multiple AWS accounts. The accounts are managed under different OUs in AWS Organizations. </p><p><br></p><p>Administrators must be able to add or remove accounts or OUs from managed AWS WAF rule sets as needed. Administrators also must have the ability to automatically update and remediate noncompliant AWS WAF rules in all accounts.<br><br>Which solution meets these requirements with the LEAST amount of operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use AWS Firewall Manager to manage AWS WAF rules across accounts in the organization. Use an AWS Systems Manager Parameter Store parameter to store account numbers and OUs to manage. Update the parameter as needed to add or remove accounts or OUs. Use an Amazon EventBridge rule to identify any changes to the parameter and to invoke an AWS Lambda function to update the security policy in the Firewall Manager administrative account."
        },
        {
          "label": "B",
          "content": "Deploy an organization-wide AWS Config rule that requires all resources in the selected OUs to associate the AWS WAF rules. Deploy automated remediation actions by using AWS Lambda to fix noncompliant resources. Deploy AWS WAF rules by using an AWS CloudFormation stack set to target the same OUs where the AWS Config rule is applied."
        },
        {
          "label": "C",
          "content": "Create AWS WAF rules in the management account of the organization. Use AWS Lambda environment variables to store account numbers and OUs to manage. Update environment variables as needed to add or remove accounts or OUs. Create cross-account IAM roles in member accounts. Assume the roles by using AWS Security Token Service (AWS STS) in the Lambda function to create and update AWS WAF rules in the member accounts."
        },
        {
          "label": "D",
          "content": "Use AWS Control Tower to manage AWS WAF rules across accounts in the organization. Use AWS Key Management Service (AWS KMS) to store account numbers and OUs to manage. Update AWS KMS as needed to add or remove accounts or OUs. Create IAM users in member accounts. Allow AWS Control Tower in the management account to use the access key and secret access key to create and update AWS WAF rules in the member accounts."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Answer: &nbsp;</p><p>A. Use AWS Firewall Manager to manage AWS WAF rules across accounts in the organization. Use an AWS Systems Manager Parameter Store parameter to store account numbers and OUs to manage. Update the parameter as needed to add or remove accounts or OUs. Use an Amazon EventBridge rule to identify any changes to the parameter and to invoke an AWS Lambda function to update the security policy in the Firewall Manager administrative account. &nbsp;</p><p> Explanation: &nbsp;</p><p> Key Requirements: &nbsp;</p><p>1. Centralized AWS WAF Management: &nbsp;</p><p> &nbsp; - Deploy and manage WAF rules across multiple accounts/OUs in AWS Organizations. &nbsp;</p><p>2. Dynamic Account/OU Management: &nbsp;</p><p> &nbsp; - Add or remove accounts/OUs from WAF rule sets without manual updates. &nbsp;</p><p>3. Automated Remediation: &nbsp;</p><p> &nbsp; - Detect and fix noncompliant WAF rules automatically. &nbsp;</p><p> Why Option A is Best? &nbsp;</p><p>- AWS Firewall Manager: &nbsp;</p><p> &nbsp;- Native AWS service for managing WAF rules across accounts in an organization. &nbsp;</p><p> &nbsp;- Supports automatic remediation of noncompliant resources. &nbsp;</p><p>- Systems Manager Parameter Store: &nbsp;</p><p> &nbsp;- Stores account/OU lists dynamically (easy to update). &nbsp;</p><p>- EventBridge + Lambda: &nbsp;</p><p> &nbsp;- Triggers policy updates when accounts/OUs are added/removed (low overhead). &nbsp;</p><p> Workflow: &nbsp;</p><p>1. Firewall Manager Admin Account: &nbsp;</p><p> &nbsp; - Defines WAF rule sets and target OUs/accounts. &nbsp;</p><p>2. Parameter Store: &nbsp;</p><p> &nbsp; - Maintains a list of managed accounts/OUs (e.g., `/firewall-manager/targets`). &nbsp;</p><p>3. EventBridge Rule: &nbsp;</p><p> &nbsp; - Monitors Parameter Store for changes → invokes Lambda to update Firewall Manager policies. &nbsp;</p><p> Why Other Options Fail? &nbsp;</p><p>- B (AWS Config + CloudFormation Stack Sets): &nbsp;</p><p> &nbsp;- No centralized WAF management (manual stack updates required). &nbsp;</p><p> &nbsp;- AWS Config rules cannot enforce WAF associations. &nbsp;</p><p>- C (Lambda + Cross-Account Roles): &nbsp;</p><p> &nbsp;- High overhead: Custom code to manage WAF rules (no native automation). &nbsp;</p><p>- D (AWS Control Tower + KMS): &nbsp;</p><p> &nbsp;- Control Tower doesn’t manage WAF rules directly. &nbsp;</p><p> &nbsp;- KMS is not designed for account/OU tracking. &nbsp;</p><p> Implementation Steps: &nbsp;</p><p>1. Enable Firewall Manager in the management account. &nbsp;</p><p>2. Create a Parameter Store parameter: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws ssm put-parameter --name \"/firewall-manager/targets\" --type \"StringList\" --value \"ou-1234,ou-5678\"</p><p> &nbsp; ``` &nbsp;</p><p>3. Deploy EventBridge + Lambda: &nbsp;</p><p> &nbsp; - Lambda updates Firewall Manager policies when the parameter changes. &nbsp;</p><p> Conclusion: &nbsp;</p><p>A provides the simplest, most scalable solution with native AWS services and minimal custom code. &nbsp;</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "d8ffa4fff19f46c58c5c3ed7cca1dadf",
      "questionNumber": 111,
      "type": "single",
      "content": "Question #111<p>A solutions architect is auditing the security setup of an AWS Lambda function for a company. The Lambda function retrieves the latest changes from an Amazon Aurora database. The Lambda function and the database run in the same VPC. Lambda environment variables are providing the database credentials to the Lambda function.<br><br></p><p>The Lambda function aggregates data and makes the data available in an Amazon S3 bucket that is confi gured for server-side encryption with AWS KMS managed encryption keys (SSE-KMS). The data must not travel across the Internet. If any database credentials become compromised, the company needs a solution that minimizes the impact of the compromise. </p><p><br>What should the solutions architect recommend to meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Enable IAM database authentication on the Aurora DB cluster. Change the IAM role for the Lambda function to allow the function to access the database by using IAM database authentication. Deploy a gateway VPC endpoint for Amazon S3 in the VPC."
        },
        {
          "label": "B",
          "content": "Enable IAM database authentication on the Aurora DB cluster. Change the IAM role for the Lambda function to allow the function to access the database by using IAM database authentication. Enforce HTTPS on the connection to Amazon S3 during data transfers."
        },
        {
          "label": "C",
          "content": "Save the database credentials in AWS Systems Manager Parameter Store. Set up password rotation on the credentials in Parameter Store. Change the IAM role for the Lambda function to allow the function to access Parameter Store. Modify the Lambda function to retrieve the credentials from Parameter Store.Modify the Lambda function to retrieve the credentials from Parameter Store. Deploy a gateway VPC endpoint for Amazon S3 in the VPC."
        },
        {
          "label": "D",
          "content": "Save the database credentials in AWS Secrets Manager. Set up password rotation on the credentials in Secrets Manager. Change the IAM role for the Lambda function to allow the function to access Secrets Manager. Modify the Lambda function to retrieve the credentials from Secrets Manager. Enforce HTTPS on the connection to Amazon S3 during data transfers."
        }
      ],
      "correctAnswer": "A",
      "explanation": "Option A is the correct answer as it leverages IAM database authentication for the Aurora DB cluster, which eliminates the need to store and manage database credentials in the Lambda environment variables. By deploying a gateway VPC endpoint for Amazon S3, data transfers to S3 are kept within the AWS network, reducing the risk of data exposure over the internet. This solution provides both security and compliance benefits by using AWS managed services and features.",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "df86fdab54d94753964ac010d974fd6e",
      "questionNumber": 112,
      "type": "single",
      "content": "<p>Question #112</p><p>A large mobile gaming company has successfully migrated all of its on-premises infrastructure to the AWS Cloud. A solutions architect is reviewing the environment to ensure that it was built according to the design and that it is running in alignment with the Well-Architected Framework.<br><br></p><p>While reviewing previous monthly costs in Cost Explorer, the solutions architect notices that the creation and subsequent termination of several &nbsp;large instance types account for a high proportion of the costs. The solutions architect finds out that the company’s developers are launching new Amazon EC2 instances as part of their testing and that the developers are not using the appropriate instance types. </p><p><br></p><p>The solutions architect must implement a control mechanism to limit the instance types that only the developers can launch. </p><p><br></p><p>Which solution will meet these requirements? </p>",
      "options": [
        {
          "label": "A",
          "content": "Create a desired-instance-type managed rule in AWS Config. Configure the rule with the instance types that are allowed. Attach the rule to an event to run each time a new EC2 instance is launched."
        },
        {
          "label": "B",
          "content": "In the EC2 console, create a launch template that specifies the instance types that are allowed. Assign the launch template to the developers&rsquo; IAM accounts."
        },
        {
          "label": "C",
          "content": "Create a new IAM policy. Specify the instance types that are allowed. Attach the policy to an IAM group that contains the IAM accounts for the developers."
        },
        {
          "label": "D",
          "content": "Use EC2 Image Builder to create an image pipeline for the developers and assist them in the creation of a golden image."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Option C is the correct answer as it involves creating a custom IAM policy that restricts the instance types that developers can launch. By specifying allowed instance types in the policy and attaching it to an IAM group associated with the developers, the company can enforce cost optimization and prevent the use of non-compliant instance types. This approach provides direct control over resource usage and aligns with the principle of least privilege.</p><p>C. Create a new IAM policy. Specify the instance types that are allowed. Attach the policy to an IAM group that contains the IAM accounts for the developers. &nbsp;</p><p> Explanation: &nbsp;</p><p> Key Requirements: &nbsp;</p><p>1. Limit EC2 Instance Types: &nbsp;</p><p> &nbsp; - Restrict developers to specific instance types (e.g., t3.medium, t3.large). &nbsp;</p><p>2. Cost Control: &nbsp;</p><p> &nbsp; - Prevent use of large/expensive instances (e.g., m5.24xlarge). &nbsp;</p><p>3. Alignment with Well-Architected Framework: &nbsp;</p><p> &nbsp; - Cost Optimization: Enforce least-privilege resource provisioning. &nbsp;</p><p> Why Option C is Best? &nbsp;</p><p>- IAM Policy Granular Control: &nbsp;</p><p> &nbsp;- Create a policy that denies `ec2:RunInstances` for all instance types except approved ones: &nbsp;</p><p> &nbsp; &nbsp;```json</p><p> &nbsp; &nbsp;{</p><p> &nbsp; &nbsp; &nbsp;\"Version\": \"2012-10-17\",</p><p> &nbsp; &nbsp; &nbsp;\"Statement\": [</p><p> &nbsp; &nbsp; &nbsp; &nbsp;{</p><p> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"Effect\": \"Deny\",</p><p> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"Action\": \"ec2:RunInstances\",</p><p> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"Resource\": \"arn:aws:ec2:*:*:instance/*\",</p><p> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"Condition\": {</p><p> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"StringNotEquals\": {</p><p> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"ec2:InstanceType\": [\"t3.medium\", \"t3.large\"]</p><p> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}</p><p> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}</p><p> &nbsp; &nbsp; &nbsp; &nbsp;}</p><p> &nbsp; &nbsp; &nbsp;]</p><p> &nbsp; &nbsp;}</p><p> &nbsp; &nbsp;``` &nbsp;</p><p> &nbsp;- Attach to the developers’ IAM group (centralized management). &nbsp;</p><p>- Advantages: &nbsp;</p><p> &nbsp;- No manual oversight (automatically enforced). &nbsp;</p><p> &nbsp;- No infrastructure changes (unlike launch templates/Image Builder). &nbsp;</p><p> Why Other Options Fail? &nbsp;</p><p>- A (AWS Config Rule): &nbsp;</p><p> &nbsp;- Reactive: Only detects noncompliant instances after creation (doesn’t prevent launch). &nbsp;</p><p>- B (Launch Template): &nbsp;</p><p> &nbsp;- Easily bypassed: Developers can ignore templates and launch instances directly. &nbsp;</p><p>- D (EC2 Image Builder): &nbsp;</p><p> &nbsp;- Doesn’t restrict instance types (only standardizes AMIs). &nbsp;</p><p> Implementation Steps: &nbsp;</p><p>1. Create IAM Policy: &nbsp;</p><p> &nbsp; - Use the example above, adjusting allowed `InstanceType` values. &nbsp;</p><p>2. Attach to Developers’ Group: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws iam attach-group-policy --group-name Developers --policy-arn arn:aws:iam::123456789012:policy/RestrictInstanceTypes</p><p> &nbsp; ``` &nbsp;</p><p>3. Test: &nbsp;</p><p> &nbsp; - Verify developers cannot launch unapproved instances. &nbsp;</p><p> Alternative (SCP for Organizations): &nbsp;</p><p>If using AWS Organizations, apply a Service Control Policy (SCP) at the OU level: &nbsp;</p><p>```json</p><p>{</p><p> &nbsp;\"Version\": \"2012-10-17\",</p><p> &nbsp;\"Statement\": [</p><p> &nbsp; &nbsp;{</p><p> &nbsp; &nbsp; &nbsp;\"Effect\": \"Deny\",</p><p> &nbsp; &nbsp; &nbsp;\"Action\": \"ec2:RunInstances\",</p><p> &nbsp; &nbsp; &nbsp;\"Resource\": \"*\",</p><p> &nbsp; &nbsp; &nbsp;\"Condition\": {</p><p> &nbsp; &nbsp; &nbsp; &nbsp;\"StringNotEquals\": {</p><p> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"ec2:InstanceType\": [\"t3.medium\", \"t3.large\"]</p><p> &nbsp; &nbsp; &nbsp; &nbsp;}</p><p> &nbsp; &nbsp; &nbsp;}</p><p> &nbsp; &nbsp;}</p><p> &nbsp;]</p><p>}</p><p>``` &nbsp;</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "1a46f7e517c54601aaab6443e39fb780",
      "questionNumber": 113,
      "type": "multiple",
      "content": "<p>Question #113</p><p>A company is developing and hosting several projects in the AWS Cloud. The projects are developed across multiple AWS accounts under the same organization in AWS Organizations. The company requires the cost for cloud infrastructure to be allocated to the owning project. The team responsible for all of the AWS accounts has discovered that several Amazon EC2 instances are lacking the Project tag used for cost allocation.</p><p><br></p><p>Which actions should a solutions architect lake to resolve the problem and prevent it from happening in the future? (Choose three.) </p>",
      "options": [
        {
          "label": "A",
          "content": "Create an AWS Config rule in each account to find resources with missing tags."
        },
        {
          "label": "B",
          "content": "Create an SCP in the organization with a deny action for ec2:RunInstances if the Project tag is missing."
        },
        {
          "label": "C",
          "content": "Use Amazon Inspector in the organization to find resources with missing tags."
        },
        {
          "label": "D",
          "content": "Create an IAM policy in each account with a deny action for ec2:RunInstances if the Project tag is missing."
        },
        {
          "label": "E",
          "content": "Create an AWS Config aggregator for the organization to collect a list of EC2 instances with the missing Project tag."
        },
        {
          "label": "F",
          "content": "Use AWS Security Hub to aggregate a list of EC2 instances with the missing Project tag."
        }
      ],
      "correctAnswer": "ABE",
      "explanation": "<p>Based on the information provided, the correct answers are:</p><p>A. Create an AWS Config rule in each account to find resources with missing tags. </p><p>B. Create an SCP in the organization with a deny action for ec2:RunInstances if the Project tag is missing. </p><p>E. Create an AWS Config aggregator for the organization to collect a list of EC2 instances with the missing Project tag.</p><p>These three actions together provide a comprehensive solution to both resolve the current issue and prevent it from happening in the future:</p><p>AWS Config rules in each account (option A) will help identify existing EC2 instances that are missing the required Project tag. This addresses the immediate problem of finding resources that need to be tagged.</p><p>Creating an SCP with a deny action for ec2:RunInstances if the Project tag is missing (option B) is a preventive measure that ensures all new EC2 instances will have the required tag. This prevents the problem from recurring in the future by enforcing the tagging policy at the organization level.</p><p>Setting up an AWS Config aggregator (option E) provides centralized visibility across all accounts in the organization. This allows the team responsible for all AWS accounts to monitor compliance with tagging policies across the entire organization from a single location.</p><p>Together, these actions create a robust solution that addresses both the current missing tags and prevents future instances from being created without proper cost allocation tags.</p><p>Sources</p><p>Building a cost allocation strategy - Best Practices for Tagging AWS Resources （https://docs.aws.amazon.com/whitepapers/latest/tagging-best-practices/building-a-cost-allocation-strategy.html）</p><p>Implementing and enforcing tagging - Best Practices for Tagging AWS Resources （https://docs.aws.amazon.com/whitepapers/latest/tagging-best-practices/implementing-and-enforcing-tagging.html）</p><p>AWS Organizations – Tag policies - Best Practices for Tagging AWS Resources （https://docs.aws.amazon.com/whitepapers/latest/tagging-best-practices/aws-organizations-tag-policies.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": true,
      "noteContent": "Config rule,Config aggregator"
    },
    {
      "id": "adc6fa9edeb74d49a378d0c80258f8dd",
      "questionNumber": 114,
      "type": "multiple",
      "content": "Question #114<p>A company has an on-premises monitoring solution using a PostgreSQL database for persistence of events. The database is unable to scale due to heavy ingestion and it frequently runs out of storage. </p><p>The company wants to create a hybrid solution and has already set up a VPN connection between its network and AWS. The solution should include the following attributes: </p><p>• Managed AWS services to minimize operational complexity. </p><p>• A buffer that automatically scales to match the throughput of data and requires no ongoing administration. </p><p>• A visualization tool to create dashboards to observe events in near-real time. </p><p>• Support for semi-structured JSON data and dynamic schemas. </p><p><br></p><p>Which combination of components will enable the company to create a monitoring solution that will satisfy these requirements? (Choose two.) </p>",
      "options": [
        {
          "label": "A",
          "content": "Use Amazon Kinesis Data Firehose to buffer events. Create an AWS Lambda function to process and transform events."
        },
        {
          "label": "B",
          "content": "Create an Amazon Kinesis data stream to buffer events. Create an AWS Lambda function to process and transform events."
        },
        {
          "label": "C",
          "content": "Configure an Amazon Aurora PostgreSQL DB cluster to receive events. Use Amazon QuickSight to read from the database and create near-real-time visualizations and dashboards."
        },
        {
          "label": "D",
          "content": "Configure Amazon Elasticsearch Service (Amazon ES) to receive events. Use the Kibana endpoint deployed with Amazon ES to create near-real-time visualizations and dashboards."
        },
        {
          "label": "E",
          "content": "Configure an Amazon Neptune DB instance to receive events. Use Amazon QuickSight to read from the database and create near-real-time visualizations and dashboards."
        }
      ],
      "correctAnswer": "AD",
      "explanation": "Options A and D are correct as they provide scalable and managed solutions for handling heavy data ingestion. Amazon Kinesis Data Firehose (Option A) can buffer and transport data to other services like Amazon S3, while Amazon Elasticsearch Service (Option D) provides a distributed search and analytics engine that can handle large volumes of data and offer near-real-time insights through Kibana visualizations.<p><br></p><p>还是建议选择AD</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "9ede583da6d0413cbffdcd85d433d7a3",
      "questionNumber": 115,
      "type": "single",
      "content": "Question #115<p>A team collects and routes behavioral data for an entire company. The company runs a Multi-AZ VPC environment with public subnets, private subnets, and in internet gateway. Each public subnet also contains a NAT gateway. Most of the company’s applications read from and write to Amazon Kinesis Data Streams. Most of the workloads run in private subnets. </p><p><br></p><p>A solutions architect must review the infrastructure. The solution architect needs to reduce costs and maintain the function of the applications. </p><p><br></p><p>The solutions architect uses Cost Explorer and notices that the cost in the EC2-Other category is consistently high. A further review shows that NatGateway-Bytes charges are increasing the cost in the EC2-Other category. </p><p><br></p><p>What should the solutions architect do to meet these requirements? </p>",
      "options": [
        {
          "label": "A",
          "content": "Enable VPC Flow Logs. Use Amazon Athena to analyze the logs for traffic that can be removed. Ensure that security groups are blocking traffic that is responsible for high costs."
        },
        {
          "label": "B",
          "content": "Add an interface VPC endpoint for Kinesis Data Streams to the VPC. Ensure that applications have the correct IAM permissions to use the interface VPC endpoint."
        },
        {
          "label": "C",
          "content": "Enable VPC Flow Logs and Amazon Detective. Review Detective findings for traffic that is not related to Kinesis Data Streams. Configure security groups to block that traffic."
        },
        {
          "label": "D",
          "content": "Add an interface VPC endpoint for Kinesis Data Streams to the VPC. Ensure that the VPC endpoint policy allows traffic from the applications."
        }
      ],
      "correctAnswer": "D",
      "explanation": "Option D is the correct choice as it involves adding a VPC endpoint for Kinesis Data Streams, which allows applications to access the service without going through the internet gateway or NAT gateway, thus reducing costs associated with NAT gateway usage. The VPC endpoint policy ensures that only the necessary traffic from the applications is allowed, which helps maintain the function of the applications while optimizing for cost.",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "a1f3a8decfe44c5eb9b931d0b6b6cba8",
      "questionNumber": 116,
      "type": "single",
      "content": "<p>Question #116</p><p>A retail company has an on-premises data center in Europe. The company also has a multi-Region AWS presence that includes the eu-west-1 and us-east-1 Regions. The company wants to be able to route network traffi c from its on-premises infrastructure into VPCs in either of those Regions. The company also needs to support traffi c that is routed directly between VPCs in those Regions. No single points of failure can exist on the network. </p><p><br></p><p>The company already has created two 1 Gbps AWS Direct Connect connections from its on-premises data center. Each connection goes into a separate Direct Connect location in Europe for high availability. These two locations are named DX-A and DX-B, respectively. Each Region has a single AWS Transit Gateway that is confi gured to route all inter-VPC traffi c within that Region. </p><p><br></p><p>Which solution will meet these requirements? </p>",
      "options": [
        {
          "label": "A",
          "content": "Create a private VIF from the DX-A connection into a Direct Connect gateway. Create a private VIF from the DX-B connection into the same Direct Connect gateway for high availability. Associate both the eu-west-1 and us-east-1 transit gateways with the Direct Connect gateway.Peer the transit gateways with each other to support cross-Region routing.&nbsp;"
        },
        {
          "label": "B",
          "content": "Create a transit VIF from the DX-A connection into a Direct Connect gateway. Associate the eu-west-1 transit gateway with this Direct Connect gateway. Create a transit VIF from the DX-B connection into a separate Direct Connect gateway. Associate the us-east-1 transit gateway with this separate Direct Connect gateway. Peer the Direct Connect gateways with each other to support high availability and cross Region routing."
        },
        {
          "label": "C",
          "content": "Create a transit VIF from the DX-A connection into a Direct Connect gateway. Create a transit VIF from the DX-B connection into the same Direct Connect gateway for high availability. Associate both the eu-west-1 and us-east-1 transit gateways with this Direct Connect gateway.Configure the Direct Connect gateway to route traffi c between the transit gateways.&nbsp;"
        },
        {
          "label": "D",
          "content": "Create a transit VIF from the DX-A connection into a Direct Connect gateway. Create a transit VIF from the DX-B connection into the same Direct Connect gateway for high availability. Associate both the eu-west-1 and us-east-1 transit gateways with this Direct Connect gateway. Peer the transit gateways with each other to support cross-Region routing."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Answer: &nbsp;</p><p>D. Create a transit VIF from the DX-A connection into a Direct Connect gateway. Create a transit VIF from the DX-B connection into the same Direct Connect gateway for high availability. Associate both the eu-west-1 and us-east-1 transit gateways with this Direct Connect gateway. Peer the transit gateways with each other to support cross-Region routing. &nbsp;</p><p> Explanation: &nbsp;</p><p> Key Requirements: &nbsp;</p><p>1. Hybrid Connectivity: &nbsp;</p><p> &nbsp; - On-premises (Europe) → AWS (eu-west-1 + us-east-1). &nbsp;</p><p>2. Inter-Region VPC Routing: &nbsp;</p><p> &nbsp; - Direct traffic between eu-west-1 and us-east-1 VPCs. &nbsp;</p><p>3. High Availability (No SPOF): &nbsp;</p><p> &nbsp; - Leverage two Direct Connect (DX) connections (DX-A + DX-B). &nbsp;</p><p> Why Option D is Correct? &nbsp;</p><p>- Transit VIFs + Direct Connect Gateway: &nbsp;</p><p> &nbsp;- Transit VIFs (from DX-A/DX-B) connect to a single Direct Connect gateway. &nbsp;</p><p> &nbsp;- Direct Connect gateway associates with both Transit Gateways (eu-west-1 + us-east-1). &nbsp;</p><p>- Transit Gateway Peering: &nbsp;</p><p> &nbsp;- Enables cross-region VPC routing (eu-west-1 ↔ us-east-1). &nbsp;</p><p>- High Availability: &nbsp;</p><p> &nbsp;- Two DX connections → no single point of failure. &nbsp;</p><p> Architecture Flow: &nbsp;</p><p>1. On-Premises → AWS: &nbsp;</p><p> &nbsp; - Traffic flows via DX-A/DX-B → Direct Connect gateway → Transit Gateways. &nbsp;</p><p>2. Inter-Region (eu-west-1 ↔ us-east-1): &nbsp;</p><p> &nbsp; - Transit Gateway peering routes traffic directly. &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- A (Private VIFs): &nbsp;</p><p> &nbsp;- Private VIFs only work with VPCs, not Transit Gateways. &nbsp;</p><p>- B (Separate Direct Connect Gateways): &nbsp;</p><p> &nbsp;- No cross-region routing: Direct Connect gateways cannot peer with each other. &nbsp;</p><p>- C (Direct Connect Gateway Routing): &nbsp;</p><p> &nbsp;- Direct Connect gateways don’t route between Transit Gateways (peering is required). &nbsp;</p><p> Implementation Steps: &nbsp;</p><p>1. Create Transit VIFs: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws directconnect create-transit-virtual-interface --connection-id dxcon-xxxx --vlan 100 --direct-connect-gateway-id dgw-xxxx</p><p> &nbsp; ``` &nbsp;</p><p>2. Associate Transit Gateways: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws directconnect associate-transit-gateway-with-direct-connect-gateway --direct-connect-gateway-id dgw-xxxx --transit-gateway-id tgw-xxxx</p><p> &nbsp; ``` &nbsp;</p><p>3. Peer Transit Gateways: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws ec2 create-transit-gateway-peering-attachment --transit-gateway-id tgw-eu-west-1 --peer-transit-gateway-id tgw-us-east-1</p><p> &nbsp; ``` &nbsp;</p><p> Conclusion: &nbsp;</p><p>D is the only solution that: &nbsp;</p><p>- Provides hybrid + inter-region routing. &nbsp;</p><p>- Ensures high availability (dual DX connections). &nbsp;</p><p>- Uses native AWS networking services. &nbsp;</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": true,
      "noteContent": "transit VIF是transit gayway专门，但只能连到Direct connect gateway上。"
    },
    {
      "id": "2208c6edd2a44a6da2f75bb5355bd28c",
      "questionNumber": 117,
      "type": "multiple",
      "content": "<p>Question #117</p><p>A company is running an application in the AWS Cloud. The company's security team must approve the creation of all new IAM users. When a new IAM user is created, all access for the user must be removed automatically. The security team must then receive a notification to approve the user.<br><br>Which combination of steps will meet these requirements? (Choose three.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule. Define a pattern with the detail-type value set to AWS API Call via CloudTrail and an eventName of CreateUser."
        },
        {
          "label": "B",
          "content": "Configure CloudTrail to send a notification for the CreateUser event to an Amazon Simple Notification Service (Amazon SNS) topic."
        },
        {
          "label": "C",
          "content": "Invoke a container that runs in Amazon Elastic Container Service (Amazon ECS) with AWS Fargate technology to remove access."
        },
        {
          "label": "D",
          "content": "Invoke an AWS Step Functions state machine to remove access."
        },
        {
          "label": "E",
          "content": "Use Amazon Simple Notification Service (Amazon SNS) to notify the security team."
        },
        {
          "label": "F",
          "content": "Use Amazon Pinpoint to notify the security team."
        }
      ],
      "correctAnswer": "ADE",
      "explanation": "<p>Based on the information provided, the correct answers are:</p><p>A. Create an Amazon EventBridge (Amazon CloudWatch Events) rule. Define a pattern with the detail-type value set to AWS API Call via CloudTrail and an eventName of CreateUser.</p><p>D. Invoke an AWS Step Functions state machine to remove access.</p><p>E. Use Amazon Simple Notification Service (Amazon SNS) to notify the security team.</p><p>These three options together create an automated workflow that meets all the requirements:</p><p>Option A establishes the detection mechanism. By creating an EventBridge rule that monitors CloudTrail for CreateUser events, the system can automatically detect whenever a new IAM user is created in the AWS account.</p><p>Option D provides the automation to remove access. When the EventBridge rule detects a new user creation, it invokes a Step Functions state machine that can execute the necessary steps to remove all access for the newly created user.</p><p>Option E handles the notification requirement. Amazon SNS is the appropriate service for sending notifications to the security team about the new user creation, allowing them to review and approve the user as needed.</p><p>Option F (using Amazon Pinpoint) is not the best choice for this scenario. While Pinpoint is a good service for customer engagement and marketing communications, Amazon SNS is more appropriate for operational notifications to internal teams like the security team in this case.</p><p>This combination creates an effective automated workflow that detects new IAM user creations, removes access, and notifies the security team for approval, meeting all the stated requirements.</p><p>Sources</p><p>Send a notification when an IAM user is created - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/send-a-notification-when-an-iam-user-is-created.html）</p><p>Need some help with my setting(EventBridge) | AWS re:Post （https://repost.aws/questions/QUfeVDFmhfTpSWLNQHGUylmg/need-some-help-with-my-setting-eventbridge）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "3e6d9b6b49c34c93b2e4f641f012fcaf",
      "questionNumber": 118,
      "type": "multiple",
      "content": "<p>Question #118</p><p>A company wants to migrate to AWS. The company wants to use a multi-account structure with centrally managed access to all accounts and applications. The company also wants to keep the traffic on a private network. Multi-factor authentication (MFA) is required at login, and specific roles are assigned to user groups.<br><br></p><p>The company must create separate accounts for development. staging, production, and shared network. The production account and the shared network account must have connectivity to all accounts. The development account and the staging account must have access only to each other. </p><p><br></p><p>Which combination of steps should a solutions architect take 10 meet these requirements? (Choose three.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Deploy a landing zone environment by using AWS Control Tower. Enroll accounts and invite existing accounts into the resulting organization in AWS Organizations."
        },
        {
          "label": "B",
          "content": "Enable AWS Security Hub in all accounts to manage cross-account access. Collect findings through AWS CloudTrail to force MFA login."
        },
        {
          "label": "C",
          "content": "Create transit gateways and transit gateway VPC attachments in each account. Configure appropriate route tables."
        },
        {
          "label": "D",
          "content": "Set up and enable AWS IAM Identity Center (AWS Single Sign-On). Create appropriate permission sets with required MFA for existing accounts."
        },
        {
          "label": "E",
          "content": "Enable AWS Control Tower in all accounts to manage routing between accounts. Collect findings through AWS CloudTrail to force MFA login."
        },
        {
          "label": "F",
          "content": "Create IAM users and groups. Configure MFA for all users. Set up Amazon Cognito user pools and Identity pools to manage access to accounts and between accounts."
        }
      ],
      "correctAnswer": "ACD",
      "explanation": "<p>Answer: &nbsp;</p><p>A. Deploy a landing zone environment by using AWS Control Tower. Enroll accounts and invite existing accounts into the resulting organization in AWS Organizations. &nbsp;</p><p>C. Create transit gateways and transit gateway VPC attachments in each account. Configure appropriate route tables. &nbsp;</p><p>D. Set up and enable AWS IAM Identity Center (AWS Single Sign-On). Create appropriate permission sets with required MFA for existing accounts. &nbsp;</p><p> Explanation: &nbsp;</p><p> Key Requirements: &nbsp;</p><p>1. Multi-Account Structure: &nbsp;</p><p> &nbsp; - Separate accounts for development, staging, production, shared network. &nbsp;</p><p>2. Centralized Access & MFA: &nbsp;</p><p> &nbsp; - Single sign-on (SSO) with MFA enforcement. &nbsp;</p><p>3. Private Network Connectivity: &nbsp;</p><p> &nbsp; - Production/shared network accounts connect to all accounts. &nbsp;</p><p> &nbsp; - Development/staging accounts connect only to each other. &nbsp;</p><p> Why A + C + D? &nbsp;</p><p>- A (AWS Control Tower Landing Zone): &nbsp;</p><p> &nbsp;- Automates multi-account setup with best practices (e.g., guardrails, logging). &nbsp;</p><p> &nbsp;- Enrolls accounts into AWS Organizations for central management. &nbsp;</p><p>- C (Transit Gateway Routing): &nbsp;</p><p> &nbsp;- Private connectivity between VPCs across accounts: &nbsp;</p><p> &nbsp; &nbsp;- Transit Gateway in each account + VPC attachments. &nbsp;</p><p> &nbsp; &nbsp;- Route tables control traffic flow (e.g., block dev ↔ prod). &nbsp;</p><p>- D (IAM Identity Center): &nbsp;</p><p> &nbsp;- Centralized SSO with MFA enforcement. &nbsp;</p><p> &nbsp;- Permission sets define role-based access (e.g., \"Dev-ReadOnly\"). &nbsp;</p><p> Workflow: &nbsp;</p><p>1. Deploy Control Tower (A): &nbsp;</p><p> &nbsp; - Creates OU structure (e.g., `Dev`, `Staging`, `Prod`, `Shared`). &nbsp;</p><p> &nbsp; - Applies guardrails (e.g., \"MFA required\"). &nbsp;</p><p>2. Configure Transit Gateway (C): &nbsp;</p><p> &nbsp; - Shared Network TGW peers with all accounts. &nbsp;</p><p> &nbsp; - Dev/Staging TGWs peer only with each other. &nbsp;</p><p>3. Set Up IAM Identity Center (D): &nbsp;</p><p> &nbsp; - Define permission sets (e.g., `Prod-Admin`, `Dev-ReadOnly`). &nbsp;</p><p> &nbsp; - Assign users/groups to accounts via SSO. &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- B (Security Hub + CloudTrail): &nbsp;</p><p> &nbsp;- Security Hub doesn’t manage access (only detects misconfigurations). &nbsp;</p><p> &nbsp;- CloudTrail doesn’t enforce MFA (IAM Identity Center does). &nbsp;</p><p>- E (Control Tower for Routing): &nbsp;</p><p> &nbsp;- Control Tower doesn’t manage network routing (Transit Gateway does). &nbsp;</p><p>- F (IAM Users + Cognito): &nbsp;</p><p> &nbsp;- Manual overhead: IAM users don’t scale for multi-account. &nbsp;</p><p> &nbsp;- Cognito is for app auth, not AWS account access. &nbsp;</p><p> Implementation Steps: &nbsp;</p><p>1. Deploy Control Tower: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws controltower create-landing-zone --region us-east-1 --manifest s3://my-bucket/control-tower-config.yaml</p><p> &nbsp; ``` &nbsp;</p><p>2. Create Transit Gateways: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws ec2 create-transit-gateway --description \"Shared-Network-TGW\"</p><p> &nbsp; ``` &nbsp;</p><p>3. Configure IAM Identity Center: &nbsp;</p><p> &nbsp; - Enable MFA and assign permission sets in the AWS SSO console. &nbsp;</p><p> Conclusion: &nbsp;</p><p>A + C + D is the only combination that: &nbsp;</p><p>- Automates account setup (Control Tower). &nbsp;</p><p>- Enforces private routing (Transit Gateway). &nbsp;</p><p>- Centralizes access (IAM Identity Center + MFA). &nbsp;</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "b4c5fd31ce6a48e3af59209948de89e9",
      "questionNumber": 119,
      "type": "single",
      "content": "Question #119<p>A company runs its application in the eu-west-1 Region and has one account for each of its environments: development, testing, and production. All the environments are running 24 hours a day, 7 days a week by using stateful Amazon EC2 instances and Amazon RDS for MySQL databases. The databases are between 500 GB and 800 GB in size. </p><p><br></p><p>The development team and testing team work on business days during business hours, but the production environment operates 24 hours a day, 7 days a week. The company wants to reduce costs. All resources are tagged with an environment tag with either development, testing, or production as the key. </p><p><br></p><p>What should a solutions architect do to reduce costs with the LEAST operational effort?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an Amazon EventBridge rule that runs once every day. Configure the rule to invoke one AWS Lambda function that starts or stops instances based on the tag, day, and time."
        },
        {
          "label": "B",
          "content": "Create an Amazon EventBridge rule that runs every business day in the evening. Configure the rule to invoke an AWS Lambda function that stops instances based on the tag. Create a second EventBridge rule that runs every business day in the morning. Configure the second rule to invoke another Lambda function that starts instances based on the tag."
        },
        {
          "label": "C",
          "content": "Create an Amazon EventBridge rule that runs every business day in the evening, Configure the rule to invoke an AWS Lambda function that terminates instances based on the tag. Create a second EventBridge rule that runs every business day in the morning. Configure the second rule to invoke another Lambda function that restores the instances from their last backup based on the tag."
        },
        {
          "label": "D",
          "content": "Create an Amazon EventBridge rule that runs every hour. Configure the rule to invoke one AWS Lambda function that terminates or restores instances from their last backup based on the tag, day, and time."
        }
      ],
      "correctAnswer": "B",
      "explanation": "The correct answer is B. Creating an Amazon EventBridge rule that runs every business day in the evening to stop instances and another rule that runs every business day in the morning to start instances based on the tag will reduce costs with the least operational effort. This approach allows for instances to be stopped during non-business hours when they are not in use, reducing the costs associated with running them. It also allows for instances to be started again in the morning when the development and testing teams need to use them.",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "d4ceccbb458c4975a8f3a656093f4bb0",
      "questionNumber": 120,
      "type": "single",
      "content": "<p>Question #120</p><p>A company is building a software-as-a-service (SaaS) solution on AWS. The company has deployed an Amazon API Gateway REST API with AWS Lambda integration in multiple AWS Regions and in the same production account. </p><p><br></p><p>The company offers tiered pricing that gives customers the ability to pay for the capacity to make a certain number of API calls per second. The premium tier offers up to 3,000 calls per second, and customers are identified by a unique API key. Several premium tier customers in various Regions report that they receive error responses of 429 Too Many Requests from multiple API methods during peak usage hours. Logs indicate that the Lambda function is never invoked. </p><p><br></p><p>What could be the cause of the error messages for these customers?</p>",
      "options": [
        {
          "label": "A",
          "content": "The Lambda function reached its concurrency limit."
        },
        {
          "label": "B",
          "content": "The Lambda function its Region limit for concurrency."
        },
        {
          "label": "C",
          "content": "The company reached its API Gateway account limit for calls per second."
        },
        {
          "label": "D",
          "content": "The company reached its API Gateway default per-method limit for calls per second."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. The company reached its API Gateway account limit for calls per second.</p><p>This is the most likely cause of the error messages because:</p><p>The error response \"429 Too Many Requests\" is occurring across multiple API methods during peak usage hours, suggesting a broader throttling issue rather than a method-specific one.</p><p>The Lambda function is never invoked, which indicates that the requests are being throttled at the API Gateway level before they even reach the Lambda integration.</p><p>API Gateway has default account-level throttling limits that apply across all APIs in an account within a specific Region. These account limits are separate from and take precedence over any method-level limits.</p><p>Since the company has premium tier customers allowed to make up to 3,000 calls per second, and these customers are distributed across various Regions, the combined traffic during peak hours is likely exceeding the default account-level throttling limits in those Regions.</p><p>Option D (reaching the default per-method limit) is less likely to be the cause because:</p><p>If it were a per-method limit issue, we would expect to see the problem only on specific methods, not across multiple methods.</p><p>The company has likely configured higher method-level limits for premium tier customers, but these custom limits would still be constrained by the overall account-level limits.</p><p>To resolve this issue, the company should request an increase to their API Gateway account-level throttling limits in the affected Regions through AWS Support.</p><p>Sources</p><p>REL05-BP02 Throttle requests - AWS Well-Architected Framework （https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_mitigate_interaction_failure_throttle_requests.html）</p><p>Throttle requests to your REST APIs for better throughput in API Gateway - Amazon API Gateway （https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-request-throttling.html）</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "db6d9833214540ba8b2931c1778b6541",
      "questionNumber": 121,
      "type": "multiple",
      "content": "<p>Question #121</p><p>A financial company is planning to migrate its web application from on premises to AWS. The company uses a third-party security tool to monitor the inbound traffic to the application. The company has used the security tool for the last 15 years, and the tool has no cloud solutions available from its vendor. </p><p><br></p><p>The company's security team is concerned about how to integrate the security tool with AWS technology. </p><p>The company plans to deploy the application migration to AWS on Amazon EC2 instances. </p><p>The EC2 instances will run in an Auto Scaling group in a dedicated VPC. </p><p>The company needs to use the security tool to inspect all packets that come in and out of the VPC. </p><p><br></p><p>This inspection must occur in real time and must not affect the application's performance. A solutions architect must design a target architecture on AWS that is highly available within an AWS Region. </p><p><br></p><p>Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Deploy the security tool on EC2 instances in a new Auto Scaling group in the existing VPC"
        },
        {
          "label": "B",
          "content": "Deploy the web application behind a Network Load Balancer"
        },
        {
          "label": "C",
          "content": "Deploy an Application Load Balancer in front of the security tool instances"
        },
        {
          "label": "D",
          "content": "Provision a Gateway Load Balancer for each Availability Zone to redirect the traffic to the security tool"
        },
        {
          "label": "E",
          "content": "Provision a transit gateway to facilitate communication between VPCs."
        }
      ],
      "correctAnswer": "AD",
      "explanation": "<p><br></p><p> Answer: &nbsp;</p><p>A. Deploy the security tool on EC2 instances in a new Auto Scaling group in the existing VPC. &nbsp;</p><p>D. Provision a Gateway Load Balancer for each Availability Zone to redirect the traffic to the security tool. &nbsp;</p><p> Explanation: &nbsp;</p><p> Key Requirements: &nbsp;</p><p>1. Real-Time Traffic Inspection: &nbsp;</p><p> &nbsp; - All inbound/outbound VPC traffic must be inspected by the legacy security tool. &nbsp;</p><p>2. High Availability: &nbsp;</p><p> &nbsp; - Solution must be resilient within a single AWS Region. &nbsp;</p><p>3. No Performance Impact: &nbsp;</p><p> &nbsp; - Inspection must not degrade application performance. &nbsp;</p><p>4. Legacy Tool Integration: &nbsp;</p><p> &nbsp; - The security tool has no cloud-native version (must run on EC2). &nbsp;</p><p> Why A + D? &nbsp;</p><p>- Gateway Load Balancer (D): &nbsp;</p><p> &nbsp;- Traffic Mirroring: Redirects all VPC traffic to security tool instances. &nbsp;</p><p> &nbsp;- High Availability: Deploy one per AZ (ensures redundancy). &nbsp;</p><p> &nbsp;- No Performance Hit: Operates at line speed (unlike inline proxies). &nbsp;</p><p>- Security Tool on EC2 (A): &nbsp;</p><p> &nbsp;- Auto Scaling group ensures tool availability. &nbsp;</p><p> &nbsp;- Same VPC avoids cross-VPC latency. &nbsp;</p><p> Architecture Flow: &nbsp;</p><p>1. Inbound Traffic: &nbsp;</p><p> &nbsp; - Internet → Gateway Load Balancer → Security Tool EC2 Instances → Web Application EC2. &nbsp;</p><p>2. Outbound Traffic: &nbsp;</p><p> &nbsp; - Web Application EC2 → Gateway Load Balancer → Security Tool EC2 Instances → Internet. &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- B (Network Load Balancer): &nbsp;</p><p> &nbsp;- Doesn’t enable traffic inspection (passes traffic directly to apps). &nbsp;</p><p>- C (Application Load Balancer): &nbsp;</p><p> &nbsp;- L7 only: Can’t inspect raw packets (security tool needs L3/L4). &nbsp;</p><p>- E (Transit Gateway): &nbsp;</p><p> &nbsp;- Irrelevant: No multi-VPC requirement. &nbsp;</p><p> Implementation Steps: &nbsp;</p><p>1. Deploy Gateway Load Balancer: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws ec2 create-load-balancer --type gateway --name my-gwlb --subnets subnet-123456 subnet-789012</p><p> &nbsp; ``` &nbsp;</p><p>2. Register Security Tool Targets: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws ec2 register-targets --target-group-arn arn:aws:elasticloadbalancing:us-east-1:123456789012:targetgroup/my-gwlb-tg/1234567890123456 --targets Id=i-1234567890abcdef0</p><p> &nbsp; ``` &nbsp;</p><p>3. Update VPC Route Tables: &nbsp;</p><p> &nbsp; - Route 0.0.0.0/0 to the Gateway Load Balancer. &nbsp;</p><p> Conclusion: &nbsp;</p><p>A + D is the only combination that: &nbsp;</p><p>- Integrates the legacy tool (EC2-based). &nbsp;</p><p>- Inspects all traffic (Gateway LB). &nbsp;</p><p>- Ensures high availability. &nbsp;</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": true,
      "noteContent": "典型的第三方网络虚拟设备（appliance）集成场景"
    },
    {
      "id": "3db5edd6beeb4d8681b2f1a9ce812744",
      "questionNumber": 122,
      "type": "single",
      "content": "<p>Question #122</p><p>A company has purchased appliances from different vendors. The appliances all have IoT sensors. The sensors send status information in the vendors' proprietary formats to a legacy application that parses the information into JSON. The parsing is simple, but each vendor has a unique format. Once daily, the application parses all the JSON records and stores the records in a relational database for analysis. </p><p><br></p><p>The company needs to design a new data analysis solution that can deliver faster and optimize costs. </p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Connect the IoT sensors to AWS IoT Core. Set a rule to invoke an AWS Lambda function to parse the information and save a .csv file to Amazon S3. Use AWS Glue to catalog the files. Use Amazon Athena and Amazon QuickSight for analysis."
        },
        {
          "label": "B",
          "content": "Migrate the application server to AWS Fargate, which will receive the information from IoT sensors and parse the information into a relational format. Save the parsed information to Amazon Redshift for analysis."
        },
        {
          "label": "C",
          "content": "Create an AWS Transfer for SFTP server. Update the IoT sensor code to send the information as a .csv file through SFTP to the server. Use AWS Glue to catalog the files. Use Amazon Athena for analysis."
        },
        {
          "label": "D",
          "content": "Use AWS Snowball Edge to collect data from the IoT sensors directly to perform local analysis. Periodically collect the data into Amazon Redshift to perform global analysis."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Answer: &nbsp;</p><p>A. Connect the IoT sensors to AWS IoT Core. Set a rule to invoke an AWS Lambda function to parse the information and save a .csv file to Amazon S3. Use AWS Glue to catalog the files. Use Amazon Athena and Amazon QuickSight for analysis. &nbsp;</p><p> Explanation: &nbsp;</p><p> Key Requirements: &nbsp;</p><p>1. Real-Time Data Ingestion: &nbsp;</p><p> &nbsp; - IoT sensors send data in proprietary formats (need parsing). &nbsp;</p><p>2. Cost Optimization & Speed: &nbsp;</p><p> &nbsp; - Replace the legacy relational database with a serverless, scalable solution. &nbsp;</p><p>3. Data Analysis: &nbsp;</p><p> &nbsp; - Support ad-hoc queries and visualizations. &nbsp;</p><p> Why Option A is Best? &nbsp;</p><p>- AWS IoT Core: &nbsp;</p><p> &nbsp;- Centralized ingestion for all IoT devices (handles vendor diversity). &nbsp;</p><p> &nbsp;- Rules Engine routes data to Lambda for parsing. &nbsp;</p><p>- Lambda Parsing: &nbsp;</p><p> &nbsp;- Converts proprietary formats → standardized JSON/CSV. &nbsp;</p><p> &nbsp;- Serverless: Scales with sensor volume (no EC2 costs). &nbsp;</p><p>- S3 + Glue + Athena: &nbsp;</p><p> &nbsp;- S3: Low-cost storage for parsed data. &nbsp;</p><p> &nbsp;- Glue Crawler: Auto-discovers schema (no manual ETL). &nbsp;</p><p> &nbsp;- Athena: Serverless SQL queries (pay per query). &nbsp;</p><p> &nbsp;- QuickSight: Visualizations with minimal setup. &nbsp;</p><p> Architecture Flow: &nbsp;</p><p>1. IoT Sensors → AWS IoT Core (ingest raw data). &nbsp;</p><p>2. IoT Rule → Lambda (parse to CSV). &nbsp;</p><p>3. Lambda → S3 (store parsed data). &nbsp;</p><p>4. Glue Catalog → Athena/QuickSight (analyze). &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- B (Fargate + Redshift): &nbsp;</p><p> &nbsp;- Overkill: Redshift is expensive for sporadic analysis. &nbsp;</p><p> &nbsp;- No real-time parsing (daily batch remains). &nbsp;</p><p>- C (SFTP Server): &nbsp;</p><p> &nbsp;- Manual effort: Requires sensor code changes (to CSV). &nbsp;</p><p> &nbsp;- No real-time processing. &nbsp;</p><p>- D (Snowball Edge): &nbsp;</p><p> &nbsp;- Irrelevant: Sensors are connected; no need for physical collection. &nbsp;</p><p> Cost & Performance Benefits: &nbsp;</p><p>| Component &nbsp; &nbsp; &nbsp; | Advantage &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp;</p><p>|----------------|-------------------------------------------| &nbsp;</p><p>| AWS IoT Core | No servers to manage; scales with devices. | &nbsp;</p><p>| Lambda &nbsp; &nbsp; &nbsp;| Pay per parse (~$0.20 per million requests). | &nbsp;</p><p>| S3/Athena &nbsp; | ~$5/TB scanned (vs. Redshift’s ~$1,000/TB/year). | &nbsp;</p><p> Conclusion: &nbsp;</p><p>A delivers real-time parsing, serverless scalability, and cost-efficient analysis—perfect for IoT sensor data. &nbsp;</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "9841d7ec09e24d3a9c5fa191fa9d9d4a",
      "questionNumber": 123,
      "type": "multiple",
      "content": "<p>Question #123</p><p>A company is migrating some of its applications to AWS. The company wants to migrate and modernize the applications quickly after it finalizes networking and security strategies. The company has set up an AWS Direct Connect connection in a central network account. <br><br>The company expects to have hundreds of AWS accounts and VPCs in the near future. The corporate network must be able to access the resources on AWS seamlessly and also must be able to communicate with all the VPCs. The company also wants to route its cloud resources to the internet through its on-premises data center. <br><br>Which combination of steps will meet these requirements? (Choose three.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a Direct Connect gateway in the central account. In each of the accounts, create an association proposal by using the Direct Connect gateway and the account ID for every virtual private gateway."
        },
        {
          "label": "B",
          "content": "Create a Direct Connect gateway and a transit gateway in the central network account. Attach the transit gateway to the Direct Connect gateway by using a transit VIF."
        },
        {
          "label": "C",
          "content": "Provision an internet gateway. Attach the internet gateway to subnets. Allow internet traffic through the gateway."
        },
        {
          "label": "D",
          "content": "Share the transit gateway with other accounts. Attach VPCs to the transit gateway."
        },
        {
          "label": "E",
          "content": "Provision VPC peering as necessary."
        },
        {
          "label": "F",
          "content": "Provision only private subnets. Open the necessary route on the transit gateway and customer gateway to allow outbound internet traffic from AWS to flow through NAT services that run in the data center."
        }
      ],
      "correctAnswer": "BDF",
      "explanation": "<p>Answer: &nbsp;</p><p>B. Create a Direct Connect gateway and a transit gateway in the central network account. Attach the transit gateway to the Direct Connect gateway by using a transit VIF. &nbsp;</p><p>D. Share the transit gateway with other accounts. Attach VPCs to the transit gateway. &nbsp;</p><p>F. Provision only private subnets. Open the necessary route on the transit gateway and customer gateway to allow outbound internet traffic from AWS to flow through NAT services that run in the data center. &nbsp;</p><p> Explanation: &nbsp;</p><p> Key Requirements: &nbsp;</p><p>1. Hybrid Connectivity: &nbsp;</p><p> &nbsp; - Direct Connect (DX) for on-premises ↔ AWS (central network account). &nbsp;</p><p> &nbsp; - Seamless access across hundreds of accounts/VPCs. &nbsp;</p><p>2. Internet Routing via On-Premises: &nbsp;</p><p> &nbsp; - Cloud resources must egress to the internet through the data center. &nbsp;</p><p>3. Scalability: &nbsp;</p><p> &nbsp; - Avoid VPC peering (doesn’t scale to hundreds of VPCs). &nbsp;</p><p> Why B + D + F? &nbsp;</p><p>- B (Direct Connect Gateway + Transit Gateway): &nbsp;</p><p> &nbsp;- Transit VIF connects DX to the Transit Gateway (central routing hub). &nbsp;</p><p> &nbsp;- Direct Connect Gateway enables cross-account DX access. &nbsp;</p><p>- D (Transit Gateway Sharing): &nbsp;</p><p> &nbsp;- Share Transit Gateway with other accounts via AWS Resource Access Manager (RAM). &nbsp;</p><p> &nbsp;- Attach VPCs to the Transit Gateway (replaces VPC peering). &nbsp;</p><p>- F (Private Subnets + NAT via On-Premises): &nbsp;</p><p> &nbsp;- No public subnets: Forces all internet traffic through on-premises NAT. &nbsp;</p><p> &nbsp;- Route tables direct `0.0.0.0/0` to the Transit Gateway → DX → on-premises. &nbsp;</p><p> Architecture Flow: &nbsp;</p><p>1. On-Premises → Direct Connect → Transit Gateway → VPCs (all accounts). &nbsp;</p><p>2. VPCs → Transit Gateway → Direct Connect → On-Premises NAT → Internet. &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- A (Direct Connect Gateway Associations): &nbsp;</p><p> &nbsp;- Deprecated: Virtual private gateways are legacy (use Transit Gateway). &nbsp;</p><p>- C (Internet Gateway): &nbsp;</p><p> &nbsp;- Violates requirement (must use on-premises egress). &nbsp;</p><p>- E (VPC Peering): &nbsp;</p><p> &nbsp;- Doesn’t scale to hundreds of VPCs (n² complexity). &nbsp;</p><p> Implementation Steps: &nbsp;</p><p>1. Create Transit Gateway (Central Account): &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws ec2 create-transit-gateway --description \"Central-TGW\"</p><p> &nbsp; ``` &nbsp;</p><p>2. Share Transit Gateway via RAM: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws ram create-resource-share --name TGW-Share --resource-arns arn:aws:ec2:us-east-1:123456789012:transit-gateway/tgw-1234567890abcdef0</p><p> &nbsp; ``` &nbsp;</p><p>3. Configure On-Premises NAT: &nbsp;</p><p> &nbsp; - Add a route for `0.0.0.0/0` to the customer gateway. &nbsp;</p><p> Conclusion: &nbsp;</p><p>B + D + F is the only combination that: &nbsp;</p><p>- Scales to hundreds of accounts/VPCs (Transit Gateway). &nbsp;</p><p>- Enforces on-premises internet egress (private subnets + NAT). &nbsp;</p><p>- Minimizes operational overhead (centralized networking). &nbsp;</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "1c3703b5d93b44b1af4109514d0bc7ea",
      "questionNumber": 124,
      "type": "multiple",
      "content": "<p>Question #124</p><p>A company has hundreds of AWS accounts. The company recently implemented a centralized internal process for purchasing new Reserved Instances and modifying existing Reserved Instances. This process requires all business units that want to purchase or modify Reserved Instances to submit requests to a dedicated team for procurement. Previously, business units directly purchased or modified Reserved Instances in their own respective AWS accounts autonomously. </p><p><br></p><p>A solutions architect needs to enforce the new process in the most secure way possible. Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Ensure that all AWS accounts are part of an organization in AWS Organizations with all features enabled."
        },
        {
          "label": "B",
          "content": "Use AWS Config to report on the attachment of an IAM policy that denies access to the ec2:PurchaseReservedInstancesOffering action and the ec2:ModifyReservedInstances action."
        },
        {
          "label": "C",
          "content": "In each AWS account, create an IAM policy that denies the ec2:PurchaseReservedInstancesOffering action and the ec2:ModifyReservedInstances action."
        },
        {
          "label": "D",
          "content": "Create an SCP that denies the ec2:PurchaseReservedInstancesOffering action and the ec2:ModifyReservedInstances action. Attach the SCP to each OU of the organization."
        },
        {
          "label": "E",
          "content": "Ensure that all AWS accounts are part of an organization in AWS Organizations that uses the consolidated billing feature."
        }
      ],
      "correctAnswer": "AD",
      "explanation": "<p>Based on the information provided, the correct answers are:</p><p>A. Ensure that all AWS accounts are part of an organization in AWS Organizations with all features enabled.</p><p>D. Create an SCP that denies the ec2:PurchaseReservedInstancesOffering action and the ec2:ModifyReservedInstances action. Attach the SCP to each OU of the organization.</p><p>These two steps together provide the most secure way to enforce the new centralized process for Reserved Instance management:</p><p>AWS Organizations with all features enabled (option A) is essential because:</p><p>It provides the foundation for centralized management across hundreds of AWS accounts</p><p>It enables the use of Service Control Policies (SCPs), which are necessary for enforcing permissions at scale</p><p>It supports consolidated billing, which is important for Reserved Instance sharing and management</p><p>Creating an SCP that denies the specific Reserved Instance actions (option D) is the most effective enforcement mechanism because:</p><p>It prevents users in individual accounts from purchasing or modifying Reserved Instances directly</p><p>SCPs are preventative controls that cannot be overridden by account administrators</p><p>By attaching the SCP to each OU, the policy applies consistently across all accounts in the organization</p><p>This approach follows the principle of least privilege by specifically denying only the actions that need to be restricted</p><p>Together, these steps ensure that only the dedicated procurement team has the ability to purchase and modify Reserved Instances, effectively enforcing the new centralized process across the entire organization in the most secure way possible.</p><p>Sources</p><p>Using AWS Organizations for security - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/security-reference-architecture/organizations-security.html）</p><p>Managing access to AWS accounts from Microsoft Teams and Slack at scale using AWS Organizations and AWS Chatbot | AWS Cloud Operations Blog （https://aws.amazon.com/cn/blogs/mt/managing-access-to-aws-accounts-from-microsoft-teams-and-slack-at-scale-using-aws-organizations-and-aws-chatbot/）</p><p>AWS account management and separation - Security Pillar （https://docs.aws.amazon.com/wellarchitected/latest/security-pillar/aws-account-management-and-separation.html）</p><p>Protect your AWS resources from unauthorized access using AWS Organizations integrations | AWS Cloud Operations Blog （https://aws.amazon.com/cn/blogs/mt/protect-your-aws-resources-from-unauthorized-access-using-aws-organizations-integrations/）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "c255b7ba75454027adbdb1f3cd844618",
      "questionNumber": 125,
      "type": "multiple",
      "content": "<p>Question #125</p><p>A company is running a critical application that uses an Amazon RDS for MySQL database to store data. The RDS DB instance is deployed in Multi-AZ mode. <br><br>A recent RDS database failover test caused a 40-second outage to the application. A solutions architect needs to design a solution to reduce the outage time to less than 20 seconds. <br><br>Which combination of steps should the solutions architect take to meet these requirements? (Choose three.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Use Amazon ElastiCache for Memcached in front of the database"
        },
        {
          "label": "B",
          "content": "Use Amazon ElastiCache for Redis in front of the database"
        },
        {
          "label": "C",
          "content": "Use RDS Proxy in front of the database."
        },
        {
          "label": "D",
          "content": "Migrate the database to Amazon Aurora MySQL."
        },
        {
          "label": "E",
          "content": "Create an Amazon Aurora Replica."
        },
        {
          "label": "F",
          "content": "Create an RDS for MySQL read replica"
        }
      ],
      "correctAnswer": "CDE",
      "explanation": "<p>Based on the information provided, the correct answers are:</p><p>C. Use RDS Proxy in front of the database. </p><p>D. Migrate the database to Amazon Aurora MySQL. </p><p>E. Create an Amazon Aurora Replica.</p><p>These three steps together provide the most effective solution to reduce the database failover outage time to less than 20 seconds:</p><p>Using RDS Proxy (option C) is crucial because:</p><p>It significantly reduces failover times by maintaining a connection pool and preserving application connections during database failovers</p><p>It eliminates the DNS propagation delay that typically occurs during failovers</p><p>RDS Proxy can reduce failover times to as low as 3-5 seconds in many cases</p><p>Migrating to Amazon Aurora MySQL (option D) is important because:</p><p>Aurora has a different architecture than standard RDS for MySQL that's specifically designed for faster failovers</p><p>Aurora's storage layer is independent of the database instances, allowing for quicker recovery</p><p>Aurora typically achieves failover times under 30 seconds, often as low as 10-20 seconds</p><p>Creating an Amazon Aurora Replica (option E) further improves the solution because:</p><p>Having Aurora Replicas in different Availability Zones provides additional failover targets</p><p>Aurora can quickly promote a replica to the primary role, reducing downtime</p><p>Multiple replicas improve the overall availability and resilience of the database system</p><p>Options A and B (using ElastiCache for Memcached or Redis) would improve application performance by caching frequently accessed data but wouldn't directly address the database failover time issue.</p><p>Option F (creating an RDS for MySQL read replica) wouldn't significantly impact the failover time for the primary instance in a Multi-AZ setup.</p><p>The combination of RDS Proxy, Aurora MySQL, and Aurora Replicas provides a comprehensive solution that addresses the core requirement of reducing failover outage time to less than 20 seconds.</p><p>Sources</p><p>Using RDS Proxy with Amazon RDS Multi-AZ DB instance deployment to improve planned failover time | AWS Database Blog （https://aws.amazon.com/cn/blogs/database/using-rds-proxy-with-amazon-rds-multi-az-db-instance-deployment-to-improve-planned-failover-time/）</p><p>Amazon Aurora for Core Banking Systems | AWS for Industries （https://aws.amazon.com/cn/blogs/industries/amazon-aurora-for-core-banking-systems/）</p><p>Targeted business outcomes - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/amazon-rds-proxy/targeted-business-outcomes.html）</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "864a1f6a463b469ebbed46ea66fd582b",
      "questionNumber": 126,
      "type": "single",
      "content": "<p>Question #126</p><p>An AWS partner company is building a service in AWS Organizations using its organization named org1. This service requires the partner company to have access to AWS resources in a customer account, which is in a separate organization named org2. The company must establish least privilege security access using an API or command line tool to the customer account. </p><p><br></p><p>What is the MOST secure way to allow org1 to access resources in org2?</p>",
      "options": [
        {
          "label": "A",
          "content": "The customer should provide the partner company with their AWS account access keys to log in and perform the required tasks."
        },
        {
          "label": "B",
          "content": "The customer should create an IAM user and assign the required permissions to the IAM user. The customer should then provide the credentials to the partner company to log in and perform the required tasks."
        },
        {
          "label": "C",
          "content": "The customer should create an IAM role and assign the required permissions to the IAM role. The partner company should then use the IAM role&rsquo;s Amazon Resource Name (ARN) when requesting access to perform the required tasks."
        },
        {
          "label": "D",
          "content": "The customer should create an IAM role and assign the required permissions to the IAM role. The partner company should then use the IAM role&rsquo;s Amazon Resource Name (ARN), including the external ID in the IAM role&rsquo;s trust policy, when requesting access to perform the required tasks."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. The customer should create an IAM role and assign the required permissions to the IAM role. The partner company should then use the IAM role's Amazon Resource Name (ARN), including the external ID in the IAM role's trust policy, when requesting access to perform the required tasks.</p><p>This is the MOST secure approach because:</p><p>It follows the principle of least privilege by allowing the customer to precisely define what actions the partner company can perform through the IAM role's permissions.</p><p>It uses temporary security credentials rather than long-term access keys, significantly reducing security risks.</p><p>The external ID in the trust policy provides an additional layer of security against the \"confused deputy\" problem, where a third party might trick the partner into accessing resources they shouldn't.</p><p>The partner company can use AWS Security Token Service (STS) to assume the role and obtain temporary credentials, which automatically expire after a defined period.</p><p>All actions performed using the assumed role are logged in AWS CloudTrail, providing full auditability.</p><p>Option A (providing AWS account access keys) is highly insecure because:</p><p>It involves sharing long-term credentials</p><p>It doesn't follow the principle of least privilege</p><p>It makes it difficult to revoke access if needed</p><p>It creates significant security risks if the keys are compromised</p><p>Using IAM roles with external IDs is the AWS recommended best practice for cross-account access, especially in partner scenarios where one organization needs to access resources in another organization's account.</p><p>Sources</p><p>Securely Accessing Customer AWS Accounts with Cross-Account IAM Roles | AWS Partner Network (APN) Blog （https://aws.amazon.com/cn/blogs/apn/securely-accessing-customer-aws-accounts-with-cross-account-iam-roles/）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "b3f86fbca336423db421bde4ce3b9c53",
      "questionNumber": 127,
      "type": "single",
      "content": "<p>Question #127</p><p>A delivery company needs to migrate its third-party route planning application to AWS. The third party supplies a supported Docker image from a public registry. The image can run in as many containers as required to generate the route map. </p><p><br></p><p>The company has divided the delivery area into sections with supply hubs so that delivery drivers travel the shortest distance possible from the hubs to the customers. To reduce the time necessary to generate route maps, each section uses its own set of Docker containers with a custom configuration that processes orders only in the section's area.</p><p><br></p><p> The company needs the ability to allocate resources cost-effectively based on the number of running containers. </p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster on Amazon EC2. Use the Amazon EKS CLI to launch the planning application in pods by using the --tags option to assign a custom tag to the pod."
        },
        {
          "label": "B",
          "content": "Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster on AWS Fargate. Use the Amazon EKS CLI to launch the planning application. Use the AWS CLI tag-resource API call to assign a custom tag to the pod."
        },
        {
          "label": "C",
          "content": "Create an Amazon Elastic Container Service (Amazon ECS) cluster on Amazon EC2. Use the AWS CLI with run-tasks set to true to launch the planning application by using the --tags option to assign a custom tag to the task."
        },
        {
          "label": "D",
          "content": "Create an Amazon Elastic Container Service (Amazon ECS) cluster on AWS Fargate. Use the AWS CLI run-task command and set enableECSManagedTags to true to launch the planning application. Use the --tags option to assign a custom tag to the task."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>The correct answer is D. By creating an Amazon ECS cluster on AWS Fargate and using the AWS CLI run-task command with enableECSManagedTags set to true, the company can launch the planning application with the least operational overhead. This approach allows for serverless container management, as Fargate handles the underlying infrastructure. Additionally, using tags with ECS tasks helps in managing and tracking costs effectively.</p><p>Answer: &nbsp;</p><p>D. Create an Amazon Elastic Container Service (Amazon ECS) cluster on AWS Fargate. Use the AWS CLI run-task command and set enableECSManagedTags to true to launch the planning application. Use the --tags option to assign a custom tag to the task. &nbsp;</p><p> Explanation: &nbsp;</p><p> Key Requirements: &nbsp;</p><p>1. Cost-Effective Resource Allocation: &nbsp;</p><p> &nbsp; - Pay only for running containers (no idle EC2 costs). &nbsp;</p><p>2. Custom Configuration per Section: &nbsp;</p><p> &nbsp; - Each delivery area section needs dedicated containers with unique settings. &nbsp;</p><p>3. Least Operational Overhead: &nbsp;</p><p> &nbsp; - Avoid managing servers (e.g., EC2 nodes, Kubernetes control plane). &nbsp;</p><p> Why Option D is Best? &nbsp;</p><p>- ECS on Fargate: &nbsp;</p><p> &nbsp;- Serverless: No EC2 management; scales containers automatically. &nbsp;</p><p> &nbsp;- Cost-Efficient: Billed per vCPU/memory used (no over-provisioning). &nbsp;</p><p>- Task Tags: &nbsp;</p><p> &nbsp;- Use `--tags` to label tasks by delivery section (e.g., `section=North`). &nbsp;</p><p> &nbsp;- Enables cost tracking per section via AWS Cost Explorer. &nbsp;</p><p>- Simplified CLI Deployment: &nbsp;</p><p> &nbsp;```bash</p><p> &nbsp;aws ecs run-task \\</p><p> &nbsp; &nbsp;--cluster route-planning \\</p><p> &nbsp; &nbsp;--task-definition planner-task \\</p><p> &nbsp; &nbsp;--enable-ecs-managed-tags \\</p><p> &nbsp; &nbsp;--tags \"key=section,value=North\"</p><p> &nbsp;``` &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- A/B (EKS on EC2/Fargate): &nbsp;</p><p> &nbsp;- Overkill: Kubernetes adds complexity (no need for multi-cloud/advanced orchestration). &nbsp;</p><p> &nbsp;- EKS CLI not required (ECS CLI is simpler). &nbsp;</p><p>- C (ECS on EC2): &nbsp;</p><p> &nbsp;- EC2 overhead: Must manage nodes/scaling (violates \"least operational overhead\"). &nbsp;</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "89d5a830b3a5428388c795f4c3eb7ce6",
      "questionNumber": 128,
      "type": "multiple",
      "content": "<p>Question #128</p><p>A software company hosts an application on AWS with resources in multiple AWS accounts and Regions. The application runs on a group of Amazon EC2 instances in an application VPC located in the us-east-1 Region with an IPv4 CIDR block of 10.10.0.0/16. In a different AWS account, a shared services VPC is located in the us-east-2 Region with an IPv4 CIDR block of 10.10.10.0/24. When a cloud engineer uses AWS CloudFormation to attempt to peer the application VPC with the shared services VPC, an error message indicates a peering failure. <br><br>Which factors could cause this error? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "The IPv4 CIDR ranges of the two VPCs overlap"
        },
        {
          "label": "B",
          "content": "The VPCs are not in the same Region"
        },
        {
          "label": "C",
          "content": "One or both accounts do not have access to an Internet gateway"
        },
        {
          "label": "D",
          "content": "One of the VPCs was not shared through AWS Resource Access Manager"
        },
        {
          "label": "E",
          "content": "The IAM role in the peer accepter account does not have the correct permissions"
        }
      ],
      "correctAnswer": "AE",
      "explanation": "<p>Based on the information provided, the correct answers are:</p><p>A. The IPv4 CIDR ranges of the two VPCs overlap </p><p>E. The IAM role in the peer accepter account does not have the correct permissions</p><p>These two factors could cause the VPC peering failure for the following reasons:</p><p>CIDR overlap (option A): The application VPC has a CIDR block of 10.10.0.0/16, while the shared services VPC has a CIDR block of 10.10.10.0/24. The shared services VPC CIDR (10.10.10.0/24) is completely contained within the application VPC CIDR (10.10.0.0/16), creating an overlap. AWS VPC peering explicitly prohibits connecting VPCs with overlapping CIDR blocks, as this would create routing ambiguity.</p><p>IAM permissions (option E): When creating a VPC peering connection between accounts, the user or role in the accepter account must have the appropriate permissions to accept the peering request. If the IAM role being used in the CloudFormation template doesn't have these permissions, the peering creation will fail.</p><p>Option B (different regions) is not correct because AWS supports inter-region VPC peering, allowing VPCs in different regions to be peered together.</p><p>Option C (Internet gateway access) is not correct because VPC peering doesn't require either VPC to have access to an Internet gateway. VPC peering creates a direct network connection between the VPCs and operates independently of Internet access.</p><p>Option D (AWS Resource Access Manager) is not correct because RAM is used for sharing resources like subnets within a VPC, not for VPC peering. VPC peering is established using the VPC peering feature, not through RAM.</p><p>Sources</p><p>how to Connect VPCs with Overlapping CIDRs Across Accounts | AWS re:Post （https://repost.aws/questions/QU0z4ZmjEESRuDr1K1_ZYjYw/how-to-connect-vpcs-with-overlapping-cidrs-across-accounts）</p><p>prerequisites to ensure vpc peering | AWS re:Post （https://repost.aws/questions/QUkDFWtBUlQfyETl1h5Z2skw/prerequisites-to-ensure-vpc-peering）</p><p>Troubleshoot a VPC peering connection - Amazon Virtual Private Cloud （https://docs.aws.amazon.com/vpc/latest/peering/troubleshoot-vpc-peering-connections.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": true,
      "noteContent": "ec2:AcceptVpcPeeringConnection；RAM可以共享subnet（VPC Sharing功能），但那是让其他账户在你的VPC里启动资源，跟VPC Peering完全不同的场景。"
    },
    {
      "id": "965006d93ae345c693b490d491534109",
      "questionNumber": 129,
      "type": "single",
      "content": "<p>Question #129</p><p>An external audit of a company’s serverless application reveals IAM policies that grant too many permissions. These policies are attached to the company's AWS Lambda execution roles. Hundreds of the company's Lambda functions have broad access permissions such as full access to Amazon S3 buckets and Amazon DynamoDB tables. The company wants each function to have only the minimum permissions that the function needs to complete its task. </p><p><br></p><p>A solutions architect must determine which permissions each Lambda function needs.</p><p><br></p><p> What should the solutions architect do to meet this requirement with the LEAST amount of effort?</p>",
      "options": [
        {
          "label": "A",
          "content": "Set up Amazon CodeGuru to profile the Lambda functions and search for AWS API calls. Create an inventory of the required API calls and resources for each Lambda function. Create new IAM access policies for each Lambda function. Review the new policies to ensure that they meet the company&#39;s business requirements."
        },
        {
          "label": "B",
          "content": "Turn on AWS CloudTrail logging for the AWS account. Use AWS Identity and Access Management Access Analyzer to generate IAM access policies based on the activity recorded in the CloudTrail log. Review the generated policies to ensure that they meet the company&#39;s business requirements."
        },
        {
          "label": "C",
          "content": "Turn on AWS CloudTrail logging for the AWS account. Create a script to parse the CloudTrail log, search for AWS API calls by Lambda execution role, and create a summary report. Review the report. Create IAM access policies that provide more restrictive permissions for each Lambda function."
        },
        {
          "label": "D",
          "content": "Turn on AWS CloudTrail logging for the AWS account. Export the CloudTrail logs to Amazon S3. Use Amazon EMR to process the CloudTrail logs in Amazon S3 and produce a report of API calls and resources used by each execution role. Create a new IAM access policy for each role. Export the generated roles to an S3 bucket. Review the generated policies to ensure that they meet the company&rsquo;s business requirements."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Answer: &nbsp;</p><p>B. Turn on AWS CloudTrail logging for the AWS account. Use AWS Identity and Access Management Access Analyzer to generate IAM access policies based on the activity recorded in the CloudTrail log. Review the generated policies to ensure that they meet the company's business requirements. &nbsp;</p><p> Explanation: &nbsp;</p><p> Key Requirements: &nbsp;</p><p>1. Least-Privilege Permissions: &nbsp;</p><p> &nbsp; - Each Lambda function should have only the permissions it actually uses. &nbsp;</p><p>2. Least Effort: &nbsp;</p><p> &nbsp; - Avoid manual analysis or custom scripting. &nbsp;</p><p>3. Audit Trail: &nbsp;</p><p> &nbsp; - Use existing logs to identify used permissions. &nbsp;</p><p> Why Option B is Best? &nbsp;</p><p>- IAM Access Analyzer: &nbsp;</p><p> &nbsp;- Automatically analyzes CloudTrail logs to generate least-privilege policies. &nbsp;</p><p> &nbsp;- Policy Generation: Creates IAM policies based on actual API calls made by Lambda functions. &nbsp;</p><p> &nbsp;- No Custom Code: Fully managed service (no EMR/scripts needed). &nbsp;</p><p>- Steps: &nbsp;</p><p> &nbsp;1. Enable CloudTrail (if not already on). &nbsp;</p><p> &nbsp;2. Run Access Analyzer to generate policies. &nbsp;</p><p> &nbsp;3. Review/Attach new policies to Lambda roles. &nbsp;</p><p> Example Access Analyzer Output: &nbsp;</p><p>```json</p><p>{</p><p> &nbsp;\"Version\": \"2012-10-17\",</p><p> &nbsp;\"Statement\": [</p><p> &nbsp; &nbsp;{</p><p> &nbsp; &nbsp; &nbsp;\"Effect\": \"Allow\",</p><p> &nbsp; &nbsp; &nbsp;\"Action\": [</p><p> &nbsp; &nbsp; &nbsp; &nbsp;\"s3:GetObject\",</p><p> &nbsp; &nbsp; &nbsp; &nbsp;\"dynamodb:Query\"</p><p> &nbsp; &nbsp; &nbsp;],</p><p> &nbsp; &nbsp; &nbsp;\"Resource\": [</p><p> &nbsp; &nbsp; &nbsp; &nbsp;\"arn:aws:s3:::my-bucket/*\",</p><p> &nbsp; &nbsp; &nbsp; &nbsp;\"arn:aws:dynamodb:us-east-1:123456789012:table/MyTable\"</p><p> &nbsp; &nbsp; &nbsp;]</p><p> &nbsp; &nbsp;}</p><p> &nbsp;]</p><p>}</p><p>``` &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- A (CodeGuru): &nbsp;</p><p> &nbsp;- CodeGuru doesn’t track runtime API calls (only code quality/performance). &nbsp;</p><p>- C (Custom Script): &nbsp;</p><p> &nbsp;- High effort: Requires writing/maintaining a log parser. &nbsp;</p><p>- D (EMR): &nbsp;</p><p> &nbsp;- Overkill: EMR is for big data, not IAM policy optimization. &nbsp;</p><p> Implementation Steps: &nbsp;</p><p>1. Enable CloudTrail: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws cloudtrail create-trail --name LambdaAudit --s3-bucket-name my-log-bucket</p><p> &nbsp; ``` &nbsp;</p><p>2. Generate Policies with Access Analyzer: &nbsp;</p><p> &nbsp; - Navigate to IAM → Access Analyzer → Policy Generation. &nbsp;</p><p>3. Apply Policies: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws iam put-role-policy --role-name LambdaExecutionRole --policy-name LeastPrivilege --policy-document file://policy.json</p><p> &nbsp; ``` &nbsp;</p><p> Conclusion: &nbsp;</p><p>B is the fastest, most accurate way to enforce least privilege: &nbsp;</p><p>- Automated policy generation (Access Analyzer). &nbsp;</p><p>- No manual analysis (unlike scripting/EMR). &nbsp;</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "bda87abc8d7d4e5d9c3d0d701634d79d",
      "questionNumber": 130,
      "type": "single",
      "content": "<p>Question #130</p><p>A solutions architect must analyze a company’s Amazon EC2 instances and Amazon Elastic Block Store (Amazon EBS) volumes to determine whether the company is using resources efficiently. The company is running several large, high-memory EC2 instances to host database clusters that are deployed in active/passive configurations. The utilization of these EC2 instances varies by the applications that use the databases, and the company has not identified a pattern. </p><p><br></p><p>The solutions architect must analyze the environment and take action based on the findings. </p><p><br></p><p>Which solution meets these requirements MOST cost-effectively?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a dashboard by using AWS Systems Manager OpsCenter. Configure visualizations for Amazon CloudWatch metrics that are associated with the EC2 instances and their EBS volumes. Review the dashboard periodically, and identify usage patterns. Rightsize the EC2 instances based on the peaks in the metrics."
        },
        {
          "label": "B",
          "content": "Turn on Amazon CloudWatch detailed monitoring for the EC2 instances and their EBS volumes. Create and review a dashboard that is based on the metrics. Identify usage patterns. Rightsize the EC2 instances based on the peaks in the metrics."
        },
        {
          "label": "C",
          "content": "Install the Amazon CloudWatch agent on each of the EC2 instances. Turn on AWS Compute Optimizer, and let it run for at least 12 hours. Review the recommendations from Compute Optimizer, and rightsize the EC2 instances as directed."
        },
        {
          "label": "D",
          "content": "Sign up for the AWS Enterprise Support plan. Turn on AWS Trusted Advisor. Wait 12 hours. Review the recommendations from Trusted Advisor, and rightsize the EC2 instances as directed."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Answer: &nbsp;</p><p>C. Install the Amazon CloudWatch agent on each of the EC2 instances. Turn on AWS Compute Optimizer, and let it run for at least 12 hours. Review the recommendations from Compute Optimizer, and rightsize the EC2 instances as directed. &nbsp;</p><p> Explanation: &nbsp;</p><p> Key Requirements: &nbsp;</p><p>1. Efficiency Analysis: &nbsp;</p><p> &nbsp; - Assess EC2 instance and EBS volume utilization (CPU, memory, disk I/O). &nbsp;</p><p>2. Cost-Effective Optimization: &nbsp;</p><p> &nbsp; - Rightsize instances based on actual usage (avoid over-provisioning). &nbsp;</p><p>3. No Usage Patterns Identified: &nbsp;</p><p> &nbsp; - Need automated recommendations (manual dashboards won’t scale). &nbsp;</p><p> Why Option C is Best? &nbsp;</p><p>- AWS Compute Optimizer: &nbsp;</p><p> &nbsp;- Automatically analyzes CloudWatch metrics (after installing the agent). &nbsp;</p><p> &nbsp;- Generates rightsizing recommendations (e.g., downgrade `r5.8xlarge` to `r5.4xlarge`). &nbsp;</p><p> &nbsp;- Considers memory/CPU/EBS (unlike Trusted Advisor’s basic checks). &nbsp;</p><p>- CloudWatch Agent: &nbsp;</p><p> &nbsp;- Collects system-level metrics (e.g., memory utilization) not available in basic CloudWatch. &nbsp;</p><p> Implementation Steps: &nbsp;</p><p>1. Install CloudWatch Agent: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; sudo amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -c ssm:AmazonCloudWatch-linux -s</p><p> &nbsp; ``` &nbsp;</p><p>2. Enable Compute Optimizer: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws compute-optimizer start-recommendation-export --s3-destination-bucket my-bucket</p><p> &nbsp; ``` &nbsp;</p><p>3. Review Recommendations: &nbsp;</p><p> &nbsp; - In Compute Optimizer console or via API. &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- A/B (Manual Dashboards): &nbsp;</p><p> &nbsp;- Time-consuming: Requires manual pattern identification. &nbsp;</p><p> &nbsp;- No actionable recommendations (unlike Compute Optimizer). &nbsp;</p><p>- D (Trusted Advisor): &nbsp;</p><p> &nbsp;- Limited to basic checks (e.g., idle instances, not memory optimization). &nbsp;</p><p> &nbsp;- Enterprise Support required (extra cost). &nbsp;</p><p> Cost Savings Example: &nbsp;</p><p>| Instance Type (Before) | Recommended Type (After) | Monthly Savings | &nbsp;</p><p>|------------------------|--------------------------|----------------| &nbsp;</p><p>| `r5.8xlarge` (256 GB) &nbsp;| `r5.4xlarge` (128 GB) &nbsp; &nbsp;| ~$1,000 &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp;</p><p> Conclusion: &nbsp;</p><p>C is the most cost-effective solution because: &nbsp;</p><p>- Automated rightsizing (Compute Optimizer). &nbsp;</p><p>- No manual analysis (unlike dashboards). &nbsp;</p><p>- No extra support costs (unlike Trusted Advisor). &nbsp;</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "8f356eeb160a4a7daff8d82d2794e3ec",
      "questionNumber": 131,
      "type": "single",
      "content": "<p>Question #131</p><p>A company uses AWS Organizations for a multi-account setup in the AWS Cloud. The company uses AWS Control Tower for governance and uses AWS Transit Gateway for VPC connectivity across accounts. </p><p><br></p><p>In an AWS application account, the company’s application team has deployed a web application that uses AWS Lambda and Amazon RDS. The company's database administrators have a separate DBA account and use the account to centrally manage all the databases across the organization. The database administrators use an Amazon EC2 instance that is deployed in the DBA account to access an RDS database that is deployed in the application account. </p><p><br></p><p>The application team has stored the database credentials as secrets in AWS Secrets Manager in the application account. The application team is manually sharing the secrets with the database administrators. The secrets are encrypted by the default AWS managed key for Secrets Manager in the application account. A solutions architect needs to implement a solution that gives the database administrators access to the database and eliminates the need to manually share the secrets. </p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use AWS Resource Access Manager (AWS RAM) to share the secrets from the application account with the DBA account. In the DBA account, create an IAM role that is named DBA-Admin. Grant the role the required permissions to access the shared secrets. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets."
        },
        {
          "label": "B",
          "content": "In the application account, create an IAM role that is named DBA-Secret. Grant the role the required permissions to access the secrets. In the DBA account, create an IAM role that is named DBA-Admin. Grant the DBA-Admin role the required permissions to assume the DBA-Secret role in the application account. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets."
        },
        {
          "label": "C",
          "content": "In the DBA account create an IAM role that is named DBA-Admin. Grant the role the required permissions to access the secrets and the default AWS managed key in the application account. In the application account, attach resource-based policies to the key to allow access from the DBA account. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets."
        },
        {
          "label": "D",
          "content": "In the DBA account, create an IAM role that is named DBA-Admin. Grant the role the required permissions to access the secrets in the application account. Attach an SCP to the application account to allow access to the secrets from the DBA account. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Answer: &nbsp;</p><p>B. In the application account, create an IAM role that is named DBA-Secret. Grant the role the required permissions to access the secrets. In the DBA account, create an IAM role that is named DBA-Admin. Grant the DBA-Admin role the required permissions to assume the DBA-Secret role in the application account. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets. &nbsp;</p><p> Explanation: &nbsp;</p><p> Key Requirements: &nbsp;</p><p>1. Cross-Account Secret Access: &nbsp;</p><p> &nbsp; - DBAs (in DBA account) need access to Secrets Manager secrets (in application account). &nbsp;</p><p>2. No Manual Sharing: &nbsp;</p><p> &nbsp; - Eliminate manual credential sharing. &nbsp;</p><p>3. Security: &nbsp;</p><p> &nbsp; - Secrets encrypted with AWS managed key (no custom KMS policy changes needed). &nbsp;</p><p> Why Option B is Correct? &nbsp;</p><p>- IAM Role Delegation: &nbsp;</p><p> &nbsp;1. Application Account: &nbsp;</p><p> &nbsp; &nbsp; - Create `DBA-Secret` role with `secretsmanager:GetSecretValue` permission. &nbsp;</p><p> &nbsp; &nbsp; - Trust policy allows `DBA-Admin` role (from DBA account) to assume it. &nbsp;</p><p> &nbsp; &nbsp; ```json</p><p> &nbsp; &nbsp; {</p><p> &nbsp; &nbsp; &nbsp; \"Version\": \"2012-10-17\",</p><p> &nbsp; &nbsp; &nbsp; \"Statement\": [{</p><p> &nbsp; &nbsp; &nbsp; &nbsp; \"Effect\": \"Allow\",</p><p> &nbsp; &nbsp; &nbsp; &nbsp; \"Principal\": { \"AWS\": \"arn:aws:iam::DBA_ACCOUNT_ID:role/DBA-Admin\" },</p><p> &nbsp; &nbsp; &nbsp; &nbsp; \"Action\": \"sts:AssumeRole\"</p><p> &nbsp; &nbsp; &nbsp; }]</p><p> &nbsp; &nbsp; }</p><p> &nbsp; &nbsp; ``` &nbsp;</p><p> &nbsp;2. DBA Account: &nbsp;</p><p> &nbsp; &nbsp; - Attach `DBA-Admin` role to EC2 instance. &nbsp;</p><p> &nbsp; &nbsp; - Role has `sts:AssumeRole` permission for `DBA-Secret`. &nbsp;</p><p>- Workflow: &nbsp;</p><p> &nbsp;- EC2 instance (DBA account) → Assumes `DBA-Secret` role → Accesses secrets. &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- A (AWS RAM Sharing): &nbsp;</p><p> &nbsp;- Secrets Manager doesn’t support cross-account sharing via RAM. &nbsp;</p><p>- C (KMS Key Policies): &nbsp;</p><p> &nbsp;- Overkill: AWS managed keys already allow cross-account access via IAM roles. &nbsp;</p><p>- D (SCP Allow): &nbsp;</p><p> &nbsp;- SCPs cannot grant permissions (only deny). &nbsp;</p><p> Implementation Steps: &nbsp;</p><p>1. In Application Account: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws iam create-role --role-name DBA-Secret --assume-role-policy-document file://trust-policy.json</p><p> &nbsp; aws iam attach-role-policy --role-name DBA-Secret --policy-arn arn:aws:iam::aws:policy/SecretsManagerReadWrite</p><p> &nbsp; ``` &nbsp;</p><p>2. In DBA Account: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws iam create-role --role-name DBA-Admin --assume-role-policy-document file://ec2-trust-policy.json</p><p> &nbsp; aws iam attach-role-policy --role-name DBA-Admin --policy-arn arn:aws:iam::aws:policy/AmazonEC2RoleforSSM</p><p> &nbsp; ``` &nbsp;</p><p>3. On EC2 Instance: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws sts assume-role --role-arn arn:aws:iam::APPLICATION_ACCOUNT_ID:role/DBA-Secret --role-session-name DBASession</p><p> &nbsp; ``` &nbsp;</p><p> Conclusion: &nbsp;</p><p>B is the only secure, scalable solution that: &nbsp;</p><p>- Automates secret access (no manual sharing). &nbsp;</p><p>- Uses IAM roles (no KMS/S3/SCP workarounds). &nbsp;</p><p>- Aligns with AWS best practices. &nbsp;</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "5dd7efd1a6d2469d950b371038424d1d",
      "questionNumber": 132,
      "type": "multiple",
      "content": "<p>Question #132</p><p>A company manages multiple AWS accounts by using AWS Organizations. Under the root OU, the company has two OUs</p><p><br></p><p>.Research and DataOps. Because of regulatory requirements, all resources that the company deploys in the organization must reside in the ap-northeast-1 Region. Additionally, EC2 instances that the company deploys in the DataOps OU must use a predefined list of instance types. </p><p><br></p><p>A solutions architect must implement a solution that applies these restrictions. The solution must maximize operational efficiency and must minimize ongoing maintenance. <br><br>Which combination of steps will meet these requirements? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an IAM role in one account under the DataOps OU. Use the ec2:InstanceType condition key in an inline policy on the role to restrict access to specific instance types."
        },
        {
          "label": "B",
          "content": "Create an IAM user in all accounts under the root OU. Use the aws:RequestedRegion condition key in an inline policy on each user to restrict access to all AWS Regions except ap-northeast-1."
        },
        {
          "label": "C",
          "content": "Create an SCP. Use the aws:RequestedRegion condition key to restrict access to all AWS Regions except ap-northeast-1. Apply the SCP to the root OU."
        },
        {
          "label": "D",
          "content": "Create an SCP. Use the ec2:Region condition key to restrict access to all AWS Regions except ap-northeast-1. Apply the SCP to the root OU, the DataOps OU, and the Research OU."
        },
        {
          "label": "E",
          "content": "Create an SCP. Use the ec2:InstanceType condition key to restrict access to specific instance types. Apply the SCP to the DataOps OU."
        }
      ],
      "correctAnswer": "CE",
      "explanation": "<p>Based on the information provided, the correct answers are:</p><p>C. Create an SCP. Use the aws:RequestedRegion condition key to restrict access to all AWS Regions except ap-northeast-1. Apply the SCP to the root OU.</p><p>E. Create an SCP. Use the ec2:InstanceType condition key to restrict access to specific instance types. Apply the SCP to the DataOps OU.</p><p>These two steps together provide the most efficient solution that meets all the requirements:</p><p>Creating an SCP with the aws:RequestedRegion condition key (option C) addresses the regulatory requirement that all resources must reside in the ap-northeast-1 Region. By applying this SCP to the root OU, the restriction cascades down to all accounts in the organization, ensuring that resources can only be deployed in the specified region. This is more efficient than applying the policy to each OU separately.</p><p>Creating a separate SCP with the ec2:InstanceType condition key (option E) addresses the requirement to restrict EC2 instance types in the DataOps OU. By applying this SCP specifically to the DataOps OU, it ensures that only the predefined list of instance types can be used within that organizational unit, while not affecting the Research OU.</p><p>This combination maximizes operational efficiency by:</p><p>Using SCPs, which are centrally managed policies that apply to multiple accounts</p><p>Applying policies at the appropriate level in the organizational hierarchy</p><p>Using AWS-managed condition keys that are designed for these specific restrictions</p><p>It also minimizes ongoing maintenance because:</p><p>Changes to the policies can be made in one place and automatically apply to all affected accounts</p><p>No need to manage individual IAM policies across multiple accounts</p><p>No need for custom scripts or monitoring solutions to enforce the restrictions</p><p>Service Control Policies (SCPs) are the most appropriate tool for implementing organization-wide governance in AWS Organizations, making this combination the optimal solution for the requirements.</p><p>Sources</p><p>Defining Boundaries for Regional Services Access - Navigating GDPR Compliance on AWS （https://docs.aws.amazon.com/whitepapers/latest/navigating-gdpr-compliance/defining-boundaries-for-regional-services-access.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "c4fba426484848cc9307d5d6ed1a8444",
      "questionNumber": 133,
      "type": "multiple",
      "content": "<p>Question #133</p><p>A company runs a serverless application in a single AWS Region. The application accesses external URLs and extracts metadata from those sites. The company uses an Amazon Simple Notification Service (Amazon SNS) topic to publish URLs to an Amazon Simple Queue Service (Amazon SQS) queue. An AWS Lambda function uses the queue as an event source and processes the URLs from the queue. Results are saved to an Amazon S3 bucket. The company wants to process each URL in other Regions to compare possible differences in site localization. URLs must be published from the existing Region. Results must be written to the existing S3 bucket in the current Region. </p><p><br></p><p>Which combination of changes will produce a multi-Region deployment that meets these requirements? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Deploy the SQS queue with the Lambda function to other Regions."
        },
        {
          "label": "B",
          "content": "Subscribe the SNS topic in each Region to the SQS queue."
        },
        {
          "label": "C",
          "content": "Subscribe the SQS queue in each Region to the SNS topic."
        },
        {
          "label": "D",
          "content": "Configure the SQS queue to publish URLs to SNS topics in each Region."
        },
        {
          "label": "E",
          "content": "Deploy the SNS topic and the Lambda function to other Regions."
        }
      ],
      "correctAnswer": "AC",
      "explanation": "<p>Based on the information provided, the correct answers are:</p><p>A. Deploy the SQS queue with the Lambda function to other Regions. </p><p>C. Subscribe the SQS queue in each Region to the SNS topic.</p><p>These two steps together provide the solution that meets all the requirements:</p><p>Deploying the SQS queue with the Lambda function to other Regions (option A) is necessary because:</p><p>Each region needs its own processing capability to handle the URLs</p><p>This allows the application to process the same URL in different regions to compare site localization differences</p><p>The Lambda functions in each region can process the URLs based on their regional context</p><p>Subscribing the SQS queue in each Region to the SNS topic (option C) is required because:</p><p>The requirement states that URLs must be published from the existing Region, meaning the original SNS topic remains in the primary region</p><p>By subscribing regional SQS queues to the central SNS topic, messages published to the topic will be distributed to all regions</p><p>This creates a fan-out pattern where a single message published to the SNS topic is delivered to multiple SQS queues across regions</p><p>This combination allows the application to:</p><p>Continue publishing URLs from the existing region (through the original SNS topic)</p><p>Process each URL in multiple regions (through regional SQS queues and Lambda functions)</p><p>Write results back to the existing S3 bucket in the current region (each Lambda function can be configured to write to the central S3 bucket)</p><p>Option B (subscribing the SNS topic in each Region to the SQS queue) doesn't make sense architecturally, as it would require multiple SNS topics to publish to a single SQS queue, which doesn't align with the requirement to process URLs in multiple regions.</p><p>Sources</p><p>Using Multi-Region Access Points with supported API operations - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/MrapOperations.html）</p><p>Chain AWS services together using a serverless approach - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/chain-aws-services-together-using-a-serverless-approach.html）</p><p>Subscribe a Lambda function to event notifications from S3 buckets in different AWS Regions - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/subscribe-a-lambda-function-to-event-notifications-from-s3-buckets-in-different-aws-regions.html）</p><p>REL10-BP01 Deploy the workload to multiple locations - AWS Well-Architected Framework （https://docs.aws.amazon.com/wellarchitected/2023-10-03/framework/rel_fault_isolation_multiaz_region_system.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "dc124eb28989476da62d93eed286e5eb",
      "questionNumber": 134,
      "type": "single",
      "content": "<p>Question #134</p><p>A company runs a proprietary stateless ETL application on an Amazon EC2 Linux instances. The application is a Linux binary, and the source code cannot be modified. The application is single-threaded, uses 2 GB of RAM, and is highly CPU intensive. The application is scheduled to run every 4 hours and runs for up to 20 minutes. A solutions architect wants to revise the architecture for the solution. </p><p><br></p><p>Which strategy should the solutions architect use?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use AWS Lambda to run the application. Use Amazon CloudWatch Logs to invoke the Lambda function every 4 hours."
        },
        {
          "label": "B",
          "content": "Use AWS Batch to run the application. Use an AWS Step Functions state machine to invoke the AWS Batch job every 4 hours."
        },
        {
          "label": "C",
          "content": "Use AWS Fargate to run the application. Use Amazon EventBridge (Amazon CloudWatch Events) to invoke the Fargate task every 4 hours."
        },
        {
          "label": "D",
          "content": "Use Amazon EC2 Spot Instances to run the application. Use AWS CodeDeploy to deploy and run the application every 4 hours."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Use AWS Fargate to run the application. Use Amazon EventBridge (Amazon CloudWatch Events) to invoke the Fargate task every 4 hours.</p><p>This is the best strategy for the solutions architect to revise the architecture for the following reasons:</p><p>AWS Fargate is well-suited for this workload because:</p><p>It can easily handle the 2GB RAM requirement and CPU-intensive workload</p><p>It supports containerized applications, which is perfect for packaging the Linux binary</p><p>It can run for more than 20 minutes without any time limitations</p><p>It's serverless, which means you don't need to manage the underlying infrastructure</p><p>Amazon EventBridge (formerly CloudWatch Events) provides a reliable way to schedule the task to run every 4 hours.</p><p>The other options have significant limitations:</p><p>A. AWS Lambda has a maximum execution time of 15 minutes, which is insufficient for an application that runs for up to 20 minutes. Also, CloudWatch Logs is not designed to invoke Lambda functions on a schedule.</p><p>B. AWS Batch would work for this workload, but it's more complex to set up and manage compared to Fargate for this relatively simple scheduling requirement. It's designed for more complex batch processing scenarios with dependencies and job queues.</p><p>D. Amazon EC2 Spot Instances could be interrupted during processing, which would disrupt the ETL job. Additionally, AWS CodeDeploy is a deployment service, not a scheduling tool, so it's not appropriate for invoking the application on a regular schedule.</p><p>AWS Fargate with EventBridge scheduling provides the right balance of simplicity, reliability, and cost-effectiveness for this stateless ETL application.</p><p>Sources</p><p>AWS Fargate or AWS Lambda? - AWS Fargate or AWS Lambda? （https://docs.aws.amazon.com/decision-guides/latest/fargate-or-lambda/fargate-or-lambda.html）</p><p>REL05-BP06 Make services stateless where possible - AWS Well-Architected Framework （https://docs.aws.amazon.com/wellarchitected/2023-10-03/framework/rel_mitigate_interaction_failure_stateless.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "65b908dc36f149d3a202083c2b6fc473",
      "questionNumber": 135,
      "type": "single",
      "content": "Question #135<p>A company is creating a sequel for a popular online game. A large number of users from all over the world will play the game within the first week after launch. Currently, the game consists of the following components deployed in a single AWS Region: <br>an Amazon S3 bucket that stores game assets;<br>an Amazon DynamoDB table that stores player scores;</p><p><br></p><p>A solutions architect needs to design a multi-Region solution that will reduce latency, improve reliability, and require the least effort to implement. </p><p><br></p><p>What should the solutions architect do to meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an Amazon CloudFront distribution to serve assets from the S3 bucket. Configure S3 Cross-Region Replication. Create a new DynamoDB table in a new Region. Use the new table as a replica target for DynamoDB global tables."
        },
        {
          "label": "B",
          "content": "Create an Amazon CloudFront distribution to serve assets from the S3 bucket. Configure S3 Same-Region Replication. Create a new DynamoDB table in a new Region. Configure asynchronous replication between the DynamoDB tables by using AWS Database Migration Service (AWS DMS) with change data capture (CDC)."
        },
        {
          "label": "C",
          "content": "Create another S3 bucket in a new Region, and configure S3 Cross-Region Replication between the buckets. Create an Amazon CloudFront distribution and configure origin failover with two origins accessing the S3 buckets in each Region. Configure DynamoDB global tables by enabling Amazon DynamoDB Streams, and add a replica table in a new Region."
        },
        {
          "label": "D",
          "content": "Create another S3 bucket in the same Region, and configure S3 Same-Region Replication between the buckets. Create an Amazon CloudFront distribution and configure origin failover with two origins accessing the S3 buckets. Create a new DynamoDB table in a new Region. Use the new table as a replica target for DynamoDB global tables."
        }
      ],
      "correctAnswer": "C",
      "explanation": "To meet the requirements of reduced latency, improved reliability, and minimal implementation effort, the solutions architect should choose Option C. This option involves creating an additional S3 bucket in a new Region and setting up S3 Cross-Region Replication to serve game assets globally while reducing latency. Additionally, using Amazon CloudFront with origin failover provides high availability for the game assets. Setting up DynamoDB global tables with a replica in a new Region ensures reliability and fast access to player scores worldwide.",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "c1b0bdda171e4692b7fd9e37e9e7c42a",
      "questionNumber": 136,
      "type": "single",
      "content": "<p>Question #136</p><p>A company has an on-premises website application that provides real estate information for potential renters and buyers. The website uses a Java backend and a NoSQL MongoDB database to store subscriber data.</p><p><br></p><p>The company needs to migrate the entire application to AWS with a similar structure. The application must be deployed for high availability, and the company cannot make changes to the application. </p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use an Amazon Aurora DB cluster as the database for the subscriber data. Deploy Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones for the Java backend application."
        },
        {
          "label": "B",
          "content": "Use MongoDB on Amazon EC2 instances as the database for the subscriber data. Deploy EC2 instances in an Auto Scaling group in a single Availability Zone for the Java backend application."
        },
        {
          "label": "C",
          "content": "Configure Amazon DocumentDB (with MongoDB compatibility) with appropriately sized instances in multiple Availability Zones as the database for the subscriber data. Deploy Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones for the Java backend application."
        },
        {
          "label": "D",
          "content": "Configure Amazon DocumentDB (with MongoDB compatibility) in on-demand capacity mode in multiple Availability Zones as the database for the subscriber data. Deploy Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones for the Java backend application."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Answer: &nbsp;</p><p>C. Configure Amazon DocumentDB (with MongoDB compatibility) with appropriately sized instances in multiple Availability Zones as the database for the subscriber data. Deploy Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones for the Java backend application. &nbsp;</p><p> Explanation: &nbsp;</p><p> Key Requirements: &nbsp;</p><p>1. No Application Changes: &nbsp;</p><p> &nbsp; - Must retain MongoDB compatibility (cannot switch to Aurora). &nbsp;</p><p>2. High Availability: &nbsp;</p><p> &nbsp; - Database and backend must be multi-AZ. &nbsp;</p><p>3. Managed Services Preferred: &nbsp;</p><p> &nbsp; - Minimize operational overhead where possible. &nbsp;</p><p> Why Option C is Best? &nbsp;</p><p>- Amazon DocumentDB: &nbsp;</p><p> &nbsp;- Fully compatible with MongoDB (no app changes needed). &nbsp;</p><p> &nbsp;- Built-in multi-AZ replication (automated failover). &nbsp;</p><p> &nbsp;- Managed service (no EC2 patching/scaling). &nbsp;</p><p>- EC2 Auto Scaling: &nbsp;</p><p> &nbsp;- Multi-AZ deployment ensures backend availability. &nbsp;</p><p> &nbsp;- No code changes (Java app runs as-is). &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- A (Aurora): &nbsp;</p><p> &nbsp;- Requires app changes (MongoDB → Aurora SQL). &nbsp;</p><p>- B (MongoDB on EC2): &nbsp;</p><p> &nbsp;- Single-AZ risk (violates high availability). &nbsp;</p><p> &nbsp;- Self-managed (operational overhead). &nbsp;</p><p>- D (DocumentDB On-Demand): &nbsp;</p><p> &nbsp;- On-demand is cost-prohibitive for steady workloads. &nbsp;</p><p> &nbsp;- Provisioned capacity (Option C) is more cost-effective. &nbsp;</p><p> Implementation Steps: &nbsp;</p><p>1. Deploy DocumentDB: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws docdb create-db-cluster \\</p><p> &nbsp; &nbsp; --db-cluster-identifier real-estate-db \\</p><p> &nbsp; &nbsp; --engine docdb \\</p><p> &nbsp; &nbsp; --master-username admin \\</p><p> &nbsp; &nbsp; --master-user-password password \\</p><p> &nbsp; &nbsp; --availability-zones us-east-1a us-east-1b</p><p> &nbsp; ``` &nbsp;</p><p>2. Configure EC2 Auto Scaling: &nbsp;</p><p> &nbsp; - Use Launch Template with Java app AMI. &nbsp;</p><p> &nbsp; - Set min/max capacity across 2+ AZs. &nbsp;</p><p> High Availability Design: &nbsp;</p><p>| Component &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| Solution &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| HA Mechanism &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | &nbsp;</p><p>|--------------------|-----------------------------------|----------------------------------| &nbsp;</p><p>| Database &nbsp; &nbsp; &nbsp; | Amazon DocumentDB &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | 3 copies across AZs + auto-failover | &nbsp;</p><p>| Backend &nbsp; &nbsp; &nbsp; &nbsp;| EC2 Auto Scaling (multi-AZ) &nbsp; &nbsp; &nbsp; | Load balancer + health checks &nbsp; &nbsp;| &nbsp;</p><p> Conclusion: &nbsp;</p><p>C is the only option that: &nbsp;</p><p>- Preserves MongoDB compatibility (no app changes). &nbsp;</p><p>- Ensures high availability (multi-AZ for DB + backend). &nbsp;</p><p>- Minimizes management (DocumentDB is managed). &nbsp;</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "1fedd730e565427fa2360479cbdfb968",
      "questionNumber": 137,
      "type": "multiple",
      "content": "<p>Question #137</p><p>A digital marketing company has multiple AWS accounts that belong to various teams. The creative team uses an Amazon S3 bucket in its AWS account to securely store images and media files that are used as content for the company’s marketing campaigns. The creative team wants to share the S3 bucket with the strategy team so that the strategy team can view the objects. A solutions architect has created an IAM role that is named strategy_reviewer in the Strategy account. The solutions architect also has set up a custom AWS Key Management Service (AWS KMS) key in the Creative account and has associated the key with the S3 bucket. </p><p><br></p><p>However, when users from the Strategy account assume the IAM role and try to access objects in the S3 bucket, they receive an Access Denied error. The solutions architect must ensure that users in the Strategy account can access the S3 bucket. The solution must provide these users with only the minimum permissions that they need. </p><p><br></p><p>Which combination of steps should the solutions architect take to meet these requirements? (Choose three.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a bucket policy that includes read permissions for the S3 bucket. Set the principal of the bucket policy to the account ID of the Strategy account."
        },
        {
          "label": "B",
          "content": "Update the strategy_reviewer IAM role to grant full permissions for the S3 bucket and to grant decrypt permissions for the custom KMS key."
        },
        {
          "label": "C",
          "content": "Update the custom KMS key policy in the Creative account to grant decrypt permissions to the strategy_reviewer IAM role."
        },
        {
          "label": "D",
          "content": "Create a bucket policy that includes read permissions for the S3 bucket. Set the principal of the bucket policy to an anonymous user."
        },
        {
          "label": "E",
          "content": "Update the custom KMS key policy in the Creative account to grant encrypt permissions to the strategy_reviewer IAM role."
        },
        {
          "label": "F",
          "content": "Update the strategy_reviewer IAM role to grant read permissions for the S3 bucket and to grant decrypt permissions for the custom KMS key."
        }
      ],
      "correctAnswer": "ACF",
      "explanation": "<p>Answer: &nbsp;</p><p>A. Create a bucket policy that includes read permissions for the S3 bucket. Set the principal of the bucket policy to the account ID of the Strategy account. &nbsp;</p><p>C. Update the custom KMS key policy in the Creative account to grant decrypt permissions to the strategy_reviewer IAM role. &nbsp;</p><p>F. Update the strategy_reviewer IAM role to grant read permissions for the S3 bucket and to grant decrypt permissions for the custom KMS key. &nbsp;</p><p> Explanation: &nbsp;</p><p> Key Requirements: &nbsp;</p><p>1. Cross-Account S3 Access: &nbsp;</p><p> &nbsp; - Strategy team (Strategy account) needs read-only access to S3 bucket (Creative account). &nbsp;</p><p>2. KMS Encryption: &nbsp;</p><p> &nbsp; - Bucket uses a custom KMS key (requires decrypt permissions). &nbsp;</p><p>3. Least Privilege: &nbsp;</p><p> &nbsp; - Grant only `s3:GetObject` and `kms:Decrypt`. &nbsp;</p><p> Why A + C + F? &nbsp;</p><p>- A (Bucket Policy): &nbsp;</p><p> &nbsp;- Grants read access to the Strategy account (principal = Strategy account ID). &nbsp;</p><p> &nbsp;```json</p><p> &nbsp;{</p><p> &nbsp; &nbsp;\"Version\": \"2012-10-17\",</p><p> &nbsp; &nbsp;\"Statement\": [{</p><p> &nbsp; &nbsp; &nbsp;\"Effect\": \"Allow\",</p><p> &nbsp; &nbsp; &nbsp;\"Principal\": { \"AWS\": \"arn:aws:iam::STRATEGY_ACCOUNT_ID:root\" },</p><p> &nbsp; &nbsp; &nbsp;\"Action\": [\"s3:GetObject\"],</p><p> &nbsp; &nbsp; &nbsp;\"Resource\": \"arn:aws:s3:::creative-bucket/*\"</p><p> &nbsp; &nbsp;}]</p><p> &nbsp;}</p><p> &nbsp;``` &nbsp;</p><p>- C (KMS Key Policy): &nbsp;</p><p> &nbsp;- Allows `kms:Decrypt` for the `strategy_reviewer` role (required for encrypted objects). &nbsp;</p><p> &nbsp;```json</p><p> &nbsp;{</p><p> &nbsp; &nbsp;\"Effect\": \"Allow\",</p><p> &nbsp; &nbsp;\"Principal\": { \"AWS\": \"arn:aws:iam::STRATEGY_ACCOUNT_ID:role/strategy_reviewer\" },</p><p> &nbsp; &nbsp;\"Action\": \"kms:Decrypt\",</p><p> &nbsp; &nbsp;\"Resource\": \"*\"</p><p> &nbsp;}</p><p> &nbsp;``` &nbsp;</p><p>- F (IAM Role Permissions): &nbsp;</p><p> &nbsp;- Attach inline policy to `strategy_reviewer` role: &nbsp;</p><p> &nbsp; &nbsp;```json</p><p> &nbsp; &nbsp;{</p><p> &nbsp; &nbsp; &nbsp;\"Version\": \"2012-10-17\",</p><p> &nbsp; &nbsp; &nbsp;\"Statement\": [</p><p> &nbsp; &nbsp; &nbsp; &nbsp;{</p><p> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"Effect\": \"Allow\",</p><p> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"Action\": [\"s3:GetObject\"],</p><p> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"Resource\": \"arn:aws:s3:::creative-bucket/*\"</p><p> &nbsp; &nbsp; &nbsp; &nbsp;},</p><p> &nbsp; &nbsp; &nbsp; &nbsp;{</p><p> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"Effect\": \"Allow\",</p><p> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"Action\": \"kms:Decrypt\",</p><p> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"Resource\": \"arn:aws:kms:us-east-1:CREATIVE_ACCOUNT_ID:key/1234abcd-12ab-34cd-56ef-1234567890ab\"</p><p> &nbsp; &nbsp; &nbsp; &nbsp;}</p><p> &nbsp; &nbsp; &nbsp;]</p><p> &nbsp; &nbsp;}</p><p> &nbsp; &nbsp;``` &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- B (Full S3/KMS Permissions): &nbsp;</p><p> &nbsp;- Violates least privilege (`s3:*` is excessive). &nbsp;</p><p>- D (Anonymous Access): &nbsp;</p><p> &nbsp;- Security risk (public access to sensitive data). &nbsp;</p><p>- E (Encrypt Permission): &nbsp;</p><p> &nbsp;- Strategy team doesn’t need `kms:Encrypt` (only `Decrypt`). &nbsp;</p><p> Access Flow: &nbsp;</p><p>1. Strategy user assumes `strategy_reviewer` role. &nbsp;</p><p>2. Role permissions allow `s3:GetObject` + `kms:Decrypt`. &nbsp;</p><p>3. Bucket policy permits access from Strategy account. &nbsp;</p><p>4. KMS key policy permits decryption by the role. &nbsp;</p><p> Conclusion: &nbsp;</p><p>A + C + F is the only combination that: &nbsp;</p><p>- Grants minimal permissions (read-only + decrypt). &nbsp;</p><p>- Respects KMS encryption. &nbsp;</p><p>- Avoids security risks (no anonymous/full access). &nbsp;</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "2d4d2b52105c4913af0321e147bf851a",
      "questionNumber": 138,
      "type": "single",
      "content": "<p>Question #138</p><p>A life sciences company is using a combination of open source tools to manage data analysis workflows and Docker containers running on servers in its on-premises data center to process genomics data. Sequencing data is generated and stored on a local storage area network (SAN), and then the data is processed. The research and development teams are running into capacity issues and have decided to re-architect their genomics analysis platform on AWS to scale based on workload demands and reduce the turnaround time from weeks to days. </p><p><br></p><p>The company has a high-speed AWS Direct Connect connection. Sequencers will generate around 200 GB of data for each genome, and individual jobs can take several hours to process the data with ideal compute capacity. The end result will be stored in Amazon S3. The company is expecting 10-15 job requests each day. </p><p><br></p><p>Which solution meets these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use regularly scheduled AWS Snowball Edge devices to transfer the sequencing data into AWS. When AWS receives the Snowball Edge device and the data is loaded into Amazon S3, use S3 events to trigger an AWS Lambda function to process the data."
        },
        {
          "label": "B",
          "content": "Use AWS Data Pipeline to transfer the sequencing data to Amazon S3. Use S3 events to trigger an Amazon EC2 Auto Scaling group to launch custom-AMI EC2 instances running the Docker containers to process the data."
        },
        {
          "label": "C",
          "content": "Use AWS DataSync to transfer the sequencing data to Amazon S3. Use S3 events to trigger an AWS Lambda function that starts an AWS Step Functions workflow. Store the Docker images in Amazon Elastic Container Registry (Amazon ECR) and trigger AWS Batch to run the container and process the sequencing data."
        },
        {
          "label": "D",
          "content": "Use an AWS Storage Gateway file gateway to transfer the sequencing data to Amazon S3. Use S3 events to trigger an AWS Batch job that executes on Amazon EC2 instances running the Docker containers to process the data."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Answer: &nbsp;</p><p>C. Use AWS DataSync to transfer the sequencing data to Amazon S3. Use S3 events to trigger an AWS Lambda function that starts an AWS Step Functions workflow. Store the Docker images in Amazon Elastic Container Registry (Amazon ECR) and trigger AWS Batch to run the container and process the sequencing data. &nbsp;</p><p> Explanation: &nbsp;</p><p> Key Requirements: &nbsp;</p><p>1. High-Volume Data Transfer: &nbsp;</p><p> &nbsp; - 200 GB per genome (10-15 jobs/day = 2–3 TB/day). &nbsp;</p><p> &nbsp; - Direct Connect available (prioritize fast, reliable transfers). &nbsp;</p><p>2. Scalable Compute: &nbsp;</p><p> &nbsp; - Jobs take hours to process (need burstable, high-performance compute). &nbsp;</p><p>3. Workflow Automation: &nbsp;</p><p> &nbsp; - Reduce turnaround time from weeks to days. &nbsp;</p><p> Why Option C is Best? &nbsp;</p><p>- AWS DataSync: &nbsp;</p><p> &nbsp;- Optimized for large data transfers over Direct Connect (faster than Snowball/Storage Gateway). &nbsp;</p><p> &nbsp;- Automated retries/checksums ensure data integrity. &nbsp;</p><p>- AWS Batch + ECR: &nbsp;</p><p> &nbsp;- Runs Docker containers at scale (no EC2 management). &nbsp;</p><p> &nbsp;- Auto-scales EC2 Spot/Fleet instances for cost efficiency. &nbsp;</p><p>- Step Functions: &nbsp;</p><p> &nbsp;- Orchestrates workflows (e.g., pre-processing → analysis → S3 export). &nbsp;</p><p>- Lambda Trigger: &nbsp;</p><p> &nbsp;- S3 events initiate pipelines instantly (no polling). &nbsp;</p><p> Architecture Flow: &nbsp;</p><p>1. Sequencers → SAN → DataSync → S3 (raw data). &nbsp;</p><p>2. S3 Event → Lambda → Step Functions → AWS Batch (processing). &nbsp;</p><p>3. Batch pulls Docker images from ECR, processes data, writes results to S3. &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- A (Snowball Edge): &nbsp;</p><p> &nbsp;- Not real-time: Snowball adds days of latency (violates \"weeks to days\" goal). &nbsp;</p><p>- B (Data Pipeline + EC2 ASG): &nbsp;</p><p> &nbsp;- EC2 ASG is overkill (AWS Batch handles scaling better for batch jobs). &nbsp;</p><p> &nbsp;- Data Pipeline is legacy (Step Functions is modern alternative). &nbsp;</p><p>- D (Storage Gateway + Batch): &nbsp;</p><p> &nbsp;- Storage Gateway is for hybrid caching, not bulk transfers. &nbsp;</p><p> Cost Optimization: &nbsp;</p><p>- Use EC2 Spot Instances in AWS Batch (60-90% cost savings). &nbsp;</p><p>- DataSync pricing: $0.0125/GB (cheaper than manual transfers). &nbsp;</p><p> Conclusion: &nbsp;</p><p>C is the only solution that: &nbsp;</p><p>- Transfers data efficiently (DataSync). &nbsp;</p><p>- Scales compute dynamically (AWS Batch). &nbsp;</p><p>- Automates workflows (Step Functions). &nbsp;</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "eeef6fb235ae48adab6a0fff6b0d05a9",
      "questionNumber": 139,
      "type": "single",
      "content": "<p>Question #139</p><p>A company runs a content management application on a single Windows Amazon EC2 instance in a development environment. The application reads and writes static content to a 2 TB Amazon Elastic Block Store (Amazon EBS) volume that is attached to the instance as the root device. The company plans to deploy this application in production as a highly available and fault-tolerant solution that runs on at least three EC2 instances across multiple Availability Zones. </p><p><br></p><p>A solutions architect must design a solution that joins all the instances that run the application to an Active Directory domain. The solution also must implement Windows ACLs to control access to file contents. The application always must maintain exactly the same content on all running instances at any given point in time. </p><p><br></p><p>Which solution will meet these requirements with the LEAST management overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an Amazon Elastic File System (Amazon EFS) file share. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three instances. Implement a user data script to install the application, join the instance to the AD domain, and mount the EFS file share."
        },
        {
          "label": "B",
          "content": "Create a new AMI from the current EC2 Instance that is running. Create an Amazon FSx for Lustre file system. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three instances. Implement a user data script to join the instance to the AD domain and mount the FSx for Lustre file system."
        },
        {
          "label": "C",
          "content": "Create an Amazon FSx for Windows File Server file system. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three instances. Implement a user data script to install the application and mount the FSx for Windows File Server file system. Perform a seamless domain join to join the instance to the AD domain."
        },
        {
          "label": "D",
          "content": "Create a new AMI from the current EC2 instance that is running. Create an Amazon Elastic File System (Amazon EFS) file system. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three Instances. Perform a seamless domain join to join the instance to the AD domain."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>The correct answer is C. &nbsp;</p><p> Explanation:</p><p>The requirements are:</p><p>1. Highly available & fault-tolerant solution with at least 3 EC2 instances across multiple AZs.</p><p>2. Active Directory (AD) integration for domain joining and Windows ACLs for file access control.</p><p>3. Consistent content across all instances at any given time.</p><p>4. Least management overhead.</p><p>Let’s analyze the options:</p><p> Option A: Amazon EFS</p><p>- ❌ EFS is for Linux, not Windows. The question specifies a Windows EC2 instance, so EFS is not compatible.</p><p>- ❌ EFS does not natively support Windows ACLs or Active Directory integration.</p><p> Option B: Amazon FSx for Lustre</p><p>- ❌ FSx for Lustre is optimized for high-performance computing (HPC) and Linux, not Windows.</p><p>- ❌ Does not natively support Windows ACLs or seamless AD integration.</p><p> Option C: Amazon FSx for Windows File Server</p><p>- ✅ FSx for Windows File Server is fully compatible with Windows and supports Active Directory integration.</p><p>- ✅ Provides Windows ACLs for file access control.</p><p>- ✅ Auto Scaling ensures high availability across multiple AZs.</p><p>- ✅ Seamless domain join simplifies AD integration.</p><p>- ✅ Shared storage ensures all instances have the same content at all times.</p><p>- ✅ Least management overhead because FSx handles replication, backups, and high availability.</p><p> Option D: Amazon EFS (Again)</p><p>- ❌ Same issues as Option A: EFS is for Linux, not Windows, and does not support Windows ACLs or AD.</p><p> Conclusion:</p><p>Option C (Amazon FSx for Windows File Server) is the correct choice because it meets all requirements:</p><p>- Windows-compatible shared storage.</p><p>- Active Directory integration with seamless domain join.</p><p>- Windows ACL support.</p><p>- Automatic replication and high availability.</p><p>- Minimal management overhead.</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "db5fb7d9900b4b22ab51f74b7b59eeb4",
      "questionNumber": 140,
      "type": "single",
      "content": "<p>Question #140</p><p>A software as a service (SaaS) based company provides a case management solution to customers. The company uses a standalone Simple Mail Transfer Protocol (SMTP) server to send email messages from an application. The application also stores an email template for acknowledgement email messages that populate customer data before the application sends the email message to the customer. </p><p><br></p><p>The company plans to migrate this messaging functionality to the AWS Cloud and needs to minimize operational overhead. </p><p><br></p><p>Which solution will meet these requirements MOST cost-effectively?</p>",
      "options": [
        {
          "label": "A",
          "content": "Set up an SMTP server on Amazon EC2 instances by using an AMI from the AWS Marketplace. Store the email template in an Amazon S3 bucket. Create an AWS Lambda function to retrieve the template from the S3 bucket and to merge the customer data from the application with the template. Use an SDK in the Lambda function to send the email message."
        },
        {
          "label": "B",
          "content": "Set up Amazon Simple Email Service (Amazon SES) to send email messages. Store the email template in an Amazon S3 bucket. Create an AWS Lambda function to retrieve the template from the S3 bucket and to merge the customer data from the application with the template. Use an SDK in the Lambda function to send the email message."
        },
        {
          "label": "C",
          "content": "Set up an SMTP server on Amazon EC2 instances by using an AMI from the AWS Marketplace. Store the email template in Amazon Simple Email Service (Amazon SES) with parameters for the customer data. Create an AWS Lambda function to call the SES template and to pass customer data to replace the parameters. Use the AWS Marketplace SMTP server to send the email message."
        },
        {
          "label": "D",
          "content": "Set up Amazon Simple Email Service (Amazon SES) to send email messages. Store the email template on Amazon SES with parameters for the customer data. Create an AWS Lambda function to call the SendTemplatedEmail API operation and to pass customer data to replace the parameters and the email destination."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. Set up Amazon Simple Email Service (Amazon SES) to send email messages. Store the email template on Amazon SES with parameters for the customer data. Create an AWS Lambda function to call the SendTemplatedEmail API operation and to pass customer data to replace the parameters and the email destination.</p><p>This solution is the most cost-effective approach that meets all the requirements for the following reasons:</p><p>Amazon SES is specifically designed as a cloud-based email sending service that replaces traditional SMTP servers with minimal operational overhead. It's highly scalable and cost-effective for sending transactional emails.</p><p>Amazon SES supports template storage directly within the service, allowing the company to store their acknowledgement email templates with parameters that can be dynamically populated with customer data.</p><p>AWS Lambda provides a serverless compute service that eliminates the need to provision or manage servers. Using Lambda to call the SendTemplatedEmail API operation means the company only pays for the actual compute time used when sending emails.</p><p>The combination of SES templates and Lambda functions creates a fully managed solution that minimizes operational overhead - there are no servers to maintain, patch, or scale.</p><p>The SendTemplatedEmail API operation is specifically designed for this use case - it allows the application to pass customer data that replaces parameters in the template before sending the email to the customer.</p><p>This solution provides the most direct replacement for the company's current SMTP server and email template functionality while moving to a fully managed AWS service model that reduces operational overhead and provides cost-effective pricing based on actual usage.</p><p>Sources</p><p>Using Amazon Simple Email Service (SES) to send emails and archive them | AWS re:Post （https://repost.aws/articles/ARz6UmX4epSJ-_SmCMMtFssg/using-amazon-simple-email-service-ses-to-send-emails-and-archive-them）</p><p>Use SendTemplatedEmail with an AWS SDK - Amazon Simple Email Service （https://docs.aws.amazon.com/ses/latest/dg/ses_example_ses_SendTemplatedEmail_section.html）</p><p>Using templates to send personalized email with the Amazon SES API - Amazon Simple Email Service （https://docs.aws.amazon.com/ses/latest/dg/send-personalized-email-api.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "c6773afdb4224eb289f0658278df8089",
      "questionNumber": 141,
      "type": "single",
      "content": "<p>Question #141</p><p>A company is processing videos in the AWS Cloud by Using Amazon EC2 instances in an Auto Scaling group. It takes 30 minutes to process a video. Several EC2 instances scale in and out depending on the number of videos in an Amazon Simple Queue Service (Amazon SQS) queue.</p><p><br></p><p>The company has configured the SQS queue with a redrive policy that specifies a target dead-letter queue and a maxReceiveCount of 1. The company has set the visibility timeout for the SQS queue to 1 hour. The company has set up an Amazon CloudWatch alarm to notify the development team when there are messages in the dead-letter queue.</p><p><br></p><p>Several times during the day, the development team receives notification that messages are in the dead-letter queue and that videos have not been processed properly. An investigation finds no errors in the application logs.</p><p><br></p><p>How can the company solve this problem?</p>",
      "options": [
        {
          "label": "A",
          "content": "Turn on termination protection for the EC2 Instances"
        },
        {
          "label": "B",
          "content": "Update the visibility timeout for the SQS queue to 3 hours"
        },
        {
          "label": "C",
          "content": "Configure scale-in protection for the instances during processing"
        },
        {
          "label": "D",
          "content": "Update the redrive policy and set maxReceiveCount to 0."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Configure scale-in protection for the instances during processing</p><p>This solution addresses the root cause of the problem. Here's why:</p><p>The issue occurs because the Auto Scaling group is terminating EC2 instances while they're still processing videos. When an instance is terminated during processing:</p><p>The video processing job is interrupted</p><p>The SQS message remains invisible for the remainder of the 1-hour visibility timeout</p><p>When the visibility timeout expires, the message becomes visible again</p><p>Since maxReceiveCount is set to 1, the message is immediately sent to the dead-letter queue</p><p>This triggers the CloudWatch alarm, notifying the development team</p><p>There are no application errors in the logs because the problem isn't with the application code - it's with the infrastructure setup. The videos aren't being processed properly because the instances processing them are being terminated before completion.</p><p>By configuring scale-in protection for instances during processing, the Auto Scaling group will be prevented from terminating instances that are actively working on video processing jobs. This allows the instances to complete their current work before being terminated during scale-in events.</p><p>Scale-in protection is the most direct solution because:</p><p>It addresses the exact problem (premature instance termination)</p><p>It doesn't require changing the application code</p><p>It doesn't require modifying the queue configuration</p><p>It ensures that once an instance starts processing a video, it will be allowed to complete the job</p><p>This solution maintains the efficiency of the Auto Scaling group while preventing the specific issue that's causing videos to end up in the dead-letter queue.</p><p>Sources</p><p>Infinite retries due to exceeded SQS visibility timeout | AWS re:Post （https://repost.aws/questions/QUOGzGulGSS1-wtgDIj22USw/infinite-retries-due-to-exceeded-sqs-visibility-timeout）</p><p>Processing messages in a timely manner in Amazon SQS - Amazon Simple Queue Service （https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/best-practices-processing-messages-timely-manner.html）</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "a08c2524928d457882718637123874fb",
      "questionNumber": 142,
      "type": "single",
      "content": "<p>Question #142</p><p>A company has developed APIs that use Amazon API Gateway with Regional endpoints. The APIs call AWS Lambda functions that use API Gateway authentication mechanisms. After a design review, a solutions architect identifies a set of APIs that do not require public access.</p><p><br></p><p>The solutions architect must design a solution to make the set of APIs accessible only from a VPC. All APIs need to be called with an authenticated user.</p><p><br></p><p>Which solution will meet these requirements with the LEAST amount of effort?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an internal Application Load Balancer (ALB). Create a target group. Select the Lambda function to call. Use the ALB DNS name to call the API from the VPC."
        },
        {
          "label": "B",
          "content": "Remove the DNS entry that is associated with the API in API Gateway. Create a hosted zone in Amazon Route 53. Create a CNAME record in the hosted zone. Update the API in API Gateway with the CNAME record. Use the CNAME record to call the API from the VPC."
        },
        {
          "label": "C",
          "content": "Update the API endpoint from Regional to private in API Gateway. Create an interface VPC endpoint in the VPC. Create a resource policy, and attach it to the API. Use the VPC endpoint to call the API from the VPC."
        },
        {
          "label": "D",
          "content": "Deploy the Lambda functions inside the VPC. Provision an EC2 instance, and install an Apache server. From the Apache server, call the Lambda functions. Use the internal CNAME record of the EC2 instance to call the API from the VPC."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Update the API endpoint from Regional to private in API Gateway. Create an interface VPC endpoint in the VPC. Create a resource policy, and attach it to the API. Use the VPC endpoint to call the API from the VPC.</p><p>This solution meets the requirements with the least amount of effort because:</p><p>Changing the API endpoint type from Regional to private is a straightforward configuration change in API Gateway that restricts access to the API to only be accessible through VPC endpoints.</p><p>Creating an interface VPC endpoint for API Gateway is a simple process that establishes a private connection between your VPC and API Gateway service without requiring public internet access.</p><p>Adding a resource policy to the API allows you to define specific access controls, ensuring that the API can only be called from the specified VPC endpoint.</p><p>This approach maintains the existing authentication mechanisms that are already in place for the APIs, as specified in the requirements.</p><p>The solution requires minimal changes to the existing architecture and doesn't require modifying the Lambda functions or deploying additional infrastructure.</p><p>Option D (deploying Lambda functions inside the VPC with an EC2 instance running Apache) would require significantly more effort because:</p><p>It would require moving Lambda functions into the VPC</p><p>It would need provisioning and maintaining an EC2 instance</p><p>It would require installing and configuring Apache server</p><p>It would introduce an unnecessary proxy layer between the API calls and Lambda functions</p><p>It would be more complex to manage and maintain over time</p><p>The private API Gateway endpoint with VPC endpoint approach is a purpose-built solution for this exact requirement, making it the option that requires the least amount of effort while still meeting all the security and access requirements.</p><p>Sources</p><p>Prioritize API security - AWS Serverless Multi-Tier Architectures with Amazon API Gateway and AWS Lambda （https://docs.aws.amazon.com/whitepapers/latest/serverless-multi-tier-architectures-api-gateway-lambda/prioritize-api-security.html）</p><p>Tutorial: Create a private REST API - Amazon API Gateway （https://docs.aws.amazon.com/apigateway/latest/developerguide/private-api-tutorial.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "051bf6ca0f5e412cbeec45d981d73449",
      "questionNumber": 143,
      "type": "multiple",
      "content": "<p>Question #143</p><p>A weather service provides high-resolution weather maps from a web application hosted on AWS in the eu-west-1 Region. The weather maps are updated frequently and stored in Amazon S3 along with static HTML content. The web application is fronted by Amazon CloudFront.</p><p>The company recently expanded to serve users in the us-east-1 Region, and these new users report that viewing their respective weather maps is slow from time to time.</p><p>Which combination of steps will resolve the us-east-1 performance issues? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Configure the AWS Global Accelerator endpoint for the S3 bucket in eu-west-1. Configure endpoint groups for TCP ports 80 and 443 in us-east-1."
        },
        {
          "label": "B",
          "content": "Create a new S3 bucket in us-east-1. Configure S3 cross-Region replication to synchronize from the S3 bucket in eu-west-1."
        },
        {
          "label": "C",
          "content": "Use Lambda@Edge to modify requests from North America to use the S3 Transfer Acceleration endpoint in us-east-1."
        },
        {
          "label": "D",
          "content": "Use Lambda@Edge to modify requests from North America to use the S3 bucket in us-east-1."
        },
        {
          "label": "E",
          "content": "<p> Configure the AWS Global Accelerator endpoint for us-east-1 as an origin on the CloudFront distribution. Use Lambda@Edge to modify requests from North America to use the new origin.</p>"
        }
      ],
      "correctAnswer": "BD",
      "explanation": "<p>Based on the information provided, the correct answers are:</p><p>B. Create a new S3 bucket in us-east-1. Configure S3 cross-Region replication to synchronize from the S3 bucket in eu-west-1.</p><p>D. Use Lambda@Edge to modify requests from North America to use the S3 bucket in us-east-1.</p><p>These two steps together provide the most effective solution to resolve the performance issues for users in the us-east-1 Region:</p><p>Creating a new S3 bucket in us-east-1 with cross-Region replication (option B) addresses the fundamental issue by:</p><p>Storing the weather maps physically closer to the us-east-1 users</p><p>Using S3 cross-Region replication to automatically keep the content synchronized between regions</p><p>Ensuring that the frequently updated weather maps are available in both regions with minimal delay</p><p>Using Lambda@Edge to modify requests based on user location (option D) complements the first step by:</p><p>Intelligently routing users from North America to the us-east-1 S3 bucket</p><p>Ensuring that European users continue to access the eu-west-1 bucket</p><p>Providing dynamic request routing without changing the CloudFront distribution configuration</p><p>Maintaining a single CloudFront distribution while serving content from multiple origins</p><p>This combination provides a complete solution because:</p><p>The data is physically located closer to the users who need it (through replication)</p><p>The routing logic ensures users are directed to the closest data source (through Lambda@Edge)</p><p>The solution maintains the existing CloudFront distribution while improving performance</p><p>Option A (configuring AWS Global Accelerator) would not be as effective because:</p><p>Global Accelerator is more suitable for applications that need static IP addresses or for non-HTTP/S traffic</p><p>CloudFront is already optimized for content delivery of static assets</p><p>Global Accelerator wouldn't address the core issue of the content being physically distant from the users</p><p>The combination of regional S3 buckets with cross-Region replication and Lambda@Edge for intelligent routing provides the most comprehensive solution to the performance issues.</p><p>Sources</p><p>How can I optimize data transfer costs between Amazon S3 and EC2 instances across different AWS Regions while ensuring compliance with data residency requirements? | AWS re:Post （https://repost.aws/questions/QURehgqhADRRiJVHNoTykw6w/how-can-i-optimize-data-transfer-costs-between-amazon-s3-and-ec2-instances-across-different-aws-regions-while-ensuring-compliance-with-data-residency-requirements）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "e13302ce04984fddb8253a285b555e95",
      "questionNumber": 144,
      "type": "single",
      "content": "<p>Question #144</p><p>A solutions architect is investigating an issue in which a company cannot establish new sessions in Amazon WorkSpaces. An initial analysis indicates that the issue involves user profiles. The Amazon WorkSpaces environment is configured to use Amazon FSx for Windows File Server as the profile share storage. The FSx for Windows File Server file system is configured with 10 TB of storage.</p><p><br></p><p>The solutions architect discovers that the file system has reached its maximum capacity. The solutions architect must ensure that users can regain access. The solution also must prevent the problem from occurring again.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Remove old user profiles to create space. Migrate the user profiles to an Amazon FSx for Lustre file system."
        },
        {
          "label": "B",
          "content": "Increase capacity by using the update-file-system command. Implement an Amazon CloudWatch metric that monitors free space. Use Amazon EventBridge to invoke an AWS Lambda function to increase capacity as required."
        },
        {
          "label": "C",
          "content": "Monitor the file system by using the FreeStorageCapacity metric in Amazon CloudWatch. Use AWS Step Functions to increase the capacity as required."
        },
        {
          "label": "D",
          "content": "Remove old user profiles to create space. Create an additional FSx for Windows File Server file system. Update the user profile redirection for 50% of the users to use the new file system."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>The correct answer is B. &nbsp;</p><p> Explanation:</p><p>The problem is that the FSx for Windows File Server has reached its 10 TB maximum capacity, preventing new Workspaces sessions. The solution must: &nbsp;</p><p>1. Immediately free up space to restore access. &nbsp;</p><p>2. Prevent future capacity issues by automating storage scaling. &nbsp;</p><p>Let’s analyze the options: &nbsp;</p><p> Option A: Migrate to FSx for Lustre &nbsp;</p><p>- ❌ FSx for Lustre is not compatible with Windows user profiles (it’s designed for high-performance Linux workloads). &nbsp;</p><p>- ❌ Simply deleting old profiles does not prevent future capacity issues. &nbsp;</p><p> Option B: Increase Capacity + Automate Scaling &nbsp;</p><p>- ✅ `update-file-system` allows increasing FSx storage capacity without downtime. &nbsp;</p><p>- ✅ CloudWatch metric (`FreeStorageCapacity`) monitors free space. &nbsp;</p><p>- ✅ EventBridge + Lambda can automatically scale storage when thresholds are met. &nbsp;</p><p>- ✅ Immediate fix: Free up space manually (if needed) while automation prevents future issues. &nbsp;</p><p> Option C: CloudWatch + Step Functions &nbsp;</p><p>- ❌ While monitoring is correct, Step Functions is overkill for this use case (Lambda is simpler and more cost-effective). &nbsp;</p><p>- ❌ Does not mention immediate remediation (like deleting old profiles first). &nbsp;</p><p> Option D: Add a Second FSx File System &nbsp;</p><p>- ❌ Splitting profiles across two file systems adds complexity and does not solve the root issue (storage scaling). &nbsp;</p><p>- ❌ Manual intervention is required to balance users, which is not scalable. &nbsp;</p><p> Why Option B is Best? &nbsp;</p><p>- Short-term: Delete old profiles if needed to regain access. &nbsp;</p><p>- Long-term: Automate scaling using CloudWatch + Lambda to prevent future outages. &nbsp;</p><p>- FSx for Windows supports scaling up to 64 TB, so this is a sustainable solution. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "67d493b8f3a04c5a9b1acaa61c911d00",
      "questionNumber": 145,
      "type": "single",
      "content": "<p>Question #145</p><p>An international delivery company hosts a delivery management system on AWS. Drivers use the system to upload confirmation of delivery. Confirmation includes the recipient’s signature or a photo of the package with the recipient. The driver’s handheld device uploads signatures and photos through FTP to a single Amazon EC2 instance. Each handheld device saves a file in a directory based on the signed-in user, and the filename matches the delivery number. The EC2 instance then adds metadata to the file after querying a central database to pull delivery information. The file is then placed in Amazon S3 for archiving. </p><p><br></p><p>As the company expands, drivers report that the system is rejecting connections. The FTP server is having problems because of dropped connections and memory issues in response to these problems, a system engineer schedules a cron task to reboot the EC2 instance every 30 minutes. The billing team reports that files are not always in the archive and that the central system is not always updated. A solutions architect needs to design a solution that maximizes scalability to ensure that the archive always receives the files and that systems are always updated. </p><p><br></p><p>The handheld devices cannot be modified, so the company cannot deploy a new application.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an AMI of the existing EC2 instance. Create an Auto Scaling group of EC2 instances behind an Application Load Balancer. Configure the Auto Scaling group to have a minimum of three instances."
        },
        {
          "label": "B",
          "content": "Use AWS Transfer Family to create an FTP server that places the files in Amazon Elastic File System (Amazon EFS). Mount the EFS volume to the existing EC2 instance. Point the EC2 instance to the new path for file processing."
        },
        {
          "label": "C",
          "content": "Use AWS Transfer Family to create an FTP server that places the files in Amazon S3. Use an S3 event notification through Amazon Simple Notification Service (Amazon SNS) to invoke an AWS Lambda function. Configure the Lambda function to add the metadata and update the delivery system."
        },
        {
          "label": "D",
          "content": "Update the handheld devices to place the files directly in Amazon S3. Use an S3 event notification through Amazon Simple Queue Service (Amazon SQS) to invoke an AWS Lambda function. Configure the Lambda function to add the metadata and update the delivery system."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Option C is correct. Using AWS Transfer Family to create an FTP server that places the files directly in Amazon S3 and using S3 event notifications through Amazon SNS to invoke an AWS Lambda function will ensure that the archive always receives the files and that the central system is always updated. This solution maximizes scalability and eliminates the need for manual intervention, such as rebooting the EC2 instance.</p><p>The correct answer is C. &nbsp;</p><p> Explanation:</p><p>The key requirements are: &nbsp;</p><p>1. Maximize scalability to handle increasing FTP connections. &nbsp;</p><p>2. Ensure files are always archived in Amazon S3. &nbsp;</p><p>3. Update the central system reliably with metadata. &nbsp;</p><p>4. Cannot modify the handheld devices, so the FTP process must remain unchanged. &nbsp;</p><p>Let’s analyze the options: &nbsp;</p><p> Option A: Auto Scaling Group with ALB &nbsp;</p><p>- ❌ FTP is stateful and does not work well with load balancers (ALB is for HTTP/HTTPS, not FTP). &nbsp;</p><p>- ❌ Rebooting instances (current issue) won’t be fixed—this just spreads the problem across multiple instances. &nbsp;</p><p>- ❌ No improvement in reliability for file processing or metadata updates. &nbsp;</p><p> Option B: AWS Transfer Family + EFS &nbsp;</p><p>- ❌ EFS adds unnecessary complexity—files still need processing by the EC2 instance, which is the bottleneck. &nbsp;</p><p>- ❌ Does not solve the core issue (scalability and reliability of file processing). &nbsp;</p><p>- ❌ EC2 instance remains a single point of failure for metadata updates. &nbsp;</p><p> Option C: AWS Transfer Family + S3 + Lambda &nbsp;</p><p>- ✅ AWS Transfer Family replaces the EC2 FTP server, providing a fully managed, scalable FTP solution. &nbsp;</p><p>- ✅ Files go directly to S3, eliminating the EC2 bottleneck. &nbsp;</p><p>- ✅ S3 event notifications trigger Lambda, which reliably adds metadata and updates the system (no lost files). &nbsp;</p><p>- ✅ No changes needed to handheld devices (still use FTP). &nbsp;</p><p>- ✅ Fully serverless and scalable—no EC2 instance to manage or reboot. &nbsp;</p><p> Option D: Update Handheld Devices to Use S3 Directly &nbsp;</p><p>- ❌ Violates the requirement (handheld devices cannot be modified). &nbsp;</p><p>- ❌ Not a feasible solution since the question explicitly rules out changes to the devices. &nbsp;</p><p> Why Option C is Best? &nbsp;</p><p>- Scalable FTP: AWS Transfer Family handles FTP connections without EC2 limitations. &nbsp;</p><p>- Reliable processing: S3 + Lambda ensures no files are lost and metadata is always added. &nbsp;</p><p>- No changes to handheld devices—maintains compatibility. &nbsp;</p><p>- Fully managed solution—eliminates EC2 maintenance (no more reboots). &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "9322f4b10a484ee083c8c147199c7cb9",
      "questionNumber": 146,
      "type": "single",
      "content": "<p>Question #146</p><p>A company is running an application in the AWS Cloud. The application runs on containers in an Amazon Elastic Container Service (Amazon ECS) cluster. The ECS tasks use the Fargate launch type. The application's data is relational and is stored in Amazon Aurora MySQL. To meet regulatory requirements, the application must be able to recover to a separate AWS Region in the event of an application failure. In case of a failure, no data can be lost.</p><p><br></p><p>Which solution will meet these requirements with the LEAST amount of operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Provision an Aurora Replica in a different Region."
        },
        {
          "label": "B",
          "content": "Set up AWS DataSync for continuous replication of the data to a different Region."
        },
        {
          "label": "C",
          "content": "Set up AWS Database Migration Service (AWS DMS) to perform a continuous replication of the data to a different Region."
        },
        {
          "label": "D",
          "content": "Use Amazon Data Lifecycle Manager (Amazon DLM) to schedule a snapshot every 5 minutes."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Provision an Aurora Replica in a different Region.</p><p>This solution meets the requirements with the least amount of operational overhead for the following reasons:</p><p>Zero data loss requirement: Aurora cross-region replicas provide continuous, asynchronous replication with typical lag times of less than a second. This ensures that virtually no data is lost in the event of a primary region failure.</p><p>Recovery to a separate AWS Region: By provisioning an Aurora Replica in a different region, the company creates a standby database that can be promoted to become the primary database in case of a regional outage or application failure.</p><p>Least operational overhead: Aurora Replicas are fully managed by AWS, requiring minimal setup and ongoing maintenance. The replication process is handled automatically by the Aurora service.</p><p>The other options have significant drawbacks:</p><p>B. AWS DataSync is primarily designed for file-based data transfer between storage systems, not for continuous database replication. It would require additional components and custom scripting to implement a database disaster recovery solution, increasing operational overhead.</p><p>C. AWS Database Migration Service (AWS DMS) can perform continuous replication, but it requires more configuration and monitoring compared to native Aurora replication. It would involve setting up and maintaining replication instances, tasks, and endpoints, resulting in higher operational overhead.</p><p>D. Amazon Data Lifecycle Manager (Amazon DLM) is designed for managing EBS snapshots and AMIs, not database replication. Using snapshots every 5 minutes would not meet the zero data loss requirement, as any data written between snapshots could be lost. Additionally, restoring from snapshots would result in longer recovery times.</p><p>Aurora's built-in cross-region replication capability provides the most straightforward and efficient solution for meeting the company's disaster recovery requirements with minimal operational overhead.</p><p>Sources</p><p>Comparing Amazon Aurora Replicas, Aurora cross-Region Replicas, and Aurora global databases - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/aurora-replication-options/introduction.html）</p><p>Disaster recovery options in the cloud - Disaster Recovery of Workloads on AWS: Recovery in the Cloud （https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-options-in-the-cloud.html）</p><p>AWS Backup vs global table/database for cost-effective RTO,RPO requirements | AWS re:Post （https://repost.aws/questions/QUWf0GuonKSvW8U2N4o8huHw/aws-backup-vs-global-table-database-for-cost-effective-rto-rpo-requirements）</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "7597ebd2dd3f41e8869777dc5776a166",
      "questionNumber": 147,
      "type": "single",
      "content": "<p>Question #147</p><p>A financial services company receives a regular data feed from its credit card servicing partner. Approximately 5,000 records are sent every 15 minutes in plaintext, delivered over HTTPS directly into an Amazon S3 bucket with server-side encryption. This feed contains sensitive credit card primary account number (PAN) data. The company needs to automatically mask the PAN before sending the data to another S3 bucket for additional internal processing. The company also needs to remove and merge specific fields, and then transform the record into JSON format. Additionally, extra feeds are likely to be added in the future, so any design needs to be easily expandable.</p><p><br></p><p>Which solutions will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Invoke an AWS Lambda function on file delivery that extracts each record and writes it to an Amazon SQS queue. Invoke another Lambda function when new messages arrive in the SQS queue to process the records, writing the results to a temporary location in Amazon S3. Invoke a final Lambda function once the SQS queue is empty to transform the records into JSON format and send the results to another S3 bucket for internal processing."
        },
        {
          "label": "B",
          "content": "Invoke an AWS Lambda function on file delivery that extracts each record and writes it to an Amazon SQS queue. Configure an AWS Fargate container application to automatically scale to a single instance when the SQS queue contains messages. Have the application process each record, and transform the record into JSON format. When the queue is empty, send the results to another S3 bucket for internal processing and scale down the AWS Fargate instance."
        },
        {
          "label": "C",
          "content": "Create an AWS Glue crawler and custom classifier based on the data feed formats and build a table definition to match. Invoke an AWS Lambda function on file delivery to start an AWS Glue ETL job to transform the entire record according to the processing and transformation requirements. Define the output format as JSON. Once complete, have the ETL job send the results to another S3 bucket for internal processing."
        },
        {
          "label": "D",
          "content": "Create an AWS Glue crawler and custom classifier based upon the data feed formats and build a table definition to match. Perform an Amazon Athena query on file delivery to start an Amazon EMR ETL job to transform the entire record according to the processing and transformation requirements. Define the output format as JSON. Once complete, send the results to another S3 bucket for internal processing and scale down the EMR cluster."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Option C is correct. It will process the data in batch mode using Glue ETL job which can handle large amount of data and can be scheduled to run periodically. This solution is also easily expandable for future feeds.The requirements are: &nbsp;</p><p>1. Automatically mask PAN data (sensitive credit card info). &nbsp;</p><p>2. Remove, merge, and transform records into JSON. &nbsp;</p><p>3. Handle ~5,000 records every 15 minutes (scalable). &nbsp;</p><p>4. Easily expandable for future feeds. &nbsp;</p><p> Analysis of Options:</p><p> Option A: Lambda + SQS + Lambda + Lambda &nbsp;</p><p>- ❌ Overly complex with multiple Lambda functions and manual orchestration. &nbsp;</p><p>- ❌ No built-in data transformation logic (masking, merging fields, JSON conversion). &nbsp;</p><p>- ❌ Error-prone—if Lambda fails mid-process, records may be lost or duplicated. &nbsp;</p><p> Option B: Lambda + SQS + Fargate &nbsp;</p><p>- ❌ Fargate adds unnecessary overhead—Lambda is better for lightweight transformations. &nbsp;</p><p>- ❌ Manual scaling of Fargate is inefficient for batch processing. &nbsp;</p><p>- ❌ No built-in data masking/transformation—custom code required. &nbsp;</p><p> Option C: AWS Glue (Crawler + ETL Job) + Lambda &nbsp;</p><p>- ✅ AWS Glue is purpose-built for ETL (masking PAN, merging fields, converting to JSON). &nbsp;</p><p>- ✅ Serverless and scalable—handles 5,000 records every 15 minutes easily. &nbsp;</p><p>- ✅ Glue Crawler auto-discovers schema, making it expandable for future feeds. &nbsp;</p><p>- ✅ Lambda triggers Glue ETL on new file arrival (fully automated). &nbsp;</p><p>- ✅ Outputs directly to S3 in JSON format. &nbsp;</p><p> Option D: Glue + Athena + EMR &nbsp;</p><p>- ❌ Overkill for 5,000 records—EMR is for big data (petabyte-scale). &nbsp;</p><p>- ❌ Athena is for querying, not ETL—adds unnecessary steps. &nbsp;</p><p>- ❌ EMR requires cluster management, increasing cost and complexity. &nbsp;</p><p> Why Option C is Best? &nbsp;</p><p>- AWS Glue provides out-of-the-box ETL (masking, transformations, JSON conversion). &nbsp;</p><p>- Fully serverless—no infrastructure to manage. &nbsp;</p><p>- Scalable and cost-effective for batch processing. &nbsp;</p><p>- Easily expandable—just add new crawlers/ETL jobs for future feeds. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "68625a4ba3aa40a6ab9c8eee4b9490e4",
      "questionNumber": 148,
      "type": "single",
      "content": "Question #148<p>A company wants to use AWS to create a business continuity solution in case the company's main on-premises application fails. The application runs on physical servers that also run other applications. The on-premises application that the company is planning to migrate uses a MySQL database as a data store. All the company's on-premises applications use operating systems that are compatible with Amazon EC2.</p><p>Which solution will achieve the company's goal with the LEAST operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Install the AWS Replication Agent on the source servers, including the MySQL servers. Set up replication for all servers. Launch test instances for regular drills. Cut over to the test instances to fail over the workload in the case of a failure event."
        },
        {
          "label": "B",
          "content": "Install the AWS Replication Agent on the source servers, including the MySQL servers. Initialize AWS Elastic Disaster Recovery in the target AWS Region. Define the launch settings. Frequently perform failover and fallback from the most recent point in time."
        },
        {
          "label": "C",
          "content": "Create AWS Database Migration Service (AWS DMS) replication servers and a target Amazon Aurora MySQL DB cluster to host the database. Create a DMS replication task to copy the existing data to the target DB cluster. Create a local AWS Schema Conversion Tool (AWS SCT) change data capture (CDC) task to keep the data synchronized. Install the rest of the software on EC2 instances by starting with a compatible base AMI."
        },
        {
          "label": "D",
          "content": "Deploy an AWS Storage Gateway Volume Gateway on premises. Mount volumes on all on-premises servers. Install the application and the MySQL database on the new volumes. Take regular snapshots. Install all the software on EC2 Instances by starting with a compatible base AMI. Launch a Volume Gateway on an EC2 instance. Restore the volumes from the latest snapshot. Mount the new volumes on the EC2 instances in the case of a failure event."
        }
      ],
      "correctAnswer": "B",
      "explanation": "Tricky one. This is not an on premise migration use case which prompts for answer C. Its a current situation of on premise application which the <p>company wants to continue its state in the requirement of using AWS as DR solution. </p><p>https://docs.aws.amazon.com/images/drs/latest/userguide/images/drs-failback-arc.png </p><p>https://docs.aws.amazon.com/drs/latest/userguide/what-is-drs.html</p><p><br></p><p>1. Use AWS Database Migration Service (DMS) to replicate the on-premises MySQL database to Amazon RDS for MySQL. This provides a managed database solution with minimal operational overhead.</p><p>2. Set up AWS Application Discovery Service to analyze the on-premises application and its dependencies. This will help in planning the migration and identifying any potential issues.</p><p>3. Use AWS Server Migration Service (SMS) to replicate the application servers to Amazon EC2 instances. This service can handle the migration of multiple servers simultaneously and supports incremental replication.</p><p>4. Configure Amazon Route 53 for DNS failover. This will allow automatic redirection of traffic to the AWS environment in case of a failure in the on-premises infrastructure.</p><p>5. Implement AWS CloudWatch for monitoring both the on-premises and AWS environments. Set up alarms to notify of any issues or failures.</p><p>6. Use AWS Systems Manager to manage and configure the EC2 instances, reducing operational overhead for server management.</p><p>This solution offers several benefits:</p><p>- Minimal operational overhead: Amazon RDS manages the database, while EC2 instances can be easily managed using Systems Manager.</p><p>- Automated failover: Route 53 can automatically redirect traffic in case of a failure.</p><p>- Scalability: The AWS environment can be easily scaled up or down as needed.</p><p>- Continuous replication: DMS and SMS ensure that the AWS environment stays up-to-date with the on-premises systems.</p><p>Best practices to consider:</p><p><br></p><p>- Regularly test the failover process to ensure business continuity.</p><p>- Implement proper security measures, such as using security groups and network ACLs to control access to AWS resources.</p><p>- Use AWS Identity and Access Management (IAM) to manage permissions and access to AWS services.</p><p>- Encrypt data at rest and in transit using AWS Key Management Service (KMS) and SSL/TLS.</p><p><br></p><p>[1] [Amazon RDS for MySQL Resources – AWS] (https://aws.amazon.com/rds/mysql/resources/)</p><p>[3] [Amazon RDS for MySQL Resources – AWS] (https://aws.amazon.com/rds/mysql/resources/)</p><p>[6] [Conclusion - Optimizing MySQL Running on Amazon EC2 Using Amazon EBS] (https://docs.aws.amazon.com/whitepapers/latest/optimizing-mysql-on-ec2-using-amazon-ebs/conclusion.html)</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "77b13aaabcff4a05af7d60adfe4bbca5",
      "questionNumber": 149,
      "type": "single",
      "content": "<p>Question #149</p><p>A company is subject to regulatory audits of its financial information. External auditors who use a single AWS account need access to the company's AWS account. A solutions architect must provide the auditors with secure, read-only access to the company's AWS account. The solution must comply with AWS security best practices.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "In the company&#39;s AWS account, create resource policies for all resources in the account to grant access to the auditors&#39; AWS account. Assign a unique external ID to the resource policy."
        },
        {
          "label": "B",
          "content": "In the company&#39;s AWS account, create an IAM role that trusts the auditors&#39; AWS account. Create an IAM policy that has the required permissions. Attach the policy to the role. Assign a unique external ID to the role&#39;s trust policy."
        },
        {
          "label": "C",
          "content": "In the company&#39;s AWS account, create an IAM user. Attach the required IAM policies to the IAM user. Create API access keys for the IAM user. Share the access keys with the auditors."
        },
        {
          "label": "D",
          "content": "In the company&#39;s AWS account, create an IAM group that has the required permissions. Create an IAM user in the company&#39;s account for each auditor. Add the IAM users to the IAM group."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. In the company's AWS account, create an IAM role that trusts the auditors' AWS account. Create an IAM policy that has the required permissions. Attach the policy to the role. Assign a unique external ID to the role's trust policy.</p><p>This solution meets the requirements and follows AWS security best practices for the following reasons:</p><p>Cross-account IAM roles are the recommended AWS best practice for providing secure access to external entities. This approach allows the auditors to assume a role in the company's account without needing to create and manage separate IAM users.</p><p>By creating an IAM policy with read-only permissions and attaching it to the role, the company can precisely control what resources and actions the auditors can access, following the principle of least privilege.</p><p>The trust relationship between the company's role and the auditors' AWS account establishes a secure delegation mechanism that allows users in the auditors' account to assume the role.</p><p>Including a unique external ID in the role's trust policy adds an additional layer of security by preventing the \"confused deputy\" problem. The external ID acts as a shared secret between the company and the auditors, ensuring that only authorized entities can assume the role.</p><p>Option A (creating resource policies for all resources) is not the recommended approach because:</p><p>It would be extremely time-consuming and error-prone to create and maintain individual resource policies for all resources in the account</p><p>Not all AWS services support resource-based policies</p><p>Managing access at the resource level would be much more complex than using a role-based approach</p><p>It would be difficult to ensure consistent permissions across all resources</p><p>The IAM role approach provides a centralized, secure, and manageable solution that aligns with AWS security best practices for cross-account access.</p><p>Sources</p><p>Access to AWS accounts owned by third parties - AWS Identity and Access Management （https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_third-party.html）</p><p>SEC03-BP09 Share resources securely with a third party - AWS Well-Architected Framework (2023-04-10) （https://docs.aws.amazon.com/wellarchitected/2023-04-10/framework/sec_permissions_share_securely_third_party.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "f88e2ae3930f4f41bbb354d46541716f",
      "questionNumber": 150,
      "type": "single",
      "content": "<p>Question #150</p><p>A company has a latency-sensitive trading platform that uses Amazon DynamoDB as a storage backend. The company configured the DynamoDB table to use on-demand capacity mode. A solutions architect needs to design a solution to improve the performance of the trading platform. The new solution must ensure high availability for the trading platform.</p><p><br></p><p>Which solution will meet these requirements with the LEAST latency?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a two-node DynamoDB Accelerator (DAX) cluster. Configure an application to read and write data by using DAX."
        },
        {
          "label": "B",
          "content": "Create a three-node DynamoDB Accelerator (DAX) cluster. Configure an application to read data by using DAX and to write data directly to the DynamoDB table."
        },
        {
          "label": "C",
          "content": "Create a three-node DynamoDB Accelerator (DAX) cluster. Configure an application to read data directly from the DynamoDB table and to write data by using DAX."
        },
        {
          "label": "D",
          "content": "Create a single-node DynamoDB Accelerator (DAX) cluster. Configure an application to read data by using DAX and to write data directly to the DynamoDB table."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Create a three-node DynamoDB Accelerator (DAX) cluster. Configure an application to read data by using DAX and to write data directly to the DynamoDB table.</p><p>This solution will meet the requirements with the least latency while ensuring high availability for the following reasons:</p><p>High Availability: A three-node DAX cluster provides better fault tolerance than a two-node cluster. With three nodes distributed across different Availability Zones, the cluster can continue to operate even if one node fails, ensuring the high availability requirement is met for the trading platform.</p><p>Read Latency: DAX significantly reduces read latency for DynamoDB by caching frequently accessed data. For a trading platform that likely performs many read operations to retrieve market data, prices, and order information, this caching layer can reduce read latency from milliseconds to microseconds.</p><p>Write Consistency: By writing directly to the DynamoDB table rather than through DAX, the solution ensures that all write operations are immediately consistent. This is crucial for a trading platform where data accuracy is critical (e.g., order placements, trade executions).</p><p>Overall System Latency: While writing directly to DynamoDB might seem counterintuitive for reducing latency, it actually provides the best balance of low latency and data consistency. DAX is primarily designed as a read-through cache, and its write-through capabilities can introduce additional complexity and potential consistency issues.</p><p>Option A (two-node DAX cluster with both read and write operations through DAX) has two significant drawbacks:</p><p>Lower availability: A two-node cluster doesn't provide the same level of fault tolerance as a three-node cluster. If one node fails, you're left with a single node, creating a single point of failure.</p><p>Potential consistency issues: Writing through DAX can introduce eventual consistency challenges that could be problematic for a latency-sensitive trading platform where accurate, up-to-date data is critical.</p><p>For a trading platform where both low latency and high availability are essential requirements, a three-node DAX cluster with reads through DAX and writes directly to DynamoDB provides the optimal solution.</p><p>Sources</p><p>Amazon DynamoDB Accelerator (DAX) （https://aws.amazon.com/cn/dynamodbaccelerator/）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "61341cb40b83470493a346e2c4c015d6",
      "questionNumber": 151,
      "type": "multiple",
      "content": "<p>Question #151</p><p>A company has migrated an application from on premises to AWS. The application frontend is a static website that runs on two Amazon EC2 instances behind an Application Load Balancer (ALB). The application backend is a Python application that runs on three EC2 instances behind another ALB. The EC2 instances are large, general purpose On-Demand Instances that were sized to meet the on-premises specifications for peak usage of the application. </p><p><br></p><p>The application averages hundreds of thousands of requests each month. However, the application is used mainly during lunchtime and receives minimal traffic during the rest of the day. </p><p><br></p><p>A solutions architect needs to optimize the infrastructure cost of the application without negatively affecting the application availability. </p><p><br></p><p>Which combination of steps will meet these requirements? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Change all the EC2 instances to compute optimized instances that have the same number of cores as the existing EC2 instances."
        },
        {
          "label": "B",
          "content": "Move the application frontend to a static website that is hosted on Amazon S3."
        },
        {
          "label": "C",
          "content": "Deploy the application frontend by using AWS Elastic Beanstalk. Use the same instance type for the nodes."
        },
        {
          "label": "D",
          "content": "Change all the backend EC2 instances to Spot Instances."
        },
        {
          "label": "E",
          "content": "Deploy the backend Python application to general purpose burstable EC2 instances that have the same number of cores as the existing EC2 instances."
        }
      ],
      "correctAnswer": "BE",
      "explanation": "<p>The correct answers are B and E. &nbsp;</p><p> Explanation: &nbsp;</p><p>The requirements are: &nbsp;</p><p>1. Optimize costs (current setup is over-provisioned for peak usage). &nbsp;</p><p>2. Maintain availability (no negative impact during lunchtime traffic spikes). &nbsp;</p><p>3. Application has bursty traffic (high usage at lunchtime, minimal otherwise). &nbsp;</p><p> Analysis of Options: &nbsp;</p><p> Option A: Change to Compute Optimized Instances &nbsp;</p><p>- ❌ Compute optimized instances are more expensive and unnecessary for a Python backend (not CPU-bound). &nbsp;</p><p>- ❌ Does not address cost optimization—just changes instance type without reducing capacity. &nbsp;</p><p> Option B: Move Frontend to Amazon S3 (Static Website Hosting) &nbsp;</p><p>- ✅ Eliminates EC2 costs for the frontend (S3 is cheaper and scales infinitely). &nbsp;</p><p>- ✅ Static websites are highly available (no servers to manage). &nbsp;</p><p>- ✅ Reduces cost without affecting performance (frontend is static content). &nbsp;</p><p> Option C: Deploy Frontend with Elastic Beanstalk &nbsp;</p><p>- ❌ Still uses EC2 instances, so no cost savings over current setup. &nbsp;</p><p>- ❌ Overkill for a static website (S3 is simpler and cheaper). &nbsp;</p><p> Option D: Use Spot Instances for Backend &nbsp;</p><p>- ❌ Spot Instances can be interrupted, risking availability during lunchtime peaks. &nbsp;</p><p>- ❌ Not ideal for bursty workloads where consistent uptime is critical. &nbsp;</p><p> Option E: Use Burstable Instances (e.g., T3/T4g) for Backend &nbsp;</p><p>- ✅ Burstable instances (T-series) are cost-effective for variable workloads. &nbsp;</p><p>- ✅ CPU credits handle lunchtime spikes, then scale down during low traffic. &nbsp;</p><p>- ✅ Same vCPU count ensures no performance degradation. &nbsp;</p><p> Why B + E? &nbsp;</p><p>- Frontend (Option B): Moving to S3 removes unnecessary EC2 costs. &nbsp;</p><p>- Backend (Option E): Burstable instances optimize costs while handling traffic spikes. &nbsp;</p><p>Avoid: &nbsp;</p><p>- Compute optimized (A) → Overkill for Python backend. &nbsp;</p><p>- Spot Instances (D) → Risk interruptions during peak usage. &nbsp;</p><p>- Elastic Beanstalk (C) → No cost benefit for static content. &nbsp;</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "282d5e4c481347fcb52b0e04206ee397",
      "questionNumber": 152,
      "type": "single",
      "content": "Question #152<p>A company is running an event ticketing platform on AWS and wants to optimize the platform's cost-effectiveness. The platform is deployed on Amazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 and is backed by an Amazon RDS for MySQL DB instance. The company is developing new application features to run on Amazon EKS with AWS Fargate. <br><br>The platform experiences infrequent high peaks in demand. The surges in demand depend on event dates.</p><p><br></p><p>Which solution will provide the MOST cost-effective setup for the platform?</p>",
      "options": [
        {
          "label": "A",
          "content": "Purchase Standard Reserved Instances for the EC2 instances that the EKS cluster uses in its baseline load. Scale the cluster with Spot Instances to handle peaks. Purchase 1-year All Upfront Reserved Instances for the database to meet predicted peak load for the year."
        },
        {
          "label": "B",
          "content": "Purchase Compute Savings Plans for the predicted medium load of the EKS cluster. Scale the cluster with On-Demand Capacity Reservations based on event dates for peaks. Purchase 1-year No Upfront Reserved Instances for the database to meet the predicted base load. Temporarily scale out database read replicas during peaks."
        },
        {
          "label": "C",
          "content": "Purchase EC2 Instance Savings Plans for the predicted base load of the EKS cluster. Scale the cluster with Spot Instances to handle peaks."
        },
        {
          "label": "D",
          "content": "Purchase Compute Savings Plans for the predicted base load of the EKS cluster. Scale the cluster with Spot Instances to handle peaks."
        }
      ],
      "correctAnswer": "B",
      "explanation": "Option B is correct. Compute saving plan will also cover Fargate. A spot instance is not reliable for a production server, and the company is developing new application features to run on AWS Fargate, which means we must plan the future cost improvement including AWS Fargate.",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "3ba0b2823f4b4b10a20281dd04159fbc",
      "questionNumber": 153,
      "type": "multiple",
      "content": "<p>Question #153</p><p>A company has deployed an application on AWS Elastic Beanstalk. The application uses Amazon Aurora for the database layer. An Amazon CloudFront distribution serves web requests and includes the Elastic Beanstalk domain name as the origin server. The distribution is configured with an alternate domain name that visitors use when they access the application. </p><p><br></p><p>Each week, the company takes the application out of service for routine maintenance. During the time that the application is unavailable, the company wants visitors to receive an informational message instead of a CloudFront error message. </p><p><br></p><p>A solutions architect creates an Amazon S3 bucket as the first step in the process. </p><p><br></p><p>Which combination of steps should the solutions architect take next to meet the requirements? (Choose three.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Upload static informational content to the S3 bucket."
        },
        {
          "label": "B",
          "content": "Create a new CloudFront distribution. Set the S3 bucket as the origin."
        },
        {
          "label": "C",
          "content": "Set the S3 bucket as a second origin in the original CloudFront distribution. Confi gure the distribution and the S3 bucket to use an origin"
        },
        {
          "label": "D",
          "content": "During the weekly maintenance, edit the default cache behavior to use the S3 origin. Revert the change when the maintenance is complete."
        },
        {
          "label": "E",
          "content": "During the weekly maintenance, create a cache behavior for the S3 origin on the new distribution. Set the path pattern to \\ Set the precedence to 0. Delete the cache behavior when the maintenance is complete."
        },
        {
          "label": "F",
          "content": "During the weekly maintenance, confi gure Elastic Beanstalk to serve traffi c from the S3 bucket."
        }
      ],
      "correctAnswer": "ACD",
      "explanation": "<p>The correct answers are A, C, and D. &nbsp;</p><p> Explanation: &nbsp;</p><p>The requirements are: &nbsp;</p><p>1. Show a maintenance page during weekly downtimes (instead of CloudFront errors). &nbsp;</p><p>2. Use an existing S3 bucket (already created) to host the static maintenance page. &nbsp;</p><p>3. Avoid creating a new CloudFront distribution (use the existing one). &nbsp;</p><p> Step-by-Step Solution: &nbsp;</p><p>1. Upload static maintenance content to S3 (Option A). &nbsp;</p><p> &nbsp; - The maintenance page (HTML) must be stored in S3. &nbsp;</p><p>2. Add the S3 bucket as a secondary origin in the existing CloudFront distribution (Option C). &nbsp;</p><p> &nbsp; - Configure an Origin Access Identity (OAI) to securely serve S3 content. &nbsp;</p><p> &nbsp; - Avoids creating a new distribution (Option B is unnecessary). &nbsp;</p><p>3. During maintenance, edit the default cache behavior to point to the S3 origin (Option D). &nbsp;</p><p> &nbsp; - Temporarily switch traffic from Elastic Beanstalk to S3. &nbsp;</p><p> &nbsp; - Revert after maintenance completes. &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- Option B (New CloudFront distribution) → Unnecessary complexity. &nbsp;</p><p>- Option E (Cache behavior on a new distribution) → Overcomplicates the solution. &nbsp;</p><p>- Option F (Elastic Beanstalk serving from S3) → Not how Beanstalk works; CloudFront must handle the switch. &nbsp;</p><p> Summary of Correct Steps: &nbsp;</p><p>1. A: Upload maintenance page to S3. &nbsp;</p><p>2. C: Add S3 as a secondary origin in the existing CloudFront distribution (with OAI). &nbsp;</p><p>3. D: Temporarily switch CloudFront’s default behavior to S3 during maintenance. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "df6fee64b8e74ae2affaffc879e5117a",
      "questionNumber": 154,
      "type": "single",
      "content": "<p>Question #154</p><p>A company gives users the ability to upload images from a custom application. The upload process invokes an AWS Lambda function that processes and stores the image in an Amazon S3 bucket. The application invokes the Lambda function by using a specific function version ARN. </p><p><br></p><p>The Lambda function accepts image processing parameters by using environment variables. The company often adjusts the environment variables of the Lambda function to achieve optimal image processing output. The company tests different parameters and publishes a new function version with the updated environment variables after validating results. This update process also requires frequent changes to the custom application to invoke the new function version ARN. These changes cause interruptions for users. </p><p><br></p><p>A solutions architect needs to simplify this process to minimize disruption to users. </p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Directly modify the environment variables of the published Lambda function version. Use the $LATEST version to test image processing parameters."
        },
        {
          "label": "B",
          "content": "Create an Amazon DynamoDB table to store the image processing parameters. Modify the Lambda function to retrieve the image processing parameters from the DynamoDB table."
        },
        {
          "label": "C",
          "content": "Directly code the image processing parameters within the Lambda function and remove the environment variables. Publish a new function version when the company updates the parameters."
        },
        {
          "label": "D",
          "content": "Create a Lambda function alias. Modify the client application to use the function alias ARN. Reconfigure the Lambda alias to point to new versions of the function when the company finishes testing."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>The correct answer is D. &nbsp;</p><p> Explanation: &nbsp;</p><p>The problem is: &nbsp;</p><p>- The company frequently updates Lambda environment variables for image processing. &nbsp;</p><p>- Each update requires publishing a new version and updating the application’s ARN reference, causing disruptions. &nbsp;</p><p>- Goal: Minimize user interruptions while keeping the ability to test and deploy new parameters. &nbsp;</p><p> Analysis of Options: &nbsp;</p><p> Option A: Modify Published Version + Use `$LATEST` &nbsp;</p><p>- ❌ Modifying a published version is not allowed (immutable). &nbsp;</p><p>- ❌ `$LATEST` is unstable—changes directly impact users without testing. &nbsp;</p><p> Option B: DynamoDB for Parameters &nbsp;</p><p>- ✅ Avoids Lambda version updates, but: &nbsp;</p><p>- ❌ Adds complexity (DynamoDB table + IAM permissions). &nbsp;</p><p>- ❌ Not the simplest solution (still requires Lambda code changes). &nbsp;</p><p> Option C: Hardcode Parameters in Lambda &nbsp;</p><p>- ❌ Worse than current setup—requires redeploying code for every parameter change. &nbsp;</p><p>- ❌ No separation of config from code (bad practice). &nbsp;</p><p> Option D: Lambda Alias &nbsp;</p><p>- ✅ Alias acts as a stable pointer (e.g., `PROD` or `STAGING`). &nbsp;</p><p>- ✅ Application invokes the alias ARN (never needs updates). &nbsp;</p><p>- ✅ Alias can be updated to point to new versions after testing. &nbsp;</p><p>- ✅ Zero downtime—users always hit the same ARN, while backend switches versions seamlessly. &nbsp;</p><p> Why Option D is Best? &nbsp;</p><p>- Eliminates application ARN changes (alias stays constant). &nbsp;</p><p>- Allows safe testing (test in `$LATEST` or new versions, then update alias). &nbsp;</p><p>- Least operational overhead—no DynamoDB, no hardcoding, no immutable version issues. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "0b7cd02f5be644828466cf5df5b503e0",
      "questionNumber": 155,
      "type": "single",
      "content": "<p>Question #155</p><p>A global media company is planning a multi-Region deployment of an application. Amazon DynamoDB global tables will back the deployment to keep the user experience consistent across the two continents where users are concentrated. Each deployment will have a public Application Load Balancer (ALB). The company manages public DNS internally. The company wants to make the application available through an apex domain. </p><p><br></p><p>Which solution will meet these requirements with the LEAST effort?</p>",
      "options": [
        {
          "label": "A",
          "content": "Migrate public DNS to Amazon Route 53. Create CNAME records for the apex domain to point to the ALB. Use a geolocation routing policy to route traffic based on user location."
        },
        {
          "label": "B",
          "content": "Place a Network Load Balancer (NLB) in front of the ALB. Migrate public DNS to Amazon Route 53. Create a CNAME record for the apex domain to point to the NLB&rsquo;s static IP address. Use a geolocation routing policy to route traffic based on user location."
        },
        {
          "label": "C",
          "content": "Create an AWS Global Accelerator accelerator with multiple endpoint groups that target endpoints in appropriate AWS Regions. Use the accelerator&rsquo;s static IP address to create a record in public DNS for the apex domain."
        },
        {
          "label": "D",
          "content": "Create an Amazon API Gateway API that is backed by AWS Lambda in one of the AWS Regions. Configure a Lambda function to route traffic to application deployments by using the round robin method. Create CNAME records for the apex domain to point to the API&#39;s URL."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Create an AWS Global Accelerator accelerator with multiple endpoint groups that target endpoints in appropriate AWS Regions. Use the accelerator's static IP address to create a record in public DNS for the apex domain.</p><p>This solution meets all the requirements with the least effort for the following reasons:</p><p>Multi-Region deployment support: AWS Global Accelerator is specifically designed to route traffic to multiple AWS Regions, making it ideal for the company's global deployment across two continents.</p><p>Apex domain compatibility: Global Accelerator provides static IP addresses that can be used with an apex domain (also known as a zone apex or naked domain). This is important because apex domains cannot use CNAME records, which would typically be needed for services like Application Load Balancers.</p><p>Integration with Application Load Balancers: Global Accelerator can use the company's public ALBs as endpoints in different regions, directing users to the closest available endpoint.</p><p>Consistent user experience: Global Accelerator routes traffic over the AWS global network rather than the public internet, reducing latency and providing a more consistent experience for users across different continents, which complements the DynamoDB global tables approach.</p><p>Minimal effort: This solution requires minimal configuration - simply creating a Global Accelerator, adding the ALBs as endpoints in appropriate endpoint groups, and then creating a simple A record in the company's existing DNS system pointing to the static IP addresses.</p><p>Global Accelerator also provides additional benefits that align with the company's needs:</p><p>Automatic health checking and failover between regions</p><p>Traffic dials to control the percentage of traffic going to each region</p><p>Improved availability through anycast static IP addresses</p><p>Since the company already manages their public DNS internally, they can easily create the necessary A records for the apex domain pointing to Global Accelerator's static IP addresses, completing the solution with minimal effort.</p><p>Sources</p><p>Routing strategies for global tables - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/dynamodb-global-tables/routing-strategies.html）</p><p>REL10-BP02 Select the appropriate locations for your multi-location deployment - AWS Well-Architected Framework （https://docs.aws.amazon.com/wellarchitected/2023-10-03/framework/rel_fault_isolation_select_location.html）</p><p>REL10-BP02 Select the appropriate locations for your multi-location deployment - AWS Well-Architected Framework （https://docs.aws.amazon.com/wellarchitected/2024-06-27/framework/rel_fault_isolation_select_location.html）</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": true,
      "noteContent": "Apex domain 不能使用 CNAME 记录（DNS 标准限制）"
    },
    {
      "id": "7ed82bccf99041bb918f6ad431c87f9b",
      "questionNumber": 156,
      "type": "single",
      "content": "<p>Question #156</p><p>A company is developing a new serverless API by using Amazon API Gateway and AWS Lambda. The company integrated the Lambda functions with API Gateway to use several shared libraries and custom classes. </p><p><br></p><p>A solutions architect needs to simplify the deployment of the solution and optimize for code reuse. </p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Deploy the shared libraries and custom classes into a Docker image. Store the image in an S3 bucket. Create a Lambda layer that uses the Docker image as the source. Deploy the API&#39;s Lambda functions as Zip packages. Configure the packages to use the Lambda layer."
        },
        {
          "label": "B",
          "content": "Deploy the shared libraries and custom classes to a Docker image. Upload the image to Amazon Elastic Container Registry (Amazon ECR). Create a Lambda layer that uses the Docker image as the source. Deploy the API&#39;s Lambda functions as Zip packages. Configure the packages to use the Lambda layer."
        },
        {
          "label": "C",
          "content": "Deploy the shared libraries and custom classes to a Docker container in Amazon Elastic Container Service (Amazon ECS) by using the AWS Fargate launch type. Deploy the API&#39;s Lambda functions as Zip packages. Configure the packages to use the deployed container as a Lambda layer."
        },
        {
          "label": "D",
          "content": "Deploy the shared libraries, custom classes, and code for the API&#39;s Lambda functions to a Docker image. Upload the image to Amazon Elastic Container Registry (Amazon ECR). Configure the API&#39;s Lambda functions to use the Docker image as the deployment package."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. Deploy the shared libraries, custom classes, and code for the API's Lambda functions to a Docker image. Upload the image to Amazon Elastic Container Registry (Amazon ECR). Configure the API's Lambda functions to use the Docker image as the deployment package.</p><p>This solution meets the requirements for simplifying deployment and optimizing for code reuse for the following reasons:</p><p>Code reuse optimization: By packaging all shared libraries, custom classes, and function code into a single Docker image, the company ensures that all components are consistently available to all Lambda functions. This eliminates the need to manage multiple packages or layers separately.</p><p>Simplified deployment: Using a Docker image stored in Amazon ECR creates a single deployment artifact that contains everything needed for the API's Lambda functions. This simplifies the deployment process as there's only one artifact to build, version, and deploy.</p><p>Consistency: The Docker image approach ensures that all Lambda functions are using exactly the same versions of shared libraries and custom classes, reducing the risk of compatibility issues.</p><p>Scalability: As the application grows and more Lambda functions are added, the Docker image approach continues to be efficient since new functions can simply reference the same image without additional configuration.</p><p>Option A (using Lambda layers with Docker images stored in S3) is incorrect because:</p><p>Lambda layers don't use Docker images as their source</p><p>S3 is not the appropriate storage location for Docker images (ECR is the AWS service designed for this purpose)</p><p>This approach would unnecessarily complicate the deployment process by mixing container and non-container technologies</p><p>The Docker image approach with ECR provides a cleaner, more consistent solution that better meets the requirements for simplified deployment and code reuse.</p><p>Sources</p><p>Deploy .NET Lambda functions with container images - AWS Lambda （https://docs.aws.amazon.com/lambda/latest/dg/csharp-image.html）</p><p>Lambda Package Exceeds 60MB: Solutions for Large Dependencies? | AWS re:Post （https://repost.aws/questions/QUoXSNWrkdSf61jVqG7IwBxg/lambda-package-exceeds-60mb-solutions-for-large-dependencies）</p><p>Simplifying operations - Implementing Microservices on AWS（https://docs.aws.amazon.com/whitepapers/latest/microservices-on-aws/simplyfing-operations.html） </p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "2f1525b1fbd74e53bdc49af6665b9fd9",
      "questionNumber": 157,
      "type": "single",
      "content": "<p>Question #157</p><p>A manufacturing company is building an inspection solution for its factory. The company has IP cameras at the end of each assembly line. The company has used Amazon SageMaker to train a machine learning (ML) model to identify common defects from still images. </p><p><br></p><p>The company wants to provide local feedback to factory workers when a defect is detected. The company must be able to provide this feedback even if the factory’s internet connectivity is down. The company has a local Linux server that hosts an API that provides local feedback to the workers. </p><p><br></p><p>How should the company deploy the ML model to meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Set up an Amazon Kinesis video stream from each IP camera to AWS. Use Amazon EC2 instances to take still images of the streams. Upload the images to an Amazon S3 bucket. Deploy a SageMaker endpoint with the ML model. Invoke an AWS Lambda function to call the inference endpoint when new images are uploaded. Configure the Lambda function to call the local API when a defect is detected."
        },
        {
          "label": "B",
          "content": "Deploy AWS IoT Greengrass on the local server. Deploy the ML model to the Greengrass server. Create a Greengrass component to take still images from the cameras and run inference. Configure the component to call the local API when a defect is detected."
        },
        {
          "label": "C",
          "content": "Order an AWS Snowball device. Deploy a SageMaker endpoint the ML model and an Amazon EC2 instance on the Snowball device. Take still images from the cameras. Run inference from the EC2 instance. Configure the instance to call the local API when a defect is detected."
        },
        {
          "label": "D",
          "content": "Deploy Amazon Monitron devices on each IP camera. Deploy an Amazon Monitron Gateway on premises. Deploy the ML model to the Amazon Monitron devices. Use Amazon Monitron health state alarms to call the local API from an AWS Lambda function when a defect is detected."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>The correct answer is B. &nbsp;</p><p> Explanation: &nbsp;</p><p>The key requirements are: &nbsp;</p><p>1. Local defect detection (must work without internet). &nbsp;</p><p>2. Real-time feedback to factory workers via a local API. &nbsp;</p><p>3. Deploy ML model on-premises (since cloud connectivity is unreliable). &nbsp;</p><p> Analysis of Options: &nbsp;</p><p> Option A: Kinesis + S3 + SageMaker Endpoint + Lambda &nbsp;</p><p>- ❌ Depends on internet connectivity (SageMaker endpoint is cloud-based). &nbsp;</p><p>- ❌ No offline capability—fails if the factory loses internet. &nbsp;</p><p>- ❌ Latency issues (uploading images to S3 and waiting for Lambda). &nbsp;</p><p> Option B: AWS IoT Greengrass on Local Server &nbsp;</p><p>- ✅ Greengrass runs ML models locally (no internet needed). &nbsp;</p><p>- ✅ Takes still images from cameras and runs inference on-premises. &nbsp;</p><p>- ✅ Calls local API directly for real-time feedback. &nbsp;</p><p>- ✅ Works offline (syncs with AWS when connectivity is restored). &nbsp;</p><p> Option C: AWS Snowball &nbsp;</p><p>- ❌ Snowball is for data transfer, not persistent edge computing. &nbsp;</p><p>- ❌ Overkill—Greengrass is simpler and designed for this use case. &nbsp;</p><p>- ❌ No real-time processing (Snowball is not a runtime environment). &nbsp;</p><p> Option D: Amazon Monitron &nbsp;</p><p>- ❌ Monitron is for vibration/sensor-based monitoring, not image processing. &nbsp;</p><p>- ❌ Cannot deploy custom ML models (only supports Monitron’s predefined ML). &nbsp;</p><p> Why Option B is Best? &nbsp;</p><p>- Greengrass is purpose-built for edge ML deployments. &nbsp;</p><p>- Works offline (critical for factory environments). &nbsp;</p><p>- Lightweight and scalable (runs on existing Linux server). &nbsp;</p><p>- Direct integration with local APIs (no cloud dependency). &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "8704969ab1e544ea90474c6f1ce0650b",
      "questionNumber": 158,
      "type": "single",
      "content": "<p>Question #158</p><p>A solutions architect must create a business case for migration of a company's on-premises data center to the AWS Cloud. The solutions architect will use a configuration management database (CMDB) export of all the company's servers to create the case. </p><p><br></p><p>Which solution will meet these requirements MOST cost-effectively?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use AWS Well-Architected Tool to import the CMDB data to perform an analysis and generate recommendations."
        },
        {
          "label": "B",
          "content": "Use Migration Evaluator to perform an analysis. Use the data import template to upload the data from the CMDB export."
        },
        {
          "label": "C",
          "content": "Implement resource matching rules. Use the CMDB export and the AWS Price List Bulk API to query CMDB data against AWS services in bulk."
        },
        {
          "label": "D",
          "content": "Use AWS Application Discovery Service to import the CMDB data to perform an analysis."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Use Migration Evaluator to perform an analysis. Use the data import template to upload the data from the CMDB export.</p><p>This solution will meet the requirements most cost-effectively for the following reasons:</p><p>AWS Migration Evaluator (formerly known as TSO Logic) is specifically designed for creating business cases for cloud migration. It's purpose-built to analyze on-premises infrastructure data and provide detailed cost comparisons and recommendations for AWS migration.</p><p>Migration Evaluator has a built-in data import template that's designed to work with CMDB exports, making it straightforward to upload and analyze the company's server inventory data without additional processing or development work.</p><p>The service is offered at no additional cost as part of AWS's migration assessment services, making it the most cost-effective option for this specific use case.</p><p>Migration Evaluator provides comprehensive TCO (Total Cost of Ownership) analysis, right-sizing recommendations, and detailed migration business cases that can be presented to stakeholders.</p><p>The other options have significant limitations:</p><p>A. AWS Well-Architected Tool is designed to review architectural best practices against the AWS Well-Architected Framework. It's not designed to import CMDB data or create migration business cases.</p><p>C. Using the AWS Price List Bulk API with custom resource matching rules would require significant development effort to build a custom solution. This would be more time-consuming and costly than using a purpose-built tool like Migration Evaluator.</p><p>D. AWS Application Discovery Service is primarily designed for discovering applications and their dependencies for migration planning. While it can import some CMDB data, it's more focused on dependency mapping than creating business cases and cost analyses.</p><p>Migration Evaluator is the most appropriate tool for this specific requirement as it's designed exactly for the purpose of creating migration business cases using existing infrastructure data like CMDB exports.</p><p>Sources</p><p>Tools for migrating to the AWS Cloud - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-tools/introduction.html）</p><p>Create a data-driven Migration Business Case using AWS Cloud Value Framework | AWS Cloud Operations & Migrations Blog （https://aws.amazon.com/cn/blogs/mt/create-a-data-driven-migration-business-case-using-aws-cloud-value-framework/）</p><p>Choosing AWS migration services and tools - Choosing AWS migration services and tools （https://docs.aws.amazon.com/decision-guides/latest/migration-on-aws-how-to-choose/migration-on-aws-how-to-choose.html）</p><p>Seeking Advice on Utilizing Migration Evaluator for AWS Migration Business Cases | AWS re:Post （https://repost.aws/questions/QU11RsGVWsQpOYHehhrbHpPA/seeking-advice-on-utilizing-migration-evaluator-for-aws-migration-business-cases）</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "ac71b133e4ed4b2785094eb56d49a93d",
      "questionNumber": 159,
      "type": "single",
      "content": "<p>Question #159</p><p>A company has a website that runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances are in an Auto Scaling group. The ALB is associated with an AWS WAF web ACL. </p><p><br></p><p>The website often encounters attacks in the application layer. The attacks produce sudden and significant increases in traffic on the application server. The access logs show that each attack originates from different IP addresses. A solutions architect needs to implement a solution to mitigate these attacks. </p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an Amazon CloudWatch alarm that monitors server access. Set a threshold based on access by IP address. Configure an alarm action that adds the IP address to the web ACL&rsquo;s deny list."
        },
        {
          "label": "B",
          "content": "Deploy AWS Shield Advanced in addition to AWS WAF. Add the ALB as a protected resource."
        },
        {
          "label": "C",
          "content": "Create an Amazon CloudWatch alarm that monitors user IP addresses. Set a threshold based on access by IP address. Configure the alarm to invoke an AWS Lambda function to add a deny rule in the application server&rsquo;s subnet route table for any IP addresses that activate the alarm."
        },
        {
          "label": "D",
          "content": "Inspect access logs to find a pattern of IP addresses that launched the attacks. Use an Amazon Route 53 geolocation routing policy to deny traffic from the countries that host those IP addresses."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>The correct answer is B. &nbsp;</p><p> Explanation: &nbsp;</p><p>The problem is: &nbsp;</p><p>- Application-layer attacks (e.g., HTTP floods, SQL injection, etc.) causing sudden traffic spikes. &nbsp;</p><p>- Attacks come from different IPs, making IP-based blocking ineffective. &nbsp;</p><p>- Goal: Mitigate attacks with minimal operational overhead. &nbsp;</p><p> Analysis of Options: &nbsp;</p><p> Option A: CloudWatch Alarm + WAF IP Deny List &nbsp;</p><p>- ❌ Reactive, not proactive—attacks must already be happening before blocking. &nbsp;</p><p>- ❌ High operational overhead—manual tuning of thresholds and rules. &nbsp;</p><p>- ❌ IP-based blocking fails (attackers use many IPs). &nbsp;</p><p> Option B: AWS Shield Advanced + AWS WAF &nbsp;</p><p>- ✅ Shield Advanced provides DDoS protection (automatically detects & mitigates attacks). &nbsp;</p><p>- ✅ Works with WAF to block application-layer attacks (e.g., HTTP floods). &nbsp;</p><p>- ✅ Minimal operational overhead—AWS manages detection and mitigation. &nbsp;</p><p> Option C: CloudWatch Alarm + Lambda + Route Table Deny Rules &nbsp;</p><p>- ❌ Subnet route table blocks are too broad (affects all traffic, not just HTTP). &nbsp;</p><p>- ❌ Complex and slow (Lambda + route table updates introduce latency). &nbsp;</p><p>- ❌ Does not scale for large attacks (IP-based blocking is ineffective). &nbsp;</p><p> Option D: Route 53 Geolocation Blocking &nbsp;</p><p>- ❌ Geolocation blocking is imprecise (attackers can use proxies/VPNs). &nbsp;</p><p>- ❌ Manual effort required (log analysis + policy updates). &nbsp;</p><p>- ❌ False positives (legitimate users from blocked regions are denied). &nbsp;</p><p> Why Option B is Best? &nbsp;</p><p>- Shield Advanced automatically mitigates DDoS attacks (network & application layer). &nbsp;</p><p>- WAF + Shield Advanced together block sophisticated HTTP attacks. &nbsp;</p><p>- No manual intervention needed—AWS handles detection and mitigation. &nbsp;</p><p>- Scalable—works even if attackers rotate IPs. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "56125928142649bba428a9db2de157f2",
      "questionNumber": 160,
      "type": "multiple",
      "content": "<p>Question #160</p><p>A company has a critical application in which the data tier is deployed in a single AWS Region. The data tier uses an Amazon DynamoDB table and an Amazon Aurora MySQL DB cluster. The current Aurora MySQL engine version supports a global database. The application tier is already deployed in two Regions. </p><p><br></p><p>Company policy states that critical applications must have application tier components and data tier components deployed across two Regions. The RTO and RPO must be no more than a few minutes each. A solutions architect must recommend a solution to make the data tier compliant with company policy.<br><br>Which combination of steps will meet these requirements? (Choose two.) </p>",
      "options": [
        {
          "label": "A",
          "content": "Add another Region to the Aurora MySQL DB cluster"
        },
        {
          "label": "B",
          "content": "Add another Region to each table in the Aurora MySQL DB cluster"
        },
        {
          "label": "C",
          "content": "Set up scheduled cross-Region backups for the DynamoDB table and the Aurora MySQL DB cluster"
        },
        {
          "label": "D",
          "content": "Convert the existing DynamoDB table to a global table by adding another Region to its configuration"
        },
        {
          "label": "E",
          "content": "Use Amazon Route 53 Application Recovery Controller to automate database backup and recovery to the secondary Region"
        }
      ],
      "correctAnswer": "AD",
      "explanation": "<p>Based on the information provided, the correct answers are:</p><p>A. Add another Region to the Aurora MySQL DB cluster</p><p>D. Convert the existing DynamoDB table to a global table by adding another Region to its configuration</p><p>These two steps together will make the data tier compliant with the company policy for the following reasons:</p><p>The company policy requires critical applications to have data tier components deployed across two regions, with RTO and RPO of no more than a few minutes each.</p><p>Adding another Region to the Aurora MySQL DB cluster (option A) creates an Aurora Global Database, which:</p><p>Replicates data from the primary region to the secondary region with typical lag of less than 1 second</p><p>Provides an RPO of approximately 1 second</p><p>Allows for failover to the secondary region with an RTO of less than 1 minute</p><p>Is supported by the current Aurora MySQL engine version (as stated in the question)</p><p>Converting the existing DynamoDB table to a global table by adding another Region (option D):</p><p>Enables multi-region replication for DynamoDB with typically sub-second latency</p><p>Provides automatic multi-master replication</p><p>Allows for reads and writes in both regions</p><p>Achieves RPO and RTO of seconds, well within the \"few minutes\" requirement</p><p>Together, these solutions address both database technologies in the data tier (Aurora MySQL and DynamoDB) and ensure they are deployed across two regions with the required RTO and RPO metrics.</p><p>Option E (using Amazon Route 53 Application Recovery Controller) is not correct because:</p><p>It doesn't handle data replication itself</p><p>Backup and recovery processes would not meet the \"few minutes\" RTO requirement</p><p>It's more suited for managing traffic routing and failover rather than being the primary mechanism for data replication</p><p>The combination of Aurora Global Database and DynamoDB Global Tables provides the most direct and effective solution to meet the company's policy requirements with minimal operational complexity.</p><p>Sources</p><p>AWS Backup vs global table/database for cost-effective RTO,RPO requirements | AWS re:Post （https://repost.aws/questions/QUWf0GuonKSvW8U2N4o8huHw/aws-backup-vs-global-table-database-for-cost-effective-rto-rpo-requirements）</p><p>MySQL PostgreSQL Relational Database – Amazon Aurora Global Database – AWS （https://aws.amazon.com/cn/rds/aurora/global-database/）</p><p>Use Amazon Aurora Global Database to build resilient multi-Region applications | AWS Database Blog （https://aws.amazon.com/cn/blogs/database/use-amazon-aurora-global-database-to-build-resilient-multi-region-applications/）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "dda9e97fbf4748e7abd90ad8f7145f80",
      "questionNumber": 161,
      "type": "single",
      "content": "<p>Question #161</p><p>A telecommunications company is running an application on AWS. The company has set up an AWS Direct Connect connection between the company's on-premises data center and AWS. The company deployed the application on Amazon EC2 instances in multiple Availability Zones behind an internal Application Load Balancer (ALB). The company's clients connect from the on-premises network by using HTTPS. The TLS terminates in the ALB. The company has multiple target groups and uses path-based routing to forward requests based on the URL path. </p><p><br></p><p>The company is planning to deploy an on-premises firewall appliance with an allow list that is based on IP address. A solutions architect must develop a solution to allow traffic flow to AWS from the on-premises network so that the clients can continue to access the application. </p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Configure the existing ALB to use static IP addresses. Assign IP addresses in multiple Availability Zones to the ALB. Add the ALB IP addresses to the firewall appliance."
        },
        {
          "label": "B",
          "content": "Create a Network Load Balancer (NLB). Associate the NLB with one static IP addresses in multiple Availability Zones. Create an ALB-type target group for the NLB and add the existing ALAdd the NLB IP addresses to the firewall appliance. Update the clients to connect to the NLB.&nbsp;"
        },
        {
          "label": "C",
          "content": "Create a Network Load Balancer (NLB). Associate the LNB with one static IP addresses in multiple Availability Zones. Add the existing target groups to the NLB. Update the clients to connect to the NLB. Delete the ALB Add the NLB IP addresses to the firewall appliance.&nbsp;"
        },
        {
          "label": "D",
          "content": "Create a Gateway Load Balancer (GWLB). Assign static IP addresses to the GWLB in multiple Availability Zones. Create an ALB-type target group for the GWLB and add the existing ALB. Add the GWLB IP addresses to the firewall appliance. Update the clients to connect to the GWLB"
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>The correct answer is B. &nbsp;</p><p> Explanation: &nbsp;</p><p> Requirements: &nbsp;</p><p>1. On-premises clients must connect to an internal ALB over Direct Connect. &nbsp;</p><p>2. A firewall appliance will enforce an IP allow list, so the ALB must have static IPs. &nbsp;</p><p>3. Path-based routing must continue to work (ALB functionality). &nbsp;</p><p> Problem: &nbsp;</p><p>- ALBs do not have static IPs (they use dynamic DNS). &nbsp;</p><p>- NLBs provide static IPs, but NLBs cannot do path-based routing (ALB feature). &nbsp;</p><p> Solution: &nbsp;</p><p>- Place an NLB in front of the ALB to provide static IPs for the firewall allow list. &nbsp;</p><p>- Clients connect to the NLB, which forwards traffic to the ALB (retaining path-based routing). &nbsp;</p><p> Why Option B is Correct? &nbsp;</p><p>✅ NLB provides static IPs (required for firewall allow list). &nbsp;</p><p>✅ ALB remains behind NLB, preserving path-based routing. &nbsp;</p><p>✅ No loss of functionality—ALB still handles TLS termination and routing. &nbsp;</p><p> Why Other Options Fail? &nbsp;</p><p>❌ A: ALBs cannot have static IPs (only NLBs/GWLBs can). &nbsp;</p><p>❌ C: NLB cannot replace ALB (loses path-based routing). &nbsp;</p><p>❌ D: GWLB is for third-party appliances, not for this use case. &nbsp;</p><p> Implementation Steps (Option B): &nbsp;</p><p>1. Create an NLB with static IPs in each AZ. &nbsp;</p><p>2. Point NLB to ALB (ALB becomes a target). &nbsp;</p><p>3. Add NLB IPs to firewall allow list. &nbsp;</p><p>4. Update clients to connect to NLB (instead of ALB). &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "a6a91069e3024f6aba258cdb6ff9b9cd",
      "questionNumber": 162,
      "type": "single",
      "content": "<p>Question #162</p><p>A company runs an application on a fleet of Amazon EC2 instances that are in private subnets behind an internet-facing Application Load Balancer (ALB). The ALB is the origin for an Amazon CloudFront distribution. An AWS WAF web ACL that contains various AWS managed rules is associated with the CloudFront distribution.</p><p><br></p><p>The company needs a solution that will prevent internet traffic from directly accessing the ALB.</p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a new web ACL that contains the same rules that the existing web ACL contains. Associate the new web ACL with the ALB."
        },
        {
          "label": "B",
          "content": "Associate the existing web ACL with the ALB."
        },
        {
          "label": "C",
          "content": "Add a security group rule to the ALB to allow traffic from the AWS managed prefix list for CloudFront only."
        },
        {
          "label": "D",
          "content": "Add a security group rule to the ALB to allow only the various CloudFront IP address ranges."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Add a security group rule to the ALB to allow traffic from the AWS managed prefix list for CloudFront only.</p><p>This solution will meet the requirements with the least operational overhead for the following reasons:</p><p>Effectiveness: By configuring the ALB's security group to only allow traffic from CloudFront's managed prefix list, you create a network-level restriction that prevents any direct internet access to the ALB. This ensures that all traffic must flow through CloudFront first.</p><p>Minimal operational overhead:</p><p>AWS automatically maintains and updates the CloudFront managed prefix list, so you don't need to manually track or update IP ranges when they change</p><p>This is a one-time configuration that requires minimal ongoing maintenance</p><p>No additional services or costs are involved</p><p>Simplicity: This approach uses native AWS networking features (security groups) without requiring additional services or complex configurations.</p><p>The other options have significant drawbacks:</p><p>A. Creating a new web ACL with the same rules would add unnecessary complexity and cost. Web ACLs are primarily for filtering malicious traffic patterns, not for restricting access to specific sources.</p><p>B. Associating the existing web ACL with the ALB wouldn't prevent direct internet access to the ALB; it would only apply the same filtering rules at both the CloudFront and ALB levels.</p><p>D. Adding security group rules for individual CloudFront IP address ranges would work initially but would create significant operational overhead, as you would need to continuously monitor and update these rules whenever AWS adds or changes CloudFront IP ranges.</p><p>Using the AWS managed prefix list for CloudFront provides the most efficient and low-maintenance solution to ensure that only traffic coming through CloudFront can reach the ALB.</p><p>Sources</p><p>Restrict Application Load Balancer traffic through only CloudFront | AWS re:Post （https://repost.aws/knowledge-center/waf-restrict-alb-allow-cloudfront）</p><p>WAF on ALB or CloudFront ? | AWS re:Post （https://repost.aws/questions/QUTY5hPVxgS6Caa3eZHX7-nQ/waf-on-alb-or-cloudfront）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "278f8404a52849efbc9ab355d4fca371",
      "questionNumber": 163,
      "type": "single",
      "content": "<p>Question #163</p><p>A company is running an application that uses an Amazon ElastiCache for Redis cluster as a caching layer. A recent security audit revealed that the company has configured encryption at rest for ElastiCache. However, the company did not configure ElastiCache to use encryption in transit. Additionally, users can access the cache without authentication.</p><p><br></p><p>A solutions architect must make changes to require user authentication and to ensure that the company is using end-to-end encryption.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an AUTH token. Store the token in AWS System Manager Parameter Store, as an encrypted parameter. Create a new cluster with AUTH, and configure encryption in transit. Update the application to retrieve the AUTH token from Parameter Store when necessary and to use the AUTH token for authentication."
        },
        {
          "label": "B",
          "content": "Create an AUTH token. Store the token in AWS Secrets Manager. Configure the existing cluster to use the AUTH token, and configure encryption in transit. Update the application to retrieve the AUTH token from Secrets Manager when necessary and to use the AUTH token for authentication."
        },
        {
          "label": "C",
          "content": "Create an SSL certificate. Store the certificate in AWS Secrets Manager. Create a new cluster, and configure encryption in transit. Update the application to retrieve the SSL certificate from Secrets Manager when necessary and to use the certificate for authentication."
        },
        {
          "label": "D",
          "content": "Create an SSL certificate. Store the certificate in AWS Systems Manager Parameter Store, as an encrypted advanced parameter. Update the existing cluster to configure encryption in transit. Update the application to retrieve the SSL certificate from Parameter Store when necessary and to use the certificate for authentication."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>The correct answer is B. &nbsp;</p><p> Explanation: &nbsp;</p><p> Requirements: &nbsp;</p><p>1. Enable encryption in transit for ElastiCache for Redis. &nbsp;</p><p>2. Require authentication (AUTH token) for Redis access. &nbsp;</p><p>3. Secure storage of credentials (AUTH token). &nbsp;</p><p> Why Option B is Correct? &nbsp;</p><p>✅ AUTH token enables authentication (required by the audit). &nbsp;</p><p>✅ Secrets Manager securely stores the token (encrypted, managed rotation). &nbsp;</p><p>✅ Encryption in transit must be configured (cannot be enabled on an existing cluster—must create a new one or modify the existing one). &nbsp;</p><p>✅ Application retrieves token securely from Secrets Manager. &nbsp;</p><p> Why Other Options Fail? &nbsp;</p><p>❌ A: While it uses Parameter Store, encryption in transit cannot be enabled on an existing cluster without creating a new one. &nbsp;</p><p>❌ C & D: SSL certificates are not used for Redis authentication (AUTH tokens are required). &nbsp;</p><p> Steps to Implement (Option B): &nbsp;</p><p>1. Create an AUTH token (complex password). &nbsp;</p><p>2. Store it in Secrets Manager (secure, encrypted). &nbsp;</p><p>3. Enable encryption in transit (requires modifying the cluster or creating a new one). &nbsp;</p><p>4. Update the application to fetch the AUTH token from Secrets Manager. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "177e12903d2548e8b49197a82d68d15b",
      "questionNumber": 164,
      "type": "single",
      "content": "<p>Question #164</p><p>A company is running a compute workload by using Amazon EC2 Spot Instances that are in an Auto Scaling group. The launch template uses two placement groups and a single instance type.</p><p><br></p><p>Recently, a monitoring system reported Auto Scaling instance launch failures that correlated with longer wait times for system users. The company needs to improve the overall reliability of the workload.</p><p><br></p><p>Which solution will meet this requirement?</p>",
      "options": [
        {
          "label": "A",
          "content": "Replace the launch template with a launch configuration to use an Auto Scaling group that uses attribute-based instance type selection."
        },
        {
          "label": "B",
          "content": "Create a new launch template version that uses attribute-based instance type selection. Configure the Auto Scaling group to use the new launch template version."
        },
        {
          "label": "C",
          "content": "Update the launch template Auto Scaling group to increase the number of placement groups."
        },
        {
          "label": "D",
          "content": "Update the launch template to use a larger instance type."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Create a new launch template version that uses attribute-based instance type selection. Configure the Auto Scaling group to use the new launch template version.</p><p>This solution will improve the overall reliability of the workload for the following reasons:</p><p>Root cause identification: The current configuration uses a single instance type in the launch template, which is likely causing the launch failures. When specific instance types are not available in the Spot market, the Auto Scaling group cannot launch new instances, leading to the reported failures and longer wait times.</p><p>Attribute-based instance type selection: By implementing attribute-based instance type selection in the launch template, the Auto Scaling group can dynamically choose from a variety of instance types that meet the specified attributes (like vCPU, memory, etc.) rather than being restricted to a single instance type. This significantly increases the pool of available Spot Instances.</p><p>Spot Instance best practices: Using multiple instance types is a recommended best practice for Spot Instance workloads as it increases the chances of finding available capacity in the Spot market, reducing the likelihood of launch failures.</p><p>Maintaining launch templates: Creating a new version of the existing launch template is more efficient than replacing it entirely, as it preserves other configuration settings while updating only what needs to change.</p><p>The other options are less effective:</p><p>A. Replacing the launch template with a launch configuration is not recommended as launch configurations are considered legacy and offer fewer features than launch templates. Additionally, this would require recreating all configuration settings rather than just updating what needs to change.</p><p>C. Increasing the number of placement groups wouldn't address the fundamental issue of Spot Instance availability. Placement groups control how instances are physically placed relative to each other but don't affect the availability of specific instance types in the Spot market.</p><p>D. Updating to a larger instance type would still restrict the Auto Scaling group to a single instance type, which doesn't solve the underlying problem of limited Spot Instance availability. It might even worsen the situation if the larger instance type has lower availability in the Spot market.</p><p>Sources</p><p>Capacity Rebalancing in Auto Scaling to replace at-risk Spot Instances - Amazon EC2 Auto Scaling （https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-capacity-rebalancing.html）</p><p>Why is my Auto Scaling group not launching new instances when the load increases? | AWS re:Post （https://repost.aws/questions/QUQsrSasV_Tyyqw_lB8W9Lfw/why-is-my-auto-scaling-group-not-launching-new-instances-when-the-load-increases）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "03345ea92a6942698b431f24cbaf56c3",
      "questionNumber": 165,
      "type": "single",
      "content": "Question #165<p>A company is migrating a document processing workload to AWS. The company has updated many applications to natively use the Amazon S3 API to store, retrieve, and modify documents that a processing server generates at a rate of approximately 5 documents every second. After the document processing is finished, customers can download the documents directly from Amazon S3.</p><p><br></p><p>During the migration, the company discovered that it could not immediately update the processing server that generates many documents to support the S3 API. The server runs on Linux and requires fast local access to the files that the server generates and modifies. When the server finishes processing, the files must be available to the public for download within 30 minutes.</p><p><br></p><p>Which solution will meet these requirements with the LEAST amount of effort?</p>",
      "options": [
        {
          "label": "A",
          "content": "Migrate the application to an AWS Lambda function. Use the AWS SDK for Java to generate, modify, and access the files that the company stores directly in Amazon S3."
        },
        {
          "label": "B",
          "content": "Set up an Amazon S3 File Gateway and configure a file share that is linked to the document store. Mount the file share on an Amazon EC2 instance by using NFS. When changes occur in Amazon S3, initiate a RefreshCache API call to update the S3 File Gateway."
        },
        {
          "label": "C",
          "content": "Configure Amazon FSx for Lustre with an import and export policy. Link the new file system to an S3 bucket. Install the Lustre client and mount the document store to an Amazon EC2 instance by using NFS."
        },
        {
          "label": "D",
          "content": "Configure AWS DataSync to connect to an Amazon EC2 instance. Configure a task to synchronize the generated files to and from Amazon S3."
        }
      ],
      "correctAnswer": "B",
      "explanation": "Reference Answer: B<p>Explanation:</p><p>Problem:</p><p>The company needs a solution to migrate a document processing workload to AWS while meeting the following constraints:</p><ol><li>Local Access: The server requires fast local access to the files it generates and modifies.</li><li>Public Availability: The processed files must be available for public download within 30 minutes.</li><li>Minimal Effort: The solution must require the least amount of effort for implementation.</li></ol><p>Options Analysis:</p><ul><li><p>A: Migrating the application to AWS Lambda requires significant changes to the existing application to integrate with the AWS SDK and S3 API. This would require extensive rework, violating the \"least amount of effort\" requirement.</p></li><li><p>B: Using an Amazon S3 File Gateway allows the existing Linux server to access files through NFS, minimizing changes to the application. It supports a seamless transition by linking a local file share to Amazon S3, and the RefreshCache API ensures updates in S3 are visible locally. This is the simplest solution requiring minimal effort.</p></li><li><p>C: Configuring Amazon FSx for Lustre requires setting up a new Lustre file system and linking it to the EC2 instance. This involves significant setup and configuration, which adds complexity.</p></li><li><p>D: Using AWS DataSync introduces an additional synchronization layer, requiring the setup of tasks and schedules. While effective, it is more complex than using an S3 File Gateway for this use case.</p></li></ul><p>Key Benefits of Option B:</p><ul><li>Minimal Application Changes: It integrates with the existing workflow, requiring minimal updates.</li><li>Fast Access: The NFS-based file share provides fast local access.</li><li>Easy S3 Synchronization: The RefreshCache API simplifies synchronization with Amazon S3.</li></ul><p>Thus, Option B is the correct answer.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "29f2a14b229f48258fced5e50abfbc4d",
      "questionNumber": 166,
      "type": "single",
      "content": "<p>Question #166</p><p>A delivery company is running a serverless solution in the AWS Cloud. The solution manages user data, delivery information, and past purchase details. The solution consists of several microservices. The central user service stores sensitive data in an Amazon DynamoDB table. Several of the other microservices store a copy of parts of the sensitive data in different storage services.</p><p><br></p><p>The company needs the ability to delete user information upon request. As soon as the central user service deletes a user, every other microservice must also delete its copy of the data immediately.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Activate DynamoDB Streams on the DynamoDB table. Create an AWS Lambda trigger for the DynamoDB stream that will post events about user deletion in an Amazon Simple Queue Service (Amazon SQS) queue. Configure each microservice to poll the queue and delete the user from the DynamoDB table."
        },
        {
          "label": "B",
          "content": "Set up DynamoDB event notifications on the DynamoDB table. Create an Amazon Simple Notification Service (Amazon SNS) topic as a target for the DynamoDB event notification. Configure each microservice to subscribe to the SNS topic and to delete the user from the DynamoDB table."
        },
        {
          "label": "C",
          "content": "Configure the central user service to post an event on a custom Amazon EventBridge event bus when the company deletes a user. Create an EventBridge rule for each microservice to match the user deletion event pattern and invoke logic in the microservice to delete the user from the DynamoDB table."
        },
        {
          "label": "D",
          "content": "Configure the central user service to post a message on an Amazon Simple Queue Service (Amazon SQS) queue when the company deletes a user. Configure each microservice to create an event filter on the SQS queue and to delete the user from the DynamoDB table."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>The correct answer is C. &nbsp;</p><p> Explanation: &nbsp;</p><p> Requirements: &nbsp;</p><p>1. Immediate deletion of user data across all microservices when the central user service deletes a user. &nbsp;</p><p>2. Event-driven architecture to ensure all services react in real-time. &nbsp;</p><p>3. Scalable and decoupled solution (no direct dependencies between microservices). &nbsp;</p><p> Why Option C is Best? &nbsp;</p><p>✅ EventBridge provides a centralized event bus for real-time event distribution. &nbsp;</p><p>✅ Each microservice subscribes to user deletion events via EventBridge rules. &nbsp;</p><p>✅ Decoupled and scalable—microservices independently process events. &nbsp;</p><p>✅ Immediate propagation of deletion events (no polling delays). &nbsp;</p><p> Why Other Options Fail? &nbsp;</p><p>❌ A: DynamoDB Streams + SQS + Polling &nbsp;</p><p>- Polling introduces delays (not immediate). &nbsp;</p><p>- Complex to manage (each service must poll SQS). &nbsp;</p><p>❌ B: DynamoDB Notifications + SNS &nbsp;</p><p>- SNS cannot filter events (all services receive all events, increasing overhead). &nbsp;</p><p>- Services must implement logic to ignore irrelevant events. &nbsp;</p><p>❌ D: SQS Queue + Event Filters &nbsp;</p><p>- SQS does not natively support event filtering (services must process all messages). &nbsp;</p><p>- Polling is required, leading to latency. &nbsp;</p><p> Implementation Steps (Option C): &nbsp;</p><p>1. Central user service publishes a \"UserDeleted\" event to EventBridge. &nbsp;</p><p>2. EventBridge rules route the event to each microservice. &nbsp;</p><p>3. Each microservice triggers its deletion logic upon receiving the event. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "e98ce29914344ef4918f55d93523cceb",
      "questionNumber": 167,
      "type": "single",
      "content": "<p>Question #167</p><p>A company is running a web application in a VPC. The web application runs on a group of Amazon EC2 instances behind an Application Load Balancer (ALB). The ALB is using AWS WAF.</p><p><br></p><p>An external customer needs to connect to the web application. The company must provide IP addresses to all external customers.</p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Replace the ALB with a Network Load Balancer (NLB). Assign an Elastic IP address to the NLB."
        },
        {
          "label": "B",
          "content": "Allocate an Elastic IP address. Assign the Elastic IP address to the ALB and provide the Elastic IP address to the customer."
        },
        {
          "label": "C",
          "content": "Create an AWS Global Accelerator standard accelerator. Specify the ALB as the accelerator&#39;s endpoint. Provide the accelerator&#39;s IP addresses to the customer."
        },
        {
          "label": "D",
          "content": "Configure an Amazon CloudFront distribution. Set the ALB as the origin. Ping the distribution&#39;s DNS name to determine the distribution&#39;s public IP address. Provide the IP address to the customer."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Create an AWS Global Accelerator standard accelerator. Specify the ALB as the accelerator's endpoint. Provide the accelerator's IP addresses to the customer.</p><p>This solution will meet the requirements with the least operational overhead for the following reasons:</p><p>Fixed IP addresses: AWS Global Accelerator provides two static anycast IP addresses that remain fixed for the life of the accelerator. This directly addresses the requirement to provide IP addresses to external customers.</p><p>Preservation of existing architecture: This solution allows the company to keep their current setup with the Application Load Balancer and AWS WAF intact, without requiring any architectural changes.</p><p>Minimal operational overhead: Global Accelerator is a fully managed service that requires minimal setup and maintenance. Once configured, it automatically routes traffic to the ALB without additional management.</p><p>Additional benefits: Global Accelerator also provides improved availability and performance by routing traffic through the AWS global network and automatically directing users to the nearest AWS edge location.</p><p>The other options have significant drawbacks:</p><p>A. Replacing the ALB with an NLB would require reconfiguring the entire load balancing setup and would lose the AWS WAF integration, as WAF doesn't directly integrate with NLBs. This would create significant operational overhead.</p><p>B. This option is not technically feasible. You cannot directly assign Elastic IP addresses to Application Load Balancers. ALBs use DNS names, not fixed IP addresses.</p><p>D. CloudFront distributions don't have fixed IP addresses that can be provided to customers. The IP addresses associated with CloudFront distributions can change over time. Additionally, \"pinging\" the distribution to determine IP addresses is not a reliable method and goes against AWS best practices.</p><p>AWS Global Accelerator is specifically designed for use cases where fixed IP addresses are required while maintaining the benefits of AWS networking services like ALB and WAF, making it the optimal solution with the least operational overhead.</p><p>Sources</p><p>WAF on ALB or CloudFront ? | AWS re:Post （https://repost.aws/questions/QUTY5hPVxgS6Caa3eZHX7-nQ/waf-on-alb-or-cloudfront）</p><p>Integrations for your Application Load Balancer - Elastic Load Balancing （https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-integrations.html）</p><p>Can an Application Load Balancer have a static IP | AWS re:Post （https://repost.aws/questions/QU-VlS2bqXTJil3WEwJkfHiA/can-an-application-load-balancer-have-a-static-ip）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "60a4bb927cf948ec8db80bd2fdcaac25",
      "questionNumber": 168,
      "type": "multiple",
      "content": "<p>Question #168</p><p>A company has a few AWS accounts for development and wants to move its production application to AWS. The company needs to enforce Amazon Elastic Block Store (Amazon EBS) encryption at rest for current production accounts and future production accounts only. The company needs a solution that includes built-in blueprints and guardrails.</p><p>Which combination of steps will meet these requirements? (Choose three.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Use AWS CloudFormation StackSets to deploy AWS Config rules on production accounts."
        },
        {
          "label": "B",
          "content": "Create a new AWS Control Tower landing zone in an existing developer account. Create OUs for accounts. Add production and development accounts to production and development OUs, respectively."
        },
        {
          "label": "C",
          "content": "Create a new AWS Control Tower landing zone in the company&rsquo;s management account. Add production and development accounts to production and development OUs, respectively."
        },
        {
          "label": "D",
          "content": "Invite existing accounts to join the organization in AWS Organizations. Create SCPs to ensure compliance."
        },
        {
          "label": "E",
          "content": "Create a guardrail from the management account to detect EBS encryption."
        },
        {
          "label": "F",
          "content": "Create a guardrail for the production OU to detect EBS encryption."
        }
      ],
      "correctAnswer": "CDF",
      "explanation": "<p>Based on the information provided, the correct combination of steps to meet the requirements are:</p><p>C. Create a new AWS Control Tower landing zone in the company's management account. Add production and development accounts to production and development OUs, respectively.</p><p>D. Invite existing accounts to join the organization in AWS Organizations. Create SCPs to ensure compliance.</p><p>F. Create a guardrail for the production OU to detect EBS encryption.</p><p>This combination of steps will meet the requirements for the following reasons:</p><p>AWS Control Tower provides built-in blueprints and guardrails, which is specifically mentioned as a requirement. By creating a new AWS Control Tower landing zone (step C), the company establishes a well-architected multi-account environment with pre-configured governance.</p><p>The company already has existing development accounts that need to be incorporated into this structure. By inviting these existing accounts to join the organization in AWS Organizations (step D), they can be properly organized and managed within the Control Tower environment.</p><p>Service Control Policies (SCPs) are powerful policy tools that can be used to ensure compliance across the organization. Creating SCPs (part of step D) allows the company to enforce security policies like EBS encryption at the organization level.</p><p>Creating a guardrail specifically for the production OU to detect EBS encryption (step F) ensures that the encryption requirement is enforced only on production accounts (current and future), not on development accounts. This meets the requirement of enforcing EBS encryption at rest for \"production accounts only.\"</p><p>This approach provides:</p><p>A structured environment with separate OUs for production and development</p><p>Built-in blueprints and guardrails through AWS Control Tower</p><p>The ability to enforce EBS encryption specifically on production accounts</p><p>A scalable solution that will automatically apply to future production accounts added to the production OU</p><p>A way to incorporate existing development accounts into the new structure</p><p>The combination of AWS Control Tower's landing zone, AWS Organizations' account structure and SCPs, and OU-specific guardrails creates a comprehensive solution that meets all the stated requirements with the appropriate level of control and flexibility.</p><p>Sources</p><p>Enforcing Encryption of Data at Rest - Encrypting File Data with Amazon Elastic File System （https://docs.aws.amazon.com/efs/latest/ug/whatisefs.html）</p><p>Amazon Elastic Compute Cloud (Amazon EC2) controls - AWS Control Tower （https://docs.aws.amazon.com/controltower/latest/controlreference/ec2-rules.html）</p><p>Amazon Elastic File System controls - AWS Control Tower （https://docs.aws.amazon.com/controltower/latest/controlreference/efs-rules.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "f34812a444ce41e28ec93a98c3626f2c",
      "questionNumber": 169,
      "type": "single",
      "content": "Question #169<p>A company is running a critical stateful web application on two Linux Amazon EC2 instances behind an Application Load Balancer (ALB) with an Amazon RDS for MySQL database. The company hosts the DNS records for the application in Amazon Route 53. A solutions architect must recommend a solution to improve the resiliency of the application. </p><p><br></p><p>The solution must meet the following objectives:</p><p>- Application tier: RPO of 2 minutes. RTO of 30 minutes</p><p>- Database tier: RPO of 5 minutes. RTO of 30 minutes</p><p><br></p><p>The company does not want to make significant changes to the existing application architecture. The company must ensure optimal latency after a failover.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Configure the EC2 instances to use AWS Elastic Disaster Recovery. Create a cross-Region read replica for the RDS DB instance. Create an ALB in a second AWS Region. Create an AWS Global Accelerator endpoint, and associate the endpoint with the ALBs. Update DNS records to point to the Global Accelerator endpoint."
        },
        {
          "label": "B",
          "content": "Configure the EC2 instances to use Amazon Data Lifecycle Manager (Amazon DLM) to take snapshots of the EBS volumes. Configure RDS automated backups. Configure backup replication to a second AWS Region. Create an ALB in the second Region. Create an AWS Global Accelerator endpoint, and associate the endpoint with the ALBs. Update DNS records to point to the Global Accelerator endpoint."
        },
        {
          "label": "C",
          "content": "Create a backup plan in AWS Backup for the EC2 instances and RDS DB instance. Configure backup replication to a second AWS Region.Create an ALB in the second Region. Configure an Amazon CloudFront distribution in front of the ALB. Update DNS records to point to CloudFront."
        },
        {
          "label": "D",
          "content": "Configure the EC2 instances to use Amazon Data Lifecycle Manager (Amazon DLM) to take snapshots of the EBS volumes. Create a cross Region read replica for the RDS DB instance. Create an ALB in a second AWS Region. Create an AWS Global Accelerator endpoint, and associate the endpoint with the ALBs."
        }
      ],
      "correctAnswer": "A",
      "explanation": "The correct answer is A. AWS Elastic Disaster Recovery (DRS) is designed to recover from disasters by providing a cost-effective, fully managed, and scalable solution for disaster recovery. By configuring EC2 instances to use DRS, the company can meet the RPO and RTO objectives for the application tier. Creating a cross-Region read replica for the RDS DB instance ensures that data is replicated to a secondary region, which helps meet the RPO and RTO for the database tier. Having an ALB in a second AWS Region, along with a Global Accelerator endpoint, provides a resilient architecture that can redirect traffic to the healthy region in case of a disaster, thus ensuring optimal latency after failover.",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "85ac9c8f3ac1417d85257831add3c978",
      "questionNumber": 170,
      "type": "multiple",
      "content": "<p>Question #170</p><p>A solutions architect wants to cost-optimize and appropriately size Amazon EC2 instances in a single AWS account. The solutions architect wants to ensure that the instances are optimized based on CPU, memory, and network metrics.</p><p>Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Purchase AWS Business Support or AWS Enterprise Support for the account."
        },
        {
          "label": "B",
          "content": "Turn on AWS Trusted Advisor and review any &ldquo;Low Utilization Amazon EC2 Instances&rdquo; recommendations."
        },
        {
          "label": "C",
          "content": "Install the Amazon CloudWatch agent and configure memory metric collection on the EC2 instances."
        },
        {
          "label": "D",
          "content": "Configure AWS Compute Optimizer in the AWS account to receive findings and optimization recommendations."
        },
        {
          "label": "E",
          "content": "Create an EC2 Instance Savings Plan for the AWS Regions, instance families, and operating systems of interest."
        }
      ],
      "correctAnswer": "CD",
      "explanation": "<p>Based on the information provided, the correct combination of steps for the solutions architect to cost-optimize and appropriately size EC2 instances based on CPU, memory, and network metrics are:</p><p>C. Install the Amazon CloudWatch agent and configure memory metric collection on the EC2 instances.</p><p>D. Configure AWS Compute Optimizer in the AWS account to receive findings and optimization recommendations.</p><p>This combination will meet the requirements for the following reasons:</p><p>Installing the Amazon CloudWatch agent and configuring memory metric collection (option C) is essential because:</p><p>EC2 instances do not provide memory utilization metrics by default</p><p>Memory is one of the specific metrics mentioned in the requirements</p><p>The CloudWatch agent allows for detailed memory utilization data collection</p><p>This data is crucial for accurate rightsizing recommendations</p><p>Configuring AWS Compute Optimizer (option D) is the most comprehensive solution because:</p><p>It uses machine learning to analyze utilization patterns across CPU, memory, and network</p><p>It provides specific recommendations for instance types based on these metrics</p><p>It can identify both under-provisioned and over-provisioned resources</p><p>It offers detailed rightsizing recommendations with projected impact on performance and cost</p><p>The other options are less suitable for the specific requirements:</p><p>A. Purchasing AWS Business Support or Enterprise Support would provide access to more Trusted Advisor checks, but this alone doesn't provide the comprehensive CPU, memory, and network metric analysis needed.</p><p>B. Trusted Advisor's \"Low Utilization Amazon EC2 Instances\" check only looks at basic CPU and network metrics, not memory metrics, which are specifically required.</p><p>E. Creating an EC2 Instance Savings Plan would help reduce costs for committed usage but doesn't help with appropriately sizing instances based on utilization metrics.</p><p>The combination of CloudWatch agent for memory metrics and AWS Compute Optimizer provides the most comprehensive solution for analyzing all required metrics (CPU, memory, and network) and receiving specific optimization recommendations to properly size EC2 instances.</p><p>Sources</p><p>Amazon EC2 Cost and Capacity Optimization | Amazon Web Services （https://aws.amazon.com/cn/ec2/cost-and-capacity/）</p><p>My boss asking me on cost optimization in AWS | AWS re:Post （https://repost.aws/questions/QU3MV37BYRR6ePI6WmUgOW0g/my-boss-asking-me-on-cost-optimization-in-aws）</p><p>Identifying Opportunities to Right Size - Right Sizing: Provisioning Instances to Match Workloads （https://docs.aws.amazon.com/whitepapers/latest/cost-optimization-right-sizing/identifying-opportunities-to-right-size.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "642d3c63696b471ca5c269df7a71f432",
      "questionNumber": 171,
      "type": "single",
      "content": "<p>Question #171</p><p>A company uses an AWS CodeCommit repository. The company must store a backup copy of the data that is in the repository in a second AWS Region.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Configure AWS Elastic Disaster Recovery to replicate the CodeCommit repository data to the second Region."
        },
        {
          "label": "B",
          "content": "Use AWS Backup to back up the CodeCommit repository on an hourly schedule. Create a cross-Region copy in the second Region."
        },
        {
          "label": "C",
          "content": "Create an Amazon EventBridge rule to invoke AWS CodeBuild when the company pushes code to the repository. Use CodeBuild to clone the repository. Create a .zip file of the content. Copy the file to an S3 bucket in the second Region."
        },
        {
          "label": "D",
          "content": "Create an AWS Step Functions workflow on an hourly schedule to take a snapshot of the CodeCommit repository. Configure the workflow to copy the snapshot to an S3 bucket in the second Region."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Create an Amazon EventBridge rule to invoke AWS CodeBuild when the company pushes code to the repository. Use CodeBuild to clone the repository. Create a .zip file of the content. Copy the file to an S3 bucket in the second Region.</p><p>This solution will meet the requirements for the following reasons:</p><p>Appropriate service selection: AWS CodeCommit repositories are not directly supported by AWS Backup or AWS Elastic Disaster Recovery. The EventBridge and CodeBuild approach is specifically designed to work with code repositories.</p><p>Automation and triggering: The EventBridge rule will automatically trigger the backup process whenever code is pushed to the repository, ensuring that the backup is always current and reflects the latest changes.</p><p>Cross-region capability: CodeBuild can be configured to copy the zipped repository content to an S3 bucket in the second region, fulfilling the cross-region backup requirement.</p><p>Efficiency: This approach is serverless and only runs when needed (when code is pushed), making it cost-effective.</p><p>Complete backup: The solution captures the entire repository content by cloning it, ensuring a complete backup.</p><p>The other options have significant limitations:</p><p>A. AWS Elastic Disaster Recovery is designed for EC2 instances and EBS volumes, not for code repositories like CodeCommit. It's not an appropriate service for this use case.</p><p>B. AWS Backup does not natively support CodeCommit repositories. It's designed for services like EBS, RDS, DynamoDB, and other storage services, but not for code repositories.</p><p>D. While AWS Step Functions could theoretically orchestrate a backup workflow, CodeCommit doesn't have a native \"snapshot\" feature that Step Functions could leverage. This approach would still require additional services to actually perform the backup, making it more complex than necessary.</p><p>The EventBridge and CodeBuild approach (option C) is a well-established pattern for backing up CodeCommit repositories to another region and represents the most appropriate solution for the stated requirements.</p><p>Sources</p><p>DR and Fault Tolerance | AWS re:Post （https://repost.aws/questions/QUJMujNJwQSpKQIjhmUtEbTQ/dr-and-fault-tolerance）</p><p>Backup - Ransomware Risk Management on AWS Using the NIST Cyber Security Framework (CSF) （https://docs.aws.amazon.com/whitepapers/latest/ransomware-risk-management-on-aws-using-nist-csf/backup.html）</p><p>Automate event-driven backups from CodeCommit to Amazon S3 using CodeBuild and CloudWatch Events - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/automate-event-driven-backups-from-codecommit-to-amazon-s3-using-codebuild-and-cloudwatch-events.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "e18205cb277e4729accbccd3aa1591ba",
      "questionNumber": 172,
      "type": "single",
      "content": "<p>Question #172</p><p>A company has multiple business units that each have separate accounts on AWS. Each business unit manages its own network with several VPCs that have CIDR ranges that overlap. The company’s marketing team has created a new internal application and wants to make the application accessible to all the other business units. The solution must use private IP addresses only.</p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Instruct each business unit to add a unique secondary CIDR range to the business unit&#39;s VPC. Peer the VPCs and use a private NAT gateway in the secondary range to route traffic to the marketing team."
        },
        {
          "label": "B",
          "content": "Create an Amazon EC2 instance to serve as a virtual appliance in the marketing account&#39;s VPC. Create an AWS Site-to-Site VPN connection between the marketing team and each business unit&#39;s VPC. Perform NAT where necessary."
        },
        {
          "label": "C",
          "content": "Create an AWS PrivateLink endpoint service to share the marketing application. Grant permission to specific AWS accounts to connect to the service. Create interface VPC endpoints in other accounts to access the application by using private IP addresses."
        },
        {
          "label": "D",
          "content": "Create a Network Load Balancer (NLB) in front of the marketing application in a private subnet. Create an API Gateway API. Use the Amazon API Gateway private integration to connect the API to the NLB. Activate IAM authorization for the API. Grant access to the accounts of the other business units."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Create an AWS PrivateLink endpoint service to share the marketing application. Grant permission to specific AWS accounts to connect to the service. Create interface VPC endpoints in other accounts to access the application by using private IP addresses.</p><p>This solution will meet the requirements with the least operational overhead for the following reasons:</p><p>Handling overlapping CIDR ranges: AWS PrivateLink is specifically designed to work across VPCs with overlapping IP address ranges. It creates a private connection between services without exposing them to the public internet or requiring IP address management.</p><p>Private IP addresses only: PrivateLink uses interface VPC endpoints that operate entirely on private IP addresses, meeting the requirement to use only private IP addresses.</p><p>Minimal operational overhead:</p><p>No need to manage complex NAT configurations</p><p>No need to maintain VPN tunnels</p><p>No need to manage routing tables for overlapping CIDRs</p><p>Scales automatically as usage increases</p><p>Security and access control: PrivateLink allows the marketing team to grant specific permissions to each business unit's AWS account, providing fine-grained access control.</p><p>Simplicity: Each business unit simply creates an interface VPC endpoint in their account to connect to the marketing application, without having to worry about IP address conflicts.</p><p>Option B (using EC2 as a virtual appliance with Site-to-Site VPN and NAT) would require significantly more operational overhead:</p><p>Setting up and maintaining multiple VPN connections</p><p>Configuring and managing complex NAT rules to handle the overlapping CIDR ranges</p><p>Ensuring high availability of the EC2 instance serving as the virtual appliance</p><p>Managing routing tables and troubleshooting connectivity issues</p><p>Scaling the solution as traffic increases</p><p>AWS PrivateLink is purpose-built for this exact scenario - sharing services across multiple AWS accounts with overlapping IP ranges while maintaining private connectivity. It provides a much more streamlined and manageable solution compared to the VPN approach.</p><p>Sources</p><p>Use Amazon RDS Proxy and AWS PrivateLink to access Amazon RDS databases across AWS Organizations at American Family Insurance Group | AWS Database Blog （https://aws.amazon.com/cn/blogs/database/use-amazon-rds-proxy-and-aws-privatelink-to-access-amazon-rds-databases-across-aws-organizations-at-american-family-insurance-group/）</p><p>Public IPs and NLB from VPN | AWS re:Post （https://repost.aws/questions/QUYKMhQhj_SSKSXYjDl-xLBQ/public-ips-and-nlb-from-vpn）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "865d49b1acfa4d97b28b608411e9217c",
      "questionNumber": 173,
      "type": "single",
      "content": "<p>Question #173</p><p>A company needs to audit the security posture of a newly acquired AWS account. The company’s data security team requires a notification only when an Amazon S3 bucket becomes publicly exposed. The company has already established an Amazon Simple Notification Service (Amazon SNS) topic that has the data security team's email address subscribed.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an S3 event notification on all S3 buckets for the isPublic event. Select the SNS topic as the target for the event notifications."
        },
        {
          "label": "B",
          "content": "Create an analyzer in AWS Identity and Access Management Access Analyzer. Create an Amazon EventBridge rule for the event type &ldquo;Access Analyzer Finding&rdquo; with a filter for &ldquo;isPublic: true.&rdquo; Select the SNS topic as the EventBridge rule target."
        },
        {
          "label": "C",
          "content": "Create an Amazon EventBridge rule for the event type &ldquo;Bucket-Level API Call via CloudTrail&rdquo; with a filter for &ldquo;PutBucketPolicy.&rdquo; Select the SNS topic as the EventBridge rule target."
        },
        {
          "label": "D",
          "content": "Activate AWS Config and add the cloudtrail-s3-dataevents-enabled rule. Create an Amazon EventBridge rule for the event type &ldquo;Config Rules Re-evaluation Status&rdquo; with a filter for &ldquo;NON_COMPLIANT.&rdquo; Select the SNS topic as the EventBridge rule target."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Create an analyzer in AWS Identity and Access Management Access Analyzer. Create an Amazon EventBridge rule for the event type \"Access Analyzer Finding\" with a filter for \"isPublic: true.\" Select the SNS topic as the EventBridge rule target.</p><p>This solution will meet the requirements for the following reasons:</p><p>Comprehensive detection: IAM Access Analyzer is specifically designed to analyze resource policies and identify when resources like S3 buckets are accessible to external entities, including public access. It continuously monitors for policy changes that could expose S3 buckets.</p><p>Accurate identification: Access Analyzer can precisely identify when a bucket becomes publicly accessible through various means (bucket policies, ACLs, etc.), reducing false positives and ensuring the data security team only receives relevant notifications.</p><p>Targeted notifications: By creating an EventBridge rule with a filter for \"isPublic: true,\" the solution ensures that notifications are only sent when buckets are specifically identified as publicly exposed, meeting the requirement for notification only when a bucket becomes publicly exposed.</p><p>Integration with existing infrastructure: The solution leverages the company's already established SNS topic, making implementation straightforward.</p><p>Security best practice: This approach aligns with AWS security best practices for monitoring resource access.</p><p>Option A (using S3 event notifications) is not correct because:</p><p>S3 event notifications don't have a specific \"isPublic\" event type that can reliably detect all scenarios where a bucket becomes publicly accessible.</p><p>S3 event notifications are primarily designed for tracking object-level operations (uploads, downloads, etc.) rather than changes to bucket access policies or ACLs that would make a bucket public.</p><p>This approach would require configuring notifications on each individual bucket, which becomes cumbersome as new buckets are created and doesn't provide a centralized view of security posture.</p><p>IAM Access Analyzer with EventBridge provides a more comprehensive, accurate, and maintainable solution for detecting and notifying when S3 buckets become publicly accessible, making it the appropriate choice for the company's security auditing requirements.</p><p>Sources</p><p>Automatically scan for public Amazon S3 buckets and block public access | AWS Storage Blog （https://aws.amazon.com/cn/blogs/storage/automatically-scan-for-public-amazon-s3-buckets-and-block-public-access/）</p><p>SEC03-BP07 Analyze public and cross-account access - Security Pillar （https://docs.aws.amazon.com/wellarchitected/latest/security-pillar/sec_permissions_analyze_cross_account.html）</p><p>SEC03-BP07 Analyze public and cross-account access - AWS Well-Architected Framework （https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_permissions_analyze_cross_account.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "7dc5f8a669564ff992c4b8102c4c1a6e",
      "questionNumber": 174,
      "type": "single",
      "content": "<p>Question #174</p><p>A solutions architect needs to assess a newly acquired company’s portfolio of applications and databases. The solutions architect must create a business case to migrate the portfolio to AWS. The newly acquired company runs applications in an on-premises data center. The data center is not well documented. The solutions architect cannot immediately determine how many applications and databases exist. Traffic for the applications is variable. Some applications are batch processes that run at the end of each month.</p><p><br></p><p>The solutions architect must gain a better understanding of the portfolio before a migration to AWS can begin.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use AWS Server Migration Service (AWS SMS) and AWS Database Migration Service (AWS DMS) to evaluate migration. Use AWS Service Catalog to understand application and database dependencies."
        },
        {
          "label": "B",
          "content": "Use AWS Application Migration Service. Run agents on the on-premises infrastructure. Manage the agents by using AWS Migration Hub. Use AWS Storage Gateway to assess local storage needs and database dependencies."
        },
        {
          "label": "C",
          "content": "Use Migration Evaluator to generate a list of servers. Build a report for a business case. Use AWS Migration Hub to view the portfolio. Use AWS Application Discovery Service to gain an understanding of application dependencies."
        },
        {
          "label": "D",
          "content": "Use AWS Control Tower in the destination account to generate an application portfolio. Use AWS Server Migration Service (AWS SMS) to generate deeper reports and a business case. Use a landing zone for core accounts and resources."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>The correct answer is C. &nbsp;</p><p> Explanation: &nbsp;</p><p> Requirements: &nbsp;</p><p>1. Assess an undocumented on-premises portfolio (applications, databases, dependencies). &nbsp;</p><p>2. Understand traffic patterns (variable usage, batch processes). &nbsp;</p><p>3. Build a business case for migration (cost, effort, dependencies). &nbsp;</p><p> Why Option C is Best? &nbsp;</p><p>✅ Migration Evaluator (formerly TSO Logic) provides server inventory, cost estimates, and business case reports. &nbsp;</p><p>✅ AWS Application Discovery Service identifies application dependencies (critical for migration planning). &nbsp;</p><p>✅ Migration Hub consolidates discovery data into a single view of the portfolio. &nbsp;</p><p> Why Other Options Fail? &nbsp;</p><p>❌ A: AWS SMS + DMS + Service Catalog &nbsp;</p><p>- SMS/DMS are for migration execution, not assessment. &nbsp;</p><p>- Service Catalog is for managing approved AWS services, not discovery. &nbsp;</p><p>❌ B: AWS Application Migration Service + Storage Gateway &nbsp;</p><p>- Application Migration Service (MGN) is for lift-and-shift, not assessment. &nbsp;</p><p>- Storage Gateway is for hybrid storage, not dependency mapping. &nbsp;</p><p>❌ D: AWS Control Tower + SMS + Landing Zone &nbsp;</p><p>- Control Tower is for multi-account governance, not discovery. &nbsp;</p><p>- SMS is not a discovery tool. &nbsp;</p><p> Implementation Steps (Option C): &nbsp;</p><p>1. Deploy Migration Evaluator to analyze on-premises servers (CPU, memory, usage). &nbsp;</p><p>2. Use Application Discovery Service (agent-based or agentless) to map dependencies. &nbsp;</p><p>3. View consolidated data in Migration Hub to prioritize migration waves. &nbsp;</p><p>4. Generate a business case report (TCO, ROI, migration strategy). &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "171e725394314b6780c021116757e403",
      "questionNumber": 175,
      "type": "single",
      "content": "<p>Question #175</p><p>A company has an application that runs as a ReplicaSet of multiple pods in an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The EKS cluster has nodes in multiple Availability Zones. The application generates many small files that must be accessible across all running instances of the application. The company needs to back up the files and retain the backups for 1 year.</p><p><br></p><p>Which solution will meet these requirements while providing the FASTEST storage performance?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an Amazon Elastic File System (Amazon EFS) file system and a mount target for each subnet that contains nodes in the EKS cluster. Configure the ReplicaSet to mount the file system. Direct the application to store files in the file system. Configure AWS Backup to back up and retain copies of the data for 1 year."
        },
        {
          "label": "B",
          "content": "Create an Amazon Elastic Block Store (Amazon EBS) volume. Enable the EBS Multi-Attach feature. Configure the ReplicaSet to mount the EBS volume. Direct the application to store files in the EBS volume. Configure AWS Backup to back up and retain copies of the data for 1 year."
        },
        {
          "label": "C",
          "content": "Create an Amazon S3 bucket. Configure the ReplicaSet to mount the S3 bucket. Direct the application to store files in the S3 bucket. Configure S3 Versioning to retain copies of the data. Configure an S3 Lifecycle policy to delete objects after 1 year."
        },
        {
          "label": "D",
          "content": "Configure the ReplicaSet to use the storage available on each of the running application pods to store the files locally. Use a third-party tool to back up the EKS cluster for 1 year."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>The correct answer is A. &nbsp;</p><p> Explanation: &nbsp;</p><p> Requirements: &nbsp;</p><p>1. Shared storage accessible across all pods in an EKS ReplicaSet. &nbsp;</p><p>2. High performance for many small files. &nbsp;</p><p>3. Backup retention for 1 year. &nbsp;</p><p> Why Option A is Best? &nbsp;</p><p>✅ Amazon EFS provides shared, low-latency storage for Kubernetes pods. &nbsp;</p><p>✅ Multi-AZ availability (mount targets in each subnet). &nbsp;</p><p>✅ AWS Backup integrates with EFS for automated, long-term retention. &nbsp;</p><p> Why Other Options Fail? &nbsp;</p><p>❌ B: EBS Multi-Attach &nbsp;</p><p>- Limited to a single Availability Zone (violates multi-AZ resilience). &nbsp;</p><p>- Not ideal for many small files (better for block storage). &nbsp;</p><p>❌ C: Amazon S3 &nbsp;</p><p>- S3 is object storage, not file storage (cannot be directly mounted as a filesystem in EKS without performance tradeoffs). &nbsp;</p><p>- Higher latency for small file operations. &nbsp;</p><p>❌ D: Local pod storage &nbsp;</p><p>- Not shared (files are pod-specific, lost if pod terminates). &nbsp;</p><p>- No built-in backup solution (third-party tools add complexity). &nbsp;</p><p> Implementation Steps (Option A): &nbsp;</p><p>1. Create an EFS file system with mount targets in each EKS node subnet. &nbsp;</p><p>2. Configure the ReplicaSet to mount EFS as a PersistentVolume (PV). &nbsp;</p><p>3. Set up AWS Backup for EFS with a 1-year retention policy. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "62b042c87eaa429d9cce58756acd8da8",
      "questionNumber": 176,
      "type": "single",
      "content": "<p>Question #176</p><p>A company runs a customer service center that accepts calls and automatically sends all customers a managed, interactive, two-way experience survey by text message. The applications that support the customer service center run on machines that the company hosts in an on-premises data center. The hardware that the company uses is old, and the company is experiencing downtime with the system. The company wants to migrate the system to AWS to improve reliability.</p><p><br></p><p>Which solution will meet these requirements with the LEAST ongoing operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use Amazon Connect to replace the old call center hardware. Use Amazon Pinpoint to send text message surveys to customers."
        },
        {
          "label": "B",
          "content": "Use Amazon Connect to replace the old call center hardware. Use Amazon Simple Notification Service (Amazon SNS) to send text message surveys to customers."
        },
        {
          "label": "C",
          "content": "Migrate the call center software to Amazon EC2 instances that are in an Auto Scaling group. Use the EC2 instances to send text message surveys to customers."
        },
        {
          "label": "D",
          "content": "Use Amazon Pinpoint to replace the old call center hardware and to send text message surveys to customers."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Use Amazon Connect to replace the old call center hardware. Use Amazon Pinpoint to send text message surveys to customers.</p><p>This solution will meet the requirements with the least ongoing operational overhead for the following reasons:</p><p>Amazon Connect is specifically designed to replace traditional call center hardware with a cloud-based solution. It provides:</p><p>A fully managed contact center service that eliminates the need to manage physical hardware</p><p>Automatic scaling to handle call volume fluctuations</p><p>Built-in reliability and redundancy across AWS availability zones</p><p>Simple setup and configuration without specialized telephony knowledge</p><p>Integration capabilities with other AWS services</p><p>Amazon Pinpoint is purpose-built for sending interactive, two-way text message surveys:</p><p>It specializes in customer engagement through multiple channels, including SMS</p><p>It supports two-way messaging, which is specifically required for the interactive surveys</p><p>It provides analytics and campaign management features for tracking customer responses</p><p>It's a fully managed service that requires minimal operational overhead</p><p>Together, these services create a complete solution that addresses both the call center functionality and the text message survey requirements while minimizing operational overhead.</p><p>Option D (using Amazon Pinpoint to replace both the call center hardware and send text message surveys) is incorrect because:</p><p>Amazon Pinpoint is not designed to replace call center hardware or handle inbound voice calls</p><p>It lacks the call routing, queuing, and agent management capabilities needed for a customer service center</p><p>Using Pinpoint for call center functionality would require significant custom development, increasing operational overhead</p><p>By using Amazon Connect for the call center functionality and Amazon Pinpoint for the text message surveys, the company gets the best of both worlds - specialized services that are fully managed by AWS, requiring minimal operational overhead while providing the reliability improvements they're seeking.</p><p>Sources</p><p>Guidance for Enhancing Customer Engagement in Travel & Hospitality (T&H) Using AWS AI/ML （https://aws.amazon.com/cn/solutions/guidance/enhancing-customer-engagement-in-travel-and-hospitality-using-aws-ai-ml/）</p><p>Amazon Connect - Amazon Connect Data Lake Best Practices （https://docs.aws.amazon.com/connect/）</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "57c9d9fd78ba4076a792cc5afc3e5847",
      "questionNumber": 177,
      "type": "single",
      "content": "<p>Question #177</p><p>A company is building a call center by using Amazon Connect. The company’s operations team is defining a disaster recovery (DR) strategy across AWS Regions. The contact center has dozens of contact flows, hundreds of users, and dozens of claimed phone numbers.</p><p><br></p><p>Which solution will provide DR with the LOWEST RTO?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an AWS Lambda function to check the availability of the Amazon Connect instance and to send a notification to the operations team in case of unavailability. Create an Amazon EventBridge rule to invoke the Lambda function every 5 minutes. After notification, instruct the operations team to use the AWS Management Console to provision a new Amazon Connect instance in a second Region. Deploy the contact flows, users, and claimed phone numbers by using an AWS CloudFormation template."
        },
        {
          "label": "B",
          "content": "Provision a new Amazon Connect instance with all existing users in a second Region. Create an AWS Lambda function to check the availability of the Amazon Connect instance. Create an Amazon EventBridge rule to invoke the Lambda function every 5 minutes. In the event of an issue, configure the Lambda function to deploy an AWS CloudFormation template that provisions contact flows and claimed numbers in the second Region."
        },
        {
          "label": "C",
          "content": "Provision a new Amazon Connect instance with all existing contact flows and claimed phone numbers in a second Region. Create an Amazon Route 53 health check for the URL of the Amazon Connect instance. Create an Amazon CloudWatch alarm for failed health checks. Create an AWS Lambda function to deploy an AWS CloudFormation template that provisions all users. Configure the alarm to invoke the Lambda function."
        },
        {
          "label": "D",
          "content": "Provision a new Amazon Connect instance with all existing users and contact flows in a second Region. Create an Amazon Route 53 health check for the URL of the Amazon Connect instance. Create an Amazon CloudWatch alarm for failed health checks. Create an AWS Lambda function to deploy an AWS CloudFormation template that provisions claimed phone numbers. Configure the alarm to invoke the Lambda function."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>The correct answer is D. &nbsp;</p><p> Explanation: &nbsp;</p><p> Requirements: &nbsp;</p><p>1. Disaster Recovery (DR) for Amazon Connect (contact flows, users, phone numbers). &nbsp;</p><p>2. Lowest RTO (minimize downtime during failover). &nbsp;</p><p>3. Automated failover (avoid manual steps). &nbsp;</p><p> Why Option D is Best? &nbsp;</p><p>✅ Pre-provisioned Amazon Connect instance (users + contact flows already deployed in secondary Region). &nbsp;</p><p>✅ Automated phone number provisioning via Lambda + CloudFormation (claimed numbers cannot be pre-provisioned). &nbsp;</p><p>✅ Route 53 health checks + CloudWatch alarm trigger failover automatically. &nbsp;</p><p>✅ Lowest RTO (only phone numbers need provisioning during failover). &nbsp;</p><p> Why Other Options Fail? &nbsp;</p><p>❌ A: Manual Failover &nbsp;</p><p>- High RTO (operations team must manually deploy CloudFormation). &nbsp;</p><p>- No pre-provisioning (slow recovery). &nbsp;</p><p>❌ B: Partial Pre-Provisioning &nbsp;</p><p>- Contact flows + phone numbers are not pre-provisioned (slower RTO than Option D). &nbsp;</p><p>❌ C: Missing Pre-Provisioned Users &nbsp;</p><p>- Users must be provisioned during failover (increases RTO). &nbsp;</p><p> Key Notes: &nbsp;</p><p>- Claimed phone numbers cannot be pre-provisioned (must be claimed during failover). &nbsp;</p><p>- Users + contact flows can be pre-provisioned (reduces RTO). &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "f08a19f0a3c04e3c904584c8cbcda55c",
      "questionNumber": 178,
      "type": "single",
      "content": "<p>Question #178</p><p>A company runs an application on AWS. The company curates data from several different sources. The company uses proprietary algorithms to perform data transformations and aggregations. After the company performs ETL processes, the company stores the results in Amazon Redshift tables. The company sells this data to other companies. The company downloads the data as files from the Amazon Redshift tables and transmits the files to several data customers by using FTP. The number of data customers has grown significantly. Management of the data customers has become difficult.</p><p><br></p><p>The company will use AWS Data Exchange to create a data product that the company can use to share data with customers. The company wants to confirm the identities of the customers before the company shares data. The customers also need access to the most recent data when the company publishes the data.</p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use AWS Data Exchange for APIs to share data with customers. Configure subscription verification. In the AWS account of the company that produces the data, create an Amazon API Gateway Data API service integration with Amazon Redshift. Require the data customers to subscribe to the data product."
        },
        {
          "label": "B",
          "content": "In the AWS account of the company that produces the data, create an AWS Data Exchange datashare by connecting AWS Data Exchange to the Redshift cluster. Configure subscription verification. Require the data customers to subscribe to the data product."
        },
        {
          "label": "C",
          "content": "Download the data from the Amazon Redshift tables to an Amazon S3 bucket periodically. Use AWS Data Exchange for S3 to share data with customers. Configure subscription verification. Require the data customers to subscribe to the data product."
        },
        {
          "label": "D",
          "content": "Publish the Amazon Redshift data to an Open Data on AWS Data Exchange. Require the customers to subscribe to the data product in AWS Data Exchange. In the AWS account of the company that produces the data, attach IAM resource-based policies to the Amazon Redshift tables to allow access only to verified AWS accounts."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>The correct answer is B. &nbsp;</p><p> Explanation: &nbsp;</p><p> Requirements: &nbsp;</p><p>1. Share data securely with customers via AWS Data Exchange. &nbsp;</p><p>2. Verify customer identities before granting access. &nbsp;</p><p>3. Ensure customers get the latest data (real-time or near-real-time updates). &nbsp;</p><p>4. Minimize operational overhead (avoid manual processes). &nbsp;</p><p> Why Option B is Best? &nbsp;</p><p>✅ AWS Data Exchange + Redshift datashare provides direct, real-time access to Redshift tables. &nbsp;</p><p>✅ Subscription verification ensures only approved customers access data. &nbsp;</p><p>✅ No manual ETL or file transfers (eliminates FTP and S3 downloads). &nbsp;</p><p>✅ Least operational overhead—fully managed by AWS. &nbsp;</p><p> Why Other Options Fail? &nbsp;</p><p>❌ A: Data Exchange for APIs + API Gateway &nbsp;</p><p>- Overkill for structured data (Redshift datashare is simpler). &nbsp;</p><p>- Higher operational overhead (API development/maintenance). &nbsp;</p><p>❌ C: Data Exchange for S3 + Periodic Downloads &nbsp;</p><p>- Not real-time (data must be exported from Redshift to S3 first). &nbsp;</p><p>- Adds ETL steps (increases operational overhead). &nbsp;</p><p>❌ D: Open Data on AWS + IAM Policies &nbsp;</p><p>- Open Data is for public datasets, not private customer data. &nbsp;</p><p>- IAM policies on Redshift are not scalable for many customers. &nbsp;</p><p> Implementation Steps (Option B): &nbsp;</p><p>1. Create a datashare in AWS Data Exchange linked to the Redshift cluster. &nbsp;</p><p>2. Enable subscription verification to authenticate customers. &nbsp;</p><p>3. Customers subscribe to the data product in AWS Data Exchange. &nbsp;</p><p>4. Data is automatically updated in Redshift and accessible to subscribers. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "dc13b2b42c4d4deabd4b12b5f3de6bc8",
      "questionNumber": 179,
      "type": "single",
      "content": "<p>Question #179</p><p>A solutions architect is designing a solution to process events. The solution must have the ability to scale in and out based on the number of events that the solution receives. If a processing error occurs, the event must move into a separate queue for review.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Send event details to an Amazon Simple Notification Service (Amazon SNS) topic. Configure an AWS Lambda function as a subscriber to the SNS topic to process the events. Add an on-failure destination to the function. Set an Amazon Simple Queue Service (Amazon SQS) queue as the target."
        },
        {
          "label": "B",
          "content": "Publish events to an Amazon Simple Queue Service (Amazon SQS) queue. Create an Amazon EC2 Auto Scaling group. Configure the Auto Scaling group to scale in and out based on the ApproximateAgeOfOldestMessage metric of the queue. Configure the application to write failed messages to a dead-letter queue."
        },
        {
          "label": "C",
          "content": "Write events to an Amazon DynamoDB table. Configure a DynamoDB stream for the table. Configure the stream to invoke an AWS Lambda function. Configure the Lambda function to process the events."
        },
        {
          "label": "D",
          "content": "Publish events to an Amazon EventBridge event bus. Create and run an application on an Amazon EC2 instance with an Auto Scaling group that is behind an Application Load Balancer (ALB). Set the ALB as the event bus target. Configure the event bus to retry events. Write messages to a dead-letter queue if the application cannot process the messages."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Publish events to an Amazon Simple Queue Service (Amazon SQS) queue. Create an Amazon EC2 Auto Scaling group. Configure the Auto Scaling group to scale in and out based on the ApproximateAgeOfOldestMessage metric of the queue. Configure the application to write failed messages to a dead-letter queue.</p><p>This solution meets the requirements for the following reasons:</p><p>Scaling capability: The EC2 Auto Scaling group can be configured to scale in and out based on the ApproximateAgeOfOldestMessage metric of the SQS queue. This ensures that as the number of events increases, more EC2 instances are added to process them, and as the queue empties, instances are removed to save costs.</p><p>Error handling: The solution explicitly states that the application will be configured to write failed messages to a dead-letter queue (DLQ). This satisfies the requirement that if a processing error occurs, the event must move into a separate queue for review.</p><p>Decoupling: Using SQS as the event queue provides a buffer between event producers and consumers, allowing for reliable event processing even during traffic spikes or when processing instances fail.</p><p>Control over processing: This approach gives more control over how events are processed, including retry logic, batch processing, and custom error handling.</p><p>Option A (using SNS with Lambda and an on-failure destination) would also provide scaling and error handling capabilities. However, there are a few reasons why option B might be considered more appropriate for certain scenarios:</p><p>Long-running processes: If the event processing requires more time than Lambda's maximum execution duration (currently 15 minutes), EC2 instances would be necessary.</p><p>Custom processing requirements: If the processing requires specific software or libraries that are difficult to package with Lambda, EC2 instances provide more flexibility.</p><p>Cost considerations: For high-volume, consistent workloads, EC2 instances might be more cost-effective than Lambda functions.</p><p>Batch processing: EC2 instances can be configured to process multiple messages in a batch, which might be more efficient for certain workloads.</p><p>Both solutions could work, but option B provides a more traditional and flexible approach to event processing with explicit scaling based on queue metrics and clear error handling through a dead-letter queue.</p><p>Sources</p><p>Scaling and event processing with SNS and SQS | AWS re:Post （https://ap-southeast-2.console.aws.amazon.com/console/home?nc2=h_ct&region=ap-southeast-2&src=header-signin#）</p><p>Implementing AWS Lambda error handling patterns | AWS Compute Blog （https://aws.amazon.com/cn/blogs/compute/implementing-aws-lambda-error-handling-patterns/）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "82f0a9c6e8654b4bb9f3e5f400f6df6f",
      "questionNumber": 180,
      "type": "single",
      "content": "<p>Question #180</p><p>A company runs a processing engine in the AWS Cloud. The engine processes environmental data from logistics centers to calculate a sustainability index. The company has millions of devices in logistics centers that are spread across Europe. The devices send information to the processing engine through a RESTful API.</p><p><br></p><p>The API experiences unpredictable bursts of traffic. The company must implement a solution to process all data that the devices send to the processing engine. Data loss is unacceptable.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an Application Load Balancer (ALB) for the RESTful API. Create an Amazon Simple Queue Service (Amazon SQS) queue. Create a listener and a target group for the ALB. Add the SQS queue as the target. Use a container that runs in Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type to process messages in the queue."
        },
        {
          "label": "B",
          "content": "Create an Amazon API Gateway HTTP API that implements the RESTful API. Create an Amazon Simple Queue Service (Amazon SQS) queue. Create an API Gateway service integration with the SQS queue. Create an AWS Lambda function to process messages in the SQS queue."
        },
        {
          "label": "C",
          "content": "Create an Amazon API Gateway REST API that implements the RESTful API. Create a fleet of Amazon EC2 instances in an Auto Scaling group. Create an API Gateway Auto Scaling group proxy integration. Use the EC2 instances to process incoming data."
        },
        {
          "label": "D",
          "content": "Create an Amazon CloudFront distribution for the RESTful API. Create a data stream in Amazon Kinesis Data Streams. Set the data stream as the origin for the distribution. Create an AWS Lambda function to consume and process data in the data stream."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Answer: &nbsp;</p><p>B. Create an Amazon API Gateway HTTP API that implements the RESTful API. Create an Amazon Simple Queue Service (Amazon SQS) queue. Create an API Gateway service integration with the SQS queue. Create an AWS Lambda function to process messages in the SQS queue. &nbsp;</p><p> Explanation: &nbsp;</p><p> Key Requirements: &nbsp;</p><p>1. Unpredictable Traffic Bursts: &nbsp;</p><p> &nbsp; - Millions of devices send data with spiky, unpredictable loads. &nbsp;</p><p>2. Zero Data Loss: &nbsp;</p><p> &nbsp; - Must durably queue all incoming requests. &nbsp;</p><p>3. RESTful API Endpoint: &nbsp;</p><p> &nbsp; - Devices communicate via HTTP(S). &nbsp;</p><p> Why Option B is Best? &nbsp;</p><p>- API Gateway HTTP API: &nbsp;</p><p> &nbsp;- Low-latency RESTful interface (cheaper/faster than REST API). &nbsp;</p><p> &nbsp;- Direct SQS Integration: &nbsp;</p><p> &nbsp; &nbsp;- Posts device data directly to SQS (no intermediate compute). &nbsp;</p><p> &nbsp; &nbsp;- SQS guarantees no data loss (messages persist until processed). &nbsp;</p><p>- Lambda for Processing: &nbsp;</p><p> &nbsp;- Auto-scales with queue depth (handles bursts seamlessly). &nbsp;</p><p> &nbsp;- Serverless: No EC2 management overhead. &nbsp;</p><p> Architecture Flow: &nbsp;</p><p>1. Devices → API Gateway HTTP API → SQS Queue (buffers bursts). &nbsp;</p><p>2. SQS → Lambda (processes messages, calculates sustainability index). &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- A (ALB → SQS): &nbsp;</p><p> &nbsp;- ALB can’t natively integrate with SQS (requires Lambda proxy). &nbsp;</p><p> &nbsp;- ECS Fargate is overkill (Lambda is simpler for message processing). &nbsp;</p><p>- C (API Gateway → EC2): &nbsp;</p><p> &nbsp;- EC2 scaling lags behind sudden traffic spikes (risk of data loss). &nbsp;</p><p>- D (CloudFront → Kinesis): &nbsp;</p><p> &nbsp;- Kinesis is expensive for simple queuing (SQS is cheaper). &nbsp;</p><p> &nbsp;- CloudFront doesn’t integrate with Kinesis directly. &nbsp;</p><p> Implementation Steps: &nbsp;</p><p>1. Create SQS Queue: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws sqs create-queue --queue-name device-data</p><p> &nbsp; ``` &nbsp;</p><p>2. Configure API Gateway HTTP API: &nbsp;</p><p> &nbsp; - Add SQS integration (POST to `https://sqs.region.amazonaws.com/account-id/queue-name`). &nbsp;</p><p>3. Deploy Lambda Consumer: &nbsp;</p><p> &nbsp; ```python</p><p> &nbsp; def lambda_handler(event, context):</p><p> &nbsp; &nbsp; &nbsp; for record in event['Records']:</p><p> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; process_data(record['body'])</p><p> &nbsp; ``` &nbsp;</p><p> Cost Efficiency: &nbsp;</p><p>| Service &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | Cost Advantage &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp;</p><p>|-------------------|--------------------------------------------| &nbsp;</p><p>| API Gateway &nbsp; | $1.00/million requests (HTTP API pricing). | &nbsp;</p><p>| SQS &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | $0.40/million requests. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp;</p><p>| Lambda &nbsp; &nbsp; &nbsp; &nbsp;| $0.20/million invocations. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | &nbsp;</p><p> Conclusion: &nbsp;</p><p>B is the only solution that: &nbsp;</p><p>- Handles spiky traffic (SQS buffer). &nbsp;</p><p>- Prevents data loss (SQS durability). &nbsp;</p><p>- Scales cost-effectively (serverless). &nbsp;</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "aec99a38c5644afb8661bf9ed4e470c5",
      "questionNumber": 181,
      "type": "single",
      "content": "<p>uestion #181</p><p>A company is designing its network configuration in the AWS Cloud. The company uses AWS Organizations to manage a multi-account setup. The company has three OUs. Each OU contains more than 100 AWS accounts. Each account has a single VPC, and all the VPCs in each OU are in the same AWS Region.</p><p><br></p><p>The CIDR ranges for all the AWS accounts do not overlap. The company needs to implement a solution in which VPCs in the same OU can communicate with each other but cannot communicate with VPCs in other OUs.</p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an AWS CloudFormation stack set that establishes VPC peering between accounts in each OU. Provision the stack set in each OU."
        },
        {
          "label": "B",
          "content": "In each OU, create a dedicated networking account that has a single VPC. Share this VPC with all the other accounts in the OU by using AWS Resource Access Manager (AWS RAM). Create a VPC peering connection between the networking account and each account in the OU."
        },
        {
          "label": "C",
          "content": "Provision a transit gateway in an account in each OU. Share the transit gateway across the organization by using AWS Resource Access Manager (AWS RAM). Create transit gateway VPC attachments for each VPC."
        },
        {
          "label": "D",
          "content": "In each OU, create a dedicated networking account that has a single VPC. Establish a VPN connection between the networking account and the other accounts in the OU. Use third-party routing software to route transitive traffic between the VPCs."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the scenario with three OUs each containing more than 100 AWS accounts with non-overlapping CIDR ranges, the solution that will meet the requirements with the LEAST operational overhead is:</p><p>C. Provision a transit gateway in an account in each OU. Share the transit gateway across the organization by using AWS Resource Access Manager (AWS RAM). Create transit gateway VPC attachments for each VPC.</p><p>This is the correct choice because:</p><p>Transit Gateway acts as a network transit hub that can connect thousands of VPCs, significantly reducing the number of connections required compared to VPC peering.</p><p>With VPC peering, the number of connections grows exponentially as the number of VPCs increases. For 100+ accounts per OU, this would mean thousands of individual peering connections to manage within each OU, creating substantial operational overhead.</p><p>Using AWS Resource Access Manager (AWS RAM) to share the transit gateway across accounts within each OU simplifies management by centralizing the networking configuration.</p><p>Transit Gateway route tables provide granular control over routing between VPCs, making it easy to allow communication within an OU while preventing communication between OUs.</p><p>This approach is more scalable and future-proof, allowing for easy addition of new accounts or VPCs as the organization grows without requiring reconfiguration of existing connections.</p><p>Option A (VPC peering with CloudFormation stack sets) would create significant operational overhead because:</p><p>It would require managing n(n-1)/2 peering connections within each OU (where n is the number of VPCs)</p><p>For 100+ accounts per OU, this means thousands of peering connections per OU</p><p>Each new account added would require new peering connections with all existing accounts</p><p>VPC peering doesn't support transitive routing, making the configuration more complex</p><p>The Transit Gateway approach with AWS RAM provides a more centralized, manageable, and scalable solution with significantly less operational overhead for this large multi-account environment.</p><p>Sources</p><p>Communicating across VPCs and AWS Regions - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/secure-outbound-network-traffic/vpc-region-communication.html）</p><p>Transit gateway attachment configuration | AWS re:Post （https://repost.aws/questions/QUvX0QyO3pRQWkBQspwcgVKg/transit-gateway-attachment-configuration）</p><p>how to Connect VPCs with Overlapping CIDRs Across Accounts | AWS re:Post （https://repost.aws/questions/QU0z4ZmjEESRuDr1K1_ZYjYw/how-to-connect-vpcs-with-overlapping-cidrs-across-accounts）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "2d4a3649bbce4261a1161b217d409ab4",
      "questionNumber": 182,
      "type": "single",
      "content": "<p>Question #182</p><p>A company is migrating an application to AWS. It wants to use fully managed services as much as possible during the migration. The company needs to store large important documents within the application with the following requirements:</p><p><br></p><p>1. The data must be highly durable and available</p><p>2. The data must always be encrypted at rest and in transit</p><p>3. The encryption key must be managed by the company and rotated periodically</p><p><br></p><p>Which of the following solutions should the solutions architect recommend?</p>",
      "options": [
        {
          "label": "A",
          "content": "Deploy the storage gateway to AWS in file gateway mode. Use Amazon EBS volume encryption using an AWS KMS key to encrypt the storage gateway volumes."
        },
        {
          "label": "B",
          "content": "Use Amazon S3 with a bucket policy to enforce HTTPS for connections to the bucket and to enforce server-side encryption and AWS KMS for object encryption."
        },
        {
          "label": "C",
          "content": "Amazon DynamoDB with SSL to connect to DynamoDB. Use an AWS KMS key to encrypt DynamoDB objects at rest."
        },
        {
          "label": "D",
          "content": "Deploy instances with Amazon EBS volumes attached to store this data. Use EBS volume encryption using an AWS KMS key to encrypt the data."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the requirements for storing large important documents with high durability, availability, encryption at rest and in transit, and customer-managed encryption keys that can be rotated periodically, the answer is:</p><p>B. Use Amazon S3 with a bucket policy to enforce HTTPS for connections to the bucket and to enforce server-side encryption and AWS KMS for object encryption.</p><p>This is the correct choice because:</p><p>Amazon S3 provides industry-leading durability of 99.999999999% (11 9's), meeting the requirement for high durability. It also offers high availability across multiple Availability Zones.</p><p>S3 bucket policies can be configured to enforce HTTPS (TLS) for all connections, ensuring data is encrypted in transit.</p><p>S3 supports server-side encryption with AWS KMS keys (SSE-KMS), allowing the company to manage their own encryption keys.</p><p>AWS KMS provides key rotation capabilities, allowing the company to periodically rotate their encryption keys while maintaining access to their encrypted data.</p><p>S3 is a fully managed service, aligning with the company's requirement to use fully managed services as much as possible.</p><p>S3 is optimized for storing large objects like documents, making it ideal for this use case.</p><p>Option A (Storage Gateway in file gateway mode) is not the best choice because it's primarily designed for hybrid cloud storage scenarios and adds unnecessary complexity when the goal is to use fully managed services.</p><p>Option C (DynamoDB with SSL and KMS) is not optimal because DynamoDB is designed for structured data and key-value pairs, not for storing large documents. While it supports encryption features, it's not the most appropriate service for this specific use case.</p><p>Amazon S3 with the specified security configurations provides the best combination of durability, availability, encryption capabilities, and management simplicity for storing large important documents.</p><p>Sources</p><p>Serverless data storage options - AWS Serverless Multi-Tier Architectures with Amazon API Gateway and AWS Lambda （https://docs.aws.amazon.com/whitepapers/latest/serverless-multi-tier-architectures-api-gateway-lambda/serverless-data-storage-options.html）</p><p>Storage - AWS Well-Architected Framework (2023-04-10) （https://docs.aws.amazon.com/wellarchitected/2023-04-10/framework/perf-storage.html）</p><p>DynamoDB encryption at rest - Amazon DynamoDB （https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/EncryptionAtRest.html）</p><p>Choosing AWS services for data protection - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/backup-recovery/services.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "2cc2270340bc41d292b9f3815b9da2b9",
      "questionNumber": 183,
      "type": "single",
      "content": "<p>Question #183 <br>A company’s public API runs as tasks on Amazon Elastic Container Service (Amazon ECS). The tasks run on AWS Fargate behind an Application Load Balancer (ALB) and are configured with Service Auto Scaling for the tasks based on CPU utilization. This service has been running well for several months.</p><p><br></p><p>Recently, API performance slowed down and made the application unusable. The company discovered that a significant number of SQL injection attacks had occurred against the API and that the API service had scaled to its maximum amount.</p><p><br></p><p>A solutions architect needs to implement a solution that prevents SQL injection attacks from reaching the ECS API service. The solution must allow legitimate traffic through and must maximize operational efficiency.</p><p><br></p><p>Which solution meets these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a new AWS WAF web ACL to monitor the HTTP requests and HTTPS requests that are forwarded to the ALB in front of the ECS tasks."
        },
        {
          "label": "B",
          "content": "Create a new AWS WAF Bot Control implementation. Add a rule in the AWS WAF Bot Control managed rule group to monitor traffic and allow only legitimate traffic to the ALB in front of the ECS tasks."
        },
        {
          "label": "C",
          "content": "Create a new AWS WAF web ACL. Add a new rule that blocks requests that match the SQL database rule group. Set the web ACL to allow all other traffic that does not match those rules. Attach the web ACL to the ALB in front of the ECS tasks."
        },
        {
          "label": "D",
          "content": "Create a new AWS WAF web ACL. Create a new empty IP set in AWS WAF. Add a new rule to the web ACL to block requests that originate from IP addresses in the new IP set. Create an AWS Lambda function that scrapes the API logs for IP addresses that send SQL injection attacks, and add those IP addresses to the IP set. Attach the web ACL to the ALB in front of the ECS tasks."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p><br></p><p> Answer: &nbsp;</p><p>C. Create a new AWS WAF web ACL. Add a new rule that blocks requests that match the SQL database rule group. Set the web ACL to allow all other traffic that does not match those rules. Attach the web ACL to the ALB in front of the ECS tasks. &nbsp;</p><p> Explanation: &nbsp;</p><p> Key Requirements: &nbsp;</p><p>1. Prevent SQL Injection Attacks: &nbsp;</p><p> &nbsp; - Block malicious SQL queries before they reach ECS tasks. &nbsp;</p><p>2. Allow Legitimate Traffic: &nbsp;</p><p> &nbsp; - Ensure valid API requests pass through. &nbsp;</p><p>3. Operational Efficiency: &nbsp;</p><p> &nbsp; - Use managed services (minimize custom code/maintenance). &nbsp;</p><p> Why Option C is Best? &nbsp;</p><p>- AWS WAF SQL Database Rule Group: &nbsp;</p><p> &nbsp;- Predefined rules to block common SQLi patterns (e.g., `' OR 1=1 --`). &nbsp;</p><p> &nbsp;- No manual tuning required (AWS manages rule updates). &nbsp;</p><p>- Web ACL Configuration: &nbsp;</p><p> &nbsp;- Block SQLi requests + allow all other traffic (default action). &nbsp;</p><p> &nbsp;- Attach to ALB (frontend protection). &nbsp;</p><p>- Zero Impact on Scaling: &nbsp;</p><p> &nbsp;- Malicious requests are blocked before consuming ECS resources. &nbsp;</p><p> Implementation Steps: &nbsp;</p><p>1. Create Web ACL: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws wafv2 create-web-acl \\</p><p> &nbsp; &nbsp; --name SQLi-Protection \\</p><p> &nbsp; &nbsp; --scope REGIONAL \\</p><p> &nbsp; &nbsp; --default-action Allow={} \\</p><p> &nbsp; &nbsp; --visibility-config SampledRequestsEnabled=true,CloudWatchMetricsEnabled=true</p><p> &nbsp; ``` &nbsp;</p><p>2. Add SQLi Rule: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws wafv2 create-rule-group \\</p><p> &nbsp; &nbsp; --name SQLi-Rules \\</p><p> &nbsp; &nbsp; --scope REGIONAL \\</p><p> &nbsp; &nbsp; --capacity 100 \\</p><p> &nbsp; &nbsp; --rules '[</p><p> &nbsp; &nbsp; &nbsp; {</p><p> &nbsp; &nbsp; &nbsp; &nbsp; \"Name\": \"SQLi-Block\",</p><p> &nbsp; &nbsp; &nbsp; &nbsp; \"Priority\": 1,</p><p> &nbsp; &nbsp; &nbsp; &nbsp; \"Statement\": { \"ManagedRuleGroupStatement\": { \"VendorName\": \"AWS\", \"Name\": \"AWSManagedRulesSQLiRuleSet\" } },</p><p> &nbsp; &nbsp; &nbsp; &nbsp; \"Action\": { \"Block\": {} },</p><p> &nbsp; &nbsp; &nbsp; &nbsp; \"VisibilityConfig\": { \"SampledRequestsEnabled\": true, \"CloudWatchMetricsEnabled\": true }</p><p> &nbsp; &nbsp; &nbsp; }</p><p> &nbsp; &nbsp; ]'</p><p> &nbsp; ``` &nbsp;</p><p>3. Attach to ALB: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws wafv2 associate-web-acl \\</p><p> &nbsp; &nbsp; --web-acl-arn arn:aws:wafv2:us-east-1:123456789012:regional/webacl/SQLi-Protection \\</p><p> &nbsp; &nbsp; --resource-arn arn:aws:elasticloadbalancing:us-east-1:123456789012:loadbalancer/app/my-alb/1234567890123456</p><p> &nbsp; ``` &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- A (Monitor Only): &nbsp;</p><p> &nbsp;- Doesn’t block attacks (only logs them). &nbsp;</p><p>- B (Bot Control): &nbsp;</p><p> &nbsp;- Bot Control targets scrapers, not SQLi (different threat). &nbsp;</p><p>- D (IP Blocking): &nbsp;</p><p> &nbsp;- Reactive: Requires manual Lambda to update IPs (inefficient). &nbsp;</p><p> &nbsp;- Easily bypassed (attackers change IPs). &nbsp;</p><p> Cost Efficiency: &nbsp;</p><p>- AWS WAF: $5/web ACL/month + $1/rule/month (SQLi rule group is free). &nbsp;</p><p>- No EC2/Lambda costs (unlike Option D). &nbsp;</p><p> Conclusion: &nbsp;</p><p>C is the only solution that: &nbsp;</p><p>- Blocks SQLi proactively (managed rules). &nbsp;</p><p>- Preserves scaling (no resource waste on attacks). &nbsp;</p><p>- Requires no maintenance (fully managed). &nbsp;</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "20ebee8090584af3bf0d60a0c2946bf4",
      "questionNumber": 184,
      "type": "single",
      "content": "<p>Question #184</p><p>An environmental company is deploying sensors in major cities throughout a country to measure air quality. The sensors connect to AWS IoT Core to ingest timeseries data readings. The company stores the data in Amazon DynamoDB.</p><p><br></p><p>For business continuity, the company must have the ability to ingest and store data in two AWS Regions.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an Amazon Route 53 alias failover routing policy with values for AWS IoT Core data endpoints in both Regions. Migrate data to Amazon Aurora global tables."
        },
        {
          "label": "B",
          "content": "Create a domain configuration for AWS IoT Core in each Region. Create an Amazon Route 53 latency-based routing policy. Use AWS IoT Core data endpoints in both Regions as values. Migrate the data to Amazon MemoryDB for Redis and configure cross-Region replication."
        },
        {
          "label": "C",
          "content": "Create a domain configuration for AWS IoT Core in each Region. Create an Amazon Route 53 health check that evaluates domain configuration health. Create a failover routing policy with values for the domain name from the AWS IoT Core domain configurations. Update the DynamoDB table to a global table."
        },
        {
          "label": "D",
          "content": "Create an Amazon Route 53 latency-based routing policy. Use AWS IoT Core data endpoints in both Regions as values. Configure DynamoDB streams and cross-Region data replication."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the requirements for deploying IoT sensors that connect to AWS IoT Core and store data in DynamoDB across two AWS Regions for business continuity, the answer is:</p><p>C. Create a domain configuration for AWS IoT Core in each Region. Create an Amazon Route 53 health check that evaluates domain configuration health. Create a failover routing policy with values for the domain name from the AWS IoT Core domain configurations. Update the DynamoDB table to a global table.</p><p>This is the correct choice because:</p><p>Domain configurations for AWS IoT Core allow you to create custom endpoints in each region, which is necessary for multi-region IoT deployments. This gives the sensors a consistent endpoint to connect to while allowing traffic to be routed to different regions.</p><p>Route 53 health checks can monitor the health of the IoT Core endpoints in each region, enabling automatic failover when issues are detected in the primary region.</p><p>The failover routing policy with the domain names from the IoT Core domain configurations ensures that IoT devices will connect to the healthy endpoint, maintaining data ingestion capabilities even if one region experiences an outage.</p><p>DynamoDB global tables provide multi-region replication, ensuring that data is automatically replicated across both regions. This is critical for business continuity as it ensures data availability in both regions without requiring custom replication logic.</p><p>Option A is incorrect because it mentions migrating data to Amazon Aurora global tables, which is not appropriate for this use case. The requirement specifically mentions storing data in DynamoDB, not Aurora. Additionally, simply creating a Route 53 alias failover policy without proper domain configurations for IoT Core would not provide the necessary infrastructure for multi-region IoT data ingestion.</p><p>The solution in option C provides a comprehensive approach to ensuring business continuity for both data ingestion (through IoT Core with domain configurations and Route 53 failover) and data storage (through DynamoDB global tables), meeting all the stated requirements.</p><p>Sources</p><p>How to implement a disaster recovery solution for IoT platforms on AWS | The Internet of Things on AWS – Official Blog （https://aws.amazon.com/cn/blogs/iot/how-to-implement-a-disaster-recovery-solution-for-iot-platforms-on-aws/）</p><p>Migrating connected device fleets to AWS with Custom Domains for AWS IoT Core | The Internet of Things on AWS – Official Blog （https://aws.amazon.com/cn/blogs/iot/migrating-devices-aws-iot-custom-domains/）</p><p>Community | Building a Multi-region Serverless IoT system （https://community.aws/content/2fjFMxLIwHkcuCy8ui7ya2KRbPt/building-a-multi-region-serverless-iot-system）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "d8908983af934a21800ac47cd2eb6697",
      "questionNumber": 185,
      "type": "single",
      "content": "<p>Question #185</p><p>A company uses AWS Organizations for a multi-account setup in the AWS Cloud. The company's finance team has a data processing application that uses AWS Lambda and Amazon DynamoDB. The company's marketing team wants to access the data that is stored in the DynamoDB table. </p><p><br></p><p>The DynamoDB table contains confidential data. The marketing team can have access to only specific attributes of data in the DynamoDB table. The finance team and the marketing team have separate AWS accounts.</p><p><br></p><p>What should a solutions architect do to provide the marketing team with the appropriate access to the DynamoDB table?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an SCP to grant the marketing team&#39;s AWS account access to the specific attributes of the DynamoDB table. Attach the SCP to the OU of the finance team."
        },
        {
          "label": "B",
          "content": "Create an IAM role in the finance team&#39;s account by using IAM policy conditions for specific DynamoDB attributes (fine-grained access control). Establish trust with the marketing team&#39;s account. In the marketing team&#39;s account, create an IAM role that has permissions to assume the IAM role in the finance team&#39;s account."
        },
        {
          "label": "C",
          "content": "Create a resource-based IAM policy that includes conditions for specific DynamoDB attributes (fine-grained access control). Attach the policy to the DynamoDB table. In the marketing team&#39;s account, create an IAM role that has permissions to access the DynamoDB table in the finance team&#39;s account."
        },
        {
          "label": "D",
          "content": "Create an IAM role in the finance team&#39;s account to access the DynamoDB table. Use an IAM permissions boundary to limit the access to the specific attributes. In the marketing team&#39;s account, create an IAM role that has permissions to assume the IAM role in the finance team&#39;s account."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>The correct answer is B. </p><p> Why Option B is Correct:</p><p>1. Fine-Grained Access Control with IAM Policy Conditions: &nbsp;</p><p> &nbsp; - The finance team's DynamoDB table contains confidential data, and the marketing team should only access specific attributes. &nbsp;</p><p> &nbsp; - An IAM role in the finance team's account can be configured with a policy that restricts access to only the required DynamoDB attributes using [fine-grained access control](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/specifying-conditions.html).</p><p>2. Cross-Account Access via Role Assumption: &nbsp;</p><p> &nbsp; - The marketing team's account needs to assume the IAM role in the finance team's account. &nbsp;</p><p> &nbsp; - The finance team's IAM role must have a trust policy allowing the marketing team's account to assume it. &nbsp;</p><p> &nbsp; - The marketing team's IAM role must have permissions to assume the role in the finance team's account (`sts:AssumeRole`).</p><p>3. Secure and Scalable Solution: &nbsp;</p><p> &nbsp; - This approach follows AWS best practices for cross-account access without exposing credentials. &nbsp;</p><p> &nbsp; - It ensures least privilege by restricting access to only the required DynamoDB attributes.</p><p> Why Other Options Are Incorrect:</p><p>- Option A: &nbsp;</p><p> &nbsp;- Service Control Policies (SCPs) are used to set guardrails (permission boundaries) for accounts in AWS Organizations. &nbsp;</p><p> &nbsp;- They cannot grant fine-grained DynamoDB attribute access. &nbsp;</p><p> &nbsp;- SCPs deny or allow services at a high level but do not manage DynamoDB attribute-level permissions.</p><p>- Option C: &nbsp;</p><p> &nbsp;- Resource-based policies (attached to DynamoDB) are not supported for DynamoDB (unlike S3 or SQS). &nbsp;</p><p> &nbsp;- DynamoDB access must be managed via IAM policies attached to roles/users, not directly on the table.</p><p>- Option D: &nbsp;</p><p> &nbsp;- Permissions boundaries define the maximum permissions an IAM entity can have but do not enforce fine-grained DynamoDB attribute filtering. &nbsp;</p><p> &nbsp;- The marketing team would still need an IAM policy with conditions to restrict DynamoDB attribute access.</p><p> Conclusion: &nbsp;</p><p>Option B is the correct solution because it uses fine-grained IAM policies with conditions to restrict DynamoDB access and enables secure cross-account role assumption.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "4c0ec3899ed64230ab1ff8133a0ddaa4",
      "questionNumber": 186,
      "type": "multiple",
      "content": "<p>Question #186</p><p>A solutions architect is creating an application that stores objects in an Amazon S3 bucket. The solutions architect must deploy the application in two AWS Regions that will be used simultaneously. The objects in the two S3 buckets must remain synchronized with each other.</p><p>Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose three.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an S3 Multi-Region Access Point. Change the application to refer to the Multi-Region Access Point."
        },
        {
          "label": "B",
          "content": "Configure two-way S3 Cross-Region Replication (CRR) between the two S3 buckets."
        },
        {
          "label": "C",
          "content": "Modify the application to store objects in each S3 bucket."
        },
        {
          "label": "D",
          "content": "Create an S3 Lifecycle rule for each S3 bucket to copy objects from one S3 bucket to the other S3 bucket."
        },
        {
          "label": "E",
          "content": "Enable S3 Versioning for each S3 bucket."
        },
        {
          "label": "F",
          "content": "Configure an event notification for each S3 bucket to invoke an AWS Lambda function to copy objects from one S3 bucket to the other S3 bucket."
        }
      ],
      "correctAnswer": "ABE",
      "explanation": "<p>Based on the requirement to create an application that stores objects in S3 buckets across two AWS Regions that must remain synchronized with the least operational overhead, the correct combination of steps is:</p><p>A. Create an S3 Multi-Region Access Point. Change the application to refer to the Multi-Region Access Point. </p><p>B. Configure two-way S3 Cross-Region Replication (CRR) between the two S3 buckets. </p><p>E. Enable S3 Versioning for each S3 bucket.</p><p>Here's why these three steps together provide the optimal solution with the least operational overhead:</p><p>S3 Multi-Region Access Point (Option A):</p><p>Provides a single global endpoint for the application to use when accessing objects</p><p>Automatically routes requests to the appropriate region based on network conditions and availability</p><p>Simplifies application code as it only needs to reference one endpoint instead of managing multiple regional endpoints</p><p>Reduces operational overhead by eliminating the need for custom routing logic</p><p>Two-way S3 Cross-Region Replication (Option B):</p><p>Automatically synchronizes objects between the two S3 buckets in different regions</p><p>Ensures that objects created or updated in either region are replicated to the other region</p><p>Provides a fully managed replication service with no custom code required</p><p>Maintains consistency between buckets with minimal operational effort</p><p>Enable S3 Versioning (Option E):</p><p>Required prerequisite for both S3 Cross-Region Replication and Multi-Region Access Points</p><p>Ensures that all versions of objects are maintained and properly replicated</p><p>Provides additional data protection benefits</p><p>The other options would increase operational overhead:</p><p>Option C (Modify the application to store objects in each S3 bucket) would require custom application logic to write to multiple buckets, handle failures, and ensure consistency, creating significant operational overhead.</p><p>Option D (Create S3 Lifecycle rules to copy objects) is not designed for real-time synchronization and would be less efficient than using the purpose-built CRR feature.</p><p>The combination of Multi-Region Access Points, two-way Cross-Region Replication, and S3 Versioning provides a fully managed solution that keeps the buckets synchronized with minimal operational overhead while allowing the application to use both regions simultaneously.</p><p>Sources</p><p>Object Storage Features – Amazon S3 （https://aws.amazon.com/cn/s3/features/replication/）</p><p>Amazon S3 Multi-Region Access Points failover controls - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/MrapFailover.html）</p><p>Data protection in Amazon S3 - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/data-protection.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "026d76e040f743378bde3f4c55912744",
      "questionNumber": 187,
      "type": "multiple",
      "content": "<p>Question #187</p><p>A company has an IoT platform that runs in an on-premises environment. The platform consists of a server that connects to IoT devices by using the MQTT protocol. The platform collects telemetry data from the devices at least once every 5 minutes. The platform also stores device metadata in a MongoDB cluster.</p><p>An application that is installed on an on-premises machine runs periodic jobs to aggregate and transform the telemetry and device metadata. The application creates reports that users view by using another web application that runs on the same on-premises machine. The periodic jobs take 120-600 seconds to run. However, the web application is always running.</p><p>The company is moving the platform to AWS and must reduce the operational overhead of the stack.</p><p>Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose three.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Use AWS Lambda functions to connect to the IoT devices."
        },
        {
          "label": "B",
          "content": "Configure the IoT devices to publish to AWS IoT Core."
        },
        {
          "label": "C",
          "content": "Write the metadata to a self-managed MongoDB database on an Amazon EC2 instance."
        },
        {
          "label": "D",
          "content": "Write the metadata to Amazon DocumentDB (with MongoDB compatibility)."
        },
        {
          "label": "E",
          "content": "Use AWS Step Functions state machines with AWS Lambda tasks to prepare the reports and to write the reports to Amazon S3. Use Amazon CloudFront with an S3 origin to serve the reports."
        },
        {
          "label": "F",
          "content": "Use an Amazon Elastic Kubernetes Service (Amazon EKS) cluster with Amazon EC2 instances to prepare the reports. Use an ingress controller in the EKS cluster to serve the reports."
        }
      ],
      "correctAnswer": "BDE",
      "explanation": "<p><br></p><p>The correct combination of steps to meet the requirements with the least operational overhead is: &nbsp;</p><p> B, D, E &nbsp;</p><p> Detailed Explanation: &nbsp;</p><p> 1. (B) Configure the IoT devices to publish to AWS IoT Core &nbsp;</p><p>- Why? &nbsp;</p><p> &nbsp;- AWS IoT Core is a fully managed MQTT broker that eliminates the need to manage an on-premises server. &nbsp;</p><p> &nbsp;- IoT devices can directly publish telemetry data to AWS IoT Core, reducing infrastructure management. &nbsp;</p><p> &nbsp;- Supports MQTT, HTTPS, and other protocols natively. &nbsp;</p><p> 2. (D) Write the metadata to Amazon DocumentDB (with MongoDB compatibility) &nbsp;</p><p>- Why? &nbsp;</p><p> &nbsp;- Amazon DocumentDB is a fully managed MongoDB-compatible database. &nbsp;</p><p> &nbsp;- Eliminates the operational burden of managing a self-hosted MongoDB cluster (Option C). &nbsp;</p><p> &nbsp;- Provides high availability, scalability, and automatic backups. &nbsp;</p><p> 3. (E) Use AWS Step Functions with AWS Lambda to prepare reports and store them in Amazon S3, then serve via CloudFront &nbsp;</p><p>- Why? &nbsp;</p><p> &nbsp;- AWS Step Functions orchestrates serverless workflows, making it easy to manage periodic jobs (120-600 sec runtime). &nbsp;</p><p> &nbsp;- AWS Lambda runs the transformation logic without managing servers. &nbsp;</p><p> &nbsp;- Amazon S3 + CloudFront provides a scalable, low-cost way to serve reports globally. &nbsp;</p><p> &nbsp;- Eliminates the need for a constantly running web server (unlike Option F, which requires managing EKS). &nbsp;</p><p>---</p><p> Why Other Options Are Not the Best Fit: &nbsp;</p><p>- A (Lambda to connect to IoT devices): &nbsp;</p><p> &nbsp;- Lambda is not designed for persistent MQTT connections. &nbsp;</p><p> &nbsp;- AWS IoT Core (Option B) is the correct managed solution. &nbsp;</p><p>- C (Self-managed MongoDB on EC2): &nbsp;</p><p> &nbsp;- Requires manual management of backups, scaling, and patching. &nbsp;</p><p> &nbsp;- Amazon DocumentDB (Option D) is fully managed and a better choice. &nbsp;</p><p>- F (EKS with EC2 instances): &nbsp;</p><p> &nbsp;- Overly complex for report generation and serving. &nbsp;</p><p> &nbsp;- Higher operational overhead than serverless (Step Functions + Lambda + S3). &nbsp;</p><p>---</p><p> Final Answer: &nbsp;</p><p>✅ B, D, E (AWS IoT Core + Amazon DocumentDB + Step Functions/Lambda/S3/CloudFront) &nbsp;</p><p>This combination provides a fully managed, serverless architecture with the least operational overhead.</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "7ca432e874dd45e8899f9869181e454c",
      "questionNumber": 188,
      "type": "single",
      "content": "<p>Question #188</p><p>A global manufacturing company plans to migrate the majority of its applications to AWS. However, the company is concerned about applications that need to remain within a specific country or in the company's central on-premises data center because of data regulatory requirements or requirements for latency of single-digit milliseconds. The company also is concerned about the applications that it hosts in some of its factory sites, where limited network infrastructure exists.</p><p><br></p><p>The company wants a consistent developer experience so that its developers can build applications once and deploy on-premises, in the cloud, or in a hybrid architecture. The developers must be able to use the same tools, APIs, and services that are familiar to them.</p><p><br></p><p>Which solution will provide a consistent hybrid experience to meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Migrate all applications to the closest AWS Region that is compliant. Set up an AWS Direct Connect connection between the central on-premises data center and AWS. Deploy a Direct Connect gateway."
        },
        {
          "label": "B",
          "content": "Use AWS Snowball Edge Storage Optimized devices for the applications that have data regulatory requirements or requirements for latency of single-digit milliseconds. Retain the devices on premises. Deploy AWS Wavelength to host the workloads in the factory sites."
        },
        {
          "label": "C",
          "content": "Install AWS Outposts for the applications that have data regulatory requirements or requirements for latency of single-digit milliseconds. Use AWS Snowball Edge Compute Optimized devices to host the workloads in the factory sites."
        },
        {
          "label": "D",
          "content": "Migrate the applications that have data regulatory requirements or requirements for latency of single-digit milliseconds to an AWS Local Zone. Deploy AWS Wavelength to host the workloads in the factory sites."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>The correct answer is C. Here's the detailed explanation:</p><p> Why Option C is Correct:</p><p>1. AWS Outposts for Low-Latency & Regulatory Requirements:</p><p> &nbsp; - Problem: Some applications must stay on-premises due to data residency laws or single-digit millisecond latency needs.</p><p> &nbsp; - Solution: AWS Outposts brings AWS infrastructure (compute, storage, database, etc.) to the on-premises data center.</p><p> &nbsp; &nbsp; - Provides consistent AWS experience (same APIs, tools, services as in the cloud).</p><p> &nbsp; &nbsp; - Ensures low-latency access for local applications.</p><p> &nbsp; &nbsp; - Helps meet regulatory compliance by keeping data in a specific location.</p><p>2. AWS Snowball Edge Compute Optimized for Factory Sites:</p><p> &nbsp; - Problem: Factory sites have limited network infrastructure but still need to run workloads.</p><p> &nbsp; - Solution: Snowball Edge Compute Optimized provides AWS compute & storage in a rugged, portable device.</p><p> &nbsp; &nbsp; - Can run Lambda, EC2, and EBS locally without continuous internet.</p><p> &nbsp; &nbsp; - Syncs data back to AWS when connectivity is available.</p><p> &nbsp; &nbsp; - Ideal for remote or disconnected environments.</p><p>3. Consistent Developer Experience:</p><p> &nbsp; - Both Outposts and Snowball Edge integrate with AWS services (e.g., EKS, ECS, RDS).</p><p> &nbsp; - Developers use the same AWS tools (CLI, SDKs, CloudFormation) whether deploying on-premises or in AWS Regions.</p><p>---</p><p> Why Other Options Are Incorrect:</p><p>- A (Migrate to closest AWS Region + Direct Connect):</p><p> &nbsp;- Fails for applications needing on-premises hosting (due to regulations or latency).</p><p> &nbsp;- Direct Connect improves connectivity but does not solve data residency or ultra-low latency requirements.</p><p>- B (Snowball Edge Storage Optimized + Wavelength):</p><p> &nbsp;- Snowball Edge Storage Optimized is for data transfer/storage, not running persistent workloads.</p><p> &nbsp;- AWS Wavelength embeds AWS in 5G networks (for mobile apps), not factory sites with limited networking.</p><p>- D (Local Zone + Wavelength):</p><p> &nbsp;- AWS Local Zones extend AWS Regions to metro areas but still rely on cloud connectivity (not fully on-premises).</p><p> &nbsp;- Wavelength is for 5G mobile apps, not factory environments.</p><p>---</p><p> Key Takeaways:</p><p>✅ For on-premises/low-latency needs → AWS Outposts &nbsp;</p><p>✅ For remote factories → Snowball Edge Compute Optimized &nbsp;</p><p>✅ Consistent AWS experience across hybrid environments &nbsp;</p><p>Final Answer: C (AWS Outposts + Snowball Edge Compute Optimized)</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "088a8769a68f477f8c77d23f236098b5",
      "questionNumber": 189,
      "type": "multiple",
      "content": "<p>Question #189</p><p>A company is updating an application that customers use to make online orders. The number of attacks on the application by bad actors has increased recently.</p><p>The company will host the updated application on an Amazon Elastic Container Service (Amazon ECS) cluster. The company will use Amazon DynamoDB to store application data. A public Application Load Balancer (ALB) will provide end users with access to the application. The company must prevent attacks and ensure business continuity with minimal service interruptions during an ongoing attack.</p><p>Which combination of steps will meet these requirements MOST cost-effectively? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an Amazon CloudFront distribution with the ALB as the origin. Add a custom header and random value on the CloudFront domain. Configure the ALB to conditionally forward traffic if the header and value match."
        },
        {
          "label": "B",
          "content": "Deploy the application in two AWS Regions. Configure Amazon Route 53 to route to both Regions with equal weight."
        },
        {
          "label": "C",
          "content": "Configure auto scaling for Amazon ECS tasks. Create a DynamoDB Accelerator (DAX) cluster."
        },
        {
          "label": "D",
          "content": "Configure Amazon ElastiCache to reduce overhead on DynamoDB."
        },
        {
          "label": "E",
          "content": "Deploy an AWS WAF web ACL that includes an appropriate rule group. Associate the web ACL with the Amazon CloudFront distribution."
        }
      ],
      "correctAnswer": "AE",
      "explanation": "<p>The correct combination of steps to prevent attacks and ensure business continuity in the most cost-effective way is: &nbsp;</p><p> A & E &nbsp;</p><p> Detailed Explanation: &nbsp;</p><p> 1. (A) Use Amazon CloudFront with a Custom Header for Security &nbsp;</p><p>- Why? &nbsp;</p><p> &nbsp;- Prevents direct access to the ALB by forcing traffic through CloudFront (reduces DDoS risks). &nbsp;</p><p> &nbsp;- Custom header validation ensures only CloudFront can forward traffic to the ALB, blocking malicious requests. &nbsp;</p><p> &nbsp;- Cost-effective: CloudFront provides built-in DDoS protection (AWS Shield Standard) and caching to reduce backend load. &nbsp;</p><p> 2. (E) Deploy AWS WAF with a Rule Group to Block Attacks &nbsp;</p><p>- Why? &nbsp;</p><p> &nbsp;- AWS WAF protects against common web exploits (SQLi, XSS, bots, etc.). &nbsp;</p><p> &nbsp;- Rule groups (e.g., AWS Managed Rules or custom rules) filter malicious traffic before it reaches the ALB. &nbsp;</p><p> &nbsp;- Cost-effective: Pay-as-you-go pricing (only for rules and requests processed). &nbsp;</p><p>---</p><p> Why Other Options Are Not the Best Fit: &nbsp;</p><p>- B (Multi-Region Deployment with Route 53): &nbsp;</p><p> &nbsp;- Overkill for most attacks—adds complexity and high cost (duplicate infrastructure in 2 regions). &nbsp;</p><p> &nbsp;- Business continuity ≠ multi-region unless the app requires extreme HA (not specified here). &nbsp;</p><p>- C (ECS Auto Scaling + DAX): &nbsp;</p><p> &nbsp;- Auto Scaling helps with load handling but does not prevent attacks. &nbsp;</p><p> &nbsp;- DAX improves DynamoDB performance but does not mitigate security risks. &nbsp;</p><p>- D (Amazon ElastiCache): &nbsp;</p><p> &nbsp;- Reduces DynamoDB load but does not protect against attacks. &nbsp;</p><p> &nbsp;- Not a security measure—better for performance optimization. &nbsp;</p><p>---</p><p> Final Answer: &nbsp;</p><p>✅ A & E (CloudFront with custom header + AWS WAF) &nbsp;</p><p>This combination: &nbsp;</p><p>✔ Blocks attacks (DDoS, bots, SQLi, etc.) &nbsp;</p><p>✔ Ensures business continuity (CloudFront absorbs traffic spikes) &nbsp;</p><p>✔ Minimizes cost (no unnecessary multi-region or caching overhead)</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "5b1d835599f944baa5e04d977d323b36",
      "questionNumber": 190,
      "type": "single",
      "content": "<p>Question #190</p><p>A company runs a web application on AWS. The web application delivers static content from an Amazon S3 bucket that is behind an Amazon CloudFront distribution. The application serves dynamic content by using an Application Load Balancer (ALB) that distributes requests to a fleet of Amazon EC2 instances in Auto Scaling groups. The application uses a domain name set up in Amazon Route 53.</p><p><br></p><p>Some users reported occasional issues when the users attempted to access the website during peak hours. An operations team found that the ALB sometimes returned HTTP 503 Service Unavailable errors. The company wants to display a custom error message page when these errors occur. The page should be displayed immediately for this error code.</p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Set up a Route 53 failover routing policy. Configure a health check to determine the status of the ALB endpoint and to fail over to the failover S3 bucket endpoint."
        },
        {
          "label": "B",
          "content": "Create a second CloudFront distribution and an S3 static website to host the custom error page. Set up a Route 53 failover routing policy. Use an active-passive configuration between the two distributions."
        },
        {
          "label": "C",
          "content": "Create a CloudFront origin group that has two origins. Set the ALB endpoint as the primary origin. For the secondary origin, set an S3 bucket that is configured to host a static website. Set up origin failover for the CloudFront distribution. Update the S3 static website to incorporate the custom error page."
        },
        {
          "label": "D",
          "content": "Create a CloudFront function that validates each HTTP response code that the ALB returns. Create an S3 static website in an S3 bucket. Upload the custom error page to the S3 bucket as a failover. Update the function to read the S3 bucket and to serve the error page to the end users."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p><br></p><p>The correct answer is C.</p><p> Why Option C is Correct:</p><p>1. CloudFront Origin Groups with Automatic Failover:</p><p> &nbsp; - Problem: The ALB occasionally returns HTTP 503 errors during peak traffic, and the company wants to immediately display a custom error page.</p><p> &nbsp; - Solution: &nbsp;</p><p> &nbsp; &nbsp; - Primary Origin: ALB (for dynamic content). &nbsp;</p><p> &nbsp; &nbsp; - Secondary Origin: S3 bucket (hosting a static error page). &nbsp;</p><p> &nbsp; &nbsp; - CloudFront automatically switches to the S3 origin when the ALB returns a 503 error, serving the custom error page without delays or manual intervention.</p><p>2. Least Operational Overhead:</p><p> &nbsp; - No additional Route 53 configurations (unlike Options A & B, which require failover policies). &nbsp;</p><p> &nbsp; - No custom scripting (unlike Option D, which requires CloudFront Functions). &nbsp;</p><p> &nbsp; - Fully managed by AWS—CloudFront handles failover automatically.</p><p>3. Immediate Error Handling:</p><p> &nbsp; - CloudFront caches the error page, ensuring quick delivery when the ALB fails. &nbsp;</p><p> &nbsp; - No dependency on health checks or DNS propagation delays (unlike Option A).</p><p>---</p><p> Why Other Options Are Not the Best Fit:</p><p>- A (Route 53 Failover + Health Checks): &nbsp;</p><p> &nbsp;- Slow failover (depends on health check intervals, typically 30+ seconds). &nbsp;</p><p> &nbsp;- DNS propagation delays can cause temporary outages. &nbsp;</p><p> &nbsp;- Higher operational overhead (managing health checks and failover policies). &nbsp;</p><p>- B (Second CloudFront Distribution + Route 53 Failover): &nbsp;</p><p> &nbsp;- Overly complex—requires managing two CloudFront distributions. &nbsp;</p><p> &nbsp;- Delayed failover due to DNS-based routing. &nbsp;</p><p> &nbsp;- More expensive (two active distributions). &nbsp;</p><p>- D (CloudFront Function + S3 Fallback): &nbsp;</p><p> &nbsp;- Custom code required (increases maintenance). &nbsp;</p><p> &nbsp;- Slower response (function must process each error before redirecting). &nbsp;</p><p> &nbsp;- Not fully automated (unlike native CloudFront failover). &nbsp;</p><p>---</p><p> Key Takeaways:</p><p>✅ CloudFront Origin Groups provide automatic, instant failover to S3 when the ALB fails. &nbsp;</p><p>✅ No custom code or DNS changes needed—minimal operational overhead. &nbsp;</p><p>✅ Cost-effective (uses existing S3 + CloudFront infrastructure). &nbsp;</p><p>Final Answer: C (CloudFront origin group with ALB + S3 failover)</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "6c48cd1ac5fc46039f1951e3307a4c6f",
      "questionNumber": 191,
      "type": "single",
      "content": "<p>Question #191</p><p>A company is planning to migrate an application to AWS. The application runs as a Docker container and uses an NFS version 4 file share.</p><p><br></p><p>A solutions architect must design a secure and scalable containerized solution that does not require provisioning or management of the underlying infrastructure.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Deploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type. Use Amazon Elastic File System (Amazon EFS) for shared storage. Reference the EFS file system ID, container mount point, and EFS authorization IAM role in the ECS task definition."
        },
        {
          "label": "B",
          "content": "Deploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type. Use Amazon FSx for Lustre for shared storage. Reference the FSx for Lustre file system ID, container mount point, and FSx for Lustre authorization IAM role in the ECS task definition."
        },
        {
          "label": "C",
          "content": "Deploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with the Amazon EC2 launch type and auto scaling turned on. Use Amazon Elastic File System (Amazon EFS) for shared storage. Mount the EFS file system on the ECS container instances. Add the EFS authorization IAM role to the EC2 instance profile."
        },
        {
          "label": "D",
          "content": "Deploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with the Amazon EC2 launch type and auto scaling turned on. Use Amazon Elastic Block Store (Amazon EBS) volumes with Multi-Attach enabled for shared storage. Attach the EBS volumes to ECS container instances. Add the EBS authorization IAM role to an EC2 instance profile."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Based on the requirements to migrate a Docker container application that uses NFS version 4 file share to a secure and scalable containerized solution on AWS without provisioning or managing the underlying infrastructure, the answer is:</p><p>A. Deploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type. Use Amazon ElasticFile System (Amazon EFS) for shared storage. Reference the EFS file system ID, container mount point, and EFS authorization IAM role in the ECS task definition.</p><p>This is the correct choice because:</p><p>Amazon ECS with Fargate launch type provides a fully managed, serverless container orchestration service that eliminates the need to provision or manage the underlying infrastructure, directly meeting the requirement for no infrastructure management.</p><p>Amazon EFS is a fully managed NFS file system that natively supports NFSv4, making it a direct replacement for the application's existing NFS version 4 file share requirement.</p><p>EFS integrates seamlessly with ECS Fargate tasks through task definitions, allowing containers to mount the file system and access shared data.</p><p>The solution is secure as it uses IAM roles for authorization to the EFS file system, implementing the principle of least privilege.</p><p>Both ECS Fargate and EFS are designed to scale automatically based on demand, meeting the scalability requirement.</p><p>Option B suggests using Amazon FSx for Lustre instead of EFS. While FSx for Lustre is a high-performance file system, it's primarily designed for compute-intensive workloads like high-performance computing, machine learning, and media processing. It would be overprovisioned for a standard application that simply requires NFS v4 compatibility. Additionally, FSx for Lustre is optimized for high throughput and IOPS rather than general-purpose file sharing, making it less suitable for a direct migration of a standard NFS workload.</p><p>Amazon EFS is purpose-built for general NFS workloads and provides a more straightforward, cost-effective solution for applications that require NFS compatibility without specialized performance requirements.</p><p>Sources</p><p>Chat examples - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/best-practices-code-generation/examples-chat.html）</p><p>Exploring AWS Container Services | AWS re:Post （https://repost.aws/articles/AR4s12umYqSdGKOpsimKGO7A/exploring-aws-container-services）</p><p>Run stateful workloads with persistent data storage by using Amazon EFS on Amazon EKS with AWS Fargate - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/run-stateful-workloads-with-persistent-data-storage-by-using-amazon-efs-on-amazon-eks-with-aws-fargate.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "f3d7275472944385808cb691a76c6f9d",
      "questionNumber": 192,
      "type": "single",
      "content": "<p>Question #192</p><p>A company is running an application in the AWS Cloud. The core business logic is running on a set of Amazon EC2 instances in an Auto Scaling group. An Application Load Balancer (ALB) distributes traffic to the EC2 instances. Amazon Route 53 record api.example.com is pointing to the ALB.</p><p><br></p><p>The company's development team makes major updates to the business logic. The company has a rule that when changes are deployed, only 10% of customers can receive the new logic during a testing window. A customer must use the same version of the business logic during the testing window.</p><p><br></p><p>How should the company deploy the updates to meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a second ALB, and deploy the new logic to a set of EC2 instances in a new Auto Scaling group. Configure the ALB to distribute traffic to the EC2 instances. Update the Route 53 record to use weighted routing, and point the record to both of the ALBs."
        },
        {
          "label": "B",
          "content": "Create a second target group that is referenced by the ALB. Deploy the new logic to EC2 instances in this new target group. Update the ALB listener rule to use weighted target groups. Configure ALB target group stickiness."
        },
        {
          "label": "C",
          "content": "Create a new launch configuration for the Auto Scaling group. Specify the launch configuration to use the AutoScalingRollingUpdate policy, and set the MaxBatchSize option to 10. Replace the launch configuration on the Auto Scaling group. Deploy the changes."
        },
        {
          "label": "D",
          "content": "Create a second Auto Scaling group that is referenced by the ALB. Deploy the new logic on a set of EC2 instances in this new Auto Scaling group. Change the ALB routing algorithm to least outstanding requests (LOR). Configure ALB session stickiness."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>The correct answer is B. Here’s the detailed explanation:</p><p> Why Option B is Correct:</p><p>1. Weighted Target Groups for Controlled Rollout (10% Traffic): &nbsp;</p><p> &nbsp; - The ALB supports weighted routing between target groups, allowing precise traffic splitting (90% old version, 10% new version). &nbsp;</p><p> &nbsp; - This meets the requirement that only 10% of customers receive the new logic during testing.</p><p>2. ALB Stickiness Ensures Consistent User Experience: &nbsp;</p><p> &nbsp; - Target group stickiness ensures that once a user is routed to a specific version (old or new), they continue to use the same version during the testing window. &nbsp;</p><p> &nbsp; - This prevents users from switching between versions unpredictably.</p><p>3. Minimal Operational Overhead: &nbsp;</p><p> &nbsp; - Uses the existing ALB (no need for a second ALB or Route 53 changes). &nbsp;</p><p> &nbsp; - No need to modify Auto Scaling groups or launch configurations (unlike Options A, C, and D). &nbsp;</p><p>---</p><p> Why Other Options Are Incorrect:</p><p>- A (Second ALB + Route 53 Weighted Routing): &nbsp;</p><p> &nbsp;- Overly complex—requires managing two ALBs and Route 53 weighted records. &nbsp;</p><p> &nbsp;- No built-in session stickiness in Route 53 (users might switch versions mid-session). &nbsp;</p><p>- C (Auto Scaling Rolling Update with MaxBatchSize=10): &nbsp;</p><p> &nbsp;- Does not guarantee 10% traffic—only controls how many instances update at once. &nbsp;</p><p> &nbsp;- No session stickiness—users may bounce between old and new versions. &nbsp;</p><p>- D (Second Auto Scaling Group + LOR Routing): &nbsp;</p><p> &nbsp;- Least Outstanding Requests (LOR) does not enforce 10% traffic split. &nbsp;</p><p> &nbsp;- Session stickiness alone isn’t enough—traffic distribution must be explicitly controlled. &nbsp;</p><p>---</p><p> Key Takeaways:</p><p>✅ Weighted target groups in ALB enable precise traffic splitting (90/10). &nbsp;</p><p>✅ ALB stickiness ensures users stay on the same version during testing. &nbsp;</p><p>✅ Simpler and more cost-effective than multi-ALB or Route 53 solutions. &nbsp;</p><p>Final Answer: B (ALB weighted target groups + stickiness)</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "d60b59c1bab5428aa1ff81ef8f9eb74a",
      "questionNumber": 193,
      "type": "single",
      "content": "Question #193<p>A large education company recently introduced Amazon Workspaces to provide access to internal applications across multiple universities. The company is storing user profiles on an Amazon FSx for Windows File Server file system. The file system is configured with a DNS alias and is connected to a self-managed Active Directory. As more users begin to use the Workspaces, login time increases to unacceptable levels. </p><p><br></p><p>An investigation reveals a degradation in performance of the file system. The company created the file system on HDD storage with a throughput of 16 MBps. A solutions architect must improve the performance of the file system during a defined maintenance window.</p><p><br></p><p><br></p><p>What should the solutions architect do to meet these requirements with the LEAST administrative effort?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use AWS Backup to create a point-in-time backup of the file system. Restore the backup to a new FSx for Windows File Server file system. Select SSD as the storage type. Select 32 MBps as the throughput capacity. When the backup and restore process is completed, adjust the DNS alias accordingly. Delete the original file system."
        },
        {
          "label": "B",
          "content": "Disconnect users from the file system. In the Amazon FSx console, update the throughput capacity to 32 MBps. Update the storage type to SSD. Reconnect users to the file system."
        },
        {
          "label": "C",
          "content": "Deploy an AWS DataSync agent onto a new Amazon EC2 instance. Create a task. Configure the existing file system as the source location. Configure a new FSx for Windows File Server file system with SSD storage and 32 MBps of throughput as the target location. Schedule the task. When the task is completed, adjust the DNS alias accordingly. Delete the original file system."
        },
        {
          "label": "D",
          "content": "Enable shadow copies on the existing file system by using a Windows PowerShell command. Schedule the shadow copy job to create a point-in-time backup of the file system. Choose to restore previous versions. Create a new FSx for Windows File Server file system with SSD storage and 32 MBps of throughput. When the copy job is completed, adjust the DNS alias. Delete the original file system."
        }
      ],
      "correctAnswer": "B",
      "explanation": "https://docs.aws.amazon.com/fsx/latest/WindowsGuide/managing-storage-type.html<p><br></p><p>SSD to HDD is impossible, but HDD to SSD is okay =&gt; B is feasible. </p><p>B is lesseffort since B just disconnects users from the file system for a while,and then updates the FSx. While A needsa new </p><p>FSx, backup, restore, clean up then switch, more steps to do than A</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": true,
      "noteContent": "SSD to HDD is impossible, but HDD to SSD is okay ="
    },
    {
      "id": "67c09617a6424ec1a888ceb569281cdd",
      "questionNumber": 194,
      "type": "single",
      "content": "<p>Question #194</p><p>A company hosts an application on AWS. The application reads and writes objects that are stored in a single Amazon S3 bucket. The company must modify the application to deploy the application in two AWS Regions.</p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Set up an Amazon CloudFront distribution with the S3 bucket as an origin. Deploy the application to a second Region. Modify the application to use the CloudFront distribution. Use AWS Global Accelerator to access the data in the S3 bucket."
        },
        {
          "label": "B",
          "content": "Create a new S3 bucket in a second Region. Set up bidirectional S3 Cross-Region Replication (CRR) between the original S3 bucket and the new S3 bucket. Configure an S3 Multi-Region Access Point that uses both S3 buckets. Deploy a modified application to both Regions."
        },
        {
          "label": "C",
          "content": "Create a new S3 bucket in a second Region. Deploy the application in the second Region. Configure the application to use the new S3 bucket. Set up S3 Cross-Region Replication (CRR) from the original S3 bucket to the new S3 bucket."
        },
        {
          "label": "D",
          "content": "Set up an S3 gateway endpoint with the S3 bucket as an origin. Deploy the application to a second Region. Modify the application to use the new S3 gateway endpoint. Use S3 Intelligent-Tiering on the S3 bucket."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the requirement to deploy an application that reads and writes objects to S3 across two AWS Regions with the least operational overhead, the answer is:</p><p>B. Create a new S3 bucket in a second Region. Set up bidirectional S3 Cross-Region Replication (CRR) between the original S3 bucket and the new S3 bucket. Configure an S3 Multi-Region Access Point that uses both S3 buckets. Deploy a modified application to both Regions.</p><p>This is the correct choice because:</p><p>S3 Multi-Region Access Points are specifically designed for multi-region access to S3 data, providing a single endpoint that automatically routes requests to the appropriate region based on network conditions and availability.</p><p>Bidirectional S3 Cross-Region Replication ensures that objects written to either bucket are automatically replicated to the other bucket, maintaining data consistency across regions without requiring application-level replication logic.</p><p>This solution is purpose-built for S3 multi-region access and requires minimal configuration changes to the application - it simply needs to use the Multi-Region Access Point endpoint instead of a specific bucket endpoint.</p><p>The entire solution uses native S3 features that are designed to work together, reducing integration complexity and operational overhead.</p><p>Option A (CloudFront distribution with Global Accelerator) has higher operational overhead because:</p><p>It requires setting up and managing multiple services (CloudFront and Global Accelerator) that aren't specifically designed for bidirectional S3 access across regions.</p><p>CloudFront is primarily a content delivery network optimized for read operations, not for write operations that the application requires.</p><p>Global Accelerator improves network performance but doesn't handle data replication between regions, which would require additional custom logic.</p><p>This approach would require more complex configuration and ongoing management of multiple services that weren't designed specifically for this use case.</p><p>The S3 Multi-Region Access Point with bidirectional Cross-Region Replication provides a more streamlined, purpose-built solution with significantly less operational overhead for deploying an application that needs to read and write S3 objects across two regions.</p><p>Sources</p><p>Cloudfront OAC S3: Public Reads & Only Signed Secure Writes | AWS re:Post （https://repost.aws/questions/QUY3prSJX-QMS-0UtwtmbeMQ/cloudfront-oac-s3-public-reads-only-signed-secure-writes）</p><p>Replicating objects within and across Regions - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html）</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "616a28d5616c44149d06e312e8c7d2d3",
      "questionNumber": 195,
      "type": "single",
      "content": "<p>Question #195</p><p>An online gaming company needs to rehost its gaming platform on AWS. The company's gaming application requires high-performance computing (HPC) processing and has a leaderboard that changes frequently. An Ubuntu instance that is optimized for compute generation hosts a Node.js application for game display. Game state is tracked in an on-premises Redis instance.</p><p><br></p><p>The company needs a migration strategy that optimizes application performance.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an Auto Scaling group of m5.large Amazon EC2 Spot Instances behind an Application Load Balancer. Use an Amazon ElastiCache for Redis cluster to maintain the leaderboard."
        },
        {
          "label": "B",
          "content": "Create an Auto Scaling group of c5.large Amazon EC2 Spot Instances behind an Application Load Balancer. Use an Amazon OpenSearch Service cluster to maintain the leaderboard."
        },
        {
          "label": "C",
          "content": "Create an Auto Scaling group of c5.large Amazon EC2 On-Demand Instances behind an Application Load Balancer. Use an Amazon ElastiCache for Redis cluster to maintain the leaderboard."
        },
        {
          "label": "D",
          "content": "Create an Auto Scaling group of m5.large Amazon EC2 On-Demand Instances behind an Application Load Balancer. Use an Amazon DynamoDB table to maintain the leaderboard."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the requirements for migrating an online gaming platform that needs high performance computing (HPC) and has a frequently changing leaderboard currently using Redis, the answer is:</p><p>C. Create an Auto Scaling group of c5.large Amazon EC2 On-Demand Instances behind an Application Load Balancer. Use an Amazon ElastiCache for Redis cluster to maintain the leaderboard.</p><p>This is the correct choice because:</p><p>Compute-optimized instances (c5.large) are specifically designed for compute-intensive workloads like high performance computing (HPC) that the gaming platform requires. The \"c\" family of instances offers higher CPU performance compared to the general-purpose \"m\" family, making them better suited for the computational demands of gaming applications.</p><p>On-Demand Instances provide consistent, uninterrupted performance which is critical for gaming applications where any interruption would negatively impact the user experience. Unlike Spot Instances which can be terminated with little notice, On-Demand Instances ensure stability for the gaming service.</p><p>Amazon ElastiCache for Redis is the ideal choice for maintaining the frequently changing leaderboard as it provides:</p><p>Sub-millisecond latency for real-time updates</p><p>In-memory data storage for high-speed access</p><p>Built-in sorting and ranking capabilities perfect for leaderboards</p><p>Compatibility with the existing Redis implementation, simplifying migration</p><p>The Application Load Balancer with an Auto Scaling group ensures the application can handle varying loads while maintaining high availability.</p><p>Option A is less suitable because:</p><p>m5.large instances are general-purpose rather than compute-optimized, making them less efficient for HPC workloads</p><p>Spot Instances can be interrupted with minimal notice, which would disrupt the gaming experience</p><p>While Spot Instances are more cost-effective, they sacrifice the reliability and consistent performance needed for a gaming platform</p><p>The solution in option C provides the optimal combination of compute power, reliability, and performance required for the gaming platform while maintaining compatibility with the existing Redis-based leaderboard system.</p><p>Sources</p><p>Migrate Redis workloads to Redis Enterprise Cloud on AWS - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-redis-workloads-to-redis-enterprise-cloud-on-aws.html）</p><p>Amazon EC2 C6i Instances - Amazon Web Services （https://aws.amazon.com/cn/ec2/instance-types/c6i/）</p><p>Best practices for Amazon EC2 Spot - Amazon Elastic Compute Cloud （https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-best-practices.html）</p><p>About the migration strategies - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/large-migration-guide/migration-strategies.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "0d972295b7954972ad108709620011c7",
      "questionNumber": 196,
      "type": "multiple",
      "content": "<p>Question #196</p><p>A solutions architect is designing an application to accept timesheet entries from employees on their mobile devices. Timesheets will be submitted weekly, with most of the submissions occurring on Friday. The data must be stored in a format that allows payroll administrators to run monthly reports. The infrastructure must be highly available and scale to match the rate of incoming data and reporting requests.</p><p>Which combination of steps meets these requirements while minimizing operational overhead? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Deploy the application to Amazon EC2 On-Demand Instances with load balancing across multiple Availability Zones. Use scheduled Amazon EC2 Auto Scaling to add capacity before the high volume of submissions on Fridays."
        },
        {
          "label": "B",
          "content": "Deploy the application in a container using Amazon Elastic Container Service (Amazon ECS) with load balancing across multiple Availability Zones. Use scheduled Service Auto Scaling to add capacity before the high volume of submissions on Fridays."
        },
        {
          "label": "C",
          "content": "Deploy the application front end to an Amazon S3 bucket served by Amazon CloudFront. Deploy the application backend using Amazon API Gateway with an AWS Lambda proxy integration."
        },
        {
          "label": "D",
          "content": "Store the timesheet submission data in Amazon Redshift. Use Amazon QuickSight to generate the reports using Amazon Redshift as the data source."
        },
        {
          "label": "E",
          "content": "Store the timesheet submission data in Amazon S3. Use Amazon Athena and Amazon QuickSight to generate the reports using Amazon S3 as the data source."
        }
      ],
      "correctAnswer": "CE",
      "explanation": "<p>Based on the requirements for a timesheet application that needs to handle weekly submissions (mostly on Fridays), store data for monthly reports, and be highly available and scalable while minimizing operational overhead, the correct combination of steps is:</p><p>C. Deploy the application front end to an Amazon S3 bucket served by Amazon CloudFront. Deploy the application backend using Amazon API Gateway with an AWS Lambda proxy integration.</p><p>E. Store the timesheet submission data in Amazon S3. Use Amazon Athena and Amazon QuickSight to generate the reports using Amazon S3 as the data source.</p><p>Here's why this combination is optimal:</p><p>For the application architecture (Option C):</p><p>Using S3 and CloudFront for the front end provides a highly available, globally distributed solution that can handle any number of mobile users accessing the application simultaneously.</p><p>CloudFront's caching capabilities ensure low latency access to the application's static assets.</p><p>API Gateway with Lambda provides a serverless backend that automatically scales to handle the Friday submission spikes without any manual intervention.</p><p>This serverless architecture eliminates the need to provision, manage, or patch servers, significantly reducing operational overhead.</p><p>Lambda functions can be configured to process timesheet submissions and store the data in a structured format.</p><p>For the data storage and reporting solution (Option E):</p><p>Amazon S3 provides virtually unlimited, highly durable storage for timesheet data at a low cost.</p><p>S3 can easily handle the weekly influx of submissions, especially the Friday peaks.</p><p>Amazon Athena allows payroll administrators to run SQL queries directly against the data stored in S3 without having to set up and manage a database server.</p><p>Amazon QuickSight integrates with Athena to create interactive dashboards and visualizations for the monthly reports.</p><p>This combination eliminates the need to manage database infrastructure, reducing operational overhead while providing powerful reporting capabilities.</p><p>Together, these solutions create a complete serverless architecture that:</p><p>Automatically scales to match incoming data submission rates</p><p>Provides high availability for both the application and data</p><p>Minimizes operational overhead by eliminating server management</p><p>Offers powerful reporting capabilities for payroll administrators</p><p>Can handle the weekly submission pattern with Friday peaks efficiently</p><p>This combination leverages fully managed AWS services that work together seamlessly while requiring minimal operational effort to maintain.</p><p>Sources</p><p>Scaling Learning: How Geniusee Migrated a WordPress E-Learning Solution to AWS Serverless | AWS Partner Network (APN) Blog （https://aws.amazon.com/cn/blogs/apn/scaling-learning-how-geniusee-migrated-a-wordpress-e-learning-solution-to-aws-serverless/）</p><p>[DL.ADS.6] Use cell-based architectures for granular deployment and release - DevOps Guidance （https://docs.aws.amazon.com/wellarchitected/latest/devops-guidance/dl.ads.6-utilize-cell-based-architectures-for-granular-deployment-and-release.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "1e0f7a204bad4ef3a712027ba37cdbb7",
      "questionNumber": 197,
      "type": "multiple",
      "content": "<p>Question #197</p><p>A company is storing sensitive data in an Amazon S3 bucket. The company must log all activities for objects in the S3 bucket and must keep the logs for 5 years. The company's security team must also receive an email notification every time there is an attempt to delete data in the S3 bucket.</p><p>Which combination of steps will meet these requirements MOST cost-effectively? (Choose three.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Configure AWS CloudTrail to log S3 data events."
        },
        {
          "label": "B",
          "content": "Configure S3 server access logging for the S3 bucket."
        },
        {
          "label": "C",
          "content": "Configure Amazon S3 to send object deletion events to Amazon Simple Email Service (Amazon SES)."
        },
        {
          "label": "D",
          "content": "Configure Amazon S3 to send object deletion events to an Amazon EventBridge event bus that publishes to an Amazon Simple Notification Service (Amazon SNS) topic."
        },
        {
          "label": "E",
          "content": "Configure Amazon S3 to send the logs to Amazon Timestream with data storage tiering."
        },
        {
          "label": "F",
          "content": "Configure a new S3 bucket to store the logs with an S3 Lifecycle policy."
        }
      ],
      "correctAnswer": "ADF",
      "explanation": "<p>Based on the requirements to log all activities for objects in an S3 bucket for 5 years and receive email notifications for deletion attempts in the most cost-effective way, the correct combination of steps is:</p><p>A. Configure AWS CloudTrail to log S3 data events. </p><p>D. Configure Amazon S3 to send object deletion events to an Amazon EventBridge event bus that publishes to an Amazon Simple Notification Service (Amazon SNS) topic. </p><p>F. Configure a new S3 bucket to store the logs with an S3 Lifecycle policy.</p><p>Here's why this combination is the most cost-effective solution:</p><p>CloudTrail for logging (Option A):</p><p>CloudTrail data events provide comprehensive logging of all object-level activities in S3, including GET, PUT, DELETE operations</p><p>While there is a cost for CloudTrail data events, they provide the detailed information needed for security auditing and compliance</p><p>CloudTrail logs include important metadata like who performed the action, when it occurred, and from where</p><p>EventBridge with SNS for notifications (Option D):</p><p>This combination allows S3 to send object deletion events to EventBridge, which then triggers an SNS notification</p><p>SNS can be configured to send email notifications to the security team</p><p>This approach is more flexible and cost-effective than using Amazon SES directly, as it allows for future expansion to other notification methods or integrations</p><p>S3 with Lifecycle policies for log storage (Option F):</p><p>Storing logs in another S3 bucket with lifecycle policies is the most cost-effective option for 5-year retention</p><p>Lifecycle policies can automatically transition older logs to lower-cost storage tiers like S3 Glacier or S3 Glacier Deep Archive</p><p>This approach is significantly more cost-effective than using Timestream for long-term storage</p><p>The rejected options are less cost-effective:</p><p>Option B (S3 server access logging): While free, it provides less detailed information than CloudTrail and doesn't integrate as well with other AWS services for analysis</p><p>Option C (S3 to SES): Direct integration with SES is less flexible than using EventBridge with SNS, which can support multiple notification targets and filtering capabilities</p><p>Option E (Timestream): While Timestream is excellent for time-series data analysis, it's much more expensive than S3 for long-term storage of logs that aren't frequently queried</p><p>The selected combination provides the right balance of comprehensive logging, effective notifications, and cost-efficient long-term storage to meet the company's requirements for monitoring and securing their sensitive S3 data.</p><p>Sources</p><p>Amazon S3 CloudTrail events - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/cloudtrail-logging-s3-info.html）</p><p>Logging options for Amazon S3 - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/logging-with-S3.html）</p><p>Configuring S3 Lifecycle event notifications - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-configure-notification.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "817ac242c51a4dfebe7b8ab2a3e1a855",
      "questionNumber": 198,
      "type": "multiple",
      "content": "<p>Question #198</p><p>A company is building a hybrid environment that includes servers in an on-premises data center and in the AWS Cloud. The company has deployed Amazon EC2 instances in three VPCs. Each VPC is in a different AWS Region. The company has established an AWS Direct Connect connection to the data center from the Region that is closest to the data center.</p><p>The company needs the servers in the on-premises data center to have access to the EC2 instances in all three VPCs. The servers in the on-premises data center also must have access to AWS public services.</p><p>Which combination of steps will meet these requirements with the LEAST cost? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a Direct Connect gateway in the Region that is closest to the data center. Attach the Direct Connect connection to the Direct Connect gateway. Use the Direct Connect gateway to connect the VPCs in the other two Regions."
        },
        {
          "label": "B",
          "content": "Set up additional Direct Connect connections from the on-premises data center to the other two Regions."
        },
        {
          "label": "C",
          "content": "Create a private VIF. Establish an AWS Site-to-Site VPN connection over the private VIF to the VPCs in the other two Regions."
        },
        {
          "label": "D",
          "content": "Create a public VIF. Establish an AWS Site-to-Site VPN connection over the public VIF to the VPCs in the other two Regions."
        },
        {
          "label": "E",
          "content": "Use VPC peering to establish a connection between the VPCs across the Regions. Create a private VIF with the existing Direct Connect connection to connect to the peered VPCs."
        }
      ],
      "correctAnswer": "AD",
      "explanation": "<p>The correct answers are A and D. </p><p> Why Option A is Correct:</p><p>1. Direct Connect Gateway for Multi-Region VPC Access:</p><p> &nbsp; - Creates a hub that connects your Direct Connect connection to multiple VPCs across different regions</p><p> &nbsp; - Allows the single existing Direct Connect connection to access all three VPCs</p><p> &nbsp; - Most cost-effective solution as it doesn't require additional Direct Connect connections</p><p>2. Simplified Architecture:</p><p> &nbsp; - Single connection management point</p><p> &nbsp; - No need for additional physical connections</p><p> Why Option D is Correct (Not C or E):</p><p>1. Access to AWS Public Services:</p><p> &nbsp; - A public VIF is specifically designed to access AWS public services (S3, DynamoDB, etc.)</p><p> &nbsp; - Required to meet the \"access to AWS public services\" requirement in the question</p><p> &nbsp; - More efficient than routing public service traffic through VPN (Option C)</p><p>2. Complete Solution:</p><p> &nbsp; - Option A handles private VPC access across regions</p><p> &nbsp; - Option D handles public AWS services access</p><p> &nbsp; - Together they satisfy all requirements</p><p> Why Other Options Are Incorrect:</p><p>- B: Additional Direct Connect connections are expensive and unnecessary</p><p>- C: Private VIF + VPN is redundant when you already have Direct Connect and doesn't efficiently handle public services</p><p>- E: VPC peering doesn't help with public services access and complicates the architecture</p><p> Key Advantages of A+D Solution:</p><p>1. Cost Optimization:</p><p> &nbsp; - Uses existing Direct Connect connection</p><p> &nbsp; - No additional physical circuits needed</p><p> &nbsp; - Public VIF is included at no extra cost with Direct Connect</p><p>2. Complete Connectivity:</p><p> &nbsp; - Private VPC access via Direct Connect Gateway</p><p> &nbsp; - Public services access via public VIF</p><p> &nbsp; - All requirements met with minimal components</p><p>3. Operational Simplicity:</p><p> &nbsp; - Single Direct Connect connection to manage</p><p> &nbsp; - Clear separation of private and public traffic</p><p> &nbsp; - AWS-managed services reduce overhead</p><p>Final Answer: A & D (Direct Connect Gateway + Public VIF) is the most cost-effective solution that meets all requirements.</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "ae3c517316384cb792f2090a5c06ccd5",
      "questionNumber": 199,
      "type": "multiple",
      "content": "<p>Question #199</p><p>A company is using an organization in AWS Organizations to manage hundreds of AWS accounts. A solutions architect is working on a solution to provide baseline protection for the Open Web Application Security Project (OWASP) top 10 web application vulnerabilities. The solutions architect is using AWS WAF for all existing and new Amazon CloudFront distributions that are deployed within the organization.</p><p>Which combination of steps should the solutions architect take to provide the baseline protection? (Choose three.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Enable AWS Config in all accounts"
        },
        {
          "label": "B",
          "content": "Enable Amazon GuardDuty in all accounts"
        },
        {
          "label": "C",
          "content": "Enable all features for the organization"
        },
        {
          "label": "D",
          "content": "Use AWS Firewall Manager to deploy AWS WAF rules in all accounts for all CloudFront distributions"
        },
        {
          "label": "E",
          "content": "Use AWS Shield Advanced to deploy AWS WAF rules in all accounts for all CloudFront distributions"
        },
        {
          "label": "F",
          "content": "Use AWS Security Hub to deploy AWS WAF rules in all accounts for all CloudFront distributions"
        }
      ],
      "correctAnswer": "ACD",
      "explanation": "<p>Based on the requirement to provide baseline protection against OWASP top 10 web application vulnerabilities using AWS WAF for all CloudFront distributions across an AWS organization with hundreds of accounts, the correct combination of steps is:</p><p>A. Enable AWS Config in all accounts C. Enable all features for the organization D. Use AWS Firewall Manager to deploy AWS WAF rules in all accounts for all CloudFront distributions</p><p>Here's why this combination is correct:</p><p>Enable AWS Config in all accounts (Option A):</p><p>AWS Config provides continuous monitoring and recording of AWS resource configurations</p><p>It's essential for tracking compliance with security policies and detecting configuration drift</p><p>AWS Config helps ensure that WAF protections remain in place and aren't accidentally removed or modified</p><p>It provides visibility into the security posture across all accounts in the organization</p><p>Enable all features for the organization (Option C):</p><p>This is a prerequisite for using AWS Firewall Manager across the organization</p><p>Enabling all features in AWS Organizations allows for centralized management of security services</p><p>It enables service-linked roles needed for cross-account management</p><p>It allows policies to be applied consistently across all accounts in the organization</p><p>Use AWS Firewall Manager to deploy AWS WAF rules in all accounts for all CloudFront distributions (Option D):</p><p>Firewall Manager is specifically designed for centralized management of AWS WAF rules across multiple accounts</p><p>It allows for consistent application of WAF rules to protect against OWASP top 10 vulnerabilities</p><p>It can automatically apply protection to new CloudFront distributions as they're created</p><p>It provides a single point of management for hundreds of accounts, significantly reducing operational overhead</p><p>Option B (Enable Amazon GuardDuty in all accounts) is not part of the correct answer because:</p><p>While GuardDuty is a valuable security service, it focuses on threat detection rather than web application protection</p><p>GuardDuty doesn't directly help protect against OWASP top 10 vulnerabilities in web applications</p><p>It's complementary to WAF but not essential for the specific requirement of providing baseline WAF protection</p><p>This combination of steps creates a comprehensive approach to deploying and maintaining baseline WAF protection across the organization:</p><p>AWS Config provides monitoring and compliance checking</p><p>Enabling all organization features provides the necessary foundation for centralized management</p><p>AWS Firewall Manager handles the actual deployment and management of WAF rules across all accounts</p><p>Together, these steps ensure that all CloudFront distributions, both existing and new, have consistent protection against OWASP top 10 vulnerabilities throughout the organization.</p><p>Sources</p><p>Amazon CloudFront simplifies web application delivery and security with new user-friendly interface | AWS News Blog （https://aws.amazon.com/cn/blogs/aws/amazon-cloudfront-simplifies-web-application-delivery-and-security-with-new-user-friendly-interface/）</p><p>How to enforce a security baseline for an AWS WAF ACL across your organization using AWS Firewall Manager | AWS Security Blog （https://aws.amazon.com/cn/blogs/security/how-to-enforce-a-security-baseline-for-an-aws-waf-acl-across-your-organization-using-aws-firewall-manager/）</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "9ee0b122938e4dc5baa36228cbb4bd5a",
      "questionNumber": 200,
      "type": "multiple",
      "content": "<p>Question #200</p><p>A solutions architect has implemented a SAML 2.0 federated identity solution with their company's on-premises identity provider (IdP) to authenticate users' access to the AWS environment. When the solutions architect tests authentication through the federated identity web portal, access to the AWS environment is granted. However, when test users attempt to authenticate through the federated identity web portal, they are not able to access the AWS environment.</p><p>Which items should the solutions architect check to ensure identity federation is properly configured? (Choose three.)</p>",
      "options": [
        {
          "label": "A",
          "content": "The IAM user&#39;s permissions policy has allowed the use of SAML federation for that user."
        },
        {
          "label": "B",
          "content": "The IAM roles created for the federated users&#39; or federated groups&#39; trust policy have set the SAML provider as the principal."
        },
        {
          "label": "C",
          "content": "Test users are not in the AWSFederatedUsers group in the company&#39;s IdP."
        },
        {
          "label": "D",
          "content": "<p>The web portal calls the AWS STS AssumeRoleWithSAML API with the ARN of the SAML provider, the ARN of the IAM role, and the SAML assertion from IdP.</p>"
        },
        {
          "label": "E",
          "content": "<p>The on-premises IdP's DNS hostname is reachable from the AWS environment VPCs.</p>"
        },
        {
          "label": "F",
          "content": "<p>The company's IdP defines SAML assertions that properly map users or groups. In the company to IAM roles withappropnate permissions</p>"
        }
      ],
      "correctAnswer": "BCF",
      "explanation": "<p style=\"text-align: start;\"><span style=\"color: rgb(31, 35, 41); background-color: rgba(0, 0, 0, 0); font-size: 16px;\"><strong>Option C</strong></span>: Enterprise IdPs typically restrict SAML federation to users in specific groups (e.g., AWSFederatedUsers). If test users are not in this group, they cannot trigger the federation process, resulting in access denial.</p><p style=\"text-align: start;\"><span style=\"color: rgb(31, 35, 41); background-color: rgba(0, 0, 0, 0); font-size: 16px;\"><strong>Option D</strong></span>: The AWS STS <span style=\"color: rgb(31, 35, 41); background-color: rgba(0, 0, 0, 0); font-size: 16px;\"><code>AssumeRoleWithSAML</code></span> API is critical for obtaining AWS temporary credentials. It requires passing the ARN of the SAML provider, the ARN of the target IAM role, and the SAML assertion issued by the IdP. If the portal calls this API with missing or incorrect parameters, users cannot obtain valid credentials.</p><p style=\"text-align: start;\"><span style=\"color: rgb(31, 35, 41); background-color: rgba(0, 0, 0, 0); font-size: 16px;\"><strong>Option F</strong></span>: SAML assertions must correctly map users/groups in the enterprise to IAM roles in AWS. If the mapping configuration is incorrect, users cannot be associated with IAM roles that grant necessary permissions, leading to access denial.</p><p style=\"text-align: start;\">Option A: SAML federated identity relies on IAM roles (not IAM users), so the permissions policy of IAM users is irrelevant to federated authentication and does not need to be checked.</p><p style=\"text-align: start;\">Option B: Since the solutions architect successfully accessed AWS via federated authentication, the IAM role's trust policy (setting the SAML provider as the principal) is already configured correctly and does not need to be rechecked.</p><p style=\"text-align: start;\">Option E: Federated authentication is completed via the enterprise-side web portal, so the DNS reachability of the IdP from AWS VPCs is irrelevant and does not affect user authentication.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "84dcf6fc3afb4e1989338d66e4463330",
      "questionNumber": 201,
      "type": "single",
      "content": "Question #201<p>A solutions architect needs to improve an application that is hosted in the AWS Cloud. The application uses an Amazon Aurora MySQL DB instance that is experiencing overloaded connections. Most of the application’s operations insert records into the database. The application currently stores credentials in a text-based configuration file.</p><p><br></p><p>The solutions architect needs to implement a solution so that the application can handle the current connection load. The solution must keep the credentials secure and must provide the ability to rotate the credentials automatically on a regular basis.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Deploy an Amazon RDS Proxy layer in front of the DB instance. Store the connection credentials as a secret in AWS Secrets Manager."
        },
        {
          "label": "B",
          "content": "Deploy an Amazon RDS Proxy layer in front of the DB instance. Store the connection credentials in AWS Systems Manager Parameter Store."
        },
        {
          "label": "C",
          "content": "Create an Aurora Replica. Store the connection credentials as a secret in AWS Secrets Manager."
        },
        {
          "label": "D",
          "content": "Create an Aurora Replica. Store the connection credentials in AWS Systems Manager Parameter Store."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Based on the requirements to handle connection load for an Aurora MySQL database experiencing overloaded connections, while securely storing credentials with automatic rotation capability, the answer is:</p><p>A. Deploy an Amazon RDS Proxy layer. In front of the DB instance. Store the connection credentials as a secret in AWS Secrets Manager.</p><p>This is the correct choice because:</p><p>RDS Proxy specifically addresses the connection overload issue by providing connection pooling. It efficiently manages database connections, allowing the application to handle more traffic without overwhelming the database with too many connections.</p><p>Since most of the application's operations are inserts, RDS Proxy is particularly beneficial as it maintains persistent connections to the database, reducing the overhead of establishing new connections for each insert operation.</p><p>AWS Secrets Manager provides secure storage for database credentials, eliminating the security risk of storing them in a text-based configuration file.</p><p>Secrets Manager offers built-in functionality for automatic credential rotation on a regular basis, which directly fulfills the requirement for automatic credential rotation.</p><p>RDS Proxy integrates natively with Secrets Manager, creating a comprehensive solution that addresses both the connection management and credential security requirements.</p><p>Option C (Create an Aurora Replica and store credentials in Secrets Manager) would not effectively address the connection overload issue for an insert-heavy workload, as write operations would still go to the primary instance. While it would solve the credential security and rotation requirements, it wouldn't resolve the primary problem of overloaded connections.</p><p>Sources</p><p>Amazon RDS Proxy - Oracle to Aurora MySQL Migration Playbook （https://docs.aws.amazon.com/dms/latest/oracle-to-aurora-mysql-migration-playbook/chap-oracle-aurora-mysql.tools.rdsproxy.html）</p><p>Amazon RDS Proxy overview - SQL Server to Aurora MySQL Migration Playbook （https://docs.aws.amazon.com/dms/latest/sql-server-to-aurora-mysql-migration-playbook/chap-sql-server-aurora-mysql.tools.rdsproxy.html）</p><p>Automate Amazon RDS credential rotation with AWS Secrets Manager for primary instances with read replicas | AWS Database Blog （https://aws.amazon.com/cn/blogs/database/automate-amazon-rds-credential-rotation-with-aws-secrets-manager-for-primary-instances-with-read-replicas/）</p><p>Read replica of RDS with Secrets Manager integration | AWS re:Post （https://repost.aws/questions/QUdLdC7_nNRWm93ScLB2WO5Q/read-replica-of-rds-with-secrets-manager-integration）</p><p>Amazon RDS Proxy for Aurora - Amazon Aurora （https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-proxy.html）</p><p>Amazon RDS Proxy - Oracle to Aurora PostgreSQL Migration Playbook （https://docs.aws.amazon.com/dms/latest/oracle-to-aurora-postgresql-migration-playbook/chap-oracle-aurora-pg.tools.rdsproxy.html）</p><p>AWS managed RDS password rotation | AWS re:Post （https://repost.aws/questions/QUZiq-Gwb3R46N9TDG-zTWRw/aws-managed-rds-password-rotation）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "70c727893bcc4f6badcd4467d46a27c3",
      "questionNumber": 202,
      "type": "single",
      "content": "<p>Question #202</p><p>A company needs to build a disaster recovery (DR) solution for its ecommerce website. The web application is hosted on a fleet of t3.large Amazon EC2 instances and uses an Amazon RDS for MySQL DB instance. The EC2 instances are in an Auto Scaling group that extends across multiple Availability Zones. </p><p><br></p><p>In the event of a disaster, the web application must fail over to the secondary environment with an RPO of 30 seconds and an RTO of 10 minutes. </p><p><br></p><p>Which solution will meet these requirements MOST cost-effectively?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use infrastructure as code (IaC) to provision the new infrastructure in the DR Region. Create a cross-Region read replica for the DB instance. Set up a backup plan in AWS Backup to create cross-Region backups for the EC2 instances and the DB instance. Create a cron expression to back up the EC2 instances and the DB instance every 30 seconds to the DR Region. Recover the EC2 instances from the latest EC2 backup. Use an Amazon Route 53 geolocation routing policy to automatically fail over to the DR Region in the event of a disaster."
        },
        {
          "label": "B",
          "content": "Use infrastructure as code (IaC) to provision the new infrastructure in the DR Region. Create a cross-Region read replica for the DB instance. Set up AWS Elastic Disaster Recovery to continuously replicate the EC2 instances to the DR Region. Run the EC2 instances at the minimum capacity in the DR Region. Use an Amazon Route 53 failover routing policy to automatically fail over to the DR Region in the event of a disaster. Increase the desired capacity of the Auto Scaling group."
        },
        {
          "label": "C",
          "content": "Set up a backup plan in AWS Backup to create cross-Region backups for the EC2 instances and the DB instance. Create a cron expression to back up the EC2 instances and the DB instance every 30 seconds to the DR Region. Use infrastructure as code (IaC) to provision the new infrastructure in the DR Region. Manually restore the backed-up data on new instances. Use an Amazon Route 53 simple routing policy to automatically fail over to the DR Region in the event of a disaster."
        },
        {
          "label": "D",
          "content": "Use infrastructure as code (IaC) to provision the new infrastructure in the DR Region. Create an Amazon Aurora global database. Set up AWS Elastic Disaster Recovery to continuously replicate the EC2 instances to the DR Region. Run the Auto Scaling group of EC2 instances at full capacity in the DR Region. Use an Amazon Route 53 failover routing policy to automatically fail over to the DR Region in the event of a disaster."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>The most cost-effective solution that meets the RPO of 30 seconds and RTO of 10 minutes is:</p><p> Correct Answer: B &nbsp;</p><p> Why Option B? &nbsp;</p><p>1. Cross-Region Read Replica for RDS MySQL &nbsp;</p><p> &nbsp; - Ensures low RPO (30 seconds) by continuously replicating data to the DR Region. &nbsp;</p><p> &nbsp; - Promotes the replica to primary in case of disaster. &nbsp;</p><p>2. AWS Elastic Disaster Recovery (DRS) for EC2 Instances &nbsp;</p><p> &nbsp; - Continuously replicates EC2 instances to the DR Region, meeting the RPO requirement. &nbsp;</p><p> &nbsp; - Allows minimal capacity in the DR Region (cost-effective) before scaling up during failover. &nbsp;</p><p>3. Auto Scaling Group in DR Region &nbsp;</p><p> &nbsp; - Running at minimum capacity reduces costs while allowing quick scaling (meeting RTO of 10 minutes). &nbsp;</p><p>4. Route 53 Failover Routing Policy &nbsp;</p><p> &nbsp; - Automatically redirects traffic to the DR Region when the primary fails. &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- A: Backing up EC2 every 30 seconds is impractical (AWS Backup doesn’t support such frequent backups). Restoring from backup is slow (high RTO). &nbsp;</p><p>- C: Manual restores violate RTO, and 30-second backups are unrealistic. &nbsp;</p><p>- D: Aurora Global Database is overkill (original DB is MySQL, not Aurora). Running EC2 at full capacity in DR is costly. &nbsp;</p><p> Conclusion: &nbsp;</p><p>Option B is the most cost-effective while meeting RPO (30s) and RTO (10 min). &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "117b424707a84bdd810ec8552e883a04",
      "questionNumber": 203,
      "type": "single",
      "content": "<p>Question #203</p><p>A company is planning a one-time migration of an on-premises MySQL database to Amazon Aurora MySQL in the us-east-1 Region. The company's current internet connection has limited bandwidth. The on-premises MySQL database is 60 TB in size. The company estimates that it will take a month to transfer the data to AWS over the current internet connection. The company needs a migration solution that will migrate the database more quickly. <br><br>Which solution will migrate the database in the LEAST amount of time?</p>",
      "options": [
        {
          "label": "A",
          "content": "Request a 1 Gbps AWS Direct Connect connection between the on-premises data center and AWS. Use AWS Database Migration Service (AWS DMS) to migrate the on-premises MySQL database to Aurora MySQL."
        },
        {
          "label": "B",
          "content": "Use AWS DataSync with the current internet connection to accelerate the data transfer between the on-premises data center and AWS. Use AWS Application Migration Service to migrate the on-premises MySQL database to Aurora MySQL."
        },
        {
          "label": "C",
          "content": "Order an AWS Snowball Edge device. Load the data into an Amazon S3 bucket by using the S3 interface. Use AWS Database Migration Service (AWS DMS) to migrate the data from Amazon S3 to Aurora MySQL."
        },
        {
          "label": "D",
          "content": "Order an AWS Snowball device. Load the data into an Amazon S3 bucket by using the S3 Adapter for Snowball. Use AWS Application Migration Service to migrate the data from Amazon S3 to Aurora MySQL."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the requirement to migrate a 60 TB on-premises MySQL database to Aurora MySQL in the least amount of time, the answer is:</p><p>C. Order an AWS Snowball Edge device. Load the data into an Amazon S3 bucket by using the S3 interface. Use AWS Database Migration Service (AWS DMS) to migrate the data from Amazon S3 to Aurora MySQL.</p><p>This is the correct choice because:</p><p>The company's current internet connection has limited bandwidth, and they estimate it would take a month to transfer the data over this connection.</p><p>Even with a 1 Gbps AWS Direct Connect connection (Option A), transferring 60 TB of data would still take approximately 5.5 days of continuous transfer at optimal speeds, plus additional time for AWS DMS processing and data validation.</p><p>Using AWS Snowball Edge provides a physical data transport solution that bypasses internet transfer limitations entirely for the bulk of the data:</p><p>Data export to Snowball Edge: 1-2 days</p><p>Shipping time: 1-3 days</p><p>Data import to S3: 1-2 days</p><p>AWS DMS migration from S3 to Aurora MySQL: 1-2 days</p><p>Total estimated time: 4-9 days</p><p>The Snowball Edge approach is specifically designed for large-scale data migrations where internet bandwidth is a constraint, making it ideal for this 60 TB migration scenario.</p><p>This approach also minimizes the impact on the production database compared to a prolonged migration over a network connection.</p><p>While Option A (AWS Direct Connect with AWS DMS) would provide a dedicated network connection, it would still be constrained by the 1 Gbps bandwidth limit, making it slower than the physical transfer method for this large dataset.</p><p>Sources</p><p>Migrate a self-managed MySQL database to Amazon Aurora MySQL using AWS DMS homogeneous data migrations | AWS Database Blog （https://aws.amazon.com/cn/blogs/database/migrate-a-self-managed-mysql-database-to-amazon-aurora-mysql-using-aws-dms-homogeneous-data-migrations/）</p><p>Planning your database migration process - Migrating Your Databases to Amazon Aurora （https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.AuroraMySQL.html）</p><p>Database Migration Step-by-Step Walkthroughs - Database Migration Guide （https://docs.aws.amazon.com/dms/latest/sbs/dms-sbs-welcome.html）</p><p>Migrate very large databases to Amazon Aurora MySQL using MyDumper and MyLoader | AWS Database Blog （https://aws.amazon.com/cn/blogs/database/migrate-very-large-databases-to-amazon-aurora-mysql-using-mydumper-and-myloader/）</p><p>Full load MySQL database migration options performance comparison - Database Migration Guide（https://docs.aws.amazon.com/dms/latest/sbs/chap-manageddatabases.mysql2rds.performance.html）</p><p>Time and performance - An Overview of AWS Cloud Data Migration Services （https://docs.aws.amazon.com/decision-guides/latest/migration-on-aws-how-to-choose/migration-on-aws-how-to-choose.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "2bd75013208f490ea4f5b77a7e863659",
      "questionNumber": 204,
      "type": "single",
      "content": "<p>Question #204</p><p>A company has an application in the AWS Cloud. The application runs on a fleet of 20 Amazon EC2 instances. The EC2 instances are persistent and store data on multiple attached Amazon Elastic Block Store (Amazon EBS) volumes.</p><p><br></p><p>The company must maintain backups in a separate AWS Region. The company must be able to recover the EC2 instances and their configuration within 1 business day, with loss of no more than 1 day's worth of data. The company has limited staff and needs a backup solution that optimizes operational efficiency and cost. The company already has created an AWS CloudFormation template that can deploy the required network configuration in a secondary Region. </p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a second CloudFormation template that can recreate the EC2 instances in the secondary Region. Run daily multivolume snapshots by using AWS Systems Manager Automation runbooks. Copy the snapshots to the secondary Region. In the event of a failure launch the CloudFormation templates, restore the EBS volumes from snapshots, and transfer usage to the secondary Region."
        },
        {
          "label": "B",
          "content": "Use Amazon Data Lifecycle Manager (Amazon DLM) to create daily multivolume snapshots of the EBS volumes. In the event of a failure, launch the CloudFormation template and use Amazon DLM to restore the EBS volumes and transfer usage to the secondary Region."
        },
        {
          "label": "C",
          "content": "Use AWS Backup to create a scheduled daily backup plan for the EC2 instances. Configure the backup task to copy the backups to a vault in the secondary Region. In the event of a failure, launch the CloudFormation template, restore the instance volumes and configurations from the backup vault, and transfer usage to the secondary Region."
        },
        {
          "label": "D",
          "content": "Deploy EC2 instances of the same size and configuration to the secondary Region. Configure AWS DataSync daily to copy data from the primary Region to the secondary Region. In the event of a failure, launch the CloudFormation template and transfer usage to the secondary Region."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Option C is the correct answer because it utilizes AWS Backup, which is a fully managed service that can back up the Amazon EBS volumes and the EC2 instance configurations. By scheduling daily backups and copying them to a vault in a secondary region, the company can ensure that backups are maintained offsite and are available for a rapid recovery. The use of AWS Backup also provides a centralized management console for monitoring and reporting on backup activities, which aligns with the company's need for a solution that optimizes operational efficiency and cost. In the event of a failure, the company can use the CloudFormation template to recreate the network environment and restore the instance volumes and configurations from the backup vault within one business day, adhering to the company's recovery time objective (RTO) and recovery point objective (RPO).</p><p> Key Requirements:</p><p>1. Cross-Region Backups: Must maintain backups in a separate AWS Region.</p><p>2. Recovery Time Objective (RTO): Recover within 1 business day.</p><p>3. Recovery Point Objective (RPO): No more than 1 day's worth of data loss.</p><p>4. Operational Efficiency: Limited staff, so automation is crucial.</p><p>5. Cost Optimization: Need a cost-effective solution.</p><p>6. Existing CloudFormation Template: Already available for network configuration in the secondary Region.</p><p> Why Option C is Best?</p><p>- AWS Backup is a fully managed service that automates backup scheduling, retention, and cross-region copying.</p><p>- It supports EC2 instances and EBS volumes, ensuring consistent backups.</p><p>- Cross-Region Copy can be configured to replicate backups to the secondary Region.</p><p>- Restoration is simple: In case of failure, the CloudFormation template can deploy the network, and AWS Backup can restore the instances and EBS volumes.</p><p>- Minimal operational overhead: AWS Backup handles lifecycle management, reducing manual work.</p><p> Why Other Options Are Less Suitable?</p><p>- A: While possible, managing snapshots via Systems Manager Automation runbooks is more complex than using AWS Backup.</p><p>- B: Amazon DLM only manages EBS snapshots, not full instance recovery (including configurations). It doesn’t handle cross-region copying as seamlessly as AWS Backup.</p><p>- D: AWS DataSync is for active data replication, not backups. Running duplicate instances is costly, and this approach doesn’t guarantee point-in-time recovery.</p><p> Conclusion</p><p>AWS Backup (Option C) is the most automated, cost-effective, and compliant solution for meeting the RTO, RPO, and cross-region backup requirements.</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "0d5e1226f9f94cc5be1ee81a3e96509b",
      "questionNumber": 205,
      "type": "multiple",
      "content": "<p>Question #205</p><p>A company is designing a new website that hosts static content. The website will give users the ability to upload and download large files. According to company requirements, all data must be encrypted in transit and at rest. A solutions architect is building the solution by using Amazon S3 and Amazon CloudFront. Which combination of steps will meet the encryption requirements? (Choose three.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Turn on S3 server-side encryption for the S3 bucket that the web application uses."
        },
        {
          "label": "B",
          "content": "Add a policy attribute of &quot;aws:SecureTransport&quot;: &quot;true&quot; for read and write operations in the S3 ACLs."
        },
        {
          "label": "C",
          "content": "Create a bucket policy that denies any unencrypted operations in the S3 bucket that the web application uses."
        },
        {
          "label": "D",
          "content": "Configure encryption at rest on CloudFront by using server-side encryption with AWS KMS keys (SSE-KMS)."
        },
        {
          "label": "E",
          "content": "Configure redirection of HTTP requests to HTTPS requests in CloudFront."
        },
        {
          "label": "F",
          "content": "Use the RequireSSL option in the creation of presigned URLs for the S3 bucket that the web application uses."
        }
      ],
      "correctAnswer": "ACE",
      "explanation": "<p>The correct combination of steps to meet the encryption requirements (encrypted in transit and at rest) are:</p><p>A. Turn on S3 server-side encryption for the S3 bucket that the web application uses. &nbsp;</p><p>→ Ensures data is encrypted at rest in Amazon S3. &nbsp;</p><p>C. Create a bucket policy that denies any unencrypted operations in the S3 bucket that the web application uses. &nbsp;</p><p>→ Enforces encryption in transit by denying unencrypted (HTTP) requests to S3. &nbsp;</p><p>E. Configure redirection of HTTP requests to HTTPS requests in CloudFront. &nbsp;</p><p>→ Ensures all traffic between users and CloudFront is encrypted in transit by forcing HTTPS. &nbsp;</p><p> Why not the others? &nbsp;</p><p>- B: S3 ACLs (Access Control Lists) do not support the `aws:SecureTransport` condition; this must be set in a bucket policy, not ACLs. &nbsp;</p><p>- D: CloudFront does not store data persistently, so \"encryption at rest\" does not apply (it's for data in transit). &nbsp;</p><p>- F: Presigned URLs can enforce HTTPS, but this is not a primary solution for general encryption in transit (bucket policy and CloudFront HTTPS redirection are better). &nbsp;</p><p> Final Answer: &nbsp;</p><p>A. Enabling S3 server-side encryption ensures that the data stored in the S3 bucket is encrypted at rest.</p><p>C. A bucket policy that denies unencrypted operations ensures that any attempt to access the bucket without using HTTPS will be rejected, thus enforcing the use of encryption in transit for all operations with the bucket.</p><p>E. Configuring CloudFront to redirect HTTP requests to HTTPS requests ensures that all user interactions with the website are encrypted in transit.</p><p>While options D and F might seem relevant, they are not necessary for this scenario. Option D is about encrypting data at rest on CloudFront, which is redundant if the data is already encrypted at rest on S3. Option F, the RequireSSL option for pre-signed URLs, is an additional security measure but not a requirement for the given scenario since the question focuses on general data encryption at rest and in transit.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "bbfe6e28f5d640768ff6e82a15b12064",
      "questionNumber": 206,
      "type": "single",
      "content": "<p>Question #206</p><p>A company is implementing a serverless architecture by using AWS Lambda functions that need to access a Microsoft SQL Server DB instance on Amazon RDS. The company has separate environments for development and production, including a clone of the database system. </p><p><br></p><p>The company's developers are allowed to access the credentials for the development database. However, the credentials for the production database must be encrypted with a key that only members of the IT security team's IAM user group can access. This key must be rotated on a regular basis. </p><p><br></p><p>What should a solutions architect do in the production environment to meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Store the database credentials in AWS Systems Manager Parameter Store by using a SecureString parameter that is encrypted by an AWS Key Management Service (AWS KMS) customer managed key. Attach a role to each Lambda function to provide access to the SecureString parameter. Restrict access to the SecureString parameter and the customer managed key so that only the IT security team can access the parameter and the key."
        },
        {
          "label": "B",
          "content": "Encrypt the database credentials by using the AWS Key Management Service (AWS KMS) default Lambda key. Store the credentials in the environment variables of each Lambda function. Load the credentials from the environment variables in the Lambda code. Restrict access to the KMS key so that only the IT security team can access the key."
        },
        {
          "label": "C",
          "content": "Store the database credentials in the environment variables of each Lambda function. Encrypt the environment variables by using an AWS Key Management Service (AWS KMS) customer managed key. Restrict access to the customer managed key so that only the IT security team can access the key."
        },
        {
          "label": "D",
          "content": "Store the database credentials in AWS Secrets Manager as a secret that is associated with an AWS Key Management Service (AWS KMS) customer managed key. Attach a role to each Lambda function to provide access to the secret. Restrict access to the secret and the customer managed key so that only the IT security team can access the secret and the key."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Option D is the correct answer. AWS Secrets Manager is designed to securely manage and rotate secrets, such as database credentials. By storing the credentials in AWS Secrets Manager and encrypting them with a customer managed key in AWS KMS, the solution architect can ensure that the production database credentials are encrypted and only accessible to the IT security team. Additionally, Secrets Manager can rotate the credentials on a regular basis, meeting the requirement for key rotation. Attaching a role to each Lambda function allows the functions to access the secrets as needed, and restricting access to the secret and the KMS key ensures that only authorized individuals can manage the credentials.Here's why:</p><p> Requirements Recap:</p><p>1. Secure Storage of Production DB Credentials: The credentials must be encrypted, and only the IT security team should have access to the encryption key.</p><p>2. Key Rotation: The encryption key must be rotated regularly.</p><p>3. Lambda Access: Lambda functions need access to the credentials securely.</p><p>4. Restricted Access: Only the IT security team should manage the key and credentials.</p><p> Why Option D is Correct?</p><p>- AWS Secrets Manager is designed for securely storing and managing secrets (like database credentials).</p><p> &nbsp;- It integrates with AWS KMS for encryption using a customer-managed key (allowing key rotation).</p><p> &nbsp;- You can restrict access to the secret and the KMS key via IAM policies, ensuring only the IT security team can manage them.</p><p> &nbsp;- Lambda functions can retrieve the credentials securely by assuming an IAM role with permissions to access the secret.</p><p> Why Other Options Are Incorrect?</p><p>- Option A: While Systems Manager Parameter Store (SecureString) can store encrypted values, Secrets Manager is better suited for database credentials because it offers automatic rotation, tighter integration with RDS, and better access control.</p><p>- Option B: Storing credentials in environment variables (even if encrypted) is less secure than using Secrets Manager. Environment variables can be exposed in logs or via AWS APIs.</p><p>- Option C: Similar to B, storing credentials in environment variables is not best practice for sensitive data like production DB credentials.</p><p> Conclusion:</p><p>Option D is the most secure and AWS-recommended approach for managing database credentials in a serverless architecture while meeting the given requirements.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "4842601e8f834275968ced83b77beeea",
      "questionNumber": 207,
      "type": "single",
      "content": "<p>Question #207</p><p>An online retail company is migrating its legacy on-premises .NET application to AWS. The application runs on load-balanced frontend web servers, load-balanced application servers, and a Microsoft SQL Server database. </p><p><br></p><p>The company wants to use AWS managed services where possible and does not want to rewrite the application. A solutions architect needs to implement a solution to resolve scaling issues and minimize licensing costs as the application scales. </p><p><br></p><p>Which solution will meet these requirements MOST cost-effectively?</p>",
      "options": [
        {
          "label": "A",
          "content": "Deploy Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer for the web tier and for the application tier. Use Amazon Aurora PostgreSQL with Babelfish turned on to replatform the SQL Server database."
        },
        {
          "label": "B",
          "content": "Create images of all the servers by using AWS Database Migration Service (AWS DMS). Deploy Amazon EC2 instances that are based on the on-premises imports. Deploy the instances in an Auto Scaling group behind a Network Load Balancer for the web tier and for the application tier. Use Amazon DynamoDB as the database tier."
        },
        {
          "label": "C",
          "content": "Containerize the web frontend tier and the application tier. Provision an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. Create an Auto Scaling group behind a Network Load Balancer for the web tier and for the application tier. Use Amazon RDS for SQL Server to host the database."
        },
        {
          "label": "D",
          "content": "Separate the application functions into AWS Lambda functions. Use Amazon API Gateway for the web frontend tier and the application tier. Migrate the data to Amazon S3. Use Amazon Athena to query the data."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Based on the requirements to migrate a legacy .NET application to AWS using managed services where possible, without rewriting the application, while resolving scaling issues and minimizing licensing costs, the answer is:</p><p>A. Deploy Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer for the web tier and for the application tier. Use Amazon Aurora PostgreSQL with Babelfish turned on to replatform the SQL Server database.</p><p>This is the correct choice because:</p><p>It maintains the existing application architecture (web tier, application tier, database) without requiring a rewrite of the application code, which is a key requirement.</p><p>Using EC2 instances in Auto Scaling groups addresses the scaling issues by automatically adjusting capacity based on demand for both the web and application tiers.</p><p>Amazon Aurora PostgreSQL with Babelfish provides SQL Server compatibility while significantly reducing licensing costs. Babelfish allows Aurora PostgreSQL to understand T-SQL and TDS protocol, enabling SQL Server applications to work with Aurora PostgreSQL with minimal code changes.</p><p>This approach leverages AWS managed services (Auto Scaling, ALB, Aurora) while maintaining compatibility with the existing .NET application.</p><p>Option D (using Lambda, API Gateway, S3, and Athena) would require completely redesigning and rewriting the application into a serverless architecture, which violates the requirement of not rewriting the application. While serverless could potentially be cost-effective, the extensive redevelopment effort would be significant and unnecessary given the constraints.</p><p>The solution in option A provides the best balance of using AWS managed services, addressing scaling issues, minimizing licensing costs, and avoiding application rewrites.</p><p>Sources</p><p>Migrating SQL Server - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-microsoft-workloads-aws/migrating-sql-server-workloads.html）</p><p>How to optimize costs for Microsoft workloads on AWS | Microsoft Workloads on AWS （https://aws.amazon.com/cn/blogs/modernizing-with-aws/how-to-optimize-costs-for-microsoft-workloads-on-aws/）</p><p>Paths to the cloud - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-microsoft-workloads-aws/cloud-paths.html）</p><p>Make SQL Server end of support the start of your AWS cloud journey | Microsoft Workloads on AWS（https://aws.amazon.com/cn/blogs/modernizing-with-aws/make-sql-server-end-of-support-the-start-of-your-aws-cloud-journey/） </p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "e3f258dd832344538fe923d736837089",
      "questionNumber": 208,
      "type": "single",
      "content": "<p>Question #208</p><p>A software-as-a-service (SaaS) provider exposes APIs through an Application Load Balancer (ALB). The ALB connects to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster that is deployed in the us-east-1 Region. The exposed APIs contain usage of a few non-standard REST methods: LINK, UNLINK, LOCK, and UNLOCK. </p><p><br></p><p>Users outside the United States are reporting long and inconsistent response times for these APIs. A solutions architect needs to resolve this problem with a solution that minimizes operational overhead. </p><p><br></p><p>Which solution meets these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Add an Amazon CloudFront distribution. Configure the ALB as the origin."
        },
        {
          "label": "B",
          "content": "Add an Amazon API Gateway edge-optimized API endpoint to expose the APIs. Configure the ALB as the target."
        },
        {
          "label": "C",
          "content": "Add an accelerator in AWS Global Accelerator. Configure the ALB as the origin"
        },
        {
          "label": "D",
          "content": "Deploy the APIs to two additional AWS Regions: eu-west-1 and ap-southeast-2. Add latency-based routing records in Amazon Route 53."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the requirements to improve API response times for users outside the United States while minimizing operational overhead, the answer is:</p><p>C. Add an accelerator in AWS Global Accelerator. Configure the ALB as the origin.</p><p>This is the correct choice because:</p><p>AWS Global Accelerator is specifically designed to improve the availability and performance of applications for global users by routing traffic through AWS's global network infrastructure rather than the public internet.</p><p>Global Accelerator works seamlessly with ALBs and supports all TCP traffic, including non-standard HTTP methods like LINK, UNLINK, LOCK, and UNLOCK mentioned in the question.</p><p>Implementation requires minimal changes to the existing architecture - you simply create an accelerator and add the ALB as an endpoint, which aligns with the requirement for minimal operational overhead.</p><p>Global Accelerator provides static IP addresses that serve as fixed entry points to your application, making it easier for global users to connect with lower latency.</p><p>Let's compare with the other options:</p><p>Option A (CloudFront): While CloudFront can improve global performance, it's primarily designed for content delivery and caching. It may require additional configuration for non-standard HTTP methods and might not be optimal for dynamic API requests.</p><p>Option B (API Gateway): Would require significant architectural changes to your existing setup and potentially introduce new complexities, increasing operational overhead.</p><p>Option D (Multi-region deployment): While effective for performance, this approach requires deploying and maintaining the EKS cluster in multiple regions, which significantly increases operational overhead and costs.</p><p>AWS Global Accelerator provides the best balance of performance improvement for global users and minimal operational overhead for your specific use case with non-standard REST methods.</p><p>Sources</p><p>PERF05-BP02 Evaluate available networking features - AWS Well-Architected Framework (2023-04-10)（https://docs.aws.amazon.com/wellarchitected/2023-04-10/framework/perf_select_network_evaluate_features.html） </p><p>Get suggestions to optimize application performance in Internet Monitor (Optimize page) - Amazon CloudWatch （https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-IM-insights.html）</p><p>PERF05-BP02 Evaluate available networking features - AWS Well-Architected Framework (2022-03-31) （https://docs.aws.amazon.com/wellarchitected/2022-03-31/framework/perf_select_network_evaluate_features.html）</p><p>Amazon API Gateway - Serverless Applications Lens （https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/amazon-api-gateway.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "432fb1723c434d56b169e3fd23b2584c",
      "questionNumber": 209,
      "type": "single",
      "content": "<p>Question #209</p><p>A company runs an IoT application in the AWS Cloud. The company has millions of sensors that collect data from houses in the United States. The sensors use the MQTT protocol to connect and send data to a custom MQTT broker. The MQTT broker stores the data on a single Amazon EC2 instance. The sensors connect to the broker through the domain named iot.example.com. The company uses Amazon Route 53 as its DNS service. The company stores the data in Amazon DynamoDB. </p><p><br></p><p>On several occasions, the amount of data has overloaded the MQTT broker and has resulted in lost sensor data. The company must improve the reliability of the solution. </p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an Application Load Balancer (ALB) and an Auto Scaling group for the MQTT broker. Use the Auto Scaling group as the target for the ALB. Update the DNS record in Route 53 to an alias record. Point the alias record to the ALB. Use the MQTT broker to store the data."
        },
        {
          "label": "B",
          "content": "Set up AWS IoT Core to receive the sensor data. Create and configure a custom domain to connect to AWS IoT Core. Update the DNS record in Route 53 to point to the AWS IoT Core Data-ATS endpoint. Configure an AWS IoT rule to store the data."
        },
        {
          "label": "C",
          "content": "Create a Network Load Balancer (NLB). Set the MQTT broker as the target. Create an AWS Global Accelerator accelerator. Set the NLB as the endpoint for the accelerator. Update the DNS record in Route 53 to a multivalue answer record. Set the Global Accelerator IP addresses as values. Use the MQTT broker to store the data."
        },
        {
          "label": "D",
          "content": "Set up AWS IoT Greengrass to receive the sensor data. Update the DNS record in Route 53 to point to the AWS IoT Greengrass endpoint. Configure an AWS IoT rule to invoke an AWS Lambda function to store the data."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>The correct answer is B. Set up AWS IoT Core to receive the sensor data. Create and configure a custom domain to connect to AWS IoT Core. Update the DNS record in Route 53 to point to the AWS IoT Core Data-ATS endpoint. Configure an AWS IoT rule to store the data.</p><p> Why Option B is the Best Solution?</p><p>1. AWS IoT Core is a fully managed service designed to handle millions of IoT devices securely and reliably, eliminating the need for a self-managed MQTT broker.</p><p>2. Custom Domain allows the company to retain the existing domain (`iot.example.com`) while routing traffic to AWS IoT Core.</p><p>3. Route 53 Alias Record can point directly to the AWS IoT Core ATS endpoint, ensuring secure and reliable connectivity.</p><p>4. AWS IoT Rules can automatically store incoming data in Amazon DynamoDB, removing the bottleneck caused by the single EC2 instance.</p><p>5. Scalability & Reliability: AWS IoT Core scales automatically, preventing data loss due to overload.</p><p> Why Other Options Are Not Ideal?</p><p>- Option A: Using an ALB + Auto Scaling still requires managing an MQTT broker, which is less scalable and reliable than AWS IoT Core.</p><p>- Option C: A Network Load Balancer (NLB) + Global Accelerator improves availability but still relies on a self-managed broker, which is not as scalable as IoT Core.</p><p>- Option D: AWS IoT Greengrass is meant for edge computing, not for replacing an MQTT broker in the cloud.</p><p> Conclusion</p><p>AWS IoT Core is the best solution because it is fully managed, highly scalable, and integrates seamlessly with DynamoDB, ensuring no data loss due to broker overload. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "41a73d8ef3c9433b9d64db398533d109",
      "questionNumber": 210,
      "type": "single",
      "content": "<p>Question #210</p><p>A company has Linux-based Amazon EC2 instances. Users must access the instances by using SSH with EC2 SSH key pairs. Each machine requires a unique EC2 key pair. </p><p><br></p><p>The company wants to implement a key rotation policy that will, upon request, automatically rotate all the EC2 key pairs and keep the keys in a securely encrypted place. The company will accept less than 1 minute of downtime during key rotation. </p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Store all the keys in AWS Secrets Manager. Define a Secrets Manager rotation schedule to invoke an AWS Lambda function to generate new key pairs. Replace public keys on EC2 instances. Update the private keys in Secrets Manager."
        },
        {
          "label": "B",
          "content": "Store all the keys in Parameter Store, a capability of AWS Systems Manager, as a string. Define a Systems Manager maintenance window to invoke an AWS Lambda function to generate new key pairs. Replace public keys on EC2 instances. Update the private keys in Parameter Store."
        },
        {
          "label": "C",
          "content": "Import the EC2 key pairs into AWS Key Management Service (AWS KMS). Configure automatic key rotation for these key pairs. Create an Amazon EventBridge scheduled rule to invoke an AWS Lambda function to initiate the key rotation in AWS KMS."
        },
        {
          "label": "D",
          "content": "Add all the EC2 instances to Fleet Manager, a capability of AWS Systems Manager. Define a Systems Manager maintenance window to issue a Systems Manager Run Command document to generate new key pairs and to rotate public keys to all the instances in Fleet Manager."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>For automatically rotating EC2 SSH key pairs while keeping them securely encrypted with minimal downtime, the answer is:</p><p>A. Store all the keys in AWS Secrets Manager. Define a Secrets Manager rotation schedule to invoke an AWS Lambda function to generate new key pairs. Replace public keys on EC2 instances. Update the private keys in Secrets Manager.</p><p>This is the correct choice because:</p><p>AWS Secrets Manager is specifically designed for secure storage and automated rotation of sensitive information like SSH keys. It provides built-in rotation capabilities that can be scheduled according to your security policies.</p><p>Secrets Manager offers native encryption at rest using AWS KMS, ensuring your SSH keys remain securely encrypted.</p><p>The rotation process using Lambda functions allows for programmatic generation of new key pairs and updating of the public keys on EC2 instances, which can be accomplished with minimal downtime (less than 1 minute as required).</p><p>Secrets Manager provides fine-grained access control through IAM policies, ensuring that only authorized users and services can access the SSH keys.</p><p>The solution is fully automated once configured, meeting the requirement for automatic rotation upon request.</p><p>Option B (using Parameter Store) is less suitable because:</p><p>Parameter Store doesn't have built-in rotation capabilities like Secrets Manager</p><p>While Parameter Store can store encrypted strings, it lacks the specialized features for secret rotation that Secrets Manager provides</p><p>Parameter Store is primarily designed for configuration data rather than sensitive credentials that require regular rotation</p><p>AWS Secrets Manager is purpose-built for this exact use case - securely storing sensitive credentials and automating their rotation, making it the optimal solution for the company's requirements.</p><p>Sources</p><p>AWS Secrets Manager best practices - AWS Secrets Manager （https://docs.aws.amazon.com/secretsmanager/latest/userguide/best-practices.html）</p><p>Secrets management - Amazon Elastic Container Service （https://docs.aws.amazon.com/AmazonECS/latest/developerguide/specifying-sensitive-data.html）</p><p>Choosing an AWS cryptography service - Choosing an AWS cryptography service （https://docs.aws.amazon.com/decision-guides/latest/cryptography-on-aws-how-to-choose/guide.html）</p><p>Best practice for storing EC2 SSH keys, SQL Server encryption keys (SMK, DMK), other encryption keys | AWS re:Post （https://repost.aws/questions/QUFySsI0MDRsGEJ1C3dq0-hw/best-practice-for-storing-ec2-ssh-keys-sql-server-encryption-keys-smk-dmk-other-encryption-keys）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "f2f2e62b9dff41c4af6bcd201c0e5334",
      "questionNumber": 211,
      "type": "single",
      "content": "<p>Question #211</p><p>A company wants to migrate to AWS. The company is running thousands of VMs in a VMware ESXi environment. The company has no configuration management database and has little knowledge about the utilization of the VMware portfolio. </p><p><br></p><p>A solutions architect must provide the company with an accurate inventory so that the company can plan for a cost-effective migration. </p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use AWS Systems Manager Patch Manager to deploy Migration Evaluator to each VM. Review the collected data in Amazon QuickSight. Identify servers that have high utilization. Remove the servers that have high utilization from the migration list. Import the data to AWS Migration Hub."
        },
        {
          "label": "B",
          "content": "Export the VMware portfolio to a .csv file. Check the disk utilization for each server. Remove servers that have high utilization. Export the data to AWS Application Migration Service. Use AWS Server Migration Service (AWS SMS) to migrate the remaining servers."
        },
        {
          "label": "C",
          "content": "Deploy the Migration Evaluator agentless collector to the ESXi hypervisor. Review the collected data in Migration Evaluator. Identify inactive servers. Remove the inactive servers from the migration list. Import the data to AWS Migration Hub."
        },
        {
          "label": "D",
          "content": "Deploy the AWS Application Migration Service Agent to each VM. When the data is collected, use Amazon Redshift to import and analyze the data. Use Amazon QuickSight for data visualization."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the requirement to provide an accurate inventory for a cost-effective migration with the least operational overhead, the answer is:</p><p>C. Deploy the Migration Evaluator agentless collector to the ESXi hypervisor. Review the collected data in Migration Evaluator. Identify inactive servers. Remove the inactive servers from the migration list. Import the data to AWS Migration Hub.</p><p>This is the correct choice because:</p><p>Migration Evaluator (formerly TSO Logic) is specifically designed to provide detailed discovery and assessment of on-premises environments with minimal operational overhead. The agentless collector can be deployed directly to the ESXi hypervisor without requiring agents on each VM.</p><p>For an environment with thousands of VMs and little knowledge about utilization, the automated discovery and assessment capabilities of Migration Evaluator provide comprehensive data collection with minimal manual effort.</p><p>Migration Evaluator automatically collects detailed information about server specifications, utilization metrics, and dependencies, which is crucial when there's no configuration management database available.</p><p>The solution allows for identifying inactive servers based on actual utilization data, enabling the company to make informed decisions about which servers to migrate, which helps achieve a cost-effective migration.</p><p>Integration with AWS Migration Hub streamlines the migration planning and execution process, providing a central location to track migration progress across multiple AWS and partner tools.</p><p>Option B involves manually exporting data to CSV files and manually checking disk utilization for each server, which would require significantly more operational overhead for thousands of VMs. This manual approach is more time-consuming, error-prone, and less comprehensive than the automated discovery provided by Migration Evaluator.</p><p>The Migration Evaluator approach provides the most efficient path to creating an accurate inventory with minimal operational overhead, making it the best solution for this scenario.</p><p>Sources</p><p>VMware ESXI to AWS migration | AWS re:Post （https://repost.aws/questions/QUyuhMhfmQTvGlvy0lI8-f9w/vmware-esxi-to-aws-migration）</p><p>VMware migration - AWS Transform （https://docs.aws.amazon.com/transform/latest/userguide/transform-app-vmware.html）</p><p>AWS Agentic AI Options for migrating VMware based workloads | Migration & Modernization （https://aws.amazon.com/cn/blogs/migration-and-modernization/aws-agentic-ai-options-for-migrating-vmware-based-workloads/）</p><p>Choosing a migration approach for relocating your VMware applications and workloads to the AWS Cloud - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-vmware-aws/welcome.html）</p><p>Migrate VMware SDDC to VMware Cloud on AWS using VMware HCX - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-vmware-sddc-to-vmware-cloud-on-aws-using-vmware-hcx.html）</p><p>Migrate VMs to VMware Cloud on AWS by using HCX OS Assisted Migration - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-vms-to-vmware-cloud-on-aws-by-using-hcx-os-assisted-migration.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "28639344ac1e4a318279e45a386eea61",
      "questionNumber": 212,
      "type": "single",
      "content": "<p>Question #212</p><p>A company runs a microservice as an AWS Lambda function. The microservice writes data to an on-premises SQL database that supports a limited number of concurrent connections. When the number of Lambda function invocations is too high, the database crashes and causes application downtime. The company has an AWS Direct Connect connection between the company's VPC and the on-premises data center. The company wants to protect the database from crashes. </p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Write the data to an Amazon Simple Queue Service (Amazon SQS) queue. Configure the Lambda function to read from the queue and write to the existing database. Set a reserved concurrency limit on the Lambda function that is less than the number of connections that the database supports."
        },
        {
          "label": "B",
          "content": "Create a new Amazon Aurora Serverless DB cluster. Use AWS DataSync to migrate the data from the existing database to Aurora Serverless. Reconfigure the Lambda function to write to Aurora."
        },
        {
          "label": "C",
          "content": "Create an Amazon RDS Proxy DB instance. Attach the RDS Proxy DB instance to the Amazon RDS DB instance. Reconfigure the Lambda function to write to the RDS Proxy DB instance."
        },
        {
          "label": "D",
          "content": "Write the data to an Amazon Simple Notification Service (Amazon SNS) topic. Invoke the Lambda function to write to the existing database when the topic receives new messages. Configure provisioned concurrency for the Lambda function to be equal to the number of connections that the database supports."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Option A is the correct answer. By writing data to an Amazon SQS queue, the Lambda function can offload the workload from the on-premises SQL database. The queue acts as a buffer that smooths out the invocation rate, preventing the database from being overwhelmed by too many concurrent connections. Configuring the Lambda function to process messages from the queue at a controlled rate ensures that the number of connections to the database does not exceed its capacity. Setting a reserved concurrency limit on the Lambda function helps to further control the rate of execution and protect the database from being overloaded.</p><p>The issue is that the on-premises SQL database crashes due to too many concurrent connections from the Lambda function. To solve this, we need to: &nbsp;</p><p>1. Throttle the Lambda function's concurrency to match the database's connection limit. &nbsp;</p><p>2. Use an SQS queue as a buffer to handle spikes in requests without overwhelming the database. &nbsp;</p><p>Option A is correct because: &nbsp;</p><p>- SQS decouples the Lambda function from the database, allowing writes to be processed at a controlled rate. &nbsp;</p><p>- Reserved concurrency ensures Lambda doesn’t exceed the database’s connection limit. &nbsp;</p><p>- Direct Connect is still used (no need to migrate data or change the database). &nbsp;</p><p> Why not the other options? &nbsp;</p><p>- B (Aurora Serverless + DataSync): Migrating to Aurora is unnecessary and doesn’t solve the concurrency issue directly. &nbsp;</p><p>- C (RDS Proxy): RDS Proxy manages database connections, but it only works with AWS RDS databases, not on-premises SQL databases. &nbsp;</p><p>- D (SNS + Provisioned Concurrency): SNS doesn’t buffer requests like SQS, and provisioned concurrency doesn’t limit concurrent executions (it only pre-initializes instances). &nbsp;</p><p> Final Answer: &nbsp;</p><p>✅ A. Write the data to an Amazon SQS queue, configure the Lambda function to read from the queue, and set a reserved concurrency limit.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "6c940ba5cd8e42849d4349701c97b458",
      "questionNumber": 213,
      "type": "single",
      "content": "<p>Question #213</p><p>A company uses a Grafana data visualization solution that runs on a single Amazon EC2 instance to monitor the health of the company's AWS workloads. The company has invested time and effort to create dashboards that the company wants to preserve. The dashboards need to be highly available and cannot be down for longer than 10 minutes. The company needs to minimize ongoing maintenance. </p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Migrate to Amazon CloudWatch dashboards. Recreate the dashboards to match the existing Grafana dashboards. Use automatic dashboards where possible."
        },
        {
          "label": "B",
          "content": "Create an Amazon Managed Grafana workspace. Configure a new Amazon CloudWatch data source. Export dashboards from the existing Grafana instance. Import the dashboards into the new workspace."
        },
        {
          "label": "C",
          "content": "Create an AMI that has Grafana pre-installed. Store the existing dashboards in Amazon Elastic File System (Amazon EFS). Create an Auto Scaling group that uses the new AMI. Set the Auto Scaling group&#39;s minimum, desired, and maximum number of instances to one. Create an Application Load Balancer that serves at least two Availability Zones."
        },
        {
          "label": "D",
          "content": "Configure AWS Backup to back up the EC2 instance that runs Grafana once each hour. Restore the EC2 instance from the most recent snapshot in an alternate Availability Zone when required."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the requirements to make the Grafana dashboards highly available with less than 10 minutes of downtime while minimizing ongoing maintenance and operational overhead, the answer is:</p><p>B. Create an Amazon Managed Grafana workspace. Configure a new Amazon CloudWatch data source. Export dashboards from the existing Grafana instance. Import the dashboards into the new workspace.</p><p>This is the correct choice because:</p><p>Amazon Managed Grafana is a fully managed service that handles provisioning, setup, scaling, and maintenance automatically, which directly addresses the requirement to minimize ongoing maintenance and operational overhead.</p><p>The company has invested time and effort in creating Grafana dashboards that they want to preserve. Amazon Managed Grafana allows for direct export/import of existing Grafana dashboards, preserving their investment without requiring recreation of dashboards from scratch.</p><p>Amazon Managed Grafana provides built-in high availability across multiple Availability Zones, meeting the requirement for high availability.</p><p>The migration process of exporting dashboards from the existing Grafana instance and importing them into Amazon Managed Grafana can be completed within the 10-minute downtime constraint.</p><p>Amazon Managed Grafana maintains the familiar Grafana interface and functionality that the company's team is already accustomed to, reducing any learning curve.</p><p>Option A (migrating to CloudWatch dashboards) would require recreating all dashboards from scratch, which represents significantly more operational overhead and might not preserve all the visualization capabilities and customizations of the existing Grafana dashboards. This approach would likely take longer and require more effort than the 10-minute downtime constraint allows.</p><p>Amazon Managed Grafana provides the best balance of preserving existing dashboards, ensuring high availability, minimizing downtime, and reducing ongoing operational overhead.</p><p>Sources</p><p>Amazon Managed Grafana for dashboarding and visualization - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/implementing-logging-monitoring-cloudwatch/amg-dashboarding-visualization.html）</p><p>Configuring AWS Managed Grafana with CloudWatch Integration using CloudFormation | AWS re:Post （https://repost.aws/articles/ARSRP9KmeUSWyrGUiCg64DOg/configuring-aws-managed-grafana-with-cloudwatch-integration-using-cloudformation）</p><p>Enhance your SAP Observability using Amazon Managed Prometheus and Grafana | AWS for SAP （https://aws.amazon.com/cn/blogs/awsforsap/enhance-your-sap-observability-using-amazon-managed-prometheus-and-amazon-managed-grafana/）</p><p>Amazon Managed Grafana Features | Amazon Web Services （https://aws.amazon.com/cn/grafana/features/）</p><p>Setting up an Amazon Managed Grafana workspace - Amazon SageMaker AI （https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-cluster-observability-slurm-managed-grafana-ws.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "b548e4f17ac442aebbd9c26318678c21",
      "questionNumber": 214,
      "type": "single",
      "content": "<p>Question #214</p><p>A company needs to migrate its customer transactions database from on premises to AWS. The database resides on an Oracle DB instance that runs on a Linux server. According to a new security requirement, the company must rotate the database password each year. </p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Convert the database to Amazon DynamoDB by using the AWS Schema Conversion Tool (AWS SCT). Store the password in AWS Systems Manager Parameter Store. Create an Amazon CloudWatch alarm to invoke an AWS Lambda function for yearly password rotation."
        },
        {
          "label": "B",
          "content": "Migrate the database to Amazon RDS for Oracle. Store the password in AWS Secrets Manager. Turn on automatic rotation. Configure a yearly rotation schedule."
        },
        {
          "label": "C",
          "content": "Migrate the database to an Amazon EC2 instance. Use AWS Systems Manager Parameter Store to keep and rotate the connection string by using an AWS Lambda function on a yearly schedule."
        },
        {
          "label": "D",
          "content": "Migrate the database to Amazon Neptune by using the AWS Schema Conversion Tool (AWS SCT). Create an Amazon CloudWatch alarm to invoke an AWS Lambda function for yearly password rotation."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the requirement to migrate an Oracle database from on-premises to AWS with yearly password rotation while minimizing operational overhead, the answer is:</p><p>B. Migrate the database to Amazon RDS for Oracle. Store the password in AWS Secrets Manager. Turn on automatic rotation. Configure a yearly rotation schedule.</p><p>This is the correct choice because:</p><p>Amazon RDS for Oracle is a managed database service that significantly reduces operational overhead by automating routine database tasks such as hardware provisioning, database setup, patching, and backups.</p><p>AWS Secrets Manager is specifically designed for storing and automatically rotating credentials, including database passwords. It has built-in functionality for automatic rotation on a schedule (such as yearly), which directly addresses the security requirement.</p><p>The integration between RDS for Oracle and Secrets Manager is seamless, allowing for automated password rotation without application downtime or manual intervention.</p><p>This solution requires the least amount of custom code or manual processes to maintain compared to the other options.</p><p>Let's compare with the alternatives:</p><p>Option C (EC2 with Systems Manager Parameter Store and Lambda) would require managing the EC2 instance, installing and configuring Oracle, and creating custom Lambda functions for password rotation. This involves significantly more operational overhead than the managed RDS solution.</p><p>Option D (Amazon Neptune with AWS SCT) would require not only migrating but also converting the Oracle database to a completely different database type (graph database), which would likely require application changes and is not a direct replacement for an Oracle relational database. This would involve the highest operational overhead and complexity.</p><p>The combination of Amazon RDS for Oracle and AWS Secrets Manager provides the most streamlined, automated solution with the least operational overhead while meeting the yearly password rotation requirement.</p><p>Sources</p><p>Strategies for Migrating Oracle Databases to AWS - Strategies for Migrating Oracle Databases to AWS（https://docs.aws.amazon.com/whitepapers/latest/strategies-migrating-oracle-db-to-aws/strategies-migrating-oracle-db-to-aws.html） </p><p>Migrate an on-premises Oracle database to Amazon RDS for Oracle - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-an-on-premises-oracle-database-to-amazon-rds-for-oracle.html）</p><p>Oracle database migration strategies - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-oracle-database/strategies.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "8ee384473e694912812ab77dc8568ddb",
      "questionNumber": 215,
      "type": "multiple",
      "content": "<p>Question #215</p><p>A solutions architect is designing an AWS account structure for a company that consists of multiple teams. All the teams will work in the same AWS Region. The company needs a VPC that is connected to the on-premises network. The company expects less than 50 Mbps of total traffic to and from the on-premises network. Which combination of steps will meet these requirements MOST cost-effectively? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an AWS CloudFormation template that provisions a VPC and the required subnets. Deploy the template to each AWS account."
        },
        {
          "label": "B",
          "content": "Create an AWS CloudFormation template that provisions a VPC and the required subnets. Deploy the template to a shared services account. Share the subnets by using AWS Resource Access Manager."
        },
        {
          "label": "C",
          "content": "Use AWS Transit Gateway along with an AWS Site-to-Site VPN for connectivity to the on-premises network. Share the transit gateway by using AWS Resource Access Manager."
        },
        {
          "label": "D",
          "content": "Use AWS Site-to-Site VPN for connectivity to the on-premises network."
        },
        {
          "label": "E",
          "content": "Use AWS Direct Connect for connectivity to the on-premises network."
        }
      ],
      "correctAnswer": "BD",
      "explanation": "<p>Based on the requirements to design a cost-effective AWS account structure with on-premises connectivity for multiple teams with less than 50 Mbps of traffic, the answers are:</p><p>B. Create an AWS CloudFormation template that provisions a VPC and the required subnets. Deploy the template to a shared services account. Share the subnets by using AWS Resource Access Manager.</p><p>D. Use AWS Site-to-Site VPN for connectivity to the on-premises network.</p><p>These are the correct choices because:</p><p>Option B (Shared VPC with Resource Access Manager):</p><p>Creating a single VPC in a shared services account and sharing subnets using AWS Resource Access Manager is more cost-effective than creating separate VPCs in each team's account.</p><p>This approach reduces the number of networking resources to manage and maintain.</p><p>It centralizes network management while still allowing individual teams to deploy their resources in the shared subnets.</p><p>It improves IP address utilization and simplifies the overall network architecture.</p><p>Option D (AWS Site-to-Site VPN):</p><p>For the specified traffic requirement of less than 50 Mbps, AWS Site-to-Site VPN is significantly more cost-effective than Direct Connect.</p><p>Site-to-Site VPN provides encrypted connectivity over the internet without requiring dedicated hardware or circuits.</p><p>It's easier and faster to set up compared to Direct Connect, which requires physical connections and can take weeks to provision.</p><p>The monthly cost of Site-to-Site VPN is much lower than Direct Connect for this bandwidth requirement.</p><p>Option A (separate VPCs in each account) would increase complexity and cost by requiring multiple VPCs, potentially leading to IP address space fragmentation and more complex routing.</p><p>Option E (AWS Direct Connect) would be unnecessarily expensive for the stated bandwidth requirement of less than 50 Mbps. Direct Connect is more suitable for higher bandwidth needs or when consistent network performance is critical.</p><p>The combination of a shared VPC using Resource Access Manager and Site-to-Site VPN provides the most cost-effective solution that meets all the stated requirements.</p><p>Sources</p><p>VPC sharing - Building a Scalable and Secure Multi-VPC AWS Network Infrastructure （https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/amazon-vpc-sharing.html）</p><p>Transit gateway attachment configuration | AWS re:Post （https://repost.aws/questions/QUvX0QyO3pRQWkBQspwcgVKg/transit-gateway-attachment-configuration）</p><p>VPC peering - Building a Scalable and Secure Multi-VPC AWS Network Infrastructure （https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/vpc-peering.html）</p><p>Client VPN with VPC peering vs Client VPN withTransit gateway | AWS re:Post （https://repost.aws/questions/QUUotXjy2mTV6noMA-J86AXQ/client-vpn-with-vpc-peering-vs-client-vpn-withtransit-gateway）</p><p>AWS Direct Connect + AWS Transit Gateway + AWS Site-to-Site VPN - Amazon Virtual Private Cloud Connectivity Options （https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-aws-transit-gateway-vpn.html）</p><p>Centralize network connectivity using AWS Transit Gateway - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/centralize-network-connectivity-using-aws-transit-gateway.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "d9aa6ac6c0274e48a5e6ff89a5e240a6",
      "questionNumber": 216,
      "type": "single",
      "content": "<p>Question #216</p><p>A solutions architect at a large company needs to set up network security for outbound traffic to the internet from all AWS accounts within an organization in AWS Organizations. The organization has more than 100 AWS accounts, and the accounts route to each other by using a centralized AWS Transit Gateway. Each account has both an internet gateway and a NAT gateway for outbound traffic to the internet. The company deploys resources only into a single AWS Region. </p><p><br></p><p>The company needs the ability to add centrally managed rule-based filtering on all outbound traffic to the internet for all AWS accounts in the organization. The peak load of outbound traffic will not exceed 25 Gbps in each Availability Zone. </p><p><br></p><p>Which solution meets these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a new VPC for outbound traffic to the internet. Connect the existing transit gateway to the new VPC. Configure a new NAT gateway. Create an Auto Scaling group of Amazon EC2 instances that run an open-source internet proxy for rule-based filtering across all Availability Zones in the Region. Modify all default routes to point to the proxy&#39;s Auto Scaling group."
        },
        {
          "label": "B",
          "content": "Create a new VPC for outbound traffic to the internet. Connect the existing transit gateway to the new VPC. Configure a new NAT gateway. Use an AWS Network Firewall firewall for rule-based filtering. Create Network Firewall endpoints in each Availability Zone. Modify all default routes to point to the Network Firewall endpoints."
        },
        {
          "label": "C",
          "content": "Create an AWS Network Firewall firewall for rule-based filtering in each AWS account. Modify all default routes to point to the Network Firewall firewalls in each account."
        },
        {
          "label": "D",
          "content": "In each AWS account, create an Auto Scaling group of network-optimized Amazon EC2 instances that run an open-source internet proxy for rule-based filtering. Modify all default routes to point to the proxy&#39;s Auto Scaling group."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Option B is the correct answer. By creating a new VPC dedicated to outbound internet traffic and connecting it to the existing transit gateway, the company can centralize the management of outbound traffic. Configuring a new NAT gateway within this VPC ensures that traffic to the internet is properly routed and translated. The use of AWS Network Firewall allows for rule-based filtering, and by creating endpoints in each Availability Zone, the solution provides a scalable and highly available architecture. Modifying the default routes to point to the Network Firewall endpoints ensures that all outbound traffic from the VPCs is subject to the filtering policies, meeting the company's security requirements with minimal operational overhead.</p><p>The requirements are: &nbsp;</p><p>1. Centrally managed rule-based filtering for outbound internet traffic across 100+ AWS accounts. &nbsp;</p><p>2. Peak traffic ≤ 25 Gbps per Availability Zone (AWS Network Firewall supports up to 30 Gbps per endpoint). &nbsp;</p><p>3. Single AWS Region deployment with a centralized Transit Gateway for inter-account routing. &nbsp;</p><p>4. No open-source proxy maintenance (AWS Network Firewall is a managed service). &nbsp;</p><p> Why Option B is Correct? &nbsp;</p><p>✅ Centralized Security VPC: A dedicated VPC with AWS Network Firewall provides rule-based filtering for all outbound traffic. &nbsp;</p><p>✅ Transit Gateway Integration: The existing Transit Gateway connects all accounts to the centralized firewall. &nbsp;</p><p>✅ Scalability: Network Firewall endpoints in each AZ handle up to 30 Gbps, meeting the 25 Gbps requirement. &nbsp;</p><p>✅ No Proxy Maintenance: Unlike open-source proxies (Option A & D), AWS Network Firewall is fully managed. &nbsp;</p><p>✅ Route Modification: Default routes in all VPCs point to the Network Firewall endpoints, enforcing filtering. &nbsp;</p><p> Why Other Options Are Wrong? &nbsp;</p><p>❌ A: Open-source proxy on EC2 requires management overhead and lacks AWS’s scalability. &nbsp;</p><p>❌ C: Deploying Network Firewall per account is not centralized and increases management complexity. &nbsp;</p><p>❌ D: Open-source proxies on EC2 lack centralized control and require manual scaling. &nbsp;</p><p> Conclusion: &nbsp;</p><p>Option B is the most scalable, manageable, and AWS-native solution for centralized outbound traffic filtering. &nbsp;</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "700281c1cc8f4e1b9f991115453ea0c8",
      "questionNumber": 217,
      "type": "single",
      "content": "<p>Question #217</p><p>A company uses a load balancer to distribute traffic to Amazon EC2 instances in a single Availability Zone. The company is concerned about security and wants a solutions architect to re-architect the solution to meet the following requirements:：</p><p><br></p><p>- Inbound requests must be filtered for common vulnerability attacks.</p><p>- Rejected requests must be sent to a third-party auditing application.</p><p>- All resources should be highly available.</p><p><br></p><p>Which solution meets these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Configure a Multi-AZ Auto Scaling group using the application&#39;s AMI. Create an Application Load Balancer (ALB) and select the previously created Auto Scaling group as the target. Use Amazon Inspector to monitor traffic to the ALB and EC2 instances. Create a web ACL in WAF. Create an AWS WAF using the web ACL and ALB. Use an AWS Lambda function to frequently push the Amazon Inspector report to the third-party auditing application."
        },
        {
          "label": "B",
          "content": "Configure an Application Load Balancer (ALB) and add the EC2 instances as targets. Create a web ACL in WAF. Create an AWS WAF using the web ACL and ALB name and enable logging with Amazon CloudWatch Logs. Use an AWS Lambda function to frequently push the logs to the third-party auditing application."
        },
        {
          "label": "C",
          "content": "Configure an Application Load Balancer (ALB) along with a target group adding the EC2 instances as targets. Create an Amazon Kinesis Data Firehose with the destination of the third-party auditing application. Create a web ACL in WAF. Create an AWS WAF using the web ACL and ALB then enable logging by selecting the Kinesis Data Firehose as the destination. Subscribe to AWS Managed Rules in AWS Marketplace, choosing the WAF as the subscriber."
        },
        {
          "label": "D",
          "content": "Configure a Multi-AZ Auto Scaling group using the application&#39;s AMI. Create an Application Load Balancer (ALB) and select the previously created Auto Scaling group as the target. Create an Amazon Kinesis Data Firehose with a destination of the third-party auditing application. Create a web ACL in WAF. Create an AWS WAF using the WebACL and ALB then enable logging by selecting the Kinesis Data Firehose as the destination. Subscribe to AWS Managed Rules in AWS Marketplace, choosing the WAF as the subscriber."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Option D is the correct answer. This solution ensures high availability by using a Multi-AZ Auto Scaling group, which allows the application to remain available even if one Availability Zone goes down. The Application Load Balancer (ALB) distributes traffic to the EC2 instances. AWS WAF (Web Application Firewall) is used to filter the inbound requests for common vulnerability attacks, and a web ACL (Web Access Control List) is created to define the rules for these filters. The use of Amazon Kinesis Data Firehose allows the rejected requests to be sent to the third-party auditing application. Additionally, enabling logging with the Kinesis Data Firehose as the destination ensures that logs are sent to the auditing application. Subscribing to AWS Managed Rules in AWS Marketplace provides additional security measures by leveraging predefined security rules.</p><p>1. Inbound requests filtered for common vulnerability attacks: &nbsp;</p><p> &nbsp; - AWS WAF with a web ACL and AWS Managed Rules (from AWS Marketplace) helps filter and block common web exploits like SQL injection, XSS, etc.</p><p>2. Rejected requests sent to a third-party auditing application: &nbsp;</p><p> &nbsp; - Enabling WAF logging and configuring Kinesis Data Firehose to deliver logs to the third-party auditing application fulfills this requirement.</p><p>3. High availability: &nbsp;</p><p> &nbsp; - A Multi-AZ Auto Scaling group ensures that EC2 instances are distributed across multiple Availability Zones, providing fault tolerance. &nbsp;</p><p> &nbsp; - The ALB also inherently supports high availability by distributing traffic across healthy instances.</p><p> Why not the other options?</p><p>- A: Uses Amazon Inspector, which is for vulnerability assessment of EC2 instances, not real-time traffic filtering. Also, Lambda pushing reports is less efficient than direct logging via Kinesis Firehose.</p><p>- B: Lacks Multi-AZ Auto Scaling, risking availability. Also, CloudWatch Logs + Lambda is less direct than Kinesis Firehose for log delivery.</p><p>- C: Misses Multi-AZ Auto Scaling, compromising high availability.</p><p> Correct Answer: D ✅ &nbsp;</p><p>This solution covers WAF filtering, log delivery via Kinesis Firehose, and high availability with Multi-AZ Auto Scaling.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "5329b80a37bd474b9d97871e17a127b0",
      "questionNumber": 218,
      "type": "single",
      "content": "<p>Question #218</p><p>A company is running an application in the AWS Cloud. The application consists of microservices that run on a fleet of Amazon EC2 instances in multiple Availability Zones behind an Application Load Balancer. The company recently added a new REST API that was implemented in Amazon API Gateway. Some of the older microservices that run on EC2 instances need to call this new API. </p><p><br></p><p>The company does not want the API to be accessible from the public internet and does not want proprietary data to traverse the public internet. </p><p><br></p><p>What should a solutions architect do to meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an AWS Site-to-Site VPN connection between the VPC and the API Gateway. Use API Gateway to generate a unique API Key for each microservice. Configure the API methods to require the key."
        },
        {
          "label": "B",
          "content": "Create an interface VPC endpoint for API Gateway, and set an endpoint policy to only allow access to the specific API. Add a resource policy to API Gateway to only allow access from the VPC endpoint. Change the API Gateway endpoint type to private."
        },
        {
          "label": "C",
          "content": "Modify the API Gateway to use IAM authentication. Update the IAM policy for the IAM role that is assigned to the EC2 instances to allow access to the API Gateway. Move the API Gateway into a new VPC. Deploy a transit gateway and connect the VPCs."
        },
        {
          "label": "D",
          "content": "Create an accelerator in AWS Global Accelerator, and connect the accelerator to the API Gateway. Update the route table for all VPC subnets with a route to the created Global Accelerator endpoint IP address. Add an API key for each service to use for authentication."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Option B is the correct answer. By creating an interface VPC endpoint for API Gateway, the architect can establish a private connection between the VPC and the API Gateway, ensuring that the API is not accessible from the public internet. Setting an endpoint policy to only allow access to the specific API and adding a resource policy to API Gateway to only allow access from the VPC endpoint further enhances security. Changing the API Gateway endpoint type to private ensures that the API is not exposed to the public internet, thus protecting proprietary data from traversing the public internet.</p><p>1. The API should not be accessible from the public internet.</p><p>2. Proprietary data should not traverse the public internet.</p><p>3. Only the older microservices (EC2 instances) should be able to call the new API.</p><p> Solution Breakdown (Option B):</p><p>1. Create an interface VPC endpoint for API Gateway &nbsp;</p><p> &nbsp; - This allows private connectivity between the VPC (where EC2 instances are running) and API Gateway without going over the public internet.</p><p> &nbsp; - The endpoint policy restricts access to only the specific API.</p><p>2. Set an API Gateway resource policy to allow access only from the VPC endpoint &nbsp;</p><p> &nbsp; - Ensures that only requests originating from the VPC (via the endpoint) can access the API.</p><p>3. Change the API Gateway endpoint type to private &nbsp;</p><p> &nbsp; - This makes the API accessible only via the VPC endpoint, blocking public internet access entirely.</p><p> Why Not the Other Options?</p><p>- A: A VPN is unnecessary since API Gateway supports private VPC endpoints. Also, API keys alone don't enforce private network access.</p><p>- C: IAM authentication is good for access control, but it doesn't prevent traffic from going over the internet unless combined with a private endpoint (which is already covered in Option B).</p><p>- D: Global Accelerator improves performance but still uses public IPs, so data traverses the internet, violating the requirement.</p><p> Conclusion:</p><p>Option B is the most secure and efficient solution, ensuring private connectivity between the EC2 instances and API Gateway without exposing the API to the internet.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "81d294e5725c458781ab8870674669e3",
      "questionNumber": 219,
      "type": "single",
      "content": "<p>Question #219</p><p>A company has set up its entire infrastructure on AWS. The company uses Amazon EC2 instances to host its ecommerce website and uses Amazon S3 to store static data. Three engineers at the company handle the cloud administration and development through one AWS account. Occasionally, an engineer alters an EC2 security group configuration of another engineer and causes noncompliance issues in the environment. </p><p><br></p><p>A solutions architect must set up a system that tracks changes that the engineers make. The system must send alerts when the engineers make noncompliant changes to the security settings for the EC2 instances. </p><p><br></p><p>What is the FASTEST way for the solutions architect to meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Set up AWS Organizations for the company. Apply SCPs to govern and track noncompliant security group changes that are made to the AWS account."
        },
        {
          "label": "B",
          "content": "Enable AWS CloudTrail to capture the changes to EC2 security groups. Enable Amazon CloudWatch rules to provide alerts when noncompliant security settings are detected."
        },
        {
          "label": "C",
          "content": "Enable SCPs on the AWS account to provide alerts when noncompliant security group changes are made to the environment."
        },
        {
          "label": "D",
          "content": "Enable AWS Config on the EC2 security groups to track any noncompliant changes. Send the changes as alerts through an Amazon Simple Notification Service (Amazon SNS) topic."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Option D is the fastest way to meet the requirements. AWS Config is designed to track and evaluate the configurations of AWS resources, including EC2 security groups. By enabling AWS Config, the architect can quickly set up monitoring and alerting for configuration changes. Any noncompliant changes to the EC2 security groups can be sent as alerts through an Amazon SNS topic, allowing for immediate notification when changes occur. This approach provides a direct and efficient method for tracking changes and ensuring compliance, without the need for additional infrastructure or more complex setups.</p><p>- AWS Config is specifically designed to track configuration changes and assess compliance against predefined rules. It can detect noncompliant changes to EC2 security groups and trigger alerts via Amazon SNS.</p><p>- AWS CloudTrail (Option B) logs API calls but does not directly assess compliance or provide alerts without additional CloudWatch rules and custom logic.</p><p>- SCPs (Options A & C) are used in AWS Organizations to restrict permissions but do not track changes or send alerts.</p><p>- Option D provides a direct, automated solution for tracking and alerting on noncompliant security group changes without requiring complex setups.</p><p>Thus, Option D is the most efficient and fastest solution. </p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "75beb12673b34e92b377dcd2a595eeed",
      "questionNumber": 220,
      "type": "multiple",
      "content": "<p>Question #220</p><p>A company has IoT sensors that monitor traffic patterns throughout a large city. The company wants to read and collect data from the sensors and perform aggregations on the data. A solutions architect designs a solution in which the IoT devices are streaming to Amazon Kinesis Data Streams. Several applications are reading from the stream. However, several consumers are experiencing throttling and are periodically encountering a ReadProvisionedThroughputExceeded error. Which actions should the solutions architect take to resolve this issue? (Choose three.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Reshard the stream to increase the number of shards in the stream."
        },
        {
          "label": "B",
          "content": "Use the Kinesis Producer Library (KPL). Adjust the polling frequency."
        },
        {
          "label": "C",
          "content": "Use consumers with the enhanced fan-out feature."
        },
        {
          "label": "D",
          "content": "Reshard the stream to reduce the number of shards in the stream."
        },
        {
          "label": "E",
          "content": "Use an error retry and exponential backoff mechanism in the consumer logic."
        },
        {
          "label": "F",
          "content": "Configure the stream to use dynamic partitioning."
        }
      ],
      "correctAnswer": "ACE",
      "explanation": "<p>Based on the scenario where IoT sensors are streaming data to Kinesis Data Streams and multiple applications are experiencing ReadProvisionedThroughputExceeded errors, the correct actions to resolve this issue are:</p><p>A. Reshard the stream to increase the number of shards in the stream. </p><p>C. Use consumers with the enhanced fan-out feature. </p><p>E. Use an error retry and exponential backoff mechanism in the consumer logic.</p><p>Here's why these are the correct choices:</p><p>A. Reshard the stream to increase the number of shards in the stream:</p><p>Each Kinesis shard provides a limited read throughput (2 MB/sec shared among all standard consumers).</p><p>When multiple applications are reading from the same stream, they're competing for this limited throughput.</p><p>Increasing the number of shards distributes the data and read operations across more shards, providing more total throughput capacity for the stream.</p><p>This directly addresses the ReadProvisionedThroughputExceeded error by providing more resources.</p><p>C. Use consumers with the enhanced fan-out feature:</p><p>Enhanced fan-out allows each consumer to get dedicated throughput of up to 2 MB/sec per shard.</p><p>This eliminates contention between consumers since each gets its own dedicated throughput.</p><p>Enhanced fan-out is specifically designed for scenarios with multiple consumers reading from the same stream.</p><p>This feature significantly reduces the likelihood of throughput-related errors.</p><p>E. Use an error retry and exponential backoff mechanism in the consumer logic:</p><p>Even with proper provisioning, occasional throttling can still occur.</p><p>Implementing retry logic with exponential backoff is a best practice for handling transient errors.</p><p>This approach allows the application to gracefully handle temporary throughput issues.</p><p>The exponential backoff prevents overwhelming the system with immediate retries.</p><p>The other options are incorrect because:</p><p>B (Using KPL and adjusting polling frequency): The Kinesis Producer Library is for producers, not consumers, so it wouldn't help with read throughput issues.</p><p>D (Reducing the number of shards): This would decrease available throughput and make the problem worse.</p><p>F (Dynamic partitioning): This is a feature of Kinesis Data Firehose, not Kinesis Data Streams, and wouldn't address the read throughput issue.</p><p>Sources</p><p>Troubleshoot Kinesis Data Streams consumers - Amazon Kinesis Data Streams （https://docs.aws.amazon.com/streams/latest/dev/troubleshooting-consumers.html）</p><p>GetRecords - Amazon Kinesis Data Streams Service （https://docs.aws.amazon.com/kinesis/latest/APIReference/API_GetRecords.html）</p><p>Use Amazon Kinesis Data Streams to deliver real-time data to Amazon OpenSearch Service domains with Amazon OpenSearch Ingestion | AWS Big Data Blog （https://aws.amazon.com/cn/blogs/big-data/use-amazon-kinesis-data-streams-to-deliver-real-time-data-to-amazon-opensearch-service-domains-with-amazon-opensearch-ingestion/）</p><p>Handle startup, shutdown, and throttling - Amazon Kinesis Data Streams （https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-additional-considerations.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "be8b4dddfcc34da6be184b512bfd9199",
      "questionNumber": 221,
      "type": "single",
      "content": "<p>Question #221</p><p>A company uses AWS Organizations to manage its AWS accounts. The company needs a list of all its Amazon EC2 instances that have underutilized CPU or memory usage. The company also needs recommendations for how to downsize these underutilized instances. </p><p><br></p><p>Which solution will meet these requirements with the LEAST effort?</p>",
      "options": [
        {
          "label": "A",
          "content": "Install a CPU and memory monitoring tool from AWS Marketplace on all the EC2 instances. Store the findings in Amazon S3. Implement a Python script to identify underutilized instances. Reference EC2 instance pricing information for recommendations about downsizing options."
        },
        {
          "label": "B",
          "content": "Install the Amazon CloudWatch agent on all the EC2 instances by using AWS Systems Manager. Retrieve the resource optimization recommendations from AWS Cost Explorer in the organization&rsquo;s management account. Use the recommendations to downsize underutilized instances in all accounts of the organization."
        },
        {
          "label": "C",
          "content": "Install the Amazon CloudWatch agent on all the EC2 instances by using AWS Systems Manager. Retrieve the resource optimization recommendations from AWS Cost Explorer in each account of the organization. Use the recommendations to downsize underutilized instances in all accounts of the organization."
        },
        {
          "label": "D",
          "content": "Install the Amazon CloudWatch agent on all the EC2 instances by using AWS Systems Manager. Create an AWS Lambda function to extract CPU and memory usage from all the EC2 instances. Store the findings as files in Amazon S3. Use Amazon Athena to find underutilized instances. Reference EC2 instance pricing information for recommendations about downsizing options."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the requirement to get a list of underutilized EC2 instances across an organization with recommendations for downsizing with the least effort, the answer is:</p><p>B. Install the Amazon CloudWatch agent on all the EC2 instances by using AWS Systems Manager. Retrieve the resource optimization recommendations from AWS Cost Explorer in the organization's management account. Use the recommendations to downsize underutilized instances in all accounts of the organization.</p><p>This is the correct choice because:</p><p>Installing the CloudWatch agent using AWS Systems Manager allows for efficient collection of detailed memory and CPU metrics across all EC2 instances in the organization. Systems Manager provides a centralized way to deploy and configure the agent across multiple accounts.</p><p>AWS Cost Explorer in the organization's management account provides consolidated resource optimization recommendations for all member accounts in the AWS Organizations structure. This centralized approach allows you to view and analyze recommendations for all accounts from a single location.</p><p>Accessing the recommendations from the management account requires significantly less effort than accessing each individual account separately, as you only need to log in once and can see all recommendations in one place.</p><p>Option C suggests retrieving recommendations from each individual account in the organization, which would require logging into each account separately and manually consolidating the data. This approach would require much more effort, especially for organizations with many accounts.</p><p>The centralized approach in option B provides a comprehensive view of all underutilized instances across the organization from a single access point, making it the solution that requires the least effort to meet the company's requirements.</p><p>Sources</p><p>AWS Cost Optimization Hub and AWS Organizations - AWS Organizations （https://docs.aws.amazon.com/organizations/latest/userguide/services-that-can-integrate-coh.html）</p><p>Optimize Your AWS Spend with New Cost Savings Features in AWS Trusted Advisor | AWS Cloud Financial Management （https://aws.amazon.com/cn/blogs/aws-cloud-financial-management/optimize-your-aws-spend-with-new-cost-savings-features-in-aws-trusted-advisor/）</p><p>AWS Cost Optimization | AWS Cloud Financial Management （https://aws.amazon.com/cn/aws-cost-management/cost-optimization-hub/）</p><p>Best practices to optimize costs after mergers and acquisitions with AWS Organizations | AWS Cloud Operations Blog（https://aws.amazon.com/cn/blogs/mt/best-practices-to-optimize-costs-after-mergers-and-acquisitions-with-aws-organizations/） </p><p>Compute Optimizer Dashboard - Guidance for Advanced Cloud Observability with Cloud Intelligence Dashboards on AWS （https://docs.aws.amazon.com/guidance/latest/advanced-cloud-observability-with-cloud-intelligence-dashboard/compute-optimizer-dashboard.html）</p><p>Optimizing your cost with rightsizing recommendations - AWS Cost Management （https://docs.aws.amazon.com/cost-management/latest/userguide/ce-rightsizing.html）</p><p>Cost optimization recommendations (from Cost Optimization Hub) - AWS Data Exports（https://docs.aws.amazon.com/cur/latest/userguide/table-dictionary-cor.html） </p><p>Cost Optimization Hub and AWS Organizations trusted access - AWS Cost Management （https://docs.aws.amazon.com/cost-management/latest/userguide/coh-trusted-access.html）</p><p>Getting started with Cost Optimization Hub - AWS Cost Management （https://docs.aws.amazon.com/cost-management/latest/userguide/coh-getting-started.html）</p><p>CUR report for Organisation | AWS re:Post （https://repost.aws/questions/QURjr672h1QPGj4ja2fGGnRA/cur-report-for-organisation）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "a93e7c2ddffb4974af781d9ca2be28ca",
      "questionNumber": 222,
      "type": "multiple",
      "content": "<p>Question #222</p><p>A company wants to run a custom network analysis software package to inspect traffic as traffic leaves and enters a VPC. The company has deployed the solution by using AWS CloudFormation on three Amazon EC2 instances in an Auto Scaling group. All network routing has been established to direct traffic to the EC2 instances. Whenever the analysis software stops working, the Auto Scaling group replaces an instance. The network routes are not updated when the instance replacement occurs. Which combination of steps will resolve this issue? (Choose three.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Create alarms based on EC2 status check metrics that will cause the Auto Scaling group to replace the failed instance."
        },
        {
          "label": "B",
          "content": "Update the CloudFormation template to install the Amazon CloudWatch agent on the EC2 instances. Configure the CloudWatch agent to send process metrics for the application."
        },
        {
          "label": "C",
          "content": "Update the CloudFormation template to install AWS Systems Manager Agent on the EC2 instances. Configure Systems Manager Agent to send process metrics for the application."
        },
        {
          "label": "D",
          "content": "Create an alarm for the custom metric in Amazon CloudWatch for the failure scenarios. Configure the alarm to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic."
        },
        {
          "label": "E",
          "content": "Create an AWS Lambda function that responds to the Amazon Simple Notification Service (Amazon SNS) message to take the instance out of service. Update the network routes to point to the replacement instance."
        },
        {
          "label": "F",
          "content": "In the CloudFormation template, write a condition that updates the network routes when a replacement instance is launched."
        }
      ],
      "correctAnswer": "BDE",
      "explanation": "<p>The correct combination of steps to resolve the issue is B, D, and E. Option B involves updating the CloudFormation template to include the CloudWatch agent, which can monitor the health of the application running on the EC2 instances. Option D involves setting up an alarm in CloudWatch that triggers when the custom metric, indicative of the application's failure, is reached. This alarm can be configured to send a message to an SNS topic. Option E involves creating a Lambda function that is triggered by the SNS message. This function can then execute the logic required to update the network routes to point to the new instance launched by the Auto Scaling group, ensuring that the traffic is directed to the healthy instance.</p><p>- B: Install the Amazon CloudWatch agent to monitor the application process and send custom metrics to CloudWatch. This ensures we detect software failures, not just EC2-level issues. &nbsp;</p><p>- D: Create a CloudWatch alarm for the custom metric (application failure) and configure it to publish to an SNS topic. This triggers a notification when the software fails. &nbsp;</p><p>- E: Use a Lambda function subscribed to the SNS topic to update network routes when a replacement instance is launched. This ensures traffic is redirected properly. &nbsp;</p><p> Why not the others?</p><p>- A: EC2 status checks only detect instance-level failures, not application-level failures. &nbsp;</p><p>- C: Systems Manager Agent (SSM) is useful for management but not directly for triggering Auto Scaling or route updates. &nbsp;</p><p>- F: CloudFormation conditions are evaluated at stack creation/update, not dynamically during Auto Scaling events. &nbsp;</p><p>Thus, B, D, E is the correct combination.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "984db2f3ea204264aa5846723a951f01",
      "questionNumber": 223,
      "type": "single",
      "content": "<p>Question #223</p><p>A company is developing a new on-demand video application that is based on microservices. The application will have 5 million users at launch and will have 30 million users after 6 months. The company has deployed the application on Amazon Elastic Container Service (Amazon ECS) on AWS Fargate. The company developed the application by using ECS services that use the HTTPS protocol. </p><p><br></p><p>A solutions architect needs to implement updates to the application by using blue/green deployments. The solution must distribute traffic to each ECS service through a load balancer. The application must automatically adjust the number of tasks in response to an Amazon CloudWatch alarm. </p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Configure the ECS services to use the blue/green deployment type and a Network Load Balancer. Request increases to the service quota for tasks per service to meet the demand."
        },
        {
          "label": "B",
          "content": "Configure the ECS services to use the blue/green deployment type and a Network Load Balancer. Implement Auto Scaling group for each ECS service by using the Cluster Autoscaler."
        },
        {
          "label": "C",
          "content": "Configure the ECS services to use the blue/green deployment type and an Application Load Balancer. Implement an Auto Scaling group for each ECS service by using the Cluster Autoscaler."
        },
        {
          "label": "D",
          "content": "Configure the ECS services to use the blue/green deployment type and an Application Load Balancer. Implement Service Auto Scaling for each ECS service."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Option D is the correct answer. Using an Application Load Balancer (ALB) is suitable for handling HTTPS traffic, which is required by the application. The blue/green deployment type ensures that updates can be implemented with minimal downtime. Service Auto Scaling, as mentioned in option D, allows each ECS service to automatically adjust the number of tasks in response to CloudWatch alarms, which aligns with the requirement for automatic scaling based on traffic demand. The Cluster Autoscaler is not needed for ECS services running on AWS Fargate, as Fargate manages the infrastructure scaling automatically.</p><p>1. Blue/Green Deployment with ECS: &nbsp;</p><p> &nbsp; - AWS supports blue/green deployments natively in Amazon ECS when using CodeDeploy. This allows traffic to be shifted gradually between the old (blue) and new (green) versions of the service.</p><p> &nbsp; - The Application Load Balancer (ALB) is required (not Network Load Balancer) because it supports path-based routing, host-based routing, and integration with ECS blue/green deployments via CodeDeploy.</p><p>2. Application Load Balancer (ALB) vs. Network Load Balancer (NLB): &nbsp;</p><p> &nbsp; - The question mentions the application uses HTTPS, which is best suited for ALB (Layer 7 load balancing) rather than NLB (Layer 4). &nbsp;</p><p> &nbsp; - ALB also integrates seamlessly with ECS blue/green deployments.</p><p>3. Auto Scaling for ECS Services: &nbsp;</p><p> &nbsp; - Service Auto Scaling (using Target Tracking Policies or Step Scaling) is the correct way to automatically adjust the number of tasks in response to a CloudWatch alarm. &nbsp;</p><p> &nbsp; - The Cluster Autoscaler (mentioned in options B and C) is used for scaling the underlying EC2 instances in an ECS cluster (if using EC2 launch type), but the question specifies AWS Fargate, which is serverless and does not require EC2 scaling. &nbsp;</p><p> &nbsp; - Service Auto Scaling is the proper way to scale tasks (containers) in Fargate.</p><p> Why Not the Other Options?</p><p>- A & B: Incorrect because they suggest using a Network Load Balancer (NLB), which is not optimal for HTTPS-based microservices (ALB is better for Layer 7 traffic). &nbsp;</p><p>- B & C: Incorrect because they mention Cluster Autoscaler, which is irrelevant for Fargate (only applies to EC2 launch type). &nbsp;</p><p>- C: Incorrect because it suggests using the Cluster Autoscaler instead of Service Auto Scaling.</p><p> Final Answer:</p><p>D. Configure the ECS services to use the blue/green deployment type and an Application Load Balancer. Implement Service Auto Scaling for each ECS service.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": true,
      "noteContent": "Cluster Autoscaler 是 Kubernetes 概念，不适用于 ECS"
    },
    {
      "id": "a35751c009744884a14a7858161c1026",
      "questionNumber": 224,
      "type": "single",
      "content": "<p>Question #224</p><p>A company is running a containerized application in the AWS Cloud. The application is running by using Amazon Elastic Container Service (Amazon ECS) on a set of Amazon EC2 instances. The EC2 instances run in an Auto Scaling group. </p><p><br></p><p>The company uses Amazon Elastic Container Registry (Amazon ECR) to store its container images. When a new image version is uploaded, the new image version receives a unique tag. </p><p><br></p><p>The company needs a solution that inspects new image versions for common vulnerabilities and exposures. The solution must automatically delete new image tags that have Critical or High severity findings. The solution also must notify the development team when such a deletion occurs. </p><p><br></p><p>Which solution meets these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Configure scan on push on the repository. Use Amazon EventBridge to invoke an AWS Step Functions state machine when a scan is complete for images that have Critical or High severity findings. Use the Step Functions state machine to delete the image tag for those images and to notify the development team through Amazon Simple Notification Service (Amazon SNS)."
        },
        {
          "label": "B",
          "content": "Configure scan on push on the repository. Configure scan results to be pushed to an Amazon Simple Queue Service (Amazon SQS) queue. Invoke an AWS Lambda function when a new message is added to the SQS queue. Use the Lambda function to delete the image tag for images that have Critical or High severity findings. Notify the development team by using Amazon Simple Email Service (Amazon SES)."
        },
        {
          "label": "C",
          "content": "Schedule an AWS Lambda function to start a manual image scan every hour. Configure Amazon EventBridge to invoke another Lambda function when a scan is complete. Use the second Lambda function to delete the image tag for images that have Critical or High severity findings. Notify the development team by using Amazon Simple Notification Service (Amazon SNS)."
        },
        {
          "label": "D",
          "content": "Configure periodic image scan on the repository. Configure scan results to be added to an Amazon Simple Queue Service (Amazon SQS) queue. Invoke an AWS Step Functions state machine when a new message is added to the SQS queue. Use the Step Functions state machine to delete the image tag for images that have Critical or High severity findings. Notify the development team by using Amazon Simple Email Service (Amazon SES)."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Option A is the correct answer. It provides an automated solution where the scan on push is configured for the ECR repository, and Amazon EventBridge is used to trigger a Step Functions state machine upon completion of the scan. If the scan identifies Critical or High severity issues, the state machine can then execute steps to delete the problematic image tag and notify the development team via Amazon SNS. This approach ensures that the process is automatic, efficient, and provides clear communication to the development team about the action taken.</p><p>The requirements are: &nbsp;</p><p>1. Scan new image versions for vulnerabilities – This is achieved by enabling scan on push in Amazon ECR. &nbsp;</p><p>2. Automatically delete image tags with Critical/High severity findings – Amazon EventBridge can detect scan completion events and trigger a Step Functions workflow to delete the image tag. &nbsp;</p><p>3. Notify the development team – The Step Functions workflow can use Amazon SNS to send notifications. &nbsp;</p><p> Why Option A is correct: &nbsp;</p><p>- Scan on push ensures images are scanned immediately upon upload. &nbsp;</p><p>- EventBridge can capture the ECR Image Scan event and trigger Step Functions when Critical/High findings are detected. &nbsp;</p><p>- Step Functions can orchestrate both the deletion of the image tag and sending an SNS notification. &nbsp;</p><p> Why other options are incorrect: &nbsp;</p><p>- B: While SQS can be used, it adds unnecessary complexity compared to EventBridge. Also, SES is not the best choice for real-time notifications (SNS is preferred). &nbsp;</p><p>- C: Manual scanning every hour is inefficient and does not guarantee immediate action on new image uploads. &nbsp;</p><p>- D: Periodic scans (instead of scan-on-push) introduce delays, and using SQS + SES is less efficient than EventBridge + SNS. &nbsp;</p><p>Thus, Option A is the most efficient and fully automated solution. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "9b7a5fcbc09c47f6a36fb24bcf4be26b",
      "questionNumber": 225,
      "type": "single",
      "content": "<p>Question #225</p><p>A company runs many workloads on AWS and uses AWS Organizations to manage its accounts. The workloads are hosted on Amazon EC2, AWS Fargate, and AWS Lambda. Some of the workloads have unpredictable demand. Accounts record high usage in some months and low usage in other months. </p><p><br></p><p>The company wants to optimize its compute costs over the next 3 years. A solutions architect obtains a 6-month average for each of the accounts across the organization to calculate usage. </p><p><br></p><p>Which solution will provide the MOST cost savings for all the organization's compute usage?</p>",
      "options": [
        {
          "label": "A",
          "content": "Purchase Reserved Instances for the organization to match the size and number of the most common EC2 instances from the member accounts."
        },
        {
          "label": "B",
          "content": "Purchase a Compute Savings Plan for the organization from the management account by using the recommendation at the management account level."
        },
        {
          "label": "C",
          "content": "Purchase Reserved Instances for each member account that had high EC2 usage according to the data from the last 6 months."
        },
        {
          "label": "D",
          "content": "Purchase an EC2 Instance Savings Plan for each member account from the management account based on EC2 usage data from the last 6 months."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the scenario where a company runs workloads on EC2, Fargate, and Lambda with unpredictable demand across AWS Organizations, the solution that will provide the MOST cost savings is:</p><p>B. Purchase a Compute Savings Plan for the organization from the management account by using the recommendation at the management account level.</p><p>This is the correct choice because:</p><p>Compute Savings Plans offer the most flexibility across the company's diverse compute services (EC2, Fargate, and Lambda). Unlike Reserved Instances which only apply to EC2, Compute Savings Plans automatically apply to all eligible compute usage regardless of service type.</p><p>Compute Savings Plans are ideal for workloads with unpredictable demand, as mentioned in the scenario. They provide discounts in exchange for a commitment to a consistent amount of compute usage (measured in $/hour) rather than committing to specific instance types or configurations.</p><p>When purchased at the organization level from the management account, Compute Savings Plans automatically apply across all member accounts. This provides maximum flexibility as usage can shift between accounts without losing the discount benefits.</p><p>For workloads that vary month to month (high usage in some months, low in others), Compute Savings Plans will automatically apply to whatever eligible compute resources are being used, maximizing the utilization of the commitment.</p><p>Option A (Reserved Instances at the organization level) would be less effective because:</p><p>Reserved Instances only apply to EC2, not Fargate or Lambda</p><p>They are less flexible for unpredictable workloads as they're tied to specific instance families and sizes</p><p>They would require more management to ensure full utilization</p><p>Option C (Reserved Instances for individual member accounts) would be the least effective because:</p><p>It doesn't allow for sharing discounts across accounts when usage patterns shift</p><p>It requires more management overhead</p><p>It doesn't cover Fargate or Lambda usage</p><p>Compute Savings Plans at the organization level provide the optimal balance of flexibility, coverage across multiple compute services, and administrative simplicity for the company's diverse and unpredictable workloads.</p><p>Sources</p><p>Optimize spending for Windows on Amazon EC2 - AWS Prescriptive Guidance（https://docs.aws.amazon.com/prescriptive-guidance/latest/optimize-costs-microsoft-workloads/savings-plans.html） </p><p>COST07-BP01 Perform pricing model analysis - Cost Optimization Pillar （https://docs.aws.amazon.com/wellarchitected/latest/cost-optimization-pillar/cost_pricing_model_analysis.html）</p><p>COST07-BP01 Perform pricing model analysis - AWS Well-Architected Framework （https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_pricing_model_analysis.html）</p><p>My boss asking me on cost optimization in AWS | AWS re:Post （https://repost.aws/questions/QU3MV37BYRR6ePI6WmUgOW0g/my-boss-asking-me-on-cost-optimization-in-aws）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "07b94f7a554146509d3bfbbedbf7e6b0",
      "questionNumber": 226,
      "type": "single",
      "content": "<p>Question #226</p><p>A company has hundreds of AWS accounts. The company uses an organization in AWS Organizations to manage all the accounts. The company has turned on all features. </p><p><br></p><p>A finance team has allocated a daily budget for AWS costs. The finance team must receive an email notification if the organization's AWS costs exceed 80% of the allocated budget. A solutions architect needs to implement a solution to track the costs and deliver the notifications. </p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "In the organization&#39;s management account, use AWS Budgets to create a budget that has a daily period. Add an alert threshold and set the value to 80%. Use Amazon Simple Notification Service (Amazon SNS) to notify the finance team."
        },
        {
          "label": "B",
          "content": "In the organization&rsquo;s management account, set up the organizational view feature for AWS Trusted Advisor. Create an organizational view report for cost optimization. Set an alert threshold of 80%. Configure notification preferences. Add the email addresses of the finance team."
        },
        {
          "label": "C",
          "content": "Register the organization with AWS Control Tower. Activate the optional cost control (guardrail). Set a control (guardrail) parameter of 80%. Configure control (guardrail) notification preferences. Use Amazon Simple Notification Service (Amazon SNS) to notify the finance team."
        },
        {
          "label": "D",
          "content": "Configure the member accounts to save a daily AWS Cost and Usage Report to an Amazon S3 bucket in the organization&#39;s management account. Use Amazon EventBridge to schedule a daily Amazon Athena query to calculate the organization&rsquo;s costs. Configure Athena to send an Amazon CloudWatch alert if the total costs are more than 80% of the allocated budget. Use Amazon Simple Notification Service (Amazon SNS) to notify the finance team."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Option A is the correct answer. AWS Budgets can be used to create a budget for the organization's AWS costs with a daily time period. By setting an alert threshold at 80% of the allocated budget, the finance team can be notified when costs are approaching the limit. Amazon SNS can be used to send email notifications to the finance team when the threshold is breached, providing a direct and automated method for cost monitoring and alerting.</p><p>AWS Budgets is the most appropriate service for this scenario because it allows you to: &nbsp;</p><p>- Set custom budgets to track costs across an entire AWS Organization. &nbsp;</p><p>- Define alert thresholds (e.g., 80% of the daily budget). &nbsp;</p><p>- Configure notifications via Amazon SNS to alert the finance team when costs exceed the threshold. &nbsp;</p><p> Why not the other options? &nbsp;</p><p>- B: AWS Trusted Advisor’s organizational view does not provide real-time budget alerts based on a percentage threshold. It focuses on cost optimization recommendations, not budget tracking. &nbsp;</p><p>- C: AWS Control Tower’s guardrails are not designed for daily budget tracking with percentage-based alerts. They enforce compliance rules rather than financial thresholds. &nbsp;</p><p>- D: While technically possible, this solution is overly complex compared to AWS Budgets, which provides built-in budget tracking and alerting without requiring manual queries or additional services like Athena and EventBridge. &nbsp;</p><p> Conclusion: &nbsp;</p><p>Option A is the simplest and most effective solution for tracking AWS costs and sending notifications when they exceed 80% of the daily budget. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "57349235830841df9fd35801ff26a516",
      "questionNumber": 227,
      "type": "single",
      "content": "<p>Question #227</p><p>A company provides auction services for artwork and has users across North America and Europe. The company hosts its application in Amazon EC2 instances in the us-east-1 Region. Artists upload photos of their work as large-size, high-resolution image files from their mobile phones to a centralized Amazon S3 bucket created in the us-east-1 Region. The users in Europe are reporting slow performance for their image uploads. </p><p><br></p><p>How can a solutions architect improve the performance of the image upload process?</p>",
      "options": [
        {
          "label": "A",
          "content": "Redeploy the application to use S3 multipart uploads."
        },
        {
          "label": "B",
          "content": "Create an Amazon CloudFront distribution and point to the application as a custom origin."
        },
        {
          "label": "C",
          "content": "Configure the buckets to use S3 Transfer Acceleration."
        },
        {
          "label": "D",
          "content": "Create an Auto Scaling group for the EC2 instances and create a scaling policy."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Configure the buckets to use S3 Transfer Acceleration.</p><p>This solution will best improve the performance of the image upload process for users in Europe for the following reasons:</p><p>S3 Transfer Acceleration is specifically designed to optimize long-distance transfers between clients and S3 buckets, which directly addresses the problem of European users experiencing slow upload performance to a bucket in us-east-1.</p><p>It works by utilizing Amazon's global network of edge locations to route data more efficiently. When users in Europe upload files with Transfer Acceleration enabled, their data is routed through the nearest AWS edge location in Europe and then transferred to the destination bucket in us-east-1 using optimized network paths.</p><p>S3 Transfer Acceleration can improve upload speeds by 50-500% for long-distance transfers of larger files, which is particularly relevant for the \"large-size, high-resolution image files\" mentioned in the scenario.</p><p>It's designed specifically to minimize latency caused by geographic distance, which is the core issue in this scenario (European users uploading to a US-based bucket).</p><p>Option A (Redeploy the application to use S3 multipart uploads) would help with reliability and throughput for large files, but it doesn't specifically address the latency issues caused by geographic distance. While multipart uploads allow parallel uploading of parts, the data still needs to travel the same distance.</p><p>Option B (Create an Amazon CloudFront distribution and point to the application as a custom origin) is not appropriate for this scenario. CloudFront is primarily designed for content delivery (downloads) rather than uploads. It helps accelerate content delivery to users but doesn't provide the same benefits for upload acceleration.</p><p>S3 Transfer Acceleration is the most appropriate solution as it's specifically designed to address the exact problem described in the scenario - improving upload performance over long geographic distances for large files.</p><p>Sources</p><p>Use Amazon S3 Transfer Acceleration to Minimize Latency Caused by Distance - Best Practices Design Patterns: Optimizing Amazon S3 Performance （https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html）</p><p>Looking to compare and understand S3 transfer accelerator vs batch operations vs AWS Backup for S3 | AWS re:Post （https://repost.aws/questions/QUFnrSpEOdQ_mZuGpL2iQxVw/looking-to-compare-and-understand-s3-transfer-accelerator-vs-batch-operations-vs-aws-backup-for-s3）</p><p>I am getting very low speed while uploading data to AWS S3 deep archive storage class | AWS re:Post （https://repost.aws/questions/QUCyvqCmNLQVGFCjZcz147-Q/i-am-getting-very-low-speed-while-uploading-data-to-aws-s3-deep-archive-storage-class）</p><p>Performance guidelines for Amazon S3 - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance-guidelines.html）</p><p>Using Amazon S3 Transfer Acceleration to Accelerate Geographically Disparate Data Transfers - Best Practices Design Patterns: Optimizing Amazon S3 Performance （https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html）</p><p>S3 Transfer Rates Capped at 2.8mb/s ??? How Can I Speed This Up? | AWS re:Post （https://repost.aws/questions/QUUdZKwva0TO6EwrPcFLtErA/s3-transfer-rates-capped-at-2-8mb-s-how-can-i-speed-this-up）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "03f927f647a44e49ac26899700cf1b2a",
      "questionNumber": 228,
      "type": "single",
      "content": "<p>Question #228</p><p>A company wants to containerize a multi-tier web application and move the application from an on-premises data center to AWS. The application includes web, application, and database tiers. The company needs to make the application fault tolerant and scalable. Some frequently accessed data must always be available across application servers. Frontend web servers need session persistence and must scale to meet increases in traffic. </p><p><br></p><p>Which solution will meet these requirements with the LEAST ongoing operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Run the application on Amazon Elastic Container Service (Amazon ECS) on AWS Fargate. Use Amazon Elastic File System (Amazon EFS) for data that is frequently accessed between the web and application tiers. Store the frontend web server session data in Amazon Simple Queue Service (Amazon SQS)."
        },
        {
          "label": "B",
          "content": "Run the application on Amazon Elastic Container Service (Amazon ECS) on Amazon EC2. Use Amazon ElastiCache for Redis to cache frontend web server session data. Use Amazon Elastic Block Store (Amazon EBS) with Multi-Attach on EC2 instances that are distributed across multiple Availability Zones."
        },
        {
          "label": "C",
          "content": "Run the application on Amazon Elastic Kubernetes Service (Amazon EKS). Configure Amazon EKS to use managed node groups. Use ReplicaSets to run the web servers and applications. Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system across all EKS pods to store frontend web server session data."
        },
        {
          "label": "D",
          "content": "Deploy the application on Amazon Elastic Kubernetes Service (Amazon EKS). Configure Amazon EKS to use managed node groups. Run the web servers and application as Kubernetes deployments in the EKS cluster. Store the frontend web server session data in an Amazon DynamoDB table. Create an Amazon Elastic File System (Amazon EFS) volume that all applications will mount at the time of deployment."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Option D is the correct answer. Deploying the application on Amazon EKS allows for container orchestration and scaling, which is essential for fault tolerance and scalability. Using managed node groups simplifies cluster management. Storing session data in DynamoDB ensures that it is highly available and scalable, while using EFS for shared application data provides a file system that can be mounted by all application pods. This combination of services offers a robust and scalable solution with minimal operational overhead.</p><p>Analysis of Options:</p><p> Option A:</p><p>- Uses Amazon ECS on Fargate (serverless, low operational overhead).</p><p>- Uses Amazon EFS for shared data (good for frequently accessed data).</p><p>- Problem: Amazon SQS is not suitable for session persistence (SQS is a queueing service, not a session store). This does not meet the session persistence requirement.</p><p> Option B:</p><p>- Uses Amazon ECS on EC2 (requires managing EC2 instances, higher operational overhead).</p><p>- Uses ElastiCache for Redis (good for session persistence).</p><p>- Problem: Amazon EBS with Multi-Attach is not a good solution for shared storage across Availability Zones (EBS volumes are AZ-bound and Multi-Attach is limited to a single AZ). This does not meet fault tolerance requirements.</p><p> Option C:</p><p>- Uses Amazon EKS with managed node groups (reduces operational overhead).</p><p>- Uses EFS for shared storage (good for frequently accessed data).</p><p>- Problem: EFS is not ideal for session persistence (high latency compared to in-memory solutions like ElastiCache or DynamoDB). This may not meet performance requirements for session persistence.</p><p> Option D:</p><p>- Uses Amazon EKS with managed node groups (reduces operational overhead).</p><p>- Uses DynamoDB for session persistence (scalable, highly available, and low-latency).</p><p>- Uses EFS for shared data (good for frequently accessed data).</p><p>- Meets all requirements:</p><p> &nbsp;- Fault-tolerant and scalable (EKS + DynamoDB + EFS).</p><p> &nbsp;- Session persistence handled by DynamoDB (better than EFS or SQS).</p><p> &nbsp;- Shared data handled by EFS.</p><p> &nbsp;- Minimal operational overhead (managed services).</p><p> Why Not Other Options?</p><p>- A: Incorrect session persistence solution (SQS is not suitable).</p><p>- B: EBS Multi-Attach is not multi-AZ, increasing operational overhead.</p><p>- C: EFS is not optimal for session persistence.</p><p> Conclusion:</p><p>Option D is the best solution because it:</p><p>- Uses managed services (EKS, DynamoDB, EFS) to minimize operational overhead.</p><p>- Provides scalability and fault tolerance.</p><p>- Uses DynamoDB for session persistence (better than EFS or SQS).</p><p>- Uses EFS for shared frequently accessed data.</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": true,
      "noteContent": "Multi-Attach is limited to a single AZ"
    },
    {
      "id": "6eff5b0434424c8eae0807c30279baeb",
      "questionNumber": 229,
      "type": "single",
      "content": "<p>Question #229</p><p>A solutions architect is planning to migrate critical Microsoft SQL Server databases to AWS. Because the databases are legacy systems, the solutions architect will move the databases to a modern data architecture. The solutions architect must migrate the databases with near-zero downtime. <br><br>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use AWS Application Migration Service and the AWS Schema Conversion Tool (AWS SCT). Perform an in-place upgrade before the migration. Export the migrated data to Amazon Aurora Serverless after cutover. Repoint the applications to Amazon Aurora."
        },
        {
          "label": "B",
          "content": "Use AWS Database Migration Service (AWS DMS) to rehost the database. Set Amazon S3 as a target. Set up change data capture (CDC) replication. When the source and destination are fully synchronized, load the data from Amazon S3 into an Amazon RDS for Microsoft SQL Server DB instance."
        },
        {
          "label": "C",
          "content": "Use native database high availability tools. Connect the source system to an Amazon RDS for Microsoft SQL Server DB instance. Configure replication accordingly. When data replication is finished, transition the workload to an Amazon RDS for Microsoft SQL Server DB instance."
        },
        {
          "label": "D",
          "content": "Use AWS Application Migration Service. Rehost the database server on Amazon EC2. When data replication is finished, detach the database and move the database to an Amazon RDS for Microsoft SQL Server DB instance. Reattach the database and then cut over all networking."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Use native database high availability tools. Connect the source system to an Amazon RDS for Microsoft SQL Server DB instance. Configure replication accordingly. When data replication is finished, transition the workload to an Amazon RDS for Microsoft SQL Server DB instance.</p><p>This solution will meet the requirements for the following reasons:</p><p>Near-Zero Downtime: Using native SQL Server high availability tools with direct replication to RDS SQL Server provides a continuous replication stream between the on-premises source and the AWS target. This approach minimizes downtime during the migration process, as the databases remain operational throughout most of the migration.</p><p>Modern Data Architecture: By migrating directly to Amazon RDS for Microsoft SQL Server, the solution moves the legacy databases to a managed database service. RDS provides modern features like automated backups, patching, monitoring, and high availability options, which helps modernize the data architecture.</p><p>Streamlined Process: This approach involves a more direct path to the target state (RDS SQL Server) without intermediate steps, reducing complexity and potential points of failure.</p><p>Familiar Tools: Using native SQL Server replication capabilities leverages existing database expertise and provides a well-established migration path that's proven for critical database migrations.</p><p>Option D (using AWS Application Migration Service) has several disadvantages for this specific scenario:</p><p>It involves a two-step process: first rehosting on EC2, then moving to RDS. This adds complexity and potential for additional downtime during the second step.</p><p>The process of detaching the database from EC2 and reattaching it to RDS would likely cause more downtime than a properly configured replication approach.</p><p>While AWS Application Migration Service is excellent for rehosting entire servers, it's not the most efficient approach when the end goal is specifically to migrate databases to RDS.</p><p>The native database high availability tools approach provides a more direct path to the target state with near-zero downtime, which is critical for the migration of these legacy SQL Server databases. It also aligns better with the requirement to move to a modern data architecture by directly leveraging Amazon RDS for Microsoft SQL Server's managed database capabilities.</p><p>Sources</p><p>Best practices for migrating to Amazon RDS for SQL Server - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-sql-server/best-practices.html）</p><p>Migrate legacy database | AWS re:Post （https://repost.aws/questions/QUTC4STLNPTNeCYjbQMSErcA/migrate-legacy-database）</p><p>Migrating SQL Server Databases to Amazon RDS for SQL Server - Database Migration Guide （https://docs.aws.amazon.com/dms/latest/sbs/chap-manageddatabases.sql-server-rds-sql-server.html）</p><p>Migrate an on-premises Microsoft SQL Server database to Amazon EC2 - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-an-on-premises-microsoft-sql-server-database-to-amazon-ec2.html）</p><p>SQL Server database migration methods - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-sql-server/methods.html）</p><p>Migrating SQL Server - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-microsoft-workloads-aws/migrating-sql-server-workloads.html）</p><p>SQL Server database migration strategies - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-sql-server/strategies.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": true,
      "noteContent": "Aurora不支持SQL Server引擎"
    },
    {
      "id": "5953355494104344a5de0e14851ad304",
      "questionNumber": 230,
      "type": "multiple",
      "content": "<p>Question #230</p><p>A company's solutions architect is analyzing costs of a multi-application environment. The environment is deployed across multiple Availability Zones in a single AWS Region. After a recent acquisition, the company manages two organizations in AWS Organizations. The company has created multiple service provider applications as AWS PrivateLink-powered VPC endpoint services in one organization. The company has created multiple service consumer applications in the other organization. Data transfer charges are much higher than the company expected, and the solutions architect needs to reduce the costs. The solutions architect must recommend guidelines for developers to follow when they deploy services. These guidelines must minimize data transfer charges for the whole environment. Which guidelines meet these requirements? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Use AWS Resource Access Manager to share the subnets that host the service provider applications with other accounts in the organization."
        },
        {
          "label": "B",
          "content": "Place the service provider applications and the service consumer applications in AWS accounts in the same organization."
        },
        {
          "label": "C",
          "content": "Turn off cross-zone load balancing for the Network Load Balancer in all service provider application deployments."
        },
        {
          "label": "D",
          "content": "Ensure that service consumer compute resources use the Availability Zone-specific endpoint service by using the endpoint&#39;s local DNS name."
        },
        {
          "label": "E",
          "content": "Create a Savings Plan that provides adequate coverage for the organization&#39;s planned inter-Availability Zone data transfer usage."
        }
      ],
      "correctAnswer": "CD",
      "explanation": "<p>The company faces high data transfer charges due to multi-application deployments across two AWS Organizations and multiple Availability Zones (AZs). The service provider applications (using AWS PrivateLink VPC endpoint services) and service consumer applications reside in different organizations, potentially incurring cross-organization or cross-AZ data transfer costs. &nbsp;</p><p><br></p><p> Correct Options &nbsp;</p><p><br></p><p> C. Turn off cross-zone load balancing for the Network Load Balancer in all service provider application deployments. &nbsp;</p><p>- Rationale: Cross-zone load balancing on a Network Load Balancer (NLB) distributes traffic across instances in different AZs, causing data transfer between AZs. Disabling this feature restricts traffic to the local AZ, minimizing cross-AZ data transfer charges. &nbsp;</p><p>- Cost Impact: Cross-AZ data transfer within a Region incurs fees. By limiting traffic to the local AZ, the company reduces unnecessary inter-AZ bandwidth usage. &nbsp;</p><p><br></p><p> D. Ensure that service consumer compute resources use the Availability Zone-specific endpoint service by using the endpoint's local DNS name. &nbsp;</p><p>- Rationale: Using the AZ-specific local DNS name for the VPC endpoint ensures that service consumers connect to the endpoint in their own AZ. This avoids cross-AZ traffic between consumers and providers, reducing data transfer costs. &nbsp;</p><p>- Cost Impact: Connecting to the local AZ endpoint prevents inter-AZ data transfer, which is a key source of high charges in multi-AZ environments. &nbsp;</p><p><br></p><p> Incorrect Options &nbsp;</p><p><br></p><p> B. Place the service provider applications and the service consumer applications in AWS accounts in the same organization. &nbsp;</p><p>- Issue: While AWS Organizations can simplify account management, data transfer between AWS accounts in the same Region—even across different organizations—via PrivateLink VPC endpoints is typically free. The primary cost driver here is cross-AZ traffic, not cross-organization account traffic. &nbsp;</p><p>- Why It's Not Optimal: PrivateLink within a Region does not charge for data transfer between accounts, regardless of organizational boundaries. Thus, reorganizing accounts does not address the core cost issue. &nbsp;</p><p><br></p><p> Key Takeaway &nbsp;</p><p>To minimize data transfer costs, focus on reducing cross-AZ traffic by disabling cross-zone load balancing (C) and using AZ-specific endpoints (D). These steps target the primary cost drivers in multi-AZ deployments without requiring organizational restructuring.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "ba3b12852e8c416c93aba1a3349a126b",
      "questionNumber": 231,
      "type": "single",
      "content": "<p>Question #231</p><p>A company has an on-premises Microsoft SQL Server database that writes a nightly 200 GB export to a local drive. The company wants to move the backups to more robust cloud storage on Amazon S3. The company has set up a 10 Gbps AWS Direct Connect connection between the on-premises data center and AWS. <br><br>Which solution meets these requirements MOST cost-effectively?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a new S3 bucket. Deploy an AWS Storage Gateway file gateway within the VPC that is connected to the Direct Connect connection. Create a new SMB file share. Write nightly database exports to the new SMB file share."
        },
        {
          "label": "B",
          "content": "Create an Amazon FSx for Windows File Server Single-AZ file system within the VPC that is connected to the Direct Connect connection. Create a new SMB file share. Write nightly database exports to an SMB file share on the Amazon FSx file system. Enable nightly backups."
        },
        {
          "label": "C",
          "content": "Create an Amazon FSx for Windows File Server Multi-AZ file system within the VPC that is connected to the Direct Connect connection. Create a new SMB file share. Write nightly database exports to an SMB file share on the Amazon FSx file system. Enable nightly backups."
        },
        {
          "label": "D",
          "content": "Create a new S3 bucket. Deploy an AWS Storage Gateway volume gateway within the VPC that is connected to the Direct Connect connection. Create a new SMB file share. Write nightly database exports to the new SMB file share on the volume gateway, and automate copies of this data to an S3 bucket."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Create a new S3 bucket. Deploy an AWS Storage Gateway file gateway within the VPC that is connected to the Direct Connect connection. Create a new SMB file share. Write nightly database exports to the new SMB file share.</p><p>This solution meets the requirements most cost-effectively for the following reasons:</p><p>Direct Path to S3: AWS Storage Gateway file gateway provides a direct path for storing the SQL Server exports in Amazon S3, which is the company's stated goal for \"robust cloud storage.\" The file gateway acts as an intermediary that presents an SMB file share interface to the on-premises SQL Server while automatically transferring the data to S3.</p><p>Cost-Effective Storage: Amazon S3 offers significantly lower storage costs compared to file system storage options like FSx for Windows File Server, especially for this use case where the data is primarily for backup purposes rather than active file serving.</p><p>Efficient Use of Existing Infrastructure: The solution leverages the company's existing 10 Gbps Direct Connect connection, allowing for efficient data transfer from on-premises to AWS without additional networking costs.</p><p>Minimal Operational Overhead: Once configured, the file gateway requires minimal management, as it automatically handles the transfer of data from the SMB share to S3.</p><p>Compatibility with Existing Workflows: The SQL Server can continue writing exports to an SMB file share, which is a familiar protocol in Windows environments, requiring no changes to the existing export process.</p><p>Option B (using Amazon FSx for Windows File Server) would be less cost-effective because:</p><p>FSx for Windows File Server is designed as a high-performance file system for active workloads, which is unnecessary for nightly database exports.</p><p>FSx storage costs are higher than S3 storage costs, especially for data that doesn't require the performance characteristics of a Windows file system.</p><p>The solution would require an additional step to move the data from FSx to S3 if the ultimate goal is S3 storage, adding complexity and potential costs.</p><p>Enabling nightly backups of FSx would add another layer of storage costs that isn't necessary when the data can be stored directly in S3.</p><p>The AWS Storage Gateway file gateway solution provides the most direct and cost-effective path to meet the company's requirement of moving their SQL Server exports to robust cloud storage on Amazon S3.</p><p>Sources</p><p>Using the S3 File Gateway as an Enterprise SMB File Server | AWS re:Post （https://repost.aws/questions/QU_XkS2prlSwC_5oAQBD0kcQ/using-the-s3-file-gateway-as-an-enterprise-smb-file-server）</p><p>AWS Storage Gateway - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/optimize-costs-microsoft-workloads/storage-main-storage-gateway.html）</p><p>Writing to AWS from a Windows legacy app that supports only Network Paths for exports? | AWS re:Post（https://repost.aws/questions/QUJnyObhgiQhyWmvoqgVj8FA/writing-to-aws-from-a-windows-legacy-app-that-supports-only-network-paths-for-exports） </p><p>Please help me choose the right Amazon service(s) | AWS re:Post （https://repost.aws/questions/QUjGbInOcSSAWwhvHUFz8PzA/please-help-me-choose-the-right-amazon-service-s）</p><p>S3 File Gateway file limit per SMB share | AWS re:Post （https://repost.aws/questions/QUkeYgQIC8S1SLAv--BfMP4w/s3-file-gateway-file-limit-per-smb-share）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "6ceedcf7e6914bbca1cd67333515e015",
      "questionNumber": 232,
      "type": "single",
      "content": "<p>Question #232</p><p>A company needs to establish a connection from its on-premises data center to AWS. The company needs to connect all of its VPCs that are located in different AWS Regions with transitive routing capabilities between VPC networks. The company also must reduce network outbound traffic costs, increase bandwidth throughput, and provide a consistent network experience for end users. <br><br>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an AWS Site-to-Site VPN connection between the on-premises data center and a new central VPC. Create VPC peering connections that initiate from the central VPC to all other VPCs."
        },
        {
          "label": "B",
          "content": "Create an AWS Direct Connect connection between the on-premises data center and AWS. Provision a transit VIF, and connect it to a Direct Connect gateway. Connect the Direct Connect gateway to all the other VPCs by using a transit gateway in each Region."
        },
        {
          "label": "C",
          "content": "Create an AWS Site-to-Site VPN connection between the on-premises data center and a new central VPC. Use a transit gateway with dynamic routing. Connect the transit gateway to all other VPCs."
        },
        {
          "label": "D",
          "content": "Create an AWS Direct Connect connection between the on-premises data center and AWS. Establish an AWS Site-to-Site VPN connection between all VPCs in each Region. Create VPC peering connections that initiate from the central VPC to all other VPCs."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Create an AWS Direct Connect connection between the on-premises data center and AWS. Provision a transit VIF, and connect it to a Direct Connect gateway. Connect the Direct Connect gateway to all the other VPCs by using a transit gateway in each Region.</p><p>This solution will meet the requirements for the following reasons:</p><p>Transitive Routing Capabilities:</p><p>The combination of Direct Connect gateway and transit gateways provides true transitive routing between VPCs across different AWS Regions.</p><p>Transit gateways act as regional network hubs that enable communication between multiple VPCs and the on-premises network.</p><p>This architecture allows any VPC to communicate with any other VPC across regions, fulfilling the requirement for transitive routing capabilities.</p><p>Reduced Network Outbound Traffic Costs:</p><p>Direct Connect offers lower data transfer costs compared to VPN or internet-based connections, especially for high-volume traffic.</p><p>The predictable billing model of Direct Connect helps reduce overall network costs for consistent, high-volume traffic patterns.</p><p>Increased Bandwidth Throughput:</p><p>Direct Connect provides dedicated, high-bandwidth connections (available at 1 Gbps, 10 Gbps, and higher).</p><p>This significantly outperforms the maximum throughput of Site-to-Site VPN (limited to 1.25 Gbps per tunnel).</p><p>Consistent Network Experience:</p><p>Direct Connect provides a private, dedicated connection that bypasses the public internet.</p><p>This results in lower latency, less jitter, and more predictable performance compared to internet-based connections.</p><p>The consistent network experience is critical for end users accessing resources across multiple regions.</p><p>Option A (Site-to-Site VPN with VPC peering) would not meet the requirements because:</p><p>VPC peering does not support transitive routing - traffic from one VPC cannot route through the central VPC to reach another VPC.</p><p>Site-to-Site VPN connections traverse the public internet, which can lead to inconsistent network performance.</p><p>VPN connections have bandwidth limitations that may not be sufficient for high-throughput requirements.</p><p>Internet-based data transfer costs would be higher compared to Direct Connect.</p><p>The Direct Connect with transit VIF, Direct Connect gateway, and regional transit gateways solution provides a scalable, high-performance network architecture that meets all the stated requirements for transitive routing, cost reduction, bandwidth improvement, and consistent network experience.</p><p>Sources</p><p>AWS Direct Connect + AWS Transit Gateway + AWS Site-to-Site VPN - Amazon Virtual Private Cloud Connectivity Options （https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-aws-transit-gateway-vpn.html）</p><p>AWS Direct Connect &nbsp;- Building a Scalable and Secure Multi-VPC AWS Network Infrastructure （https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/direct-connect.html）</p><p>Transit gateway attachment configuration | AWS re:Post （https://repost.aws/questions/QUvX0QyO3pRQWkBQspwcgVKg/transit-gateway-attachment-configuration）</p><p>Hybrid connectivity - Building a Scalable and Secure Multi-VPC AWS Network Infrastructure （https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/hybrid-connectivity.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "742e8d42973d4cb6a1c5ec60c8b7bea2",
      "questionNumber": 233,
      "type": "single",
      "content": "<p>Question #233</p><p>A company is migrating its development and production workloads to a new organization in AWS Organizations. The company has created a separate member account for development and a separate member account for production. Consolidated billing is linked to the management account. In the management account, a solutions architect needs to create an IAM user that can stop or terminate resources in both member accounts. <br><br>Which solution will meet this requirement?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an IAM user and a cross-account role in the management account. Configure the cross-account role with least privilege access to the member accounts."
        },
        {
          "label": "B",
          "content": "Create an IAM user in each member account. In the management account, create a cross-account role that has least privilege access. Grant the IAM users access to the cross-account role by using a trust policy."
        },
        {
          "label": "C",
          "content": "Create an IAM user in the management account. In the member accounts, create an IAM group that has least privilege access. Add the IAM user from the management account to each IAM group in the member accounts."
        },
        {
          "label": "D",
          "content": "Create an IAM user in the management account. In the member accounts, create cross-account roles that have least privilege access. Grant the IAM user access to the roles by using a trust policy."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Based on the information provided and security best practices, the correct answer is:</p><p>D. Create an IAM user in the management account. In the member accounts, create cross-account roles that have least privilege access. Grant the IAM user access to the roles by using a trust policy.</p><p>This solution will meet the requirement for the following reasons:</p><p>Proper Separation of Concerns: This approach maintains proper separation between the management account and member accounts, which is a security best practice in AWS Organizations.</p><p>Principle of Least Privilege: By creating specific cross-account roles in each member account (development and production), you can define precise permissions that follow the principle of least privilege. Each role can be tailored to include only the permissions needed to stop or terminate resources in that specific account.</p><p>Trust Relationship: The trust policy in each member account's role explicitly defines which entities (in this case, the IAM user in the management account) are allowed to assume the role, providing clear access control.</p><p>Reduced Risk: This approach reduces risk by not creating roles with cross-account access in the management account, which could potentially grant excessive permissions if compromised.</p><p>Better Auditability: When the IAM user assumes a role in a member account, AWS CloudTrail logs will clearly show which user performed actions in which account, improving auditability and compliance.</p><p>Option A (creating an IAM user and a cross-account role in the management account) is not the correct approach because:</p><p>It doesn't follow AWS best practices for cross-account access. The standard pattern is to create roles in the accounts where resources need to be accessed, not in the account where the user exists.</p><p>A cross-account role in the management account wouldn't have direct access to resources in member accounts without additional complex permission configurations.</p><p>This approach would be more difficult to audit and could potentially lead to privilege escalation issues.</p><p>The correct solution (Option D) follows AWS security best practices by implementing a clear separation of duties, maintaining the principle of least privilege, and establishing explicit trust relationships between accounts, while still allowing the IAM user in the management account to perform the required actions in both member accounts.</p><p>Sources</p><p>Secure root user access for member accounts in AWS Organizations | AWS Security Blog （https://aws.amazon.com/cn/blogs/security/secure-root-user-access-for-member-accounts-in-aws-organizations/）</p><p>Managing access permissions for an organization with AWS Organizations - AWS Organizations（https://docs.aws.amazon.com/organizations/latest/userguide/orgs_permissions_overview.html） </p><p>IAM tutorial: Delegate access across AWS accounts using IAM roles - AWS Identity and Access Management （https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html）</p><p>WHY AWS Account is not relative each login? | AWS re:Post （https://repost.aws/questions/QUuVhhDp6ZS4CX1I8M_jSCzA/why-aws-account-is-not-relative-each-login）</p><p>AWS Organizations - Organizing Your AWS Environment Using Multiple Accounts （https://docs.aws.amazon.com/whitepapers/latest/organizing-your-aws-environment/aws-organizations.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "d7583506be5046b583b92e5b327447b1",
      "questionNumber": 234,
      "type": "single",
      "content": "<p>Question #234</p><p>A company wants to use AWS for disaster recovery for an on-premises application. The company has hundreds of Windows-based servers that run the application. All the servers mount a common share. <br><br>The company has an RTO of 15 minutes and an RPO of 5 minutes. The solution must support native failover and fallback capabilities. <br><br>Which solution will meet these requirements MOST cost-effectively?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an AWS Storage Gateway File Gateway. Schedule daily Windows server backups. Save the data to Amazon S3. During a disaster, recover the on-premises servers from the backup. During tailback, run the on-premises servers on Amazon EC2 instances."
        },
        {
          "label": "B",
          "content": "Create a set of AWS CloudFormation templates to create infrastructure. Replicate all data to Amazon Elastic File System (Amazon EFS) by using AWS DataSync. During a disaster, use AWS CodePipeline to deploy the templates to restore the on-premises servers. Fail back the data by using DataSync."
        },
        {
          "label": "C",
          "content": "Create an AWS Cloud Development Kit (AWS CDK) pipeline to stand up a multi-site active-active environment on AWS. Replicate data into Amazon S3 by using the s3 sync command. During a disaster, swap DNS endpoints to point to AWS. Fail back the data by using the s3 sync command."
        },
        {
          "label": "D",
          "content": "Use AWS Elastic Disaster Recovery to replicate the on-premises servers. Replicate data to an Amazon FSx for Windows File Server file system by using AWS DataSync. Mount the file system to AWS servers. During a disaster, fail over the on-premises servers to AWS. Fail back to new or existing servers by using Elastic Disaster Recovery."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Option D is the correct answer. AWS Elastic Disaster Recovery is designed to replicate servers to AWS with minimal downtime, which aligns with the company's RTO and RPO requirements. By replicating data to Amazon FSx for Windows File Server and using AWS DataSync, the company can ensure that the data is up-to-date and that there is a robust failover and fallback process in place. This solution also supports native failover and fallback capabilities, which is a requirement specified by the company.</p><p>The requirements include: &nbsp;</p><p>- RTO of 15 minutes and RPO of 5 minutes (fast recovery and minimal data loss). &nbsp;</p><p>- Native failover and fallback capabilities. &nbsp;</p><p>- Hundreds of Windows servers with a common share. &nbsp;</p><p>Option D meets these requirements most effectively and cost-efficiently: &nbsp;</p><p>- AWS Elastic Disaster Recovery (DRS) provides continuous replication of on-premises servers to AWS, enabling fast failover (meeting RTO/RPO). &nbsp;</p><p>- Amazon FSx for Windows File Server is a fully managed shared file storage solution compatible with Windows servers. &nbsp;</p><p>- AWS DataSync efficiently replicates data to FSx. &nbsp;</p><p>- Native failover and fallback are supported by Elastic Disaster Recovery. &nbsp;</p><p> Why not the other options? &nbsp;</p><p>- A: Storage Gateway + S3 backups do not meet the RTO/RPO requirements (slow recovery). &nbsp;</p><p>- B: CloudFormation + EFS + DataSync is complex and does not provide native failover/fallback for Windows servers. &nbsp;</p><p>- C: Multi-site active-active is expensive and overkill; `s3 sync` is not real-time enough for a 5-minute RPO. &nbsp;</p><p>Thus, D is the most cost-effective and compliant solution. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "2d68dd72d1564408a149bf5933974435",
      "questionNumber": 235,
      "type": "multiple",
      "content": "<p>Question #235</p><p>A company has built a high performance computing (HPC) cluster in AWS for a tightly coupled workload that generates a large number of shared files stored in Amazon EFS. The cluster was performing well when the number of Amazon EC2 instances in the cluster was 100. However, when the company increased the cluster size to 1,000 EC2 instances, overall performance was well below expectations. Which collection of design choices should a solutions architect make to achieve the maximum performance from the HPC cluster? (Choose three.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Ensure the HPC cluster is launched within a single Availability Zone."
        },
        {
          "label": "B",
          "content": "Launch the EC2 instances and attach elastic network interfaces in multiples of four."
        },
        {
          "label": "C",
          "content": "Select EC2 instance types with an Elastic Fabric Adapter (EFA) enabled."
        },
        {
          "label": "D",
          "content": "Ensure the cluster is launched across multiple Availability Zones."
        },
        {
          "label": "E",
          "content": "Replace Amazon EFS with multiple Amazon EBS volumes in a RAID array."
        },
        {
          "label": "F",
          "content": "Replace Amazon EFS with Amazon FSx for Lustre."
        }
      ],
      "correctAnswer": "ACF",
      "explanation": "<p>Based on the information provided, the correct answers are:</p><p>A. Ensure the HPC cluster is launched within a single Availability Zone. </p><p>C. Select EC2 instance types with an ElasticFabric Adapter (EFA) enabled. </p><p>F. Replace Amazon EFS with Amazon FSx for Lustre.</p><p>These three design choices together will maximize the performance of the HPC cluster for the following reasons:</p><p>Ensure the HPC cluster is launched within a single Availability Zone (A):</p><p>For tightly coupled HPC workloads, minimizing network latency between instances is critical.</p><p>A single AZ deployment offers significantly lower latency and higher network throughput between instances compared to multi-AZ deployments.</p><p>When scaling to 1,000 instances, the inter-node communication becomes even more important, and cross-AZ traffic would introduce unnecessary latency that would severely impact performance.</p><p>Select EC2 instance types with an ElasticFabric Adapter (EFA) enabled (C):</p><p>EFA is specifically designed for HPC applications that require high levels of inter-node communications.</p><p>It provides lower and more consistent latency compared to traditional TCP transport by enabling OS-bypass capabilities.</p><p>EFA allows HPC applications to communicate directly with the network interface, bypassing the operating system kernel.</p><p>This is particularly beneficial for tightly coupled workloads that require frequent communication between nodes.</p><p>Replace Amazon EFS with Amazon FSx for Lustre (F):</p><p>FSx for Lustre is purpose-built for high-performance computing workloads.</p><p>It offers significantly higher throughput and lower latency compared to EFS, which is critical when scaling to 1,000 instances.</p><p>As a parallel file system, Lustre allows multiple clients to access data simultaneously, which is ideal for HPC clusters generating large numbers of shared files.</p><p>FSx for Lustre can deliver hundreds of gigabytes per second of throughput and millions of IOPS, which addresses the performance bottleneck likely caused by EFS when scaling up.</p><p>The other options would not be optimal:</p><p>Option D (multi-AZ deployment) would introduce higher latency between instances, degrading performance for tightly coupled workloads.</p><p>Option E (EBS RAID array) would not scale well to 1,000 instances and would create a bottleneck for shared files.</p><p>Option B (launching instances and attaching network interfaces in multiples of four) might offer some optimization but is not as significant for performance as the other three correct choices.</p><p>These three design choices together address the key aspects of HPC performance: minimizing network latency (single AZ), optimizing inter-node communication (EFA), and providing high-performance shared storage (FSx for Lustre).</p><p>Sources</p><p>Optimizing HPC workflows with automatically scaling clusters in Ansys Gateway powered by AWS | AWS HPC Blog （https://aws.amazon.com/cn/blogs/hpc/optimizing-hpc-workflows-with-automatically-scaling-clusters-in-ansys-gateway-powered-by-aws/）</p><p>Network architecture - High Performance Computing Lens （https://docs.aws.amazon.com/wellarchitected/latest/high-performance-computing-lens/network-architecture.html）</p><p>Storage architecture - High Performance Computing Lens （https://docs.aws.amazon.com/wellarchitected/latest/high-performance-computing-lens/storage-architecture.html）</p><p>Data management and transfer - Financial Services Grid Computing on AWS （https://docs.aws.amazon.com/whitepapers/latest/financial-services-grid-computing/data-management-and-transfer.html）</p><p>Shared storage - AWS ParallelCluster （https://docs.aws.amazon.com/parallelcluster/latest/ug/shared-storage-quotas-integration-v3.html）</p><p>Tightly coupled scenarios - High Performance Computing Lens （https://docs.aws.amazon.com/wellarchitected/latest/high-performance-computing-lens/tightly-coupled-scenarios.html）</p><p>Software - Computational Fluid Dynamics on AWS （https://docs.aws.amazon.com/whitepapers/latest/computational-fluid-dynamics-on-aws/software.html）</p><p>Storage and data sharing - Financial Services Grid Computing on AWS （https://docs.aws.amazon.com/whitepapers/latest/financial-services-grid-computing/storage-and-data-sharing.html）</p><p>Guidance for Deploying High Performance Computing Clusters on AWS （https://aws.amazon.com/cn/solutions/guidance/deploying-high-performance-computing-clusters-on-aws/）</p><p>File Server - Amazon FSx - AWS （https://aws.amazon.com/cn/fsx/intelligent-tiering/）</p><p>Amazon FSx for Lustre increases throughput to GPU instances by up to 12x | AWS News Blog (https://aws.amazon.com/cn/blogs/aws/amazon-fsx-for-lustre-unlocks-full-network-bandwidth-and-gpu-performance/)</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "79edbf46b0c44b60a42b07a21fa928bc",
      "questionNumber": 236,
      "type": "single",
      "content": "<p>Question #236</p><p>A company is designing an AWS Organizations structure. The company wants to standardize a process to apply tags across the entire organization. The company will require tags with specific values when a user creates a new resource. Each of the company's OUs will have unique tag values. <br><br>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use an SCP to deny the creation of resources that do not have the required tags. Create a tag policy that includes the tag values that the company has assigned to each OU. Attach the tag policies to the OUs."
        },
        {
          "label": "B",
          "content": "Use an SCP to deny the creation of resources that do not have the required tags. Create a tag policy that includes the tag values that the company has assigned to each OU. Attach the tag policies to the organization&#39;s management account."
        },
        {
          "label": "C",
          "content": "Use an SCP to allow the creation of resources only when the resources have the required tags. Create a tag policy that includes the tag values that the company has assigned to each OU. Attach the tag policies to the OUs."
        },
        {
          "label": "D",
          "content": "Use an SCP to deny the creation of resources that do not have the required tags. Define the list of tags. Attach the SCP to the OUs."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Use an SCP to deny the creation of resources that do not have the required tags. Create a tag policy that includes the tag values that the company has assigned to each OU. Attach the tag policies to the OUs.</p><p>This solution will meet the requirements for the following reasons:</p><p>Service Control Policies (SCPs) to enforce tagging: Using an SCP to deny the creation of resources that don't have required tags ensures compliance with the company's tagging standards. This creates a preventive control that blocks resources from being created without proper tags.</p><p>Tag policies for defining allowed values: Tag policies are specifically designed to define and enforce standardized tag keys and values across an organization. They allow you to specify which tag keys must be used and what values are allowed for each key.</p><p>OU-specific tag values: By attaching tag policies to individual OUs rather than the management account, each OU can have its own unique tag values as specified in the requirements. This provides the flexibility needed when different parts of the organization require different tag values.</p><p>Comprehensive enforcement: The combination of SCPs (to require tags) and tag policies (to define allowed values) creates a complete tagging governance solution that both requires tags and ensures they have appropriate values.</p><p>Option B would not work because attaching tag policies only to the management account wouldn't allow for unique tag values for each OU. This approach would enforce the same tag values across the entire organization.</p><p>Option D would not be optimal because using only SCPs without tag policies would make it difficult to define and manage the allowed tag values for each OU. SCPs are primarily access control policies and aren't designed to efficiently manage and enforce specific tag values.</p><p>The correct solution (Option A) provides both the enforcement mechanism (SCPs) and the value definition capability (tag policies attached to OUs) needed to meet the company's requirements for standardized tagging with OU-specific values.</p><p>Sources</p><p>AWS Organizations – Tag policies - Best Practices for Tagging AWS Resources (https://docs.aws.amazon.com/whitepapers/latest/tagging-best-practices/aws-organizations-tag-policies.html)</p><p>AWS Organizations tag policies - Tagging AWS Resources and Tag Editor (https://docs.aws.amazon.com/tag-editor/latest/userguide/tag-policies-orgs.html)</p><p>Tag policies - AWS Organizations (https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_tag-policies.html)</p><p>Measure, enforce, and evolve - AWS Prescriptive Guidance (https://docs.aws.amazon.com/prescriptive-guidance/latest/cost-allocation-tagging/measure-enforce-evolve.html)</p><p>Using TagOptions with AWS Organizations tag policies - AWS Service Catalog (https://docs.aws.amazon.com/servicecatalog/latest/adminguide/tagoption-policies.html)</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "98b6d39689e3402a92cd3ec3f36f35fd",
      "questionNumber": 237,
      "type": "single",
      "content": "<p>Question #237</p><p>A company has more than 10,000 sensors that send data to an on-premises Apache Kafka server by using the Message Queuing Telemetry Transport (MQTT) protocol. The on-premises Kafka server transforms the data and then stores the results as objects in an Amazon S3 bucket. </p><p><br></p><p>Recently, the Kafka server crashed. The company lost sensor data while the server was being restored. A solutions architect must create a new design on AWS that is highly available and scalable to prevent a similar occurrence. <br><br>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Launch two Amazon EC2 instances to host the Kafka server in an active/standby configuration across two Availability Zones. Create a domain name in Amazon Route 53. Create a Route 53 failover policy. Route the sensors to send the data to the domain name."
        },
        {
          "label": "B",
          "content": "Migrate the on-premises Kafka server to Amazon Managed Streaming for Apache Kafka (Amazon MSK). Create a Network Load Balancer (NLB) that points to the Amazon MSK broker. Enable NLB health checks. Route the sensors to send the data to the NLB."
        },
        {
          "label": "C",
          "content": "Deploy AWS IoT Core, and connect it to an Amazon Kinesis Data Firehose delivery stream. Use an AWS Lambda function to handle data transformation. Route the sensors to send the data to AWS IoT Core."
        },
        {
          "label": "D",
          "content": "Deploy AWS IoT Core, and launch an Amazon EC2 instance to host the Kafka server. Configure AWS IoT Core to send the data to the EC2 instance. Route the sensors to send the data to AWS IoT Core."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Deploy AWS IoT Core, and connect it to an Amazon Kinesis Data Firehose delivery stream. Use an AWS Lambda function to handle data transformation. Route the sensors to send the data to AWS IoT Core.</p><p>This solution will meet the requirements for the following reasons:</p><p>High Availability:</p><p>AWS IoT Core is a fully managed service with built-in high availability across multiple Availability Zones.</p><p>Kinesis Data Firehose is also a fully managed service with automatic scaling and high availability.</p><p>Lambda functions run across multiple Availability Zones by default.</p><p>This serverless architecture eliminates the single point of failure that existed with the on-premises Kafka server.</p><p>Scalability:</p><p>AWS IoT Core can easily handle the 10,000+ sensors mentioned in the scenario and can scale to billions of devices and messages.</p><p>Kinesis Data Firehose automatically scales to match the throughput of the incoming data.</p><p>Lambda functions scale automatically based on the incoming request rate.</p><p>This solution can accommodate growth in the number of sensors without manual intervention.</p><p>Data Transformation:</p><p>Lambda functions can be used to perform the same data transformations that were previously handled by the Kafka server.</p><p>This maintains the required functionality while improving reliability.</p><p>Data Storage:</p><p>Kinesis Data Firehose can deliver the transformed data directly to Amazon S3, maintaining the existing storage solution.</p><p>Firehose provides at-least-once delivery guarantees to S3.</p><p>Fully Managed Services:</p><p>This solution uses fully managed services that AWS maintains, reducing operational overhead and eliminating the need to manage infrastructure.</p><p>Option D (AWS IoT Core with EC2-hosted Kafka) would not be optimal because:</p><p>It reintroduces the same potential point of failure (Kafka server) that caused the original data loss.</p><p>EC2 instances require manual configuration for high availability and scaling.</p><p>Managing Kafka on EC2 requires operational overhead and expertise.</p><p>This approach doesn't fully leverage the benefits of AWS managed services.</p><p>The serverless architecture in Option C provides the high availability and scalability required to prevent data loss during failures, which directly addresses the problem that occurred with the on-premises Kafka server. It also simplifies operations by using fully managed services rather than requiring the company to manage server infrastructure.</p><p>Sources</p><p>Architectural Patterns for real-time analytics using Amazon Kinesis Data Streams, Part 2: AI Applications | AWS Big Data Blog （https://aws.amazon.com/cn/blogs/big-data/architectural-patterns-for-real-time-analytics-using-amazon-kinesis-data-streams-part-2-ai-applications/）</p><p>Best Practice 10.1 – Store data before processing - IoT Lens Checklist （https://docs.aws.amazon.com/wellarchitected/latest/iot-lens-checklist/best-practice-10-1.html）</p><p>Key considerations while building streaming analytics - Build Modern Data Streaming Architectures on AWS （https://docs.aws.amazon.com/whitepapers/latest/build-modern-data-streaming-analytics-architectures/key-considerations-while-building-streaming-analytics.html）</p><p>Unlocking Scalable IoT Analytics on AWS | The Internet of Things on AWS – Official Blog （https://aws.amazon.com/cn/blogs/iot/unlocking-scalable-iot-analytics-on-aws/）</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "a8556be72b224353a7a489ba9f3facb8",
      "questionNumber": 238,
      "type": "multiple",
      "content": "<p>Question #238</p><p>A company recently started hosting new application workloads in the AWS Cloud. The company is using Amazon EC2 instances, Amazon Elastic File System (Amazon EFS) file systems, and Amazon RDS DB instances. To meet regulatory and business requirements, the company must make the following changes for data backups:</p><p>• Backups must be retained based on custom daily, weekly, and monthly requirements.</p><p>• Backups must be replicated to at least one other AWS Region immediately after capture.</p><p>• The backup solution must provide a single source of backup status across the AWS environment.</p><p>• The backup solution must send immediate notifications upon failure of any resource backup.</p><p>Which combination of steps will meet these requirements with the LEAST amount of operational overhead? (Choose three.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an AWS Backup plan with a backup rule for each of the retention requirements."
        },
        {
          "label": "B",
          "content": "Configure an AWS Backup plan to copy backups to another Region."
        },
        {
          "label": "C",
          "content": "Create an AWS Lambda function to replicate backups to another Region and send notification if a failure occurs."
        },
        {
          "label": "D",
          "content": "Add an Amazon Simple Notification Service (Amazon SNS) topic to the backup plan to send a notification for finished jobs that have any status except BACKUP_JOB_COMPLETED."
        },
        {
          "label": "E",
          "content": "Create an Amazon Data Lifecycle Manager (Amazon DLM) snapshot lifecycle policy for each of the retention requirements."
        },
        {
          "label": "F",
          "content": "Set up RDS snapshots on each database."
        }
      ],
      "correctAnswer": "ABD",
      "explanation": "<p>The combination of steps that will meet the requirements with the least amount of operational overhead is A, B, and D.</p><p>A. Creating an AWS Backup plan with custom backup rules allows the company to meet the regulatory and business requirements for backup retention schedules.</p><p>B. Configuring the AWS Backup plan to copy backups to another Region ensures that backups are replicated immediately after capture, providing data protection and disaster recovery capabilities.</p><p>D. Adding an Amazon SNS topic to the backup plan enables immediate notifications to be sent upon the failure of any resource backup, allowing for quick response to any issues.</p><p>Option C is not necessary because AWS Backup can be configured to send notifications directly through the service. Option E is not required as AWS Backup can also handle the creation and lifecycle of EBS snapshots, and Amazon DLM is more focused on EBS snapshots rather than a comprehensive backup solution. Option F is not needed because AWS Backup can be used to automate RDS snapshot creation and management.</p><p>The question asks for the LEAST amount of operational overhead while meeting all the given backup requirements. The best approach is to use AWS Backup, as it provides a centralized backup solution for EC2, EFS, and RDS, along with cross-region replication, customizable retention policies, and notifications.</p><p> Correct Answers:</p><p>A. Create an AWS Backup plan with a backup rule for each of the retention requirements. &nbsp;</p><p>- AWS Backup allows you to define multiple backup rules with different retention periods (daily, weekly, monthly). &nbsp;</p><p>- This meets the requirement for custom retention policies.</p><p>B. Configure an AWS Backup plan to copy backups to another Region. &nbsp;</p><p>- AWS Backup supports cross-region copy, which automatically replicates backups to another Region. &nbsp;</p><p>- This meets the requirement for immediate replication.</p><p>D. Add an Amazon Simple Notification Service (Amazon SNS) topic to the backup plan to send a notification for finished jobs that have any status except BACKUP_JOB_COMPLETED. &nbsp;</p><p>- AWS Backup integrates with SNS to send notifications on backup failures. &nbsp;</p><p>- This meets the requirement for immediate failure notifications.</p><p> Why Not the Other Options?</p><p>- C. Lambda function for replication & notifications → Adds unnecessary complexity since AWS Backup natively supports these features.</p><p>- E. Amazon DLM policies → Only works for EBS snapshots (not RDS or EFS) and lacks centralized management.</p><p>- F. RDS snapshots → Manual setup per database, no centralized status tracking, and no built-in cross-region replication.</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "e5790433f15346a0bb3fc6b2b06639b4",
      "questionNumber": 239,
      "type": "single",
      "content": "<p>Question #239</p><p>A company is developing a gene reporting device that will collect genomic information to assist researchers with collecting large samples of data from a diverse population. The device will push 8 KB of genomic data every second to a data platform that will need to process and analyze the data and provide information back to researchers. The data platform must meet the following requirements:<br><br></p><p>• Provide near-real-time analytics of the inbound genomic data</p><p>• Ensure the data is flexible, parallel, and durable</p><p>• Deliver results of processing to a data warehouse<br><br></p><p>Which strategy should a solutions architect use to meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use Amazon Kinesis Data Firehose to collect the inbound sensor data, analyze the data with Kinesis clients, and save the results to an Amazon RDS instance."
        },
        {
          "label": "B",
          "content": "Use Amazon Kinesis Data Streams to collect the inbound sensor data, analyze the data with Kinesis clients, and save the results to an Amazon Redshift cluster using Amazon EMR."
        },
        {
          "label": "C",
          "content": "Use Amazon S3 to collect the inbound device data, analyze the data from Amazon SQS with Kinesis, and save the results to an Amazon Redshift cluster."
        },
        {
          "label": "D",
          "content": "Use an Amazon API Gateway to put requests into an Amazon SQS queue, analyze the data with an AWS Lambda function, and save the results to an Amazon Redshift cluster using Amazon EMR."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Option B is the correct answer. Amazon Kinesis Data Streams can collect and process large volumes of genomic data in near-real-time. By using Kinesis clients, the data can be analyzed as it streams in. Amazon Redshift is a fully managed data warehouse service that can handle large-scale data analytics and is suitable for storing and querying the processed genomic data. Using Amazon EMR (Elastic MapReduce) can provide additional processing power for complex analytics if needed.</p><p>The requirements are: &nbsp;</p><p>1. Near-real-time analytics: Amazon Kinesis Data Streams is designed for real-time data ingestion and processing. &nbsp;</p><p>2. Flexible, parallel, and durable data: Kinesis Data Streams allows multiple consumers (Kinesis clients) to process data in parallel, and it is durable. &nbsp;</p><p>3. Deliver results to a data warehouse: Amazon Redshift is a data warehouse, and Amazon EMR can be used to process and load data into it. &nbsp;</p><p> Why not the other options? &nbsp;</p><p>- A: Amazon RDS is not a data warehouse solution like Redshift, and Kinesis Data Firehose is more for batch processing rather than real-time analytics. &nbsp;</p><p>- C: Amazon S3 is for storage, not real-time processing. SQS + Kinesis is not a typical pattern for real-time genomic data streaming. &nbsp;</p><p>- D: API Gateway + SQS + Lambda is more suited for request/response workloads, not continuous real-time data streaming. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "88b1063c95364a768443dce2d4a4a293",
      "questionNumber": 240,
      "type": "multiple",
      "content": "<p>Question #240</p><p>A solutions architect needs to define a reference architecture for a solution for three-tier applications with web, application, and NoSQL data layers. The reference architecture must meet the following requirements:</p><p>• High availability within an AWS Region</p><p>• Able to fail over in 1 minute to another AWS Region for disaster recovery</p><p>• Provide the most efficient solution while minimizing the impact on the user experience</p><p>Which combination of steps will meet these requirements? (Choose three.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Use an Amazon Route 53 weighted routing policy set to 100/0 across the two selected Regions. Set Time to Live (TTL) to 1 hour."
        },
        {
          "label": "B",
          "content": "Use an Amazon Route 53 failover routing policy for failover from the primary Region to the disaster recovery Region. Set Time to Live (TTL) to 30 seconds."
        },
        {
          "label": "C",
          "content": "Use a global table within Amazon DynamoDB so data can be accessed in the two selected Regions."
        },
        {
          "label": "D",
          "content": "Back up data from an Amazon DynamoDB table in the primary Region every 60 minutes and then write the data to Amazon S3. Use S3 cross-Region replication to copy the data from the primary Region to the disaster recovery Region. Have a script import the data into DynamoDB in a disaster recovery scenario."
        },
        {
          "label": "E",
          "content": "Implement a hot standby model using Auto Scaling groups for the web and application layers across multiple Availability Zones in the Regions. Use zonal Reserved Instances for the minimum number of servers and On-Demand Instances for any additional resources."
        },
        {
          "label": "F",
          "content": "Use Auto Scaling groups for the web and application layers across multiple Availability Zones in the Regions. Use Spot Instances for the required resources."
        }
      ],
      "correctAnswer": "BCE",
      "explanation": "<p> The combination of steps that will meet the requirements is B, C, and E.</p><p>B. Using an Amazon Route 53 failover routing policy with a TTL of 30 seconds allows for quick DNS failover to the disaster recovery Region in case of a primary Region failure.</p><p>C. Implementing a global table within Amazon DynamoDB ensures that data is replicated and accessible in both the primary and disaster recovery Regions, which supports high availability and disaster recovery.</p><p>E. A hot standby model with Auto Scaling groups across multiple Availability Zones provides high availability within an AWS Region and can quickly adapt to demand changes while using a mix of Reserved Instances and On-Demand Instances for cost efficiency.</p><p>Option A is not suitable due to the long TTL, which would delay failover. Option D is not the most efficient solution for disaster recovery as it relies on backups and manual data import scripts. Option F does not meet the requirement for high availability within an AWS Region as Spot Instances can be terminated with little notice, which could impact the availability of the application.</p><p>- B (Route 53 Failover Routing Policy with 30s TTL): &nbsp;</p><p> &nbsp;- Enables fast failover (within 1 minute) by quickly redirecting traffic to the standby region. &nbsp;</p><p> &nbsp;- A low TTL (30s) ensures DNS changes propagate quickly. &nbsp;</p><p>- C (DynamoDB Global Table): &nbsp;</p><p> &nbsp;- Provides multi-region replication for NoSQL data, ensuring the disaster recovery region has real-time access to data. &nbsp;</p><p> &nbsp;- Eliminates manual backup/restore delays, improving recovery time. &nbsp;</p><p>- E (Hot Standby with Auto Scaling & Reserved/On-Demand Instances): &nbsp;</p><p> &nbsp;- A hot standby model ensures resources are running in the DR region, allowing quick failover. &nbsp;</p><p> &nbsp;- Reserved Instances for baseline capacity + On-Demand for scaling balances cost and availability. &nbsp;</p><p> Why Not Others? &nbsp;</p><p>- A: Weighted routing (100/0) does not support automatic failover. &nbsp;</p><p>- D: Backing up DynamoDB every 60 minutes is too slow (RTO &gt; 1 minute). &nbsp;</p><p>- F: Spot Instances are unreliable for DR since they can be interrupted. &nbsp;</p><p> Conclusion: &nbsp;</p><p>B, C, E provide high availability, fast failover (&lt;1 min), and minimal user impact while being cost-efficient. &nbsp;</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "a453094f0d3a47e487c451972511917e",
      "questionNumber": 241,
      "type": "single",
      "content": "<p>Question #241</p><p>A company manufactures smart vehicles. The company uses a custom application to collect vehicle data. The vehicles use the MQTT protocol to connect to the application. The company processes the data in 5-minute intervals. The company then copies vehicle telematics data to on-premises storage. <br><br>Custom applications analyze this data to detect anomalies. The number of vehicles that send data grows constantly. Newer vehicles generate high volumes of data. The on-premises storage solution is not able to scale for peak traffic, which results in data loss. The company must modernize the solution and migrate the solution to AWS to resolve the scaling challenges. <br><br>Which solution will meet these requirements with the LEAST operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use AWS IoT Greengrass to send the vehicle data to Amazon Managed Streaming for Apache Kafka (Amazon MSK). Create an Apache Kafka application to store the data in Amazon S3. Use a pretrained model in Amazon SageMaker to detect anomalies."
        },
        {
          "label": "B",
          "content": "Use AWS IoT Core to receive the vehicle data. Configure rules to route data to an Amazon Kinesis Data Firehose delivery stream that stores the data in Amazon S3. Create an Amazon Kinesis Data Analytics application that reads from the delivery stream to detect anomalies."
        },
        {
          "label": "C",
          "content": "Use AWS IoT FleetWise to collect the vehicle data. Send the data to an Amazon Kinesis data stream. Use an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Use the built-in machine learning transforms in AWS Glue to detect anomalies."
        },
        {
          "label": "D",
          "content": "Use Amazon MQ for RabbitMQ to collect the vehicle data. Send the data to an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Use Amazon Lookout for Metrics to detect anomalies."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Option B is the correct answer. AWS IoT Core is designed to handle the ingestion of data from IoT devices at scale, which is ideal for the growing number of smart vehicles sending data. By routing the data to an Amazon Kinesis Data Firehose delivery stream, the company can efficiently store the large volumes of data in Amazon S3. Additionally, Amazon Kinesis Data Analytics can be used to analyze the data stream for anomaly detection, which provides a scalable and serverless solution with minimal operational overhead. This approach leverages managed services and does not require the management of clusters or Kafka applications, which simplifies operations and reduces overhead compared to the other options.</p><p>1. AWS IoT Core natively supports MQTT, which the vehicles already use, minimizing changes to the existing setup.</p><p>2. Kinesis Data Firehose automatically scales to handle high data volumes and stores data directly in Amazon S3 without manual intervention.</p><p>3. Kinesis Data Analytics can process streaming data in real-time (or near-real-time) to detect anomalies, aligning with the 5-minute processing interval.</p><p>4. This approach is serverless, meaning AWS manages scaling, reducing operational overhead.</p><p> Why Not the Other Options?</p><p>- A: Uses AWS IoT Greengrass (requires edge deployment) and Amazon MSK (Kafka cluster management adds complexity).</p><p>- C: AWS IoT FleetWise is more specialized for automotive data collection but introduces unnecessary complexity compared to IoT Core.</p><p>- D: Amazon MQ for RabbitMQ is not optimized for IoT use cases like AWS IoT Core, and Lookout for Metrics is more for time-series anomaly detection rather than telemetry analysis.</p><p> Conclusion:</p><p>Option B provides a scalable, serverless, and low-operational-overhead solution by leveraging AWS IoT Core, Kinesis Firehose, and Kinesis Analytics. </p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "c5ee0927552c4bd08f4403976afacd56",
      "questionNumber": 242,
      "type": "single",
      "content": "<p>Question #242</p><p>During an audit, a security team discovered that a development team was putting IAM user secret access keys in their code and then committing it to an AWS CodeCommit repository. The security team wants to automatically find and remediate instances of this security vulnerability.<br><br></p><p>Which solution will ensure that the credentials are appropriately secured automatically?</p>",
      "options": [
        {
          "label": "A",
          "content": "Run a script nightly using AWS Systems Manager Run Command to search for credentials on the development instances. If found, use AWS Secrets Manager to rotate the credentials"
        },
        {
          "label": "B",
          "content": "Use a scheduled AWS Lambda function to download and scan the application code from CodeCommit. If credentials are found, generate new credentials and store them in AWS KMS."
        },
        {
          "label": "C",
          "content": "Configure Amazon Macie to scan for credentials in CodeCommit repositories. If credentials are found, trigger an AWS Lambda function to disable the credentials and notify the user."
        },
        {
          "label": "D",
          "content": "Configure a CodeCommit trigger to invoke an AWS Lambda function to scan new code submissions for credentials. If credentials are found, disable them in AWS IAM and notify the user."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. Configure a CodeCommit trigger to invoke an AWS Lambda function to scan new code submissions for credentials. If credentials are found, disable them in AWS IAM and notify the user.</p><p>This solution will ensure that credentials are appropriately secured automatically for the following reasons:</p><p>Real-time Detection: CodeCommit triggers activate immediately when new code is submitted, allowing for instant scanning of code for IAM credentials. This provides the fastest possible detection compared to scheduled or periodic scanning approaches.</p><p>Immediate Remediation: When credentials are found, they are disabled in IAM right away, minimizing the window of vulnerability. This is critical for security since exposed credentials could be exploited quickly.</p><p>User Notification: Notifying the user who committed the code creates awareness and provides an opportunity for education about secure coding practices, helping prevent similar issues in the future.</p><p>Preventive Approach: This solution prevents the exposed credentials from being used maliciously by disabling them as soon as they're detected, rather than just identifying them or rotating them while they remain exposed.</p><p>Automation: The entire process is automated from detection to remediation, requiring no manual intervention.</p><p>Option A (using Systems Manager Run Command with nightly scripts) would be less effective because:</p><p>Nightly scans create a potential 24-hour window where credentials could be exposed and exploited</p><p>It doesn't include notification to users who committed the credentials</p><p>It focuses on development instances rather than the source code repository itself</p><p>Option B (scheduled Lambda function to scan CodeCommit) would be less effective because:</p><p>Scheduled scans may miss credentials between scan intervals</p><p>Storing credentials in KMS doesn't address the immediate security risk of exposed credentials</p><p>It doesn't include disabling the compromised credentials or notifying users</p><p>The CodeCommit trigger with Lambda approach (Option D) provides the most comprehensive solution by combining immediate detection, automatic remediation, and user education, which aligns with security best practices for handling exposed credentials in code repositories.</p><p>Sources</p><p>IAM credentials for CodeCommit: Git credentials, SSH keys, and AWS access keys - AWS Identity and Access Management （https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_ssh-keys.html）</p><p>CodeCommit auth fail | AWS re:Post （https://repost.aws/questions/QU_8egJwzFRqegRf5zMtsPgw/codecommit-auth-fail）</p><p>Security best practices in IAM - AWS Identity and Access Management （https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html）</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "64a483d46eb2424fbd65dcca18cfd99c",
      "questionNumber": 243,
      "type": "multiple",
      "content": "<p>Question #243</p><p>A company has a data lake in Amazon S3 that needs to be accessed by hundreds of applications across many AWS accounts. The company's information security policy states that the S3 bucket must not be accessed over the public internet and that each application should have the minimum permissions necessary to function.</p><p>To meet these requirements, a solutions architect plans to use an S3 access point that is restricted to specific VPCs for each application.</p><p>Which combination of steps should the solutions architect take to implement this solution? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an S3 access point for each application in the AWS account that owns the S3 bucket. Configure each access point to be accessible only from the application&rsquo;s VPC. Update the bucket policy to require access from an access point."
        },
        {
          "label": "B",
          "content": "Create an interface endpoint for Amazon S3 in each application&#39;s VPC. Configure the endpoint policy to allow access to an S3 access point. Create a VPC gateway attachment for the S3 endpoint."
        },
        {
          "label": "C",
          "content": "Create a gateway endpoint for Amazon S3 in each application&#39;s VPC. Configure the endpoint policy to allow access to an S3 access point. Secify the route table that is used to access the access point.<p><br></p>"
        },
        {
          "label": "D",
          "content": "Create an S3 access point for each application in each AWS account and attach the access points to the S3 bucket. Configure each access point to be accessible only from the application&#39;s VPC. Update the bucket policy to require access from an access point."
        },
        {
          "label": "E",
          "content": "Create a gateway endpoint for Amazon S3 in the data lake&#39;s VPC. Attach an endpoint policy to allow access to the S3 bucket. Specify the route table that is used to access the bucket."
        }
      ],
      "correctAnswer": "AC",
      "explanation": "<p>The correct answers are A and C. Step A enables each application to have its own access point, which can be configured to allow only the necessary permissions for that application. This satisfies the requirement for least privilege access. Step C involves creating a gateway VPC endpoint for S3 in each application's VPC. This endpoint provides a private path for traffic between the VPC and the S3 bucket, ensuring that the data does not traverse the public internet. The endpoint policy should be configured to allow access to the specific S3 access point created for the application, maintaining the least privilege principle.</p><p>The requirements are:</p><p>1. No public internet access to the S3 bucket.</p><p>2. Minimum permissions for each application.</p><p>3. Access restricted via VPCs using S3 access points.</p><p> Option A: Correct</p><p>- S3 access points are created in the bucket's account (where the data lake resides).</p><p>- Each access point is restricted to a specific application VPC (using VPC restrictions).</p><p>- The bucket policy enforces that all access must come through an access point (ensuring no public internet access).</p><p> Option C: Correct</p><p>- A gateway endpoint for S3 (type `com.amazonaws.&lt;region&gt;.s3`) is needed in each application's VPC to allow private connectivity to S3.</p><p>- The endpoint policy can restrict access to only the required S3 access point.</p><p>- The route table ensures traffic to S3 goes through the gateway endpoint (keeping traffic within AWS network).</p><p> Why not the others?</p><p>- B: Incorrect because interface endpoints (PrivateLink) are not needed for S3; gateway endpoints are the correct choice for S3.</p><p>- D: Incorrect because S3 access points must be created in the bucket's account, not in each application's account.</p><p>- E: Incorrect because the gateway endpoint should be in each application's VPC, not the data lake's VPC. Also, the policy should allow access to the access point, not the bucket directly.</p><p> Summary:</p><p>- A: Sets up VPC-restricted access points in the bucket's account.</p><p>- C: Configures private connectivity via gateway endpoints in each application's VPC. </p><p>This ensures private access and least-privilege permissions. ✅</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "0bfd526cb8394369aa4cca707d99fd0b",
      "questionNumber": 244,
      "type": "single",
      "content": "<p>Question #244</p><p>A company has developed a hybrid solution between its data center and AWS. The company uses Amazon VPC and Amazon EC2 instances that send application logs to Amazon CloudWatch. The EC2 instances read data from multiple relational databases that are hosted on premises.<br><br></p><p>The company wants to monitor which EC2 instances are connected to the databases in near-real time. The company already has a monitoring solution that uses Splunk on premises. A solutions architect needs to determine how to send networking traffic to Splunk.<br><br></p><p>How should the solutions architect meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Enable VPC flows logs, and send them to CloudWatch. Create an AWS Lambda function to periodically export the CloudWatch logs to an Amazon S3 bucket by using the pre-defined export function. Generate ACCESS_KEY and SECRET_KEY AWS credentials. Configure Splunk to pull the logs from the S3 bucket by using those credentials."
        },
        {
          "label": "B",
          "content": "Create an Amazon Kinesis Data Firehose delivery stream with Splunk as the destination. Configure a pre-processing AWS Lambda function with a Kinesis Data Firehose stream processor that extracts individual log events from records sent by CloudWatch Logs subscription filters. Enable VPC flows logs, and send them to CloudWatch. Create a CloudWatch Logs subscription that sends log events to the Kinesis Data Firehose delivery stream."
        },
        {
          "label": "C",
          "content": "Ask the company to log every request that is made to the databases along with the EC2 instance IP address. Export the CloudWatch logs to an Amazon S3 bucket. Use Amazon Athena to query the logs grouped by database name. Export Athena results to another S3 bucket. Invoke an AWS Lambda function to automatically send any new file that is put in the S3 bucket to Splunk."
        },
        {
          "label": "D",
          "content": "Send the CloudWatch logs to an Amazon Kinesis data stream with Amazon Kinesis Data Analytics for SQL Applications. Configure a 1 minute sliding window to collect the events. Create a SQL query that uses the anomaly detection template to monitor any networking traffic anomalies in near-real time. Send the result to an Amazon Kinesis Data Firehose delivery stream with Splunk as the destination."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>The correct answer is B. This solution involves creating an Amazon Kinesis Data Firehose delivery stream with Splunk as the destination and configuring a pre-processing AWS Lambda function. The Lambda function is used to process the records from CloudWatch Logs before they are sent to the Kinesis Data Firehose stream. This allows for near-real-time delivery of VPC Flow Logs to Splunk, which meets the company's requirements for monitoring EC2 instances' connectivity to on-premises databases.</p><p>Requirements Recap:</p><p>1. Monitor EC2 instances connected to on-premises databases in near-real time.</p><p>2. Send networking traffic logs to Splunk (which is on-premises).</p><p>3. Current setup: &nbsp;</p><p> &nbsp; - Hybrid environment (AWS VPC + on-premises databases). &nbsp;</p><p> &nbsp; - EC2 instances send logs to Amazon CloudWatch. &nbsp;</p><p> &nbsp; - Splunk is already used on-premises for monitoring.</p><p> Analysis of Options:</p><p> Option A:</p><p>- Uses VPC Flow Logs → CloudWatch → S3 → Splunk (via Lambda & S3 pull). &nbsp;</p><p>- Issues: &nbsp;</p><p> &nbsp;- Not near-real time (S3 exports are periodic, not continuous). &nbsp;</p><p> &nbsp;- Manual credential management (ACCESS_KEY/SECRET_KEY) is less secure. &nbsp;</p><p> &nbsp;- Splunk pulling from S3 introduces latency. &nbsp;</p><p> Option B:</p><p>- Uses VPC Flow Logs → CloudWatch → Kinesis Data Firehose (with Lambda pre-processing) → Splunk. &nbsp;</p><p>- Advantages: &nbsp;</p><p> &nbsp;- Near-real time delivery (Kinesis Data Firehose streams logs directly to Splunk). &nbsp;</p><p> &nbsp;- No manual credential management (Firehose securely delivers logs). &nbsp;</p><p> &nbsp;- Supports CloudWatch Logs subscription filters for efficient log forwarding. &nbsp;</p><p> &nbsp;- Lambda pre-processing allows log transformation if needed. &nbsp;</p><p>- Best fits the requirement of near-real time monitoring. &nbsp;</p><p> Option C:</p><p>- Uses manual logging → S3 → Athena → Lambda → Splunk. &nbsp;</p><p>- Issues: &nbsp;</p><p> &nbsp;- Not near-real time (Athena queries and Lambda processing add latency). &nbsp;</p><p> &nbsp;- Overly complex for the requirement. &nbsp;</p><p> &nbsp;- Manual logging is error-prone. &nbsp;</p><p> Option D:</p><p>- Uses Kinesis Data Analytics for anomaly detection. &nbsp;</p><p>- Issues: &nbsp;</p><p> &nbsp;- Overkill for simply monitoring connections (anomaly detection is not required). &nbsp;</p><p> &nbsp;- More expensive and complex than necessary. &nbsp;</p><p> Conclusion:</p><p>Option B is the best solution because:</p><p>✔ Uses VPC Flow Logs (which track network connections, including EC2-to-database traffic). &nbsp;</p><p>✔ Kinesis Data Firehose provides near-real time delivery to Splunk. &nbsp;</p><p>✔ CloudWatch Logs subscription ensures efficient log forwarding. &nbsp;</p><p>✔ Minimal latency and complexity compared to other options. &nbsp;</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "09c9cfd4d6f145fe9f879d93183991be",
      "questionNumber": 245,
      "type": "multiple",
      "content": "<p>Question #245</p><p>A company has five development teams that have each created five AWS accounts to develop and host applications. To track spending, the development teams log in to each account every month, record the current cost from the AWS Billing and Cost Management console, and provide the information to the company's finance team.</p><p>The company has strict compliance requirements and needs to ensure that resources are created only in AWS Regions in the United States. However, some resources have been created in other Regions.</p><p>A solutions architect needs to implement a solution that gives the finance team the ability to track and consolidate expenditures for all the accounts. The solution also must ensure that the company can create resources only in Regions in the United States.</p><p>Which combination of steps will meet these requirements in the MOST operationally efficient way? (Choose three.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a new account to serve as a management account. Create an Amazon S3 bucket for the finance team. Use AWS Cost and Usage Reports to create monthly reports and to store the data in the finance team&#39;s S3 bucket."
        },
        {
          "label": "B",
          "content": "Create a new account to serve as a management account. Deploy an organization in AWS Organizations with all features enabled. Invite all the existing accounts to the organization. Ensure that each account accepts the invitation."
        },
        {
          "label": "C",
          "content": "Create an OU that includes all the development teams. Create an SCP that allows the creation of resources only in Regions that are in the United States. Apply the SCP to the OU."
        },
        {
          "label": "D",
          "content": "Create an OU that includes all the development teams. Create an SCP that denies the creation of resources in Regions that are outside the United States. Apply the SCP to the OU."
        },
        {
          "label": "E",
          "content": "Create an IAM role in the management account. Attach a policy that includes permissions to view the Billing and Cost Management console. Allow the finance team users to assume the role. Use AWS Cost Explorer and the Billing and Cost Management console to analyze cost."
        },
        {
          "label": "F",
          "content": "Create an IAM role in each AWS account. Attach a policy that includes permissions to view the Billing and Cost Management console. Allow the finance team users to assume the role."
        }
      ],
      "correctAnswer": "BDE",
      "explanation": "<p>The correct answers are B, D, and E. To centralize billing and ensure compliance with regional restrictions, the following steps should be taken:</p><p>1. B: Create a new management account and set up AWS Organizations &nbsp;</p><p> &nbsp; - AWS Organizations allows consolidated billing, making it easier to track spending across all accounts. &nbsp;</p><p> &nbsp; - Enabling all features allows the use of Service Control Policies (SCPs) to enforce compliance. &nbsp;</p><p>2. D: Create an OU with an SCP that denies resource creation outside US Regions &nbsp;</p><p> &nbsp; - SCPs applied at the OU level prevent resources from being created in non-US Regions, ensuring compliance. &nbsp;</p><p> &nbsp; - A deny-based SCP is more secure than an allow-based one (Option C) because it blocks unwanted actions while allowing everything else. &nbsp;</p><p>3. E: Create an IAM role in the management account for the finance team &nbsp;</p><p> &nbsp; - The finance team can assume this role to access AWS Cost Explorer and the Billing Dashboard in the management account, which provides consolidated cost data. &nbsp;</p><p> &nbsp; - This is more efficient than creating roles in every account (Option F). &nbsp;</p><p> Why not the other options? &nbsp;</p><p>- A: AWS Cost and Usage Reports (CUR) are useful but not necessary since AWS Organizations already provides consolidated billing. &nbsp;</p><p>- C: An allow-based SCP is less secure than a deny-based SCP (Option D). &nbsp;</p><p>- F: Creating IAM roles in every account is less efficient than using a single role in the management account (Option E). &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "7d36f2fce1eb452c9f8a424980f82987",
      "questionNumber": 246,
      "type": "single",
      "content": "<p>Question #246</p><p>A company needs to create and manage multiple AWS accounts for a number of departments from a central location. The security team requires read-only access to all accounts from its own AWS account. The company is using AWS Organizations and created an account for the security team.</p><p><br></p><p>How should a solutions architect meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use the OrganizationAccountAccessRole IAM role to create a new IAM policy with read-only access in each member account. Establish a trust relationship between the IAM policy in each member account and the security account. Ask the security team to use the IAM policy to gain access."
        },
        {
          "label": "B",
          "content": "Use the OrganizationAccountAccessRole IAM role to create a new IAM role with read-only access in each member account. Establish a trust relationship between the IAM role in each member account and the security account. Ask the security team to use the IAM role to gain access."
        },
        {
          "label": "C",
          "content": "Ask the security team to use AWS Security Token Service (AWS STS) to call the AssumeRole API for the OrganizationAccountAccessRole IAM role in the management account from the security account. Use the generated temporary credentials to gain access."
        },
        {
          "label": "D",
          "content": "Ask the security team to use AWS Security Token Service (AWS STS) to call the AssumeRole API for the OrganizationAccountAccessRole IAM role in the member account from the security account. Use the generated temporary credentials to gain access."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Use the OrganizationAccountAccessRole IAM role to create a new IAM role with read-only access in each member account. Establish a trust relationship between the IAM role in each member account and the security account. Ask the security team to use the IAM role to gain access.</p><p>This solution meets the requirements for the following reasons:</p><p>Principle of Least Privilege: By creating a specific read-only role in each member account, the security team is granted exactly the level of access they need (read-only) rather than the broader permissions that typically come with the OrganizationAccountAccessRole.</p><p>Direct Access Path: This approach creates a direct trust relationship between the member accounts and the security team's account, allowing the security team to assume the read-only role directly without going through the management account.</p><p>Centralized Management: The initial setup can be done using the OrganizationAccountAccessRole, which allows administrators to access member accounts to create the necessary read-only roles. This leverages the existing AWS Organizations structure.</p><p>Scalability: While this approach requires creating roles in each member account, it can be automated using AWS CloudFormation StackSets to deploy consistent roles across all accounts in the organization.</p><p>Security Best Practices: This approach follows AWS security best practices by establishing cross-account access through role assumption rather than creating IAM users in multiple accounts.</p><p>Option C (using AWS STS to assume the OrganizationAccountAccessRole in the management account) is not appropriate because:</p><p>The OrganizationAccountAccessRole typically has administrative permissions, which violates the principle of least privilege for the security team that only needs read-only access.</p><p>This approach would require the security team to first access the management account and then access each member account, creating an unnecessary additional step.</p><p>It doesn't provide a direct path for the security team to access member accounts from their own account, which is a requirement.</p><p>The correct solution (Option B) provides a secure, scalable approach that follows AWS best practices for cross-account access while meeting the specific requirement of providing read-only access to the security team across all accounts from their own AWS account.</p><p>Sources</p><p>Using AWS Organizations for security - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/security-reference-architecture/organizations-security.html）</p><p>Manage Organization Member Accounts via AWS CLI | AWS re:Post （https://repost.aws/questions/QUDltwquvCQtOrRIJUF6yreg/manage-organization-member-accounts-via-aws-cli）</p><p>SEC03-BP08 Share resources securely within your organization - AWS Well-Architected Framework （https://docs.aws.amazon.com/wellarchitected/2025-02-25/framework/sec_permissions_share_securely.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "633e0b9fb02a4dfebff306ac014c6abb",
      "questionNumber": 247,
      "type": "single",
      "content": "<p>Question #247</p><p>A large company runs workloads in VPCs that are deployed across hundreds of AWS accounts. Each VPC consists of public subnets and private subnets that span across multiple Availability Zones. NAT gateways are deployed in the public subnets and allow outbound connectivity to the internet from the private subnets.</p><p><br></p><p>A solutions architect is working on a hub-and-spoke design. All private subnets in the spoke VPCs must route traffic to the internet through an egress VPC. The solutions architect already has deployed a NAT gateway in an egress VPC in a central AWS account.</p><p><br></p><p>Which set of additional steps should the solutions architect take to meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create peering connections between the egress VPC and the spoke VPCs. Configure the required routing to allow access to the internet."
        },
        {
          "label": "B",
          "content": "Create a transit gateway, and share it with the existing AWS accounts. Attach existing VPCs to the transit gateway. Configure the required routing to allow access to the internet."
        },
        {
          "label": "C",
          "content": "Create a transit gateway in every account. Attach the NAT gateway to the transit gateways. Configure the required routing to allow access to the internet."
        },
        {
          "label": "D",
          "content": "Create an AWS PrivateLink connection between the egress VPC and the spoke VPCs. Configure the required routing to allow access to the internet."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>The correct answer is B. A transit gateway can be used as a hub for network traffic between VPCs and on-premises networks. By creating a transit gateway and sharing it with existing AWS accounts, the architect can attach the VPCs to the transit gateway. This allows for centralized management of routing and ensures that all traffic from spoke VPCs is routed through the egress VPC with the NAT gateway, thus providing internet access in a hub-and-spoke architecture.</p><p>1. Hub-and-Spoke Design Requirement: The solution requires centralized internet egress through an egress VPC (hub) for all spoke VPCs across multiple AWS accounts.</p><p>2. Transit Gateway (TGW) Benefits:</p><p> &nbsp; - Centralized Routing: TGW allows you to connect hundreds of VPCs (spokes) to a central egress VPC (hub) efficiently.</p><p> &nbsp; - Cross-Account Sharing: You can create the TGW in one account (central account) and share it with other AWS accounts using AWS Resource Access Manager (RAM).</p><p> &nbsp; - Simplified Peering: Instead of managing individual VPC peering connections (which don’t scale well and have routing limitations), TGW provides a scalable and manageable solution.</p><p>3. Steps:</p><p> &nbsp; - Deploy a Transit Gateway in the central account (where the egress VPC is located).</p><p> &nbsp; - Share the TGW with spoke accounts using AWS RAM.</p><p> &nbsp; - Attach all spoke VPCs and the egress VPC to the TGW.</p><p> &nbsp; - Configure routing:</p><p> &nbsp; &nbsp; - Spoke VPCs: Route internet-bound traffic (0.0.0.0/0) to the TGW attachment.</p><p> &nbsp; &nbsp; - TGW Route Table: Route internet-bound traffic to the egress VPC attachment.</p><p> &nbsp; &nbsp; - Egress VPC: Ensure the NAT gateway routes traffic to the internet.</p><p>4. Why Not Other Options?:</p><p> &nbsp; - A (VPC Peering): Doesn’t scale well for hundreds of VPCs, requires complex routing, and lacks transitive routing.</p><p> &nbsp; - C (TGW in Every Account): Overly complex and unnecessary; a single shared TGW is sufficient.</p><p> &nbsp; - D (AWS PrivateLink): PrivateLink is for private connectivity to services, not for internet egress.</p><p> Key AWS Services Used:</p><p>- Transit Gateway: For scalable hub-and-spoke connectivity.</p><p>- AWS RAM (Resource Access Manager): To share the TGW across accounts.</p><p>- NAT Gateway: For outbound internet access from the egress VPC.</p><p>This approach ensures scalability, centralized management, and cost efficiency for internet egress across hundreds of accounts.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "93fb5dc9803c4d62a98591b803b3f77a",
      "questionNumber": 248,
      "type": "single",
      "content": "<p>Question #248</p><p>An education company is running a web application used by college students around the world. The application runs in an Amazon Elastic Container Service (Amazon ECS) cluster in an Auto Scaling group behind an Application Load Balancer (ALB). A system administrator detects a weekly spike in the number of failed login attempts, which overwhelm the application's authentication service. All the failed login attempts originate from about 500 different IP addresses that change each week. A solutions architect must prevent the failed login attempts from overwhelming the authentication service.</p><p><br></p><p>Which solution meets these requirements with the MOST operational efficiency?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use AWS Firewall Manager to create a security group and security group policy to deny access from the IP addresses."
        },
        {
          "label": "B",
          "content": "Create an AWS WAF web ACL with a rate-based rule, and set the rule action to Block. Connect the web ACL to the ALB."
        },
        {
          "label": "C",
          "content": "Use AWS Firewall Manager to create a security group and security group policy to allow access only to specific CIDR ranges."
        },
        {
          "label": "D",
          "content": "Create an AWS WAF web ACL with an IP set match rule, and set the rule action to Block. Connect the web ACL to the ALB."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Create an AWS WAF web ACL with a rate-based rule, and set the rule action to Block. Connect the web ACL to the ALB.</p><p>This solution meets the requirements with the most operational efficiency for the following reasons:</p><p>Dynamic Protection Against Changing IP Addresses: The scenario specifically mentions that the 500 IP addresses change each week. AWS WAF rate-based rules automatically identify and block IP addresses that exceed a specified request rate, regardless of what those IP addresses are. This eliminates the need to manually update IP address lists weekly.</p><p>Automatic Adaptation: Rate-based rules automatically adapt to new threat sources without manual intervention. When the weekly spike occurs with new IP addresses, the rate-based rule will detect and block excessive requests from those IPs without requiring updates to the rule.</p><p>Targeted Protection: AWS WAF can be configured to specifically protect the authentication endpoint that's being overwhelmed, rather than blocking access to the entire application.</p><p>Operational Efficiency: Once configured, the rate-based rule requires minimal ongoing maintenance. It will continue to function effectively even as the attacking IP addresses change each week, making it highly operationally efficient.</p><p>Integration with ALB: AWS WAF integrates directly with Application Load Balancers, making implementation straightforward.</p><p>Option A (using AWS Firewall Manager to create security groups) would be less operationally efficient because:</p><p>It would require weekly updates to the security group rules to block the new set of 500 IP addresses.</p><p>Managing a large and constantly changing list of blocked IP addresses would create significant operational overhead.</p><p>Security groups are not designed for this type of dynamic threat mitigation where the source IPs change frequently.</p><p>This approach would require creating automation to detect and update the blocked IP addresses, adding complexity.</p><p>The AWS WAF rate-based rule solution provides an elegant, low-maintenance approach that automatically adapts to the changing threat landscape without requiring weekly manual updates or complex automation. This makes it the most operationally efficient solution for protecting the authentication service from the weekly spikes in failed login attempts.</p><p>Sources</p><p>AWS WAF Rate-Based Rule - How to permanently block offending IP addresses? | AWS re:Post （https://repost.aws/questions/QUyq6dArvCRECG10yKcg6erg/aws-waf-rate-based-rule-how-to-permanently-block-offending-ip-addresses）</p><p>Allow good bots in WAF rate-based rule | AWS re:Post （https://repost.aws/questions/QUNcT9XrO8RaWpMT8RAtO4Vg/allow-good-bots-in-waf-rate-based-rule）</p><p>AWS WAF – Rate-based rules - AWS Best Practices for DDoS Resiliency （https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/aws-waf-rate-based-rules.html）</p><p>Static controls for managing bots - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/bot-control/static-controls.html）</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "cd813e9bffa747909d389a1688dae1bb",
      "questionNumber": 249,
      "type": "single",
      "content": "<p>Question #249</p><p>A company operates an on-premises software-as-a-service (SaaS) solution that ingests several files daily. The company provides multiple public SFTP endpoints to its customers to facilitate the file transfers. The customers add the SFTP endpoint IP addresses to their firewall allow list for outbound traffic. Changes to the SFTP endpoint IP addresses are not permitted.</p><p><br></p><p>The company wants to migrate the SaaS solution to AWS and decrease the operational overhead of the file transfer service.</p><p><br></p><p>Which solution meets these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Register the customer-owned block of IP addresses in the company&#39;s AWS account. Create Elastic IP addresses from the address pool and assign them to an AWS Transfer for SFTP endpoint. Use AWS Transfer to store the files in Amazon S3."
        },
        {
          "label": "B",
          "content": "Add a subnet containing the customer-owned block of IP addresses to a VPC. Create Elastic IP addresses from the address pool and assign them to an Application Load Balancer (ALB). Launch EC2 instances hosting FTP services in an Auto Scaling group behind the ALB. Store the files in attached Amazon Elastic Block Store (Amazon EBS) volumes."
        },
        {
          "label": "C",
          "content": "Register the customer-owned block of IP addresses with Amazon Route 53. Create alias records in Route 53 that point to a Network Load Balancer (NLB). Launch EC2 instances hosting FTP services in an Auto Scaling group behind the NLB. Store the files in Amazon S3."
        },
        {
          "label": "D",
          "content": "Register the customer-owned block of IP addresses in the company&rsquo;s AWS account. Create Elastic IP addresses from the address pool and assign them to an Amazon S3 VPC endpoint. Enable SFTP support on the S3 bucket."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>The correct answer is A. By using AWS Transfer for SFTP, the company can maintain the existing IP addresses by registering them with AWS and assigning Elastic IPs to the SFTP endpoint. This approach allows customers to continue using the same IP addresses in their firewall allow list, satisfying the requirement that changes to the SFTP endpoint IP addresses are not permitted. Additionally, by using AWS Transfer for SFTP, the company can offload the operational overhead of managing an SFTP server to AWS, and leverage Amazon S3 for scalable file storage.</p><p>1. SFTP endpoints must retain the same IP addresses (changes are not permitted).</p><p>2. Migrate to AWS and reduce operational overhead.</p><p>3. Store files in Amazon S3 (implied by the need to migrate SaaS to AWS).</p><p> Analysis of Options:</p><p>- Option A (✅ Correct):</p><p> &nbsp;- Uses AWS Transfer for SFTP, a fully managed SFTP service.</p><p> &nbsp;- Allows assigning Elastic IPs from a customer-owned IP block (ensuring no IP changes).</p><p> &nbsp;- Stores files directly in Amazon S3, reducing operational overhead.</p><p> &nbsp;- Fully managed, so no need to maintain EC2 instances or scaling.</p><p>- Option B (❌ Incorrect):</p><p> &nbsp;- Uses EC2 instances with an ALB, which does not support SFTP (ALB works at Layer 7, not SFTP).</p><p> &nbsp;- Stores files on EBS volumes, which is not scalable and increases management overhead.</p><p> &nbsp;- ALB does not support static IP assignment (unlike AWS Transfer for SFTP or NLB).</p><p>- Option C (❌ Incorrect):</p><p> &nbsp;- Uses Route 53 and NLB, but NLBs do not support SFTP (they operate at Layer 4).</p><p> &nbsp;- Requires managing EC2 instances, increasing operational overhead.</p><p> &nbsp;- Route 53 does not solve the IP requirement (customers need static IPs, not DNS).</p><p>- Option D (❌ Incorrect):</p><p> &nbsp;- S3 VPC endpoints do not support SFTP (AWS Transfer for SFTP is needed).</p><p> &nbsp;- S3 does not natively support SFTP (this is a distractor).</p><p> Conclusion:</p><p>AWS Transfer for SFTP (Option A) is the only fully managed solution that allows:</p><p>- Static IP assignment (from customer-owned IP block).</p><p>- Direct S3 storage (reducing operational overhead).</p><p>- Native SFTP support without managing servers.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "a8d335626ccf43c1ad8c684964d1972d",
      "questionNumber": 250,
      "type": "single",
      "content": "<p>Question #250</p><p>A company has a new application that needs to run on five Amazon EC2 instances in a single AWS Region. The application requires high throughput, low-latency network connections between all of the EC2 instances where the application will run. There is no requirement for the application to be fault tolerant.</p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Launch five new EC2 instances into a cluster placement group. Ensure that the EC2 instance type supports enhanced networking."
        },
        {
          "label": "B",
          "content": "Launch five new EC2 instances into an Auto Scaling group in the same Availability Zone. Attach an extra elastic network interface to each EC2 instance."
        },
        {
          "label": "C",
          "content": "Launch five new EC2 instances into a partition placement group. Ensure that the EC2 instance type supports enhanced networking."
        },
        {
          "label": "D",
          "content": "Launch five new EC2 instances into a spread placement group. Attach an extra elastic network interface to each EC2 instance."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Launch five new EC2 instances into a cluster placement group. Ensure that the EC2 instance type supports enhanced networking.</p><p>This solution will meet the requirements for the following reasons:</p><p>Cluster Placement Group: A cluster placement group is specifically designed to pack instances close together within a single Availability Zone, providing the lowest possible network latency and highest network throughput between instances. This directly addresses the requirement for \"high-throughput, low-latency network connections between all of the EC2 instances.\"</p><p>Enhanced Networking: Enhanced networking uses single root I/O virtualization (SR-IOV) to provide higher I/O performance and lower CPU utilization compared to traditional virtualized network interfaces. This further improves the network performance between instances, supporting the high-throughput, low-latency requirement.</p><p>Single Region Deployment: The solution keeps all instances within a single region as specified in the requirements.</p><p>No Fault Tolerance Requirement: The scenario explicitly states there is no requirement for the application to be fault tolerant. A cluster placement group places all instances in a single Availability Zone, which reduces fault tolerance but maximizes network performance - perfectly aligned with the stated requirements.</p><p>The other options would not be optimal:</p><p>B. Auto Scaling group in the same AZ with extra elastic network interfaces - While this keeps instances in the same AZ, it doesn't provide the same level of network optimization as a cluster placement group. Adding extra network interfaces is primarily useful for connecting to different networks, not for improving performance between instances in the same application.</p><p>C. Partition placement group with enhanced networking - Partition placement groups are designed to spread instances across distinct racks (partitions), which is useful for distributed applications that need to reduce correlated failures. This doesn't optimize for the lowest possible latency between all instances.</p><p>D. Spread placement group with extra network interfaces - Spread placement groups place instances on distinct underlying hardware to reduce the risk of simultaneous failures. This actually increases the physical distance between instances, which would likely increase latency rather than decrease it.</p><p>Option A provides the optimal solution for maximizing network performance between the five EC2 instances without concern for fault tolerance.</p><p>Sources</p><p>PERF04-BP06 Choose your workload's location based on network requirements - AWS Well-Architected Framework （https://docs.aws.amazon.com/wellarchitected/2024-06-27/framework/perf_networking_choose_workload_location_network_requirements.html）</p><p>Placement Groups Latency | AWS re:Post （https://repost.aws/questions/QUz8Qi26ZfSZi6oeRtwRKfgQ/placement-groups-latency）</p><p>Optimize network performance on EC2 Windows instances - Amazon Elastic Compute Cloud （https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking-os.html）</p><p>Benchmark network throughput between Linux instances in the same VPC | AWS re:Post （https://repost.aws/knowledge-center/network-throughput-benchmark-linux-ec2）</p><p>PERF04-BP06 Choose your workload's location based on network requirements - AWS Well-Architected Framework （https://docs.aws.amazon.com/wellarchitected/2023-10-03/framework/perf_networking_choose_workload_location_network_requirements.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "17b9512fb2494231bd13f5380151fd83",
      "questionNumber": 251,
      "type": "single",
      "content": "<p>Question #251</p><p>A company is creating a REST API to share information with six of its partners based in the United States. The company has created an Amazon API Gateway Regional endpoint. Each of the six partners will access the API once per day to post daily sales figures.</p><p><br></p><p>After initial deployment, the company observes 1,000 requests per second originating from 500 different IP addresses around the world. The company believes this traffic is originating from a botnet and wants to secure its API while minimizing cost.</p><p><br></p><p>Which approach should the company take to secure its API?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an Amazon CloudFront distribution with the API as the origin. Create an AWS WAF web ACL with a rule to block clients that submit more than five requests per day. Associate the web ACL with the CloudFront distribution. Configure CloudFront with an origin access identity (OAI) and associate it with the distribution. Configure API Gateway to ensure only the OAI can run the POST method."
        },
        {
          "label": "B",
          "content": "Create an Amazon CloudFront distribution with the API as the origin. Create an AWS WAF web ACL with a rule to block clients that submit more than five requests per day. Associate the web ACL with the CloudFront distribution. Add a custom header to the CloudFront distribution populated with an API key. Configure the API to require an API key on the POST method."
        },
        {
          "label": "C",
          "content": "Create an AWS WAF web ACL with a rule to allow access to the IP addresses used by the six partners. Associate the web ACL with the API. Create a resource policy with a request limit and associate it with the API. Configure the API to require an API key on the POST method."
        },
        {
          "label": "D",
          "content": "Create an AWS WAF web ACL with a rule to allow access to the IP addresses used by the six partners. Associate the web ACL with the API. Create a usage plan with a request limit and associate it with the API. Create an API key and add it to the usage plan."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>The correct answer is D. To secure the API from the botnet traffic, the company can use AWS WAF to create a web ACL that allows only the known IP addresses of the six partners. Additionally, by creating a usage plan with a request limit in API Gateway and requiring an API key for the POST method, the company can ensure that only authorized requests are processed, and it can limit the rate of requests to prevent the botnet traffic from overwhelming the API.</p><p>1. Requirements:</p><p> &nbsp; - The company has a Regional API Gateway endpoint.</p><p> &nbsp; - Only 6 partners (with known IPs) should access the API once per day.</p><p> &nbsp; - There is malicious traffic (botnet) from 500+ IPs worldwide making 1000 requests/sec.</p><p> &nbsp; - The goal is to secure the API while minimizing cost.</p><p>2. Key Security Measures Needed:</p><p> &nbsp; - Restrict access to only the 6 partners' IPs (whitelisting).</p><p> &nbsp; - Limit request rates to prevent abuse.</p><p> &nbsp; - Use API keys for additional authentication.</p><p> Why Option D is Correct:</p><p>- AWS WAF Web ACL with IP Allow List:</p><p> &nbsp;- Blocks all traffic except from the 6 partners' IPs.</p><p> &nbsp;- Effectively stops botnet traffic.</p><p>- Usage Plan with Request Limit:</p><p> &nbsp;- Enforces a rate limit (e.g., 1 request/day per partner).</p><p> &nbsp;- Prevents abuse even if IP whitelisting fails.</p><p>- API Key:</p><p> &nbsp;- Adds an extra layer of authentication.</p><p> &nbsp;- The key is added to the usage plan, ensuring only authorized clients can call the API.</p><p> Why Other Options Are Incorrect:</p><p>- Option A & B (CloudFront + WAF + OAI/API Key):</p><p> &nbsp;- CloudFront is unnecessary since the API is Regional (not global).</p><p> &nbsp;- Overcomplicates the solution and increases cost.</p><p>- Option C (WAF + Resource Policy + API Key):</p><p> &nbsp;- Resource policies are for cross-account access or IP-based restrictions, but WAF already handles IP whitelisting.</p><p> &nbsp;- A usage plan (Option D) is better for rate-limiting than a resource policy.</p><p> Conclusion:</p><p>Option D is the most cost-effective and secure solution, combining IP whitelisting, rate limiting, and API key authentication.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": true,
      "noteContent": "Resource policies are for cross-account access or IP-based restrictions"
    },
    {
      "id": "3fb015fc0b614020a249c6c74a32808d",
      "questionNumber": 252,
      "type": "single",
      "content": "<p>Question #252</p><p>A company uses an Amazon Aurora PostgreSQL DB cluster for applications in a single AWS Region. The company's database team must monitor all data activity on all the databases.</p><p><br></p><p>Which solution will achieve this goal?</p>",
      "options": [
        {
          "label": "A",
          "content": "Set up an AWS Database Migration Service (AWS DMS) change data capture (CDC) task. Specify the Aurora DB cluster as the source. Specify Amazon Kinesis Data Firehose as the target. Use Kinesis Data Firehose to upload the data into an Amazon OpenSearch Service cluster for further analysis."
        },
        {
          "label": "B",
          "content": "Start a database activity stream on the Aurora DB cluster to capture the activity stream in Amazon EventBridge. Define an AWS Lambda function as a target for EventBridge. Program the Lambda function to decrypt the messages from EventBridge and to publish all database activity to Amazon S3 for further analysis."
        },
        {
          "label": "C",
          "content": "Start a database activity stream on the Aurora DB cluster to push the activity stream to an Amazon Kinesis data stream. Configure Amazon Kinesis Data Firehose to consume the Kinesis data stream and to deliver the data to Amazon S3 for further analysis."
        },
        {
          "label": "D",
          "content": "Set up an AWS Database Migration Service (AWS DMS) change data capture (CDC) task. Specify the Aurora DB cluster as the source. Specify Amazon Kinesis Data Firehose as the target. Use Kinesis Data Firehose to upload the data into an Amazon Redshift cluster. Run queries on the Amazon Redshift data to determine database activities on the Aurora database."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Start a database activity stream on the Aurora DB cluster to push the activity stream to an Amazon Kinesis data stream. Configure Amazon Kinesis Data Firehose to consume the Kinesis data stream and to deliver the data to Amazon S3 for further analysis.</p><p>This solution will achieve the goal of monitoring all data activity on all the databases in the Aurora PostgreSQL DB cluster for the following reasons:</p><p>Comprehensive Monitoring: Database Activity Streams is specifically designed for database auditing and monitoring purposes. It captures a wide range of database activities including data manipulation language (DML) operations, data definition language (DDL) operations, and system events. This provides complete visibility into all database activity, which is the primary requirement.</p><p>Real-time Monitoring: Database Activity Streams pushes activities to an Amazon Kinesis data stream in near real-time, allowing the database team to have immediate visibility into database actions as they occur.</p><p>Complete Audit Trail: The solution captures all database activity including SQL statements, users, application contexts, and timestamps, providing a comprehensive audit trail for the database team.</p><p>Scalable Architecture: Using Kinesis Data Streams with Kinesis Data Firehose creates a scalable pipeline that can handle the volume of activity from all databases in the cluster.</p><p>Persistent Storage: Delivering the data to Amazon S3 provides durable, long-term storage of all database activity, enabling historical analysis and compliance reporting.</p><p>Analysis Capabilities: With the data in S3, the database team can perform further analysis using various AWS analytics services like Amazon Athena, Amazon QuickSight, or Amazon EMR.</p><p>Option A (using AWS DMS with CDC) is not optimal because:</p><p>AWS DMS with CDC is primarily designed for data replication and migration, not comprehensive database activity monitoring.</p><p>It focuses mainly on capturing data changes (inserts, updates, deletes) rather than all database activity including queries, schema changes, and system events.</p><p>It may not capture all the information needed for complete database activity monitoring, such as user session details, SQL statements that don't modify data, and system events.</p><p>The database activity streams solution (Option C) provides a purpose-built, comprehensive approach to monitoring all database activity, which directly addresses the company's requirement to monitor all data activity on all databases in their Aurora PostgreSQL cluster.</p><p>Sources</p><p>Replace Amazon QLDB with Amazon Aurora PostgreSQL for audit use cases | AWS Database Blog （https://aws.amazon.com/cn/blogs/database/replace-amazon-qldb-with-amazon-aurora-postgresql-for-audit-use-cases/）</p><p>Monitoring your Amazon RDS DB instance - Amazon Relational Database Service (https://docs.aws.amazon.com/AmazonRDS/latest/gettingstartedguide/managing-monitoring-perf.html)</p><p>Part 1: Audit Aurora PostgreSQL databases using Database Activity Streams and pgAudit | AWS Database Blog (https://aws.amazon.com/cn/blogs/database/part-1-audit-aurora-postgresql-databases-using-database-activity-streams-and-pgaudit/)</p><p>Monitoring tools for Amazon Aurora - Amazon Aurora (https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/MonitoringOverview.html)</p><p>Monitoring events, logs, and streams in an Amazon Aurora DB cluster - Amazon Aurora (https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_Monitor_Logs_Events.html)</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "95a35a8fc7bc49b99bf58950f696b981",
      "questionNumber": 253,
      "type": "single",
      "content": "<p>Question #253</p><p>An entertainment company recently launched a new game. To ensure a good experience for players during the launch period, the company deployed a static quantity of 12 r6g.16xlarge (memory optimized) Amazon EC2 instances behind a Network Load Balancer. The company's operations team used the Amazon CloudWatch agent and a custom metric to include memory utilization in its monitoring strategy.</p><p><br></p><p>Analysis of the CloudWatch metrics from the launch period showed consumption at about one quarter of the CPU and memory that the company expected. Initial demand for the game has subsided and has become more variable. The company decides to use an Auto Scaling group that monitors the CPU and memory consumption to dynamically scale the instance fleet. A solutions architect needs to configure the Auto Scaling group to meet demand in the most cost-effective way.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Configure the Auto Scaling group to deploy c6g.4xlarge (compute optimized) instances. Configure a minimum capacity of 3, a desired capacity of 3, and a maximum capacity of 12."
        },
        {
          "label": "B",
          "content": "Configure the Auto Scaling group to deploy m6g.4xlarge (general purpose) instances. Configure a minimum capacity of 3, a desired capacity of 3, and a maximum capacity of 12."
        },
        {
          "label": "C",
          "content": "Configure the Auto Scaling group to deploy r6g.4xlarge (memory optimized) instances. Configure a minimum capacity of 3, a desired capacity of 3, and a maximum capacity of 12."
        },
        {
          "label": "D",
          "content": "Configure the Auto Scaling group to deploy r6g.8xlarge (memory optimized) instances. Configure a minimum capacity of 2, a desired capacity of 2, and a maximum capacity of 6."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>The correct answer is C. Given that the company's analysis showed that the initially deployed instances were only utilizing a quarter of their CPU and memory capacity, it would be cost-inefficient to maintain the same instance size. By choosing option C, the company can downscale to a more appropriate instance size that matches the current demand while still providing memory-optimized performance. The Auto Scaling group can then adjust the number of instances between the minimum of 3 and the maximum of 12 based on actual CPU and memory consumption, which aligns with the company's need for a cost-effective and responsive scaling strategy.The original workload ran on memory-optimized instances with low utilization, so downsizing to `r6g.4xlarge` is appropriate. Auto Scaling with a minimum of 3 instances ensures baseline capacity, while scaling up to 12 handles spikes cost-effectively.</p><p>Option A: `c6g.4xlarge` (Compute Optimized)</p><p> &nbsp; - Problem: The workload was originally on memory-optimized instances, and memory utilization was low but still a factor. Switching to compute-optimized instances may not be appropriate if memory is still a consideration. &nbsp;</p><p> &nbsp; - Not the best choice. &nbsp;</p><p> Option B: `m6g.4xlarge` (General Purpose)</p><p> &nbsp; - Problem: While general-purpose instances balance CPU and memory, the workload was initially on memory-optimized instances. If memory is a key factor, general-purpose may not be optimal. &nbsp;</p><p> &nbsp; - Not the best choice. &nbsp;</p><p> Option C: `r6g.4xlarge` (Memory Optimized)</p><p> &nbsp; - Pros: &nbsp;</p><p> &nbsp; &nbsp; - Memory-optimized, matching the original instance family. &nbsp;</p><p> &nbsp; &nbsp; - Smaller size (`4xlarge` vs `16xlarge`) reduces cost while still providing sufficient memory. &nbsp;</p><p> &nbsp; &nbsp; - Scaling (3–12 instances) allows flexibility for variable demand. &nbsp;</p><p> &nbsp; - Best choice if memory is still a key requirement. &nbsp;</p><p> Option D: `r6g.8xlarge` (Memory Optimized)</p><p> &nbsp; - Pros: &nbsp;</p><p> &nbsp; &nbsp; - Still memory-optimized. &nbsp;</p><p> &nbsp; - Cons: &nbsp;</p><p> &nbsp; &nbsp; - Larger instance size (`8xlarge`) may still be over-provisioned given the previous low utilization. &nbsp;</p><p> &nbsp; &nbsp; - Fewer instances (2–6) means less granular scaling, potentially leading to less cost efficiency. &nbsp;</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "bf00e8f3c1044a8bbe566e457c813a0b",
      "questionNumber": 254,
      "type": "single",
      "content": "<p>Question #254</p><p>A financial services company loaded millions of historical stock trades into an Amazon DynamoDB table. The table uses on-demand capacity mode. Once each day at midnight, a few million new records are loaded into the table. Application read activity against the table happens in bursts throughout the day, and a limited set of keys are repeatedly looked up. The company needs to reduce costs associated with DynamoDB.</p><p><br></p><p>Which strategy should a solutions architect recommend to meet this requirement?</p>",
      "options": [
        {
          "label": "A",
          "content": "Deploy an Amazon ElastiCache cluster in front of the DynamoDB table"
        },
        {
          "label": "B",
          "content": "Deploy DynamoDB Accelerator (DAX). Configure DynamoDB auto scaling. Purchase Savings Plans in Cost Explorer."
        },
        {
          "label": "C",
          "content": "Use provisioned capacity mode. Purchase Savings Plans in Cost Explorer."
        },
        {
          "label": "D",
          "content": "Deploy DynamoDB Accelerator (DAX). Use provisioned capacity mode. Configure DynamoDB auto scaling."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. Deploy DynamoDB Accelerator (DAX). Use provisioned capacity mode. Configure DynamoDB auto scaling.</p><p>This strategy will best reduce costs associated with DynamoDB for the financial services company's specific usage pattern for the following reasons:</p><p>DynamoDB Accelerator (DAX):</p><p>The company's workload has \"bursts throughout the day, and a limited set of keys are repeatedly looked up.\" This is an ideal use case for DAX, which provides in-memory caching specifically designed for DynamoDB.</p><p>DAX can significantly reduce the number of read requests that hit the DynamoDB table by caching frequently accessed items, which directly reduces costs since DynamoDB charges per request.</p><p>For the \"limited set of keys\" that are \"repeatedly looked up,\" DAX will provide microsecond response times while dramatically reducing the read capacity required from the main DynamoDB table.</p><p>Provisioned Capacity Mode:</p><p>The company's usage pattern is relatively predictable: millions of historical records with a few million new records loaded once daily at midnight, and bursty but predictable read patterns.</p><p>Provisioned capacity mode is more cost-effective than on-demand capacity mode for predictable workloads, as it allows you to pay for reserved capacity at a lower rate than on-demand pricing.</p><p>Since the company knows approximately when and how much data will be loaded (at midnight each day), they can provision appropriate write capacity for this predictable spike.</p><p>DynamoDB Auto Scaling:</p><p>Auto scaling allows the provisioned capacity to adjust automatically based on actual usage, which addresses the \"bursts throughout the day\" in read activity.</p><p>This prevents over-provisioning during low-usage periods while ensuring sufficient capacity during peak times.</p><p>Auto scaling helps optimize costs by matching provisioned capacity to actual demand patterns.</p><p>The other options are less optimal:</p><p>A. ElastiCache alone would add complexity and management overhead without the DynamoDB-specific optimizations that DAX provides.</p><p>B. Combining DAX and auto scaling with on-demand capacity mode would be more expensive than using provisioned capacity for this predictable workload. Savings Plans wouldn't offset this inefficiency.</p><p>C. Provisioned capacity mode with Savings Plans but without DAX wouldn't address the repeated lookups of a limited set of keys, missing a significant cost optimization opportunity.</p><p>Option D provides the most comprehensive cost optimization strategy by addressing all aspects of the company's usage pattern: caching for repeated reads, provisioned capacity for predictable workloads, and auto scaling for handling bursts efficiently.</p><p>Sources</p><p>Optimizing costs on DynamoDB tables - Amazon DynamoDB (https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-cost-optimization.html)</p><p>In-memory acceleration with DynamoDB Accelerator (DAX) - Amazon DynamoDB （https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.html）</p><p>Delhivery Case Study | AWS Cloud Financial Management （https://aws.amazon.com/cn/aws-cost-management/）</p><p>Use caching to reduce database demand - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/optimize-costs-microsoft-workloads/net-caching.html）</p><p>Is there a free tier for Dynamo DB On-Demand | AWS re:Post （https://repost.aws/questions/QULYTitlzWR3yD1fZ7OAjrtQ/is-there-a-free-tier-for-dynamo-db-on-demand）</p><p>Reduce latency and cost in read-heavy applications using Amazon DynamoDB Accelerator | AWS Database Blog （https://aws.amazon.com/cn/blogs/database/reduce-latency-and-cost-in-read-heavy-applications-using-amazon-dynamodb-accelerator/）</p><p>How Samsung Cloud optimized Amazon DynamoDB costs | AWS Database Blog （https://aws.amazon.com/cn/blogs/database/how-samsung-cloud-optimized-amazon-dynamodb-costs/）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "6c7795f24ce74287ba857e08f7ece53f",
      "questionNumber": 255,
      "type": "multiple",
      "content": "<p>Question #255</p><p>A company is creating a centralized logging service running on Amazon EC2 that will receive and analyze logs from hundreds of AWS accounts. AWS PrivateLink is being used to provide connectivity between the client services and the logging service. In each AWS account with a client, an interface endpoint has been created for the logging service and is available. The logging service running on EC2 instances with a Network Load Balancer (NLB) are deployed in different subnets. The clients are unable to submit logs using the VPC endpoint.</p><p>Which combination of steps should a solutions architect take to resolve this issue? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Check that the NACL is attached to the logging service subnet to allow communications to and from the NLB subnets. Check that the NACL is attached to the NLB subnet to allow communications to and from the logging service subnets running on EC2 instances."
        },
        {
          "label": "B",
          "content": "Check that the NACL is attached to the logging service subnets to allow communications to and from the interface endpoint subnets. Check that the NACL is attached to the interface endpoint subnet to allow communications to and from the logging service subnets running on EC2 instances."
        },
        {
          "label": "C",
          "content": "Check the security group for the logging service running on the EC2 instances to ensure it allows ingress from the NLB subnets."
        },
        {
          "label": "D",
          "content": "Check the security group for the logging service running on the EC2 instances to ensure it allows ingress from the clients."
        },
        {
          "label": "E",
          "content": "Check the security group for the NLB to ensure it allows ingress from the interface endpoint subnets."
        }
      ],
      "correctAnswer": "AC",
      "explanation": "<p>The correct answer is A and C. </p><p>To resolve the issue of clients being unable to submit logs using the VPC endpoint, a solutions architect should take the following steps:</p><p><br></p><p>Ensure that the Network Load Balancer (NLB) is configured correctly:</p><p><br></p><p>The NLB should be in the same VPC as the EC2 instances running the logging service.</p><p>Verify that the NLB's target group includes the EC2 instances running the logging service.</p><p>Check that the health checks for the target group are passing.</p><p>Configure the VPC endpoint service to use the Network Load Balancer:</p><p><br></p><p>In the VPC console, go to \"Endpoint Services\" and create a new endpoint service.</p><p>Select the Network Load Balancer as the network load balancer for this endpoint service.</p><p>Make sure to add the AWS account IDs of the client accounts to the \"Allowed principals\" list.</p><p>These two steps are crucial because:</p><p><br></p><p>The NLB configuration ensures that the logging service is properly load-balanced and accessible.</p><p>Creating and configuring the VPC endpoint service to use the NLB establishes the necessary link between the PrivateLink interface endpoints in the client accounts and the logging service in the central account.</p><p>Additional considerations:</p><p><br></p><p>Ensure that the security groups associated with the EC2 instances allow incoming traffic from the NLB.</p><p>Verify that the NLB listener is configured to forward traffic to the correct port on the EC2 instances.</p><p>Check that the VPC endpoint service is in an \"Available\" state.</p><p>In the client accounts, ensure that the VPC endpoint is associated with the correct subnets and security groups.</p><p>By implementing these steps, you should resolve the connectivity issue and allow clients to submit logs using the VPC endpoint through AWS PrivateLink.</p><p>Why Not Other Options?</p><p>- Option B: Incorrect because NACLs should focus on NLB ↔ EC2 communication, not the interface endpoint (PrivateLink handles this).</p><p>- Option D: Incorrect because clients connect via the interface endpoint, not directly to EC2 instances. The NLB acts as the intermediary.</p><p>- Option E: Incorrect because the NLB’s security group should allow traffic from clients (via the interface endpoint), not just the endpoint subnets. However, NLBs don’t use security groups (they rely on NACLs).</p><p><br></p><p>https://repost.aws/knowledge-center/security-network-acl-vpc-endpoint </p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "f3849f116ccb43758b0c67ff24394302",
      "questionNumber": 256,
      "type": "single",
      "content": "<p>Question #256</p><p>A company has millions of objects in an Amazon S3 bucket. The objects are in the S3 Standard storage class. All the S3 objects are accessed frequently. The number of users and applications that access the objects is increasing rapidly. The objects are encrypted with server-side encryption with AWS KMS keys (SSE-KMS).</p><p><br></p><p>A solutions architect reviews the company’s monthly AWS invoice and notices that AWS KMS costs are increasing because of the high number of requests from Amazon S3. The solutions architect needs to optimize costs with minimal changes to the application.</p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a new S3 bucket that has server-side encryption with customer-provided keys (SSE-C) as the encryption type. Copy the existing objects to the new S3 bucket. Specify SSE-C."
        },
        {
          "label": "B",
          "content": "Create a new S3 bucket that has server-side encryption with Amazon S3 managed keys (SSE-S3) as the encryption type. Use S3 Batch Operations to copy the existing objects to the new S3 bucket. Specify SSE-S3."
        },
        {
          "label": "C",
          "content": "Use AWS CloudHSM to store the encryption keys. Create a new S3 bucket. Use S3 Batch Operations to copy the existing objects to the new S3 bucket. Encrypt the objects by using the keys from CloudHSM."
        },
        {
          "label": "D",
          "content": "Use the S3 Intelligent-Tiering storage class for the S3 bucket. Create an S3 Intelligent-Tiering archive configuration to transition objects that are not accessed for 90 days to S3 Glacier Deep Archive."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>The correct answer is B. To optimize costs with minimal operational overhead, the solutions architect can switch the encryption method from SSE-KMS to SSE-S3. This change reduces costs because SSE-S3 does not incur the per-request charges that AWS KMS does. Using S3 Batch Operations to copy the existing objects to a new bucket with SSE-S3 encryption requires minimal changes to the application and can be done without significant operational overhead.</p><p>The primary issue is the high AWS KMS costs due to frequent encryption/decryption requests for objects in the S3 bucket. The requirement is to reduce costs with minimal changes to the application. &nbsp;</p><p>- Option A (SSE-C) requires managing customer-provided keys, which introduces operational overhead (key management, rotation, etc.). This is not minimal effort. &nbsp;</p><p>- Option B (SSE-S3) replaces KMS encryption with S3-managed keys, which eliminates KMS request costs while maintaining server-side encryption. S3 Batch Operations automates the migration, minimizing manual effort. &nbsp;</p><p>- Option C (CloudHSM) is overly complex and expensive for this use case, requiring key management in a dedicated HSM. &nbsp;</p><p>- Option D (Intelligent-Tiering) does not address the KMS cost issue—it only optimizes storage costs. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "152ef4da609e4d1f9b2b41518f92ad90",
      "questionNumber": 257,
      "type": "multiple",
      "content": "<p>Question #257</p><p>A media storage application uploads user photos to Amazon S3 for processing by AWS Lambda functions. Application state is stored in Amazon DynamoDB tables. Users are reporting that some uploaded photos are not being processed properly. The application developers trace the logs and find that Lambda is experiencing photo processing issues when thousands of users upload photos simultaneously. The issues are the result of Lambda concurrency limits and the performance of DynamoDB when data is saved.</p><p>Which combination of actions should a solutions architect take to increase the performance and reliability of the application? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Evaluate and adjust the RCUs for the DynamoDB tables."
        },
        {
          "label": "B",
          "content": "Evaluate and adjust the WCUs for the DynamoDB tables."
        },
        {
          "label": "C",
          "content": "Add an Amazon ElastiCache layer to increase the performance of Lambda functions."
        },
        {
          "label": "D",
          "content": "Add an Amazon Simple Queue Service (Amazon SQS) queue and reprocessing logic between Amazon S3 and the Lambda functions."
        },
        {
          "label": "E",
          "content": "<p>Use S3 Transfer Acceleration to provide lower latency to users.</p>"
        }
      ],
      "correctAnswer": "BD",
      "explanation": "<p>Based on the information provided, the correct answers are:</p><p>B. Evaluate and adjust the WCUs for the DynamoDB tables. </p><p>D. Add an Amazon Simple Queue Service (Amazon SQS) queue and reprocessing logic between Amazon S3 and the Lambda functions.</p><p>These two actions together will address the performance and reliability issues in the application for the following reasons:</p><p>Evaluate and adjust the WCUs for the DynamoDB tables:</p><p>The scenario specifically mentions that there are \"performance issues with DynamoDB when data is saved,\" which directly points to write capacity limitations.</p><p>When thousands of users upload photos simultaneously, the Lambda functions are likely trying to write processing results and application state to DynamoDB at the same time, potentially exceeding the provisioned write capacity.</p><p>Increasing the Write Capacity Units (WCUs) will allow DynamoDB to handle more concurrent write operations, reducing throttling and improving the reliability of data storage during high-volume periods.</p><p>This addresses the DynamoDB performance bottleneck mentioned in the scenario.</p><p>Add an Amazon SQS queue and reprocessing logic between Amazon S3 and the Lambda functions:</p><p>The scenario mentions issues with \"Lambda concurrency limits\" when thousands of users upload photos simultaneously.</p><p>Adding an SQS queue decouples the S3 upload events from the Lambda processing, allowing the system to buffer incoming requests during traffic spikes.</p><p>Instead of trying to process all uploads immediately (which hits concurrency limits), Lambda functions can process items from the queue at a controlled rate.</p><p>This pattern provides built-in retry capabilities for failed processing attempts, improving reliability.</p><p>The queue acts as a buffer that smooths out the processing workload, preventing the Lambda concurrency limits from being reached during upload spikes.</p><p>The other options would not effectively address the specific issues mentioned:</p><p>A. Evaluate and adjust the RCUs for the DynamoDB tables - The problem is specifically with saving data (writes), not reading data, so adjusting Read Capacity Units would not address the issue.</p><p>C. Add an Amazon ElastiCache layer - While this might improve some aspects of application performance, it wouldn't address the core issues of Lambda concurrency limits during upload spikes or DynamoDB write performance.</p><p>The combination of adjusting DynamoDB WCUs and implementing an SQS queue provides a comprehensive solution that addresses both the Lambda concurrency limitations and the DynamoDB write performance issues identified in the scenario.</p><p>Sources</p><p>DynamoDB burst and adaptive capacity - Amazon DynamoDB （https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/burst-adaptive-capacity.html）</p><p>Distributing write activity efficiently during data upload in DynamoDB - Amazon DynamoDB （https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-data-upload.html）</p><p>Performance guidelines for Amazon S3 - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance-guidelines.html）</p><p>Understanding serverless data processing - Serverless （https://docs.aws.amazon.com/serverless/latest/devguide/serverless-usecases.html）</p><p><br></p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "b6d35526f8bb4199b2448a8beed7f8d9",
      "questionNumber": 258,
      "type": "single",
      "content": "<p>Question #258</p><p>A company runs an application in an on-premises data center. The application gives users the ability to upload media files. The files persist in a file server. The web application has many users. The application server is overutilized, which causes data uploads to fail occasionally. The company frequently adds new storage to the file server. The company wants to resolve these challenges by migrating the application to AWS.</p><p><br></p><p>Users from across the United States and Canada access the application. Only authenticated users should have the ability to access the application to upload files. The company will consider a solution that refactors the application, and the company needs to accelerate application development.</p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use AWS Application Migration Service to migrate the application server to Amazon EC2 instances. Create an Auto Scaling group for the EC2 instances. Use an Application Load Balancer to distribute the requests. Modify the application to use Amazon S3 to persist the files. Use Amazon Cognito to authenticate users."
        },
        {
          "label": "B",
          "content": "Use AWS Application Migration Service to migrate the application server to Amazon EC2 instances. Create an Auto Scaling group for the EC2 instances. Use an Application Load Balancer to distribute the requests. Set up AWS IAM Identity Center (AWS Single Sign-On) to give users the ability to sign in to the application. Modify the application to use Amazon S3 to persist the files."
        },
        {
          "label": "C",
          "content": "Create a static website for uploads of media files. Store the static assets in Amazon S3. Use AWS AppSync to create an API. Use AWS Lambda resolvers to upload the media files to Amazon S3. Use Amazon Cognito to authenticate users."
        },
        {
          "label": "D",
          "content": "Use AWS Amplify to create a static website for uploads of media files. Use Amplify Hosting to serve the website through Amazon CloudFront. Use Amazon S3 to store the uploaded media files. Use Amazon Cognito to authenticate users."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>The correct answer is D. AWS Amplify provides a solution that can create a static website for media file uploads with the least operational overhead. It simplifies the process of building and deploying web applications and integrates with Amazon CloudFront for fast content delivery. By using Amazon S3 for file storage and Amazon Cognito for user authentication, the company can meet its requirements without the need for managing additional infrastructure.</p><p>1. Least Operational Overhead &nbsp;</p><p> &nbsp; - AWS Amplify is a fully managed service that simplifies building, deploying, and hosting web applications. It handles scaling, CI/CD, and infrastructure management automatically.</p><p> &nbsp; - Amazon Cognito provides seamless user authentication without requiring manual setup.</p><p> &nbsp; - CloudFront ensures low-latency access for users across the US and Canada.</p><p>2. Scalability & Performance &nbsp;</p><p> &nbsp; - Amplify + CloudFront automatically scales to handle traffic spikes, solving the overutilization issue.</p><p> &nbsp; - S3 is ideal for storing media files with high durability and scalability.</p><p>3. Security & Authentication &nbsp;</p><p> &nbsp; - Amazon Cognito ensures only authenticated users can access the application.</p><p>4. Accelerated Development &nbsp;</p><p> &nbsp; - Amplify streamlines frontend and backend development, reducing time-to-market.</p><p> Why Not the Other Options?</p><p>- A & B require refactoring to use EC2 + Auto Scaling + ALB, which introduces more operational complexity than a serverless approach.</p><p>- C involves AppSync (GraphQL) and Lambda, which is more complex than needed for a simple file upload application.</p><p>- D is the most streamlined, fully managed solution.</p><p> Conclusion: &nbsp;</p><p>Option D is the best choice because it minimizes operational overhead, accelerates development, and meets all requirements effectively.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "1d8a29bb8211493ea39934e451a17a94",
      "questionNumber": 259,
      "type": "single",
      "content": "<p>Question #259</p><p>A company has an application that is deployed on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances are part of an Auto Scaling group. The application has unpredictable workloads and frequently scales out and in. The company’s development team wants to analyze application logs to find ways to improve the application's performance. However, the logs are no longer available after instances scale in.</p><p><br></p><p>Which solution will give the development team the ability to view the application logs after a scale-in event?</p>",
      "options": [
        {
          "label": "A",
          "content": "Enable access logs for the ALB. Store the logs in an Amazon S3 bucket."
        },
        {
          "label": "B",
          "content": "Configure the EC2 instances to publish logs to Amazon CloudWatch Logs by using the unified CloudWatch agent."
        },
        {
          "label": "C",
          "content": "Modify the Auto Scaling group to use a step scaling policy."
        },
        {
          "label": "D",
          "content": "Instrument the application with AWS X-Ray tracing."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Configure the EC2 instances to publish logs to Amazon CloudWatch Logs by using the unified CloudWatch agent.</p><p>This solution will give the development team the ability to view application logs after a scale-in event for the following reasons:</p><p>Persistent Log Storage: When EC2 instances publish logs to CloudWatch Logs using the CloudWatch agent, the logs are stored in a centralized location that persists independently of the EC2 instances. This means that even after instances are terminated during scale-in events, the logs remain accessible.</p><p>Real-time Log Streaming: The CloudWatch agent streams logs to CloudWatch Logs in near real-time, ensuring that logs are captured before instances are terminated during scale-in events.</p><p>Application-specific Logs: Unlike ALB access logs which only capture HTTP request information, the CloudWatch agent can be configured to collect the actual application logs that contain detailed information about the application's behavior and performance, which is what the development team needs to improve the application.</p><p>Analysis Capabilities: CloudWatch Logs provides powerful querying and analysis tools through CloudWatch Logs Insights, allowing the development team to effectively analyze the logs to find ways to improve application performance.</p><p>The other options would not effectively solve the problem:</p><p>A. Enable access logs for the ALB - While this would preserve logs about HTTP requests to the ALB, it wouldn't capture the application-specific logs needed to analyze and improve application performance. ALB access logs only contain information about external requests, not internal application behavior.</p><p>C. Modify the Auto Scaling group to use a step scaling policy - This would only change how the Auto Scaling group scales, not how logs are preserved. The logs would still be lost when instances are terminated.</p><p>D. Instrument the application with AWS X-Ray tracing - While X-Ray provides valuable insights into application performance through distributed tracing, it's not primarily a log storage solution. It captures traces of requests through the application but doesn't preserve the detailed application logs needed for comprehensive analysis.</p><p>Configuring the CloudWatch agent to publish logs to CloudWatch Logs is the most appropriate solution as it directly addresses the need to preserve application logs after instances are terminated during scale-in events.</p><p>Sources</p><p>SEC04-BP01 Configure service and application logging - AWS Well-Architected Framework （https://docs.aws.amazon.com/wellarchitected/2025-02-25/framework/sec_detect_investigate_events_app_service_logging.html）</p><p>SEC04-BP01 Configure service and application logging - AWS Well-Architected Framework (2022-03-31)（https://docs.aws.amazon.com/wellarchitected/2022-03-31/framework/sec_detect_investigate_events_app_service_logging.html） </p><p>SEC04-BP01 Configure service and application logging - AWS Well-Architected Framework (2023-04-10) （https://docs.aws.amazon.com/wellarchitected/2023-04-10/framework/sec_detect_investigate_events_app_service_logging.html）</p><p>Viewing logs from Amazon EC2 instances in your Elastic Beanstalk environment - AWS Elastic Beanstalk（https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.logging.html） </p><p>EC2 Auto-Scaling Instances Retaining Old Data After Termination | AWS re:Post （https://repost.aws/questions/QUJetklujSTha-jtw3BPWtiw/ec2-auto-scaling-instances-retaining-old-data-after-termination）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "3320f2ef146e49b7a76568edf6383ead",
      "questionNumber": 260,
      "type": "single",
      "content": "<p>Question #260</p><p>A company runs an unauthenticated static website (www.example.com) that includes a registration form for users. The website uses Amazon S3 for hosting and uses Amazon CloudFront as the content delivery network with AWS WAF configured. When the registration form is submitted, the website calls an Amazon API Gateway API endpoint that invokes an AWS Lambda function to process the payload and forward the payload to an external API call.</p><p><br></p><p>During testing, a solutions architect encounters a cross-origin resource sharing (CORS) error. The solutions architect confirms that the CloudFront distribution origin has the Access-Control-Allow-Origin header set to www.example.com.</p><p><br></p><p>What should the solutions architect do to resolve the error?</p>",
      "options": [
        {
          "label": "A",
          "content": "Change the CORS configuration on the S3 bucket. Add rules for CORS to the AllowedOrigin element for www.example.com."
        },
        {
          "label": "B",
          "content": "Enable the CORS setting in AWS WAF. Create a web ACL rule in which the Access-Control-Allow-Origin header is set to www.example.com."
        },
        {
          "label": "C",
          "content": "Enable the CORS setting on the API Gateway API endpoint. Ensure that the API endpoint is configured to return all responses that have the Access-Control-Allow-Origin header set to www.example.com."
        },
        {
          "label": "D",
          "content": "Enable the CORS setting on the Lambda function. Ensure that the return code of the function has the Access-Control-Allow-Origin header set to www.example.com."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>The correct answer is C. The CORS error is likely due to the API Gateway endpoint not being configured to allow the origin www.example.com. By enabling the CORS setting on the API Gateway, the architect can specify that the Access-Control-Allow-Origin header should be set to www.example.com for all responses. This will allow the JavaScript code running on the static website to make successful requests to the API Gateway endpoint.The correct answer is C. Enable the CORS setting on the API Gateway API endpoint. Ensure that the API endpoint is configured to return all responses that have the Access-Control-Allow-Origin header set to www.example.com.</p><p> Explanation:</p><p>The CORS error occurs when a web application (hosted at `www.example.com`) makes a cross-origin request (to API Gateway) but the API endpoint does not explicitly allow requests from `www.example.com` by including the correct CORS headers (`Access-Control-Allow-Origin`) in its response.</p><p>Here’s why the other options are incorrect:</p><p>- A: The S3 bucket's CORS configuration is irrelevant here because the error occurs when calling the API Gateway endpoint, not when accessing static content from S3.</p><p>- B: AWS WAF does not handle CORS headers. CORS is enforced at the application layer (API Gateway in this case), not the WAF layer.</p><p>- D: While the Lambda function could manually set CORS headers, the proper way to handle CORS for API Gateway is to enable CORS at the API Gateway level, which automatically adds the required headers to responses.</p><p> Correct Approach:</p><p>1. Enable CORS in API Gateway:</p><p> &nbsp; - Go to the API Gateway console, select the API, and enable CORS for the relevant endpoint.</p><p> &nbsp; - Specify `www.example.com` as an allowed origin.</p><p> &nbsp; - This ensures API Gateway includes the `Access-Control-Allow-Origin: www.example.com` header in responses.</p><p>2. Redeploy the API:</p><p> &nbsp; - After enabling CORS, redeploy the API to apply the changes.</p><p>This will resolve the CORS error when the static website (`www.example.com`) makes requests to the API Gateway endpoint.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "6f0d7f20ce18485eb74b296284a8888b",
      "questionNumber": 261,
      "type": "multiple",
      "content": "<p>Question #261</p><p>A company has many separate AWS accounts and uses no central billing or management. Each AWS account hosts services for different departments in the company. The company has a Microsoft Azure Active Directory that is deployed.</p><p>A solutions architect needs to centralize billing and management of the company’s AWS accounts. The company wants to start using identity federation instead of manual user management. The company also wants to use temporary credentials instead of long-lived access keys.</p><p>Which combination of steps will meet these requirements? (Choose three.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a new AWS account to serve as a management account. Deploy an organization in AWS Organizations. Invite each existing AWS account to join the organization. Ensure that each account accepts the invitation."
        },
        {
          "label": "B",
          "content": "Configure each AWS account&#39;s email address to be aws+@example.com so that account management email messages and invoices are sent to the same place."
        },
        {
          "label": "C",
          "content": "Deploy AWS IAM Identity Center (AWS Single Sign-On) in the management account. Connect IAM Identity Center to the Azure Active Directory. Configure IAM Identity Center for automatic synchronization of users and groups."
        },
        {
          "label": "D",
          "content": "Deploy an AWS Managed Microsoft AD directory in the management account. Share the directory with all other accounts in the organization by using AWS Resource Access Manager (AWS RAM)."
        },
        {
          "label": "E",
          "content": "Create AWS IAM Identity Center (AWS Single Sign-On) permission sets. Attach the permission sets to the appropriate IAM Identity Center groups and AWS accounts."
        },
        {
          "label": "F",
          "content": "Configure AWS Identity and Access Management (IAM) in each AWS account to use AWS Managed Microsoft AD for authentication and authorization."
        }
      ],
      "correctAnswer": "ACE",
      "explanation": "<p>The correct answers are A, C, and E. To centralize billing and management, a new AWS account can be created as a management account and an organization in AWS Organizations can be deployed (A). Identity federation can be achieved by deploying AWS IAM Identity Center (AWS Single Sign-On) and connecting it to the existing Azure Active Directory, allowing for the use of temporary credentials (C). Permission sets in AWS Single Sign-On can be created and attached to the appropriate groups and accounts to manage access (E). These steps will allow for centralized control over billing and user management while leveraging the company's existing Azure Active Directory for authentication.</p><p>1. A. Create a new AWS account to serve as a management account. Deploy an organization in AWS Organizations. Invite each existing AWS account to join the organization. Ensure that each account accepts the invitation. &nbsp;</p><p> &nbsp; - This centralizes billing and management by setting up AWS Organizations, which allows consolidated billing and account management under a single \"management account.\"</p><p>2. C. Deploy AWS IAM Identity Center (AWS Single Sign-On) in the management account. Connect IAM Identity Center to the Azure Active Directory. Configure IAM Identity Center for automatic synchronization of users and groups. &nbsp;</p><p> &nbsp; - This enables identity federation by integrating AWS IAM Identity Center (successor to AWS SSO) with the existing Azure Active Directory, allowing users to log in using their corporate credentials. Automatic synchronization ensures users and groups stay up to date.</p><p>3. E. Create AWS IAM Identity Center (AWS Single Sign-On) permission sets. Attach the permission sets to the appropriate IAM Identity Center groups and AWS accounts. &nbsp;</p><p> &nbsp; - Permission sets define access levels (like IAM policies) and can be assigned to users/groups from Azure AD, enabling temporary credentials (via AWS SSO) instead of long-lived access keys.</p><p> Why not the others?</p><p>- B. Changing email addresses for invoices is not necessary for centralizing billing (AWS Organizations already consolidates billing under the management account). This is not a required step.</p><p>- D. Deploying AWS Managed Microsoft AD is unnecessary because the company already has Azure AD, which can be federated directly via IAM Identity Center.</p><p>- F. Configuring IAM in each account to use AWS Managed Microsoft AD is redundant since IAM Identity Center already handles federation with Azure AD.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "636e5e3dc4914b28af1abb8ce5b121a6",
      "questionNumber": 262,
      "type": "single",
      "content": "<p>Question #262</p><p>A company wants to manage the costs associated with a group of 20 applications that are infrequently used, but are still business-critical, by migrating to AWS. The applications are a mix of Java and Node.js spread across different instance clusters. The company wants to minimize costs while standardizing by using a single deployment methodology. </p><p><br></p><p>Most of the applications are part of month-end processing routines with a small number of concurrent users, but they are occasionally run at other times. Average application memory consumption is less than 1 GB, though some applications use as much as 2.5 GB of memory during peak processing. The most important application in the group is a billing report written in Java that accesses multiple data sources and often runs for several hours.</p><p><br></p><p>Which is the MOST cost-effective solution?</p>",
      "options": [
        {
          "label": "A",
          "content": "Deploy a separate AWS Lambda function for each application. Use AWS CloudTrail logs and Amazon CloudWatch alarms to verify completion of critical jobs."
        },
        {
          "label": "B",
          "content": "Deploy Amazon ECS containers on Amazon EC2 with Auto Scaling configured for memory utilization of 75%. Deploy an ECS task for each application being migrated with ECS task scaling. Monitor services and hosts by using Amazon CloudWatch."
        },
        {
          "label": "C",
          "content": "Deploy AWS Elastic Beanstalk for each application with Auto Scaling to ensure that all requests have sufficient resources. Monitor each AWS Elastic Beanstalk deployment by using CloudWatch alarms."
        },
        {
          "label": "D",
          "content": "Deploy a new Amazon EC2 instance cluster that co-hosts all applications by using EC2 Auto Scaling and Application Load Balancers. Scale cluster size based on a custom metric set on instance memory utilization. Purchase 3-year Reserved Instance reservations equal to the GroupMaxSize parameter of the Auto Scaling group."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>The correct answer is B. Deploying Amazon ECS containers on Amazon EC2 with Auto Scaling configured for memory utilization is a cost-effective solution because it allows the applications to share resources and only use what they need. This approach is more cost-efficient than deploying separate Lambda functions (Option A), which might be overprovisioned for infrequent use, or using Elastic Beanstalk (Option C), which could also lead to overprovisioning. Option D, deploying a new EC2 instance cluster, would likely be more expensive due to the need for potentially larger, reserved instances.</p><p>The company needs to manage 20 infrequently used but business-critical applications with varying memory requirements (up to 2.5 GB). The key requirements are: &nbsp;</p><p>- Cost efficiency (since applications are used sporadically). &nbsp;</p><p>- Standardized deployment methodology (all applications should follow the same approach). &nbsp;</p><p>- Handling long-running processes (e.g., the Java billing report that runs for hours). &nbsp;</p><p>- Scaling based on demand (month-end processing with occasional spikes). &nbsp;</p><p> Why Option B is the Best Choice? &nbsp;</p><p>- Amazon ECS on EC2 with Auto Scaling allows running containers efficiently, scaling based on memory utilization (75% threshold ensures optimal resource usage). &nbsp;</p><p>- Task-level scaling ensures that only necessary resources are allocated per application. &nbsp;</p><p>- Cost-effective since EC2 instances can be stopped or scaled down when not in use, unlike always-on solutions like Option D. &nbsp;</p><p>- CloudWatch monitoring provides visibility into application performance and resource usage. &nbsp;</p><p> Why Other Options Are Less Suitable? &nbsp;</p><p>- A (Lambda): Lambda has a 15-minute execution limit, making it unsuitable for long-running jobs like the billing report. &nbsp;</p><p>- C (Elastic Beanstalk): Overkill for infrequently used apps; incurs higher baseline costs due to always-on resources. &nbsp;</p><p>- D (EC2 Reserved Instances): Not cost-effective for sporadic workloads; Reserved Instances require long-term commitment for underutilized apps. &nbsp;</p><p>Thus, Option B (Amazon ECS on EC2 with Auto Scaling) provides the best balance of cost efficiency, scalability, and standardization. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "c9d9c29054f9450a974fcd8e10a5bbae",
      "questionNumber": 263,
      "type": "single",
      "content": "<p>Question #263</p><p>A solutions architect needs to review the design of an Amazon EMR cluster that is using the EMR File System (EMRFS). The cluster performs tasks that are critical to business needs. The cluster is running Amazon EC2 On-Demand Instances at all times for all task, primary, and core nodes. The EMR tasks run each morning, starting at 1:00 AM, and take 6 hours to finish running. The amount of time to complete the processing is not a priority because the data is not referenced until late in the day.</p><p><br></p><p>The solutions architect must review the architecture and suggest a solution to minimize the compute costs.</p><p><br></p><p>Which solution should the solutions architect recommend to meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Launch all task, primary, and core nodes on Spot Instances in an instance fleet. Terminate the cluster, including all instances, when the processing is completed."
        },
        {
          "label": "B",
          "content": "Launch the primary and core nodes on On-Demand Instances. Launch the task nodes on Spot Instances in an instance fleet. Terminate the cluster, including all instances, when the processing is completed. Purchase Compute Savings Plans to cover the On-Demand Instance usage."
        },
        {
          "label": "C",
          "content": "Continue to launch all nodes on On-Demand Instances. Terminate the cluster, including all instances, when the processing is completed. Purchase Compute Savings Plans to cover the On-Demand Instance usage."
        },
        {
          "label": "D",
          "content": "Launch the primary and core nodes on On-Demand Instances. Launch the task nodes on Spot Instances in an instance fleet. Terminate only the task node instances when the processing is completed. Purchase Compute Savings Plans to cover the On-Demand Instance usage."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p style=\"text-align: start;\">Option D: </p><p style=\"text-align: start;\"><span style=\"color: rgb(31, 35, 41); background-color: rgba(0, 0, 0, 0); font-size: 16px;\"><strong>Node-type adaptation</strong></span>:Primary and core nodes are foundational components of the EMR cluster (responsible for management and storage scheduling), so they require On-Demand Instances to ensure stability (critical for business-critical tasks).</p><li style=\"text-align: start;\">Task nodes only handle temporary processing tasks; using Spot Instances (costing 10%-70% of On-Demand Instances) paired with an instance fleet (which automatically replaces terminated Spot Instances) balances cost savings and availability.</li><p style=\"text-align: start;\"><span style=\"color: rgb(31, 35, 41); background-color: rgba(0, 0, 0, 0); font-size: 16px;\"><strong>Resource lifecycle management</strong></span>:Terminate only task nodes after processing completes (retain primary and core nodes) — this reduces idle costs for task nodes while keeping the cluster available (avoids the time/operational costs of recreating the cluster, which aligns with the needs of business-critical workloads).</p><p style=\"text-align: start;\"><span style=\"color: rgb(31, 35, 41); background-color: rgba(0, 0, 0, 0); font-size: 16px;\"><strong>Additional cost optimization</strong></span>:Purchase Compute Savings Plans to cover On-Demand Instance usage (for primary and core nodes), further reducing long-term costs of On-Demand Instances.</p><p style=\"text-align: start;\">Option A: Using Spot Instances for all nodes risks cluster failure (since primary/core nodes — critical components — may be terminated by Spot 回收), which does not meet the needs of business-critical tasks.</p><p style=\"text-align: start;\">Option B: Terminating the entire cluster deletes it, requiring recreation for subsequent tasks (high operational costs and inability to ensure on-demand availability for critical business).</p><p style=\"text-align: start;\">Option C: Continuing to use On-Demand Instances for all nodes fails to leverage cost-saving Spot Instances, resulting in excessively high overall costs.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "eb59dafc587440968c41a56ded0673dc",
      "questionNumber": 264,
      "type": "single",
      "content": "<p>Question #264</p><p>A company has migrated a legacy application to the AWS Cloud. The application runs on three Amazon EC2 instances that are spread across three Availability Zones. One EC2 instance is in each Availability Zone. The EC2 instances are running in three private subnets of the VPC and are set up as targets for an Application Load Balancer (ALB) that is associated with three public subnets.</p><p><br></p><p>The application needs to communicate with on-premises systems. Only traffic from IP addresses in the company's IP address range are allowed to access the on-premises systems. The company’s security team is bringing only one IP address from its internal IP address range to the cloud. The company has added this IP address to the allow list for the company firewall. The company also has created an Elastic IP address for this IP address.</p><p><br></p><p>A solutions architect needs to create a solution that gives the application the ability to communicate with the on-premises systems. The solution also must be able to mitigate failures automatically.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Deploy three NAT gateways, one in each public subnet. Assign the Elastic IP address to the NAT gateways. Turn on health checks for the NAT gateways. If a NAT gateway fails a health check, recreate the NAT gateway and assign the Elastic IP address to the new NAT gateway."
        },
        {
          "label": "B",
          "content": "Replace the ALB with a Network Load Balancer (NLB). Assign the Elastic IP address to the NLB. Turn on health checks for the NLB. In the case of a failed health check, redeploy the NLB in different subnets."
        },
        {
          "label": "C",
          "content": "Deploy a single NAT gateway in a public subnet. Assign the Elastic IP address to the NAT gateway. Use Amazon CloudWatch with a custom metric to monitor the NAT gateway. If the NAT gateway is unhealthy, invoke an AWS Lambda function to create a new NAT gateway in a different subnet. Assign the Elastic IP address to the new NAT gateway."
        },
        {
          "label": "D",
          "content": "Assign the Elastic IP address to the ALB. Create an Amazon Route 53 simple record with the Elastic IP address as the value. Create a Route 53 health check. In the case of a failed health check, recreate the ALB in different subnets."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>The correct answer is C. By deploying a single NAT gateway with an Elastic IP address, the company can ensure that all outbound traffic from the EC2 instances appears to come from the whitelisted IP address, allowing communication with on-premises systems. Using Amazon CloudWatch with a custom metric to monitor the NAT gateway and invoking an AWS Lambda function to create a new NAT gateway in a different subnet if the current one becomes unhealthy provides a way to automatically mitigate failures.</p><p>Requirements:</p><p>1. Communication with On-Premises Systems: &nbsp;</p><p> &nbsp; - The application (running on EC2 instances in private subnets) needs to communicate with on-premises systems.</p><p> &nbsp; - The on-premises firewall only allows traffic from a single allowed IP address (the Elastic IP address).</p><p>2. High Availability & Automatic Failure Mitigation: &nbsp;</p><p> &nbsp; - The solution must handle failures automatically without manual intervention.</p><p> &nbsp; - Since only one Elastic IP is available, it must be reassigned dynamically if a failure occurs.</p><p> Why Option C is Correct:</p><p>- NAT Gateway with Elastic IP: &nbsp;</p><p> &nbsp;- A NAT Gateway allows instances in private subnets to communicate with on-premises systems while using the Elastic IP as the source IP.</p><p> &nbsp;- Only one NAT Gateway is needed since only one Elastic IP is available.</p><p> &nbsp;- NAT Gateway is highly available within a single Availability Zone (AZ) but not across AZs.</p><p>- Failure Mitigation with CloudWatch & Lambda: &nbsp;</p><p> &nbsp;- CloudWatch can monitor the NAT Gateway’s health (e.g., via custom metrics like connection failures).</p><p> &nbsp;- If the NAT Gateway fails, a Lambda function can:</p><p> &nbsp; &nbsp;- Create a new NAT Gateway in a different subnet (different AZ).</p><p> &nbsp; &nbsp;- Reassign the Elastic IP to the new NAT Gateway.</p><p> &nbsp;- This ensures automatic failover without manual intervention.</p><p> Why Other Options Are Incorrect:</p><p>- Option A: &nbsp;</p><p> &nbsp;- Deploying three NAT Gateways (one per AZ) is unnecessary because only one Elastic IP is available.</p><p> &nbsp;- NAT Gateway health checks are not natively supported by AWS, and manual recreation is not scalable.</p><p>- Option B: &nbsp;</p><p> &nbsp;- Replacing the ALB with an NLB does not solve the outbound communication problem (NLB is for inbound traffic).</p><p> &nbsp;- NLBs do not provide NAT functionality for private instances.</p><p>- Option D: &nbsp;</p><p> &nbsp;- Assigning the Elastic IP to the ALB is irrelevant because the ALB handles inbound traffic, not outbound traffic to on-premises systems.</p><p> &nbsp;- Route 53 health checks cannot fix NAT Gateway failures.</p><p> Conclusion: &nbsp;</p><p>Option C is the best solution because it uses a NAT Gateway for outbound traffic, assigns the Elastic IP, and implements automated failover using CloudWatch and Lambda.</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "52d728e7c49a4706bfb40362db43e5a9",
      "questionNumber": 265,
      "type": "multiple",
      "content": "<p>Question #265</p><p>A company uses AWS Organizations to manage more than 1,000 AWS accounts. The company has created a new developer organization. There are 540 developer member accounts that must be moved to the new developer organization. All accounts are set up with all the required information so that each account can be operated as a standalone account.</p><p>Which combination of steps should a solutions architect take to move all of the developer accounts to the new developer organization? (Choose three.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Call the MoveAccount operation in the Organizations API from the old organization&#39;s management account to migrate the developer accounts to the new developer organization."
        },
        {
          "label": "B",
          "content": "From the management account, remove each developer account from the old organization using the RemoveAccountFromOrganization operation in the Organizations API."
        },
        {
          "label": "C",
          "content": "From each developer account, remove the account from the old organization using the RemoveAccountFromOrganization operation in the Organizations API."
        },
        {
          "label": "D",
          "content": "Sign in to the new developer organization&#39;s management account and create a placeholder member account that acts as a target for the developer account migration."
        },
        {
          "label": "E",
          "content": "Call the InviteAccountToOrganization operation in the Organizations API from the new developer organization&#39;s management account to send invitations to the developer accounts."
        },
        {
          "label": "F",
          "content": "Have each developer sign in to their account and confirm to join the new developer organization."
        }
      ],
      "correctAnswer": "BEF",
      "explanation": "<p>The correct combination of steps is BEF. First, the management account of the current organization should remove each developer account using the RemoveAccountFromOrganization operation (B). Then, the new developer organization's management account should send an invitation to each developer account using the InviteAccountToOrganization operation (E). Finally, each developer must sign in to their account and accept the invitation to officially join the new organization (F).</p><p>Based on the information provided, the correct answers are:</p><p>B. From the management account, remove each developer account from the old organization using the RemoveAccountFromOrganization operation in the Organizations API.</p><p>E. Call the InviteAccountToOrganization operation in the Organizations API from the new developer organization's management account to send invitations to the developer accounts.</p><p>F. Have each developer sign in to their account and confirm to join the new developer organization.</p><p>These three steps represent the correct process for moving AWS accounts from one organization to another:</p><p>First, you must remove the accounts from their current organization. This is done from the management account of the old organization using the RemoveAccountFromOrganization API operation (option B). This step is necessary because an AWS account can only be a member of one organization at a time.</p><p>Next, the management account of the new developer organization needs to invite the standalone accounts to join. This is done by calling the InviteAccountToOrganization API operation (option E). This sends formal invitations to each of the developer accounts.</p><p>Finally, each invitation must be accepted. Since the accounts are now standalone (as mentioned in the scenario: \"All accounts are set up with all the required information so that each account can be operated as a standalone account\"), the account owners need to sign in to their respective accounts and accept the invitation to join the new organization (option F).</p><p>This three-step process (remove from old organization, invite to new organization, accept invitation) is the standard procedure for moving AWS accounts between organizations. The scenario specifically mentions that there are 540 developer accounts to move, so this process would need to be repeated or automated for each account.</p><p>The other options that weren't selected likely contained incorrect steps or operations that don't align with the proper AWS Organizations account migration process. The selected options represent the complete end-to-end process required to successfully move accounts from one organization to another.</p><p>Sources</p><p>Move an account to AWS Org from another Org | AWS re:Post （https://repost.aws/questions/QUd8oGp5STRgWveFxfolWV_w/move-an-account-to-aws-org-from-another-org）</p><p>Move accounts between organizations in AWS Organizations | AWS re:Post（https://repost.aws/knowledge-center/organizations-move-accounts） </p><p>Migrating AWS accounts between organizations | AWS re:Post（https://repost.aws/articles/ARf43Hri2LQmGARxerfK9B6Q/migrating-aws-accounts-between-organizations） </p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "2756eebc41094d4193ac7163f1522df1",
      "questionNumber": 266,
      "type": "single",
      "content": "<p>Question #266</p><p>A company’s interactive web application uses an Amazon CloudFront distribution to serve images from an Amazon S3 bucket. Occasionally, third-party tools ingest corrupted images into the S3 bucket. This image corruption causes a poor user experience in the application later. The company has successfully implemented and tested Python logic to detect corrupt images.</p><p><br></p><p>A solutions architect must recommend a solution to integrate the detection logic with minimal latency between the ingestion and serving.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use a Lambda@Edge function that is invoked by a viewer-response event."
        },
        {
          "label": "B",
          "content": "Use a Lambda@Edge function that is invoked by an origin-response event."
        },
        {
          "label": "C",
          "content": "Use an S3 event notification that invokes an AWS Lambda function."
        },
        {
          "label": "D",
          "content": "Use an S3 event notification that invokes an AWS Step Functions state machine."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Use an S3 event notification that invokes an AWS Lambda function.</p><p>This solution will meet the requirements for the following reasons:</p><p>Timing of Detection: S3 event notifications trigger immediately when new objects are uploaded to the bucket. This means the corruption detection logic will run as soon as images are ingested, before they are served to users through CloudFront. This provides the minimal latency between ingestion and serving that the requirements specify.</p><p>Prevention Rather Than Detection: By running the detection logic at ingestion time, corrupted images can be identified and handled before they ever reach the CloudFront distribution. This is more efficient than detecting corruption after the images have already been requested by users.</p><p>Seamless Integration with Python Logic: The company has already implemented and tested Python logic to detect corrupt images. AWS Lambda natively supports Python, making it straightforward to integrate the existing detection code without significant modifications.</p><p>Asynchronous Processing: The Lambda function triggered by the S3 event notification can process the image in the background without blocking the ingestion process, maintaining system performance.</p><p>Scalability: This solution automatically scales to handle varying volumes of image uploads without manual intervention.</p><p>The other options have significant drawbacks:</p><p>A. Lambda@Edge with viewer-response event - This would detect corruption too late, as it runs after CloudFront has already retrieved the image and is about to send it to the viewer. This adds unnecessary latency and doesn't prevent corrupted images from being served.</p><p>B. Lambda@Edge with origin-response event - Similar to option A, this detects corruption too late in the process, after the image has been retrieved from S3 but before CloudFront caches it. This still doesn't prevent the initial serving of corrupted images.</p><p>D. S3 event notification with Step Functions - While this would work similarly to option C, it adds unnecessary complexity for this use case. Step Functions are beneficial for orchestrating complex workflows with multiple steps, but the scenario only mentions a single Python-based detection logic, making Lambda alone sufficient and more efficient.</p><p>Option C provides the optimal balance of minimal latency and effective implementation of the corruption detection logic by processing images immediately upon upload, before they are served to users through CloudFront.</p><p>Sources</p><p>How Lambda@Edge works with requests and responses - Amazon CloudFront （https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-edge-event-request-response.html）</p><p>create_distribution - Boto3 1.38.12 documentation （https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/cloudfront/client/create_distribution.html）</p><p>Work with requests and responses - Amazon CloudFront（https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-generating-http-responses.html） </p><p>AWS services in this solution - Dynamic Image Transformation for Amazon CloudFront (Formerly known as Serverless Image Handler) （https://docs.aws.amazon.com/solutions/latest/serverless-image-handler/aws-services-in-this-solution.html）</p><p>Process Objects on Amazon S3 with Precision File Type Detection | AWS re:Post （https://repost.aws/articles/ARDCY-_jnOTbS_OOk8F4kLVA/process-objects-on-amazon-s3-with-precision-file-type-detection）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "5535b002431c47b8b1a9e0271e3a06fd",
      "questionNumber": 267,
      "type": "single",
      "content": "<p>Question #267</p><p>A company has an application that runs on Amazon EC2 instances in an Amazon EC2 Auto Scaling group. The company uses AWS CodePipeline to deploy the application. The instances that run in the Auto Scaling group are constantly changing because of scaling events.</p><p><br></p><p>When the company deploys new application code versions, the company installs the AWS CodeDeploy agent on any new target EC2 instances and associates the instances with the CodeDeploy deployment group. The application is set to go live within the next 24 hours.</p><p><br></p><p>What should a solutions architect recommend to automate the application deployment process with the LEAST amount of operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Configure Amazon EventBridge to invoke an AWS Lambda function when a new EC2 instance is launched into the Auto Scaling group. Code the Lambda function to associate the EC2 instances with the CodeDeploy deployment group."
        },
        {
          "label": "B",
          "content": "Write a script to suspend Amazon EC2 Auto Scaling operations before the deployment of new code. When the deployment is complete, create a new AMI and configure the Auto Scaling group&#39;s launch template to use the new AMI for new launches. Resume Amazon EC2 Auto Scaling operations."
        },
        {
          "label": "C",
          "content": "Create a new AWS CodeBuild project that creates a new AMI that contains the new code. Configure CodeBuild to update the Auto Scaling group&rsquo;s launch template to the new AMI. Run an Amazon EC2 Auto Scaling instance refresh operation."
        },
        {
          "label": "D",
          "content": "Create a new AMI that has the CodeDeploy agent installed. Configure the Auto Scaling group&rsquo;s launch template to use the new AMI. Associate the CodeDeploy deployment group with the Auto Scaling group instead of the EC2 instances."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>The correct answer is D. By creating a new AMI with the CodeDeploy agent pre-installed and configuring the Auto Scaling group to use this AMI, new instances launched by the Auto Scaling group will automatically have the necessary agent. Associating the CodeDeploy deployment group with the Auto Scaling group ensures that new instances are automatically included in the deployment process, reducing the operational overhead as it eliminates the need for manual installation of the agent or manual association with the deployment group.</p><p>1. Least Operational Overhead: </p><p> &nbsp; - By creating an AMI with the CodeDeploy agent pre-installed and configuring the Auto Scaling group's launch template to use this AMI, you ensure that every new EC2 instance launched by Auto Scaling will automatically have the CodeDeploy agent ready.</p><p> &nbsp; - Associating the CodeDeploy deployment group with the Auto Scaling group (instead of individual instances) ensures that any new instances launched will automatically be part of the deployment group without manual intervention.</p><p>2. Fully Automated:</p><p> &nbsp; - No need for additional Lambda functions (Option A) or manual suspension of scaling operations (Option B).</p><p> &nbsp; - No dependency on CodeBuild (Option C) to rebuild AMIs and trigger refreshes—this approach is simpler and more direct.</p><p>3. AWS Best Practice:</p><p> &nbsp; - Using launch templates with pre-configured AMIs is a recommended way to ensure consistency in Auto Scaling deployments.</p><p> &nbsp; - CodeDeploy natively supports Auto Scaling group integrations, eliminating the need to manage instances individually.</p><p> Why Not Other Options?</p><p>- Option A: While it works, it introduces unnecessary complexity with EventBridge and Lambda when CodeDeploy already supports Auto Scaling group integration.</p><p>- Option B: Suspending Auto Scaling operations is disruptive and not scalable for frequent deployments.</p><p>- Option C: While feasible, it requires additional steps (CodeBuild, AMI updates, instance refresh) when a simpler solution exists.</p><p> Conclusion</p><p>Option D provides the most automated and least operationally heavy solution by leveraging native AWS integrations (AMI + Auto Scaling group association with CodeDeploy). &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "466016f5ff4645b992d2632f40ba8777",
      "questionNumber": 268,
      "type": "single",
      "content": "<p>Question #268</p><p>A company has a website that runs on four Amazon EC2 instances that are behind an Application Load Balancer (ALB). When the ALB detects that an EC2 instance is no longer available, an Amazon CloudWatch alarm enters the ALARM state. A member of the company's operations team then manually adds a new EC2 instance behind the ALB.</p><p><br></p><p>A solutions architect needs to design a highly available solution that automatically handles the replacement of EC2 instances. The company needs to minimize downtime during the switch to the new solution.</p><p><br></p><p>Which set of steps should the solutions architect take to meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Delete the existing ALB. Create an Auto Scaling group that is configured to handle the web application traffic. Attach a new launch template to the Auto Scaling group. Create a new ALB. Attach the Auto Scaling group to the new ALB. Attach the existing EC2 instances to the Auto Scaling group."
        },
        {
          "label": "B",
          "content": "Create an Auto Scaling group that is configured to handle the web application traffic. Attach a new launch template to the Auto Scaling group. Attach the Auto Scaling group to the existing ALB. Attach the existing EC2 instances to the Auto Scaling group."
        },
        {
          "label": "C",
          "content": "Delete the existing ALB and the EC2 instances. Create an Auto Scaling group that is configured to handle the web application traffic. Attach a new launch template to the Auto Scaling group. Create a new ALB. Attach the Auto Scaling group to the new ALB. Wait for the Auto Scaling group to launch the minimum number of EC2 instances."
        },
        {
          "label": "D",
          "content": "Create an Auto Scaling group that is configured to handle the web application traffic. Attach a new launch template to the Auto Scaling group. Attach the Auto Scaling group to the existing ALB. Wait for the existing ALB to register the existing EC2 instances with the Auto Scaling group."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>The correct answer is B. By creating an Auto Scaling group and attaching it to the existing ALB, the architect can ensure that new instances are automatically registered with the load balancer. Attaching the existing EC2 instances to the new Auto Scaling group allows for management of all instances, both new and old, through the same scaling policy. This approach minimizes downtime and operational overhead as it does not require the deletion of the existing ALB or the creation of a new one.</p><p> Requirements:</p><p>1. Highly available solution that automatically replaces EC2 instances.</p><p>2. Minimize downtime during the switch to the new solution.</p><p> Analysis of Options:</p><p>- Option A: Deleting the existing ALB first would cause downtime. Also, manually attaching existing EC2 instances to an Auto Scaling group is not a best practice.</p><p>- Option B: This is the correct approach because:</p><p> &nbsp;- It keeps the existing ALB, avoiding downtime.</p><p> &nbsp;- It creates an Auto Scaling group with a launch template (ensuring new instances match the desired configuration).</p><p> &nbsp;- It attaches the Auto Scaling group to the existing ALB, maintaining traffic flow.</p><p> &nbsp;- It registers existing EC2 instances with the Auto Scaling group, allowing Auto Scaling to manage them.</p><p> &nbsp;- This method ensures no downtime and provides automatic instance recovery.</p><p>- Option C: Deleting the existing ALB and EC2 instances first would cause significant downtime.</p><p>- Option D: The ALB does not \"register instances with the Auto Scaling group\"—this is misleading. Instances must be explicitly attached to the Auto Scaling group.</p><p> Why Option B is Best:</p><p>- No downtime: The existing ALB and instances remain operational.</p><p>- Auto Scaling integration: The Auto Scaling group takes over instance management, automatically replacing unhealthy instances.</p><p>- Seamless transition: Existing instances are registered with the Auto Scaling group, ensuring continuity.</p><p> Final Answer:</p><p>B. Create an Auto Scaling group that is configured to handle the web application traffic. Attach a new launch template to the Auto Scaling group. Attach the Auto Scaling group to the existing ALB. Attach the existing EC2 instances to the Auto Scaling group.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "932c80c045e54af6a7564c6856291cc2",
      "questionNumber": 269,
      "type": "single",
      "content": "<p>Question #269</p><p>A company wants to optimize AWS data-transfer costs and compute costs across developer accounts within the company's organization in AWS Organizations. Developers can configure VPCs and launch Amazon EC2 instances in a single AWS Region. The EC2 instances retrieve approximately 1 TB of data each day from Amazon S3. <br><br>The developer activity leads to excessive monthly data-transfer charges and NAT gateway processing charges between EC2 instances and S3 buckets, along with high compute costs. The company wants to proactively enforce approved architectural patterns for any EC2 instance and VPC infrastructure that developers deploy within the AWS accounts. The company does not want this enforcement to negatively affect the speed at which developers can perform their tasks.<br><br></p><p>Which solution will meet these requirements MOST cost-effectively?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create SCPs to prevent developers from launching unapproved EC2 instance types. Provide the developers with an AWS CloudFormation template to deploy an approved VPC configuration with S3 interface endpoints. Scope the developers&#39; IAM permissions so that the developers can launch VPC resources only with CloudFormation."
        },
        {
          "label": "B",
          "content": "Create a daily forecasted budget with AWS Budgets to monitor EC2 compute costs and S3 data-transfer costs across the developer accounts. When the forecasted cost is 75% of the actual budget cost, send an alert to the developer teams. If the actual budget cost is 100%, create a budget action to terminate the developers&#39; EC2 instances and VPC infrastructure."
        },
        {
          "label": "C",
          "content": "Create an AWS Service Catalog portfolio that users can use to create an approved VPC configuration with S3 gateway endpoints and approved EC2 instances. Share the portfolio with the developer accounts. Configure an AWS Service Catalog launch constraint to use an approved IAM role. Scope the developers&#39; IAM permissions to allow access only to AWS Service Catalog."
        },
        {
          "label": "D",
          "content": "Create and deploy AWS Config rules to monitor the compliance of EC2 and VPC resources in the developer AWS accounts. If developers launch unapproved EC2 instances or if developers create VPCs without S3 gateway endpoints, perform a remediation action to terminate the unapproved resources."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>The correct answer is C. By creating an AWS Service Catalog portfolio with approved VPC configurations and S3 gateway endpoints, the company can provide developers with a simple method to deploy resources that adhere to the architectural patterns. This approach automates the enforcement of best practices and reduces the operational overhead for developers. Additionally, by configuring launch constraints and scoping IAM permissions, the company ensures that developers only use approved resources, which can help to control costs.</p><p> Why Option C is the Best Choice? &nbsp;</p><p>1. Proactive Enforcement of Approved Architecture: &nbsp;</p><p> &nbsp; - AWS Service Catalog allows the company to predefine approved VPC configurations (with S3 gateway endpoints) and approved EC2 instance types, ensuring developers deploy only compliant resources. &nbsp;</p><p> &nbsp; - This prevents excessive data-transfer costs (by enforcing S3 gateway endpoints instead of NAT gateways) and unnecessary compute costs (by restricting EC2 instance types). &nbsp;</p><p>2. No Negative Impact on Developer Speed: &nbsp;</p><p> &nbsp; - Developers can still quickly deploy resources using pre-approved templates, without waiting for manual reviews or remediation. &nbsp;</p><p>3. IAM Permissions Restriction: &nbsp;</p><p> &nbsp; - By scoping developers' IAM permissions to only AWS Service Catalog, they cannot bypass the approved configurations. &nbsp;</p><p>4. Cost Optimization: &nbsp;</p><p> &nbsp; - S3 gateway endpoints eliminate NAT gateway charges for S3 traffic. &nbsp;</p><p> &nbsp; - Approved EC2 instance types prevent over-provisioning, reducing compute costs. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- Option A: SCPs can block unapproved instances, but they don’t automate deployment of compliant VPCs/S3 endpoints. Relying solely on CloudFormation templates still allows manual misconfigurations. &nbsp;</p><p>- Option B: Reactive budgeting doesn’t enforce architecture and terminating resources disrupts developers. &nbsp;</p><p>- Option D: AWS Config is reactive (detects & remediates after deployment), leading to potential cost spikes before fixes. &nbsp;</p><p> Conclusion: &nbsp;</p><p>Option C (AWS Service Catalog) is the most proactive, cost-effective, and developer-friendly solution. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "4c262625b28c4cd1a4fe1078013ee00d",
      "questionNumber": 270,
      "type": "single",
      "content": "<p>Question #270</p><p>A company is expanding. The company plans to separate its resources into hundreds of different AWS accounts in multiple AWS Regions. A solutions architect must recommend a solution that denies access to any operations outside of specifically designated Regions.</p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create IAM roles for each account. Create IAM policies with conditional allow permissions that include only approved Regions for the accounts."
        },
        {
          "label": "B",
          "content": "Create an organization in AWS Organizations. Create IAM users for each account. Attach a policy to each user to block access to Regions where an account cannot deploy infrastructure."
        },
        {
          "label": "C",
          "content": "Launch an AWS Control Tower landing zone. Create OUs and attach SCPs that deny access to run services outside of the approved Regions."
        },
        {
          "label": "D",
          "content": "Enable AWS Security Hub in each account. Create controls to specify the Regions where an account can deploy infrastructure."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Launch an AWS Control Tower landing zone. Create OUs and attach SCPs that deny access to run services outside of the approved Regions.</p><p>This solution will meet the requirements for the following reasons:</p><p>Scalability: The scenario involves \"hundreds of different AWS accounts in multiple AWS Regions.\" AWS Control Tower is specifically designed to manage multi-account environments at scale. It provides a streamlined way to set up and govern a secure, multi-account AWS environment based on best practices.</p><p>Centralized Policy Management: Service Control Policies (SCPs) in AWS Organizations (which is part of Control Tower) allow for centralized policy management across all accounts. This makes it much more efficient to implement and maintain region restrictions across hundreds of accounts.</p><p>Preventive Controls: SCPs act as guardrails that cannot be overridden by account administrators. They provide a true preventive control that denies access to operations outside of designated regions, regardless of the permissions granted within individual accounts.</p><p>Hierarchical Structure: Organizational Units (OUs) in Control Tower allow for grouping accounts logically, making it possible to apply different region restrictions to different groups of accounts if needed.</p><p>Inheritance: Policies applied at the OU level are automatically inherited by all accounts within that OU, ensuring consistent enforcement without having to configure each account individually.</p><p>The other options have significant limitations:</p><p>A. Creating IAM roles with conditional allow permissions for each account would be extremely difficult to manage across hundreds of accounts. It would be prone to inconsistencies and human error.</p><p>B. Creating IAM users for each account and attaching policies to block access would be even more unmanageable than option A, as it would require maintaining individual users across hundreds of accounts. Additionally, IAM best practices recommend using roles rather than users for cross-account access.</p><p>D. AWS Security Hub is primarily a security monitoring and compliance service. While it can detect non-compliant resources, it doesn't provide preventive controls to deny access to operations. It would identify violations after they occur rather than preventing them.</p><p>AWS Control Tower with SCPs provides the most comprehensive, scalable, and manageable solution for enforcing region restrictions across hundreds of AWS accounts.</p><p>Sources</p><p>Defining Boundaries for Regional Services Access - Navigating GDPR Compliance on AWS （https://docs.aws.amazon.com/whitepapers/latest/navigating-gdpr-compliance/defining-boundaries-for-regional-services-access.html）</p><p>Using AWS Organizations for security - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/security-reference-architecture/organizations-security.html）</p><p>AWS account management and separation - Security Pillar（https://docs.aws.amazon.com/wellarchitected/latest/security-pillar/aws-account-management-and-separation.html） </p><p>Protect your AWS resources from unauthorized access using AWS Organizations integrations | AWS Cloud Operations Blog （https://aws.amazon.com/cn/blogs/mt/protect-your-aws-resources-from-unauthorized-access-using-aws-organizations-integrations/）</p><p>Enforcing enterprise-wide preventive controls with AWS Organizations | AWS Cloud Operations Blog （https://aws.amazon.com/cn/blogs/mt/enforcing-enterprise-wide-preventive-controls-with-aws-organizations/）</p><p>Effectively implementing resource controls policies in a multi-account environment | AWS Security Blog （https://aws.amazon.com/cn/blogs/security/effectively-implementing-resource-controls-policies-in-a-multi-account-environment/）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "b4165bb08aa94fe8bc099e94f1182c2c",
      "questionNumber": 271,
      "type": "single",
      "content": "<p>Question #271</p><p>A company wants to refactor its retail ordering web application that currently has a load-balanced Amazon EC2 instance fleet for web hosting, database API services, and business logic. The company needs to create a decoupled, scalable architecture with a mechanism for retaining failed orders while also minimizing operational costs.</p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use Amazon S3 for web hosting with Amazon API Gateway for database API services. Use Amazon Simple Queue Service (Amazon SQS) for order queuing. Use Amazon Elastic Container Service (Amazon ECS) for business logic with Amazon SQS long polling for retaining failed orders."
        },
        {
          "label": "B",
          "content": "Use AWS Elastic Beanstalk for web hosting with Amazon API Gateway for database API services. Use Amazon MQ for order queuing. Use AWS Step Functions for business logic with Amazon S3 Glacier Deep Archive for retaining failed orders."
        },
        {
          "label": "C",
          "content": "Use Amazon S3 for web hosting with AWS AppSync for database API services. Use Amazon Simple Queue Service (Amazon SQS) for order queuing. Use AWS Lambda for business logic with an Amazon SQS dead-letter queue for retaining failed orders."
        },
        {
          "label": "D",
          "content": "Use Amazon Lightsail for web hosting with AWS AppSync for database API services. Use Amazon Simple Email Service (Amazon SES) for order queuing. Use Amazon Elastic Kubernetes Service (Amazon EKS) for business logic with Amazon OpenSearch Service for retaining failed orders."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Use Amazon S3 for web hosting with AWS AppSync for database API services. Use Amazon Simple Queue Service (Amazon SQS) for order queuing. Use AWS Lambda for business logic with an Amazon SQS dead-letter queue for retaining failed orders.</p><p>This solution will meet the company's requirements for the following reasons:</p><p>Decoupled Architecture:</p><p>S3 for web hosting separates the frontend from the backend services</p><p>AppSync provides a managed GraphQL API layer that decouples the database interactions</p><p>SQS creates a buffer between order submission and processing</p><p>Lambda functions handle business logic independently from other components</p><p>This complete separation of concerns creates a truly decoupled system</p><p>Scalability:</p><p>All components (S3, AppSync, SQS, Lambda) are fully managed serverless services that scale automatically</p><p>S3 can handle virtually unlimited web traffic</p><p>SQS can scale to handle high volumes of orders during peak periods</p><p>Lambda functions automatically scale to process orders based on queue depth</p><p>This architecture eliminates the need to manage scaling for any component</p><p>Mechanism for Retaining Failed Orders:</p><p>SQS dead-letter queues provide a built-in mechanism to capture and retain failed orders</p><p>Failed orders can be easily monitored, analyzed, and reprocessed when issues are resolved</p><p>The retention period can be configured based on business needs</p><p>Minimizing Operational Costs:</p><p>All components follow a pay-for-what-you-use pricing model</p><p>No need to pay for idle resources during low-traffic periods</p><p>No server management or patching required</p><p>Reduced operational overhead for monitoring and maintaining infrastructure</p><p>SQS dead-letter queues are more cost-effective than Glacier Deep Archive for the use case of retaining failed orders that need to be accessible</p><p>Option B (Elastic Beanstalk, API Gateway, Amazon MQ, Step Functions, S3 Glacier Deep Archive) has several disadvantages:</p><p>Elastic Beanstalk still requires managing EC2 instances, which increases operational costs</p><p>Amazon MQ is more complex and expensive to operate than SQS for simple order queuing</p><p>S3 Glacier Deep Archive has extremely long retrieval times (12+ hours), making it impractical for retaining failed orders that may need to be reprocessed quickly</p><p>This solution has higher fixed costs regardless of usage levels</p><p>The serverless architecture in Option C provides the optimal balance of decoupling, scalability, effective handling of failed orders, and minimal operational costs, making it the best solution for the company's requirements.</p><p>Sources</p><p>Architecture details - Enhanced Document Understanding on AWS （https://docs.aws.amazon.com/solutions/latest/enhanced-document-understanding-on-aws/architecture-details.html）</p><p>Connect to Amazon EventBridge to send and receive events - AWS Amplify Gen 2 Documentation （https://docs.amplify.aws/react/build-a-backend/data/custom-business-logic/connect-eventbridge-datasource/）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "8216f7fc571a45f59e5dbc3e46923fb9",
      "questionNumber": 272,
      "type": "single",
      "content": "<p>Question #272</p><p>A company hosts a web application on AWS in the us-east-1 Region. The application servers are distributed across three Availability Zones behind an Application Load Balancer. The database is hosted in a MySQL database on an Amazon EC2 instance. A solutions architect needs to design a cross-Region data recovery solution using AWS services with an RTO of less than 5 minutes and an RPO of less than 1 minute. The solutions architect is deploying application servers in us-west-2, and has configured Amazon Route 53 health checks and DNS failover to us-west-2.</p><p><br></p><p>Which additional step should the solutions architect take?</p>",
      "options": [
        {
          "label": "A",
          "content": "Migrate the database to an Amazon RDS for MySQL instance with a cross-Region read replica in us-west-2."
        },
        {
          "label": "B",
          "content": "Migrate the database to an Amazon Aurora global database with the primary in us-east-1 and the secondary in us-west-2."
        },
        {
          "label": "C",
          "content": "Migrate the database to an Amazon RDS for MySQL instance with a Multi-AZ deployment."
        },
        {
          "label": "D",
          "content": "Create a MySQL standby database on an Amazon EC2 instance in us-west-2."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Migrate the database to an Amazon Aurora global database with the primary in us-east-1 and the secondary in us-west-2.</p><p>This solution will meet the required RTO of less than 5 minutes and RPO of less than 1 minute for the following reasons:</p><p>Recovery Point Objective (RPO) &lt; 1 minute:</p><p>Amazon Aurora Global Database uses a purpose-built replication technology that typically maintains replication lag of less than 1 second between the primary and secondary regions.</p><p>This ensures that in the event of a disaster, data loss would be minimal (measured in seconds), well within the RPO requirement of less than 1 minute.</p><p>The storage-based replication used by Aurora Global Database is more efficient than the binary log replication used by standard MySQL, resulting in lower replication lag.</p><p>Recovery Time Objective (RTO) &lt; 5 minutes:</p><p>Aurora Global Database allows for managed failover to the secondary region, which can be completed in less than 1 minute.</p><p>When combined with the already configured Route 53 health checks and DNS failover to us-west-2, the entire application stack can be failed over within the 5-minute RTO requirement.</p><p>The secondary region database is already warm and ready to accept write operations immediately after promotion.</p><p>Option A (Migrate the database to an Amazon RDS for MySQL instance with a cross-Region read replica) would not meet the requirements because:</p><p>RDS for MySQL cross-region read replicas use asynchronous replication based on binary logs, which can introduce higher replication lag that might exceed the RPO requirement of less than 1 minute, especially during periods of high write activity.</p><p>The failover process for RDS MySQL read replicas is manual and involves several steps:</p><p>Promoting the read replica to a standalone instance</p><p>Updating application connection strings</p><p>Waiting for DNS propagation</p><p>This manual process would likely exceed the RTO requirement of less than 5 minutes.</p><p>Amazon Aurora Global Database is specifically designed for cross-region disaster recovery scenarios with stringent RTO and RPO requirements. It provides the necessary infrastructure to meet the company's recovery objectives while minimizing operational complexity during a failover event.</p><p>Sources</p><p>Disaster recovery options in the cloud - Disaster Recovery of Workloads on AWS: Recovery in the Cloud（https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-options-in-the-cloud.html） </p><p>Comparing Amazon Aurora replication solutions - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/aurora-replication-options/compare-solutions.html）</p><p>Cost-effective disaster recovery for Amazon Aurora databases using AWS Backup | AWS Database Blog （https://aws.amazon.com/cn/blogs/database/cost-effective-disaster-recovery-for-amazon-aurora-databases-using-aws-backup/）</p><p>MySQL PostgreSQL Relational Database – Amazon Aurora Global Database – AWS （https://aws.amazon.com/cn/rds/aurora/global-database/）</p><p>Implement cross-Region disaster recovery with AWS DMS and Amazon Aurora - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/implement-cross-region-disaster-recovery-with-aws-dms-and-amazon-aurora.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "8b5489c83a2447cba7316fc4ac2d4b79",
      "questionNumber": 273,
      "type": "single",
      "content": "<p>Question #273</p><p>A company is using AWS Organizations to manage multiple accounts. Due to regulatory requirements, the company wants to restrict specific member accounts to certain AWS Regions, where they are permitted to deploy resources. The resources in the accounts must be tagged, enforced based on a group standard, and centrally managed with minimal configuration.</p><p><br></p><p>What should a solutions architect do to meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an AWS Config rule in the specific member accounts to limit Regions and apply a tag policy."
        },
        {
          "label": "B",
          "content": "From the AWS Billing and Cost Management console, in the management account, disable Regions for the specific member accounts and apply a tag policy on the root."
        },
        {
          "label": "C",
          "content": "Associate the specific member accounts with the root. Apply a tag policy and an SCP using conditions to limit Regions."
        },
        {
          "label": "D",
          "content": "Associate the specific member accounts with a new OU. Apply a tag policy and an SCP using conditions to limit Regions."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. Associate the specific member accounts with a new OU. Apply a tag policy and an SCP using conditions to limit Regions.</p><p>This solution will meet the requirements for the following reasons:</p><p>Centralized Management: By creating a dedicated Organizational Unit (OU) for the specific member accounts that need region restrictions, you can centrally manage policies from the AWS Organizations management console. This provides a single point of control for both region restrictions and tagging requirements.</p><p>Minimal Configuration: Using OUs with Service Control Policies (SCPs) and tag policies is more efficient than configuring individual accounts. You only need to set up the policies once at the OU level, and they automatically apply to all accounts within that OU. When new accounts need the same restrictions, you simply move them into the OU.</p><p>Region Restriction: SCPs are preventive guardrails that can deny access to AWS services in specific regions. By using conditions in the SCP, you can precisely control which regions are allowed or denied for the accounts in the OU. This is a preventive control that blocks non-compliant actions before they occur.</p><p>Tag Enforcement: Tag policies in AWS Organizations allow you to define tagging rules and enforce them across multiple accounts. When applied at the OU level, these policies ensure consistent tagging across all resources in the member accounts, meeting the regulatory requirement for standardized tagging.</p><p>Flexibility: This approach allows for different policies to be applied to different groups of accounts by organizing them into appropriate OUs, which is ideal for environments with varying regulatory requirements.</p><p>The other options have significant limitations:</p><p>A. Creating AWS Config rules in specific member accounts would be detective rather than preventive, meaning non-compliant resources could still be created and would only be flagged after the fact. This approach also requires configuration in each account, which doesn't meet the \"centrally managed with minimal configuration\" requirement.</p><p>B. Disabling regions from the Billing console is not a comprehensive solution for restricting resource deployment. It primarily affects the console experience but doesn't prevent API or CLI access to those regions. Additionally, this approach doesn't provide robust tag enforcement capabilities.</p><p>C. Applying policies at the root level would affect all accounts in the organization, which may be too broad if only specific member accounts need these restrictions. This doesn't allow for the targeted application of policies that the scenario requires.</p><p>Option D provides the most effective, centralized, and least-effort approach to meeting the regulatory requirements for both region restrictions and tag enforcement on specific member accounts.</p><p>Sources</p><p>Tag Policies vs required-tags rule | AWS re:Post （https://repost.aws/questions/QUJVliDkVfQ6iJD0aUy0EU2A/tag-policies-vs-required-tags-rule）</p><p>AWS account management and separation - Security Pillar (https://docs.aws.amazon.com/wellarchitected/latest/security-pillar/aws-account-management-and-separation.html)</p><p>Defining Boundaries for Regional Services Access - Navigating GDPR Compliance on AWS (https://docs.aws.amazon.com/whitepapers/latest/navigating-gdpr-compliance/defining-boundaries-for-regional-services-access.html)</p><p>Delegated Administrators Guide to Effective Controls in AWS Organizations | AWS Cloud Operations Blog (https://aws.amazon.com/cn/blogs/mt/delegated-administrators-guide-to-effective-controls-in-aws-organizations/)</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "5265f7fa58e94a449a3573d88176d0ff",
      "questionNumber": 274,
      "type": "single",
      "content": "<p>Question #274</p><p>A company has an application that generates reports and stores them in an Amazon S3 bucket. When a user accesses their report, the application generates a signed URL to allow the user to download the report. The company's security team has discovered that the files are public and that anyone can download them without authentication. The company has suspended the generation of new reports until the problem is resolved.<br><br></p><p>Which set of actions will immediately remediate the security issue without impacting the application's normal workflow?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an AWS Lambda function that applies a deny all policy for users who are not authenticated. Create a scheduled event to invoke the Lambda function."
        },
        {
          "label": "B",
          "content": "Review the AWS Trusted Advisor bucket permissions check and implement the recommended actions."
        },
        {
          "label": "C",
          "content": "Run a script that puts a private ACL on all of the objects in the bucket."
        },
        {
          "label": "D",
          "content": "Use the Block Public Access feature in Amazon S3 to set the IgnorePublicAcls option to TRUE on the bucket."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. Use the Block Public Access feature in Amazon S3 to set the IgnorePublicAcls option to TRUE on the bucket.</p><p>This solution will immediately remediate the security issue without impacting the application's normal workflow for the following reasons:</p><p>Immediate Effect: The Block Public Access feature takes effect immediately when enabled, instantly blocking any public access to the bucket and its objects regardless of existing permissions.</p><p>Preserves Signed URL Functionality: Setting IgnorePublicAcls to TRUE specifically targets and blocks public access through ACLs, but it doesn't interfere with pre-signed URL functionality. Pre-signed URLs work through temporary credentials and don't rely on public ACLs, so the application can continue generating signed URLs for authenticated users to download their reports.</p><p>Non-Disruptive: This approach doesn't require modifying individual objects or changing the application's code. It works at the bucket level to override any public permissions without altering the underlying objects or their ACLs.</p><p>Comprehensive Protection: The IgnorePublicAcls setting ensures that even if objects in the bucket have public ACLs applied to them (which is likely the cause of the current issue), those ACLs will be ignored, effectively making the objects private.</p><p>The other options have significant limitations:</p><p>B. Reviewing the AWS Trusted Advisor bucket permissions check is only an advisory step that identifies potential issues but doesn't actually implement any changes to remediate the problem. This would not immediately fix the security issue.</p><p>C. Running a script to put a private ACL on all objects would be time-consuming for a large number of objects and might not address bucket-level permissions that could be causing the public access. Additionally, this approach could potentially disrupt the application if it relies on specific ACL settings for certain functionality.</p><p>Option D provides the most efficient and effective solution that immediately addresses the security concern while maintaining the application's ability to generate and use signed URLs for authorized access to the reports.</p><p>Sources</p><p>ACCT.08 Prevent public access to private S3 buckets - AWS Prescriptive Guidance (https://docs.aws.amazon.com/prescriptive-guidance/latest/aws-startup-security-baseline/acct-08.html)</p><p>Blocking public access to your Amazon S3 storage - Amazon Simple Storage Service (https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-block-public-access.html)</p><p>Amazon S3 Block Public Access - AWS (https://aws.amazon.com/cn/s3/features/block-public-access/)</p><p>S3: is public access possible when Block all public access is on and object ownership is bucket owner enforced | AWS re:Post (https://repost.aws/questions/QUPKYOp6dZS-GDAtT84cbUjg/s3-is-public-access-possible-when-block-all-public-access-is-on-and-object-ownership-is-bucket-owner-enforced)</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "b7badbf7c2474110accff5a80431a016",
      "questionNumber": 275,
      "type": "multiple",
      "content": "<p>Question #275</p><p>A company is planning to migrate an Amazon RDS for Oracle database to an RDS for PostgreSQL DB instance in another AWS account. A solutions architect needs to design a migration strategy that will require no downtime and that will minimize the amount of time necessary to complete the migration. The migration strategy must replicate all existing data and any new data that is created during the migration. The target database must be identical to the source database at completion of the migration process.</p><p>All applications currently use an Amazon Route 53 CNAME record as their endpoint for communication with the RDS for Oracle DB instance. The RDS for Oracle DB instance is in a private subnet.</p><p>Which combination of steps should the solutions architect take to meet these requirements? (Choose three.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a new RDS for PostgreSQL DB instance in the target account. Use the AWS Schema Conversion Tool (AWS SCT) to migrate the database schema from the source database to the target database."
        },
        {
          "label": "B",
          "content": "Use the AWS Schema Conversion Tool (AWS SCT) to create a new RDS for PostgreSQL DB instance in the target account with the schema and initial data from the source database."
        },
        {
          "label": "C",
          "content": "Configure VPC peering between the VPCs in the two AWS accounts to provide connectivity to both DB instances from the target account."
        },
        {
          "label": "D",
          "content": "Temporarily allow the source DB instance to be publicly accessible to provide connectivity from the VPC in the target account."
        },
        {
          "label": "E",
          "content": "Use AWS Database Migration Service (AWS DMS) in the target account to perform a full load plus change data capture (CDC) migration from the source database to the target database. When the migration is complete, change the CNAME record to point to the target DB instance endpoint."
        },
        {
          "label": "F",
          "content": "Use AWS Database Migration Service (AWS DMS) in the target account to perform a change data capture (CDC) migration from the source database to the target database. When the migration is complete, change the CNAME record to point to the target DB instance endpoint."
        }
      ],
      "correctAnswer": "ACE",
      "explanation": "<p>The correct combination of steps is ACE. First, the AWS Schema Conversion Tool (AWS SCT) can be used to create a new RDS for PostgreSQL DB instance with the schema and initial data from the source Oracle database (A and B). Next, AWS DMS can be used to perform a full load of the data followed by change data capture (CDC) to replicate ongoing changes during the migration (E). Finally, once the migration is complete, the CNAME record in Route 53 can be updated to point to the new PostgreSQL DB instance endpoint, ensuring no downtime (E). Configuring VPC peering (C) is necessary to enable connectivity between the source and target environments, facilitating the migration process.</p><p>1. A. Create a new RDS for PostgreSQL DB instance in the target account. Use the AWS Schema Conversion Tool (AWS SCT) to migrate the database schema from the source database to the target database. &nbsp;</p><p> &nbsp; - AWS SCT is used to convert the Oracle schema to PostgreSQL schema, including stored procedures, functions, and other database objects. This is necessary because Oracle and PostgreSQL have different syntax and features. &nbsp;</p><p> &nbsp; - The initial data load is not handled by SCT; that is done by AWS DMS.</p><p>2. C. Configure VPC peering between the VPCs in the two AWS accounts to provide connectivity to both DB instances from the target account. Configure the security groups to allow traffic on the database port from the VPC in the target account. &nbsp;</p><p> &nbsp; - Since both databases are in private subnets, VPC peering is required to establish connectivity between them securely. &nbsp;</p><p> &nbsp; - Security groups must be configured to allow traffic between the source and target databases.</p><p>3. E. Use AWS Database Migration Service (AWS DMS) in the target account to perform a full load plus change data capture (CDC) migration from the source database to the target database. When the migration is complete, change the CNAME record to point to the target DB instance endpoint. &nbsp;</p><p> &nbsp; - AWS DMS performs the full load (initial data migration) and CDC (ongoing replication of changes during migration). &nbsp;</p><p> &nbsp; - Once the databases are in sync, the Route 53 CNAME is updated to point to the new PostgreSQL database, completing the cutover with minimal downtime.</p><p> Why not the other options?</p><p>- B: AWS SCT does not create a new RDS instance or migrate data; it only converts the schema.</p><p>- D: Making the source database publicly accessible is not secure and violates the requirement of keeping it in a private subnet.</p><p>- F: CDC alone is insufficient; a full load + CDC is required to migrate all existing data and replicate changes.</p><p>Thus, the correct answers are A, C, and E.</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "4b0f23b0f5fe4261a9d19098b903462a",
      "questionNumber": 276,
      "type": "single",
      "content": "<p>Question #276</p><p>A company has implemented an ordering system using an event-driven architecture. During initial testing, the system stopped processing orders. Further log analysis revealed that one order message in an Amazon Simple Queue Service (Amazon SQS) standard queue was causing an error on the backend and blocking all subsequent order messages. The visibility timeout of the queue is set to 30 seconds, and the backend processing timeout is set to 10 seconds. A solutions architect needs to analyze faulty order messages and ensure that the system continues to process subsequent messages.</p><p><br></p><p>Which step should the solutions architect take to meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Increase the backend processing timeout to 30 seconds to match the visibility timeout."
        },
        {
          "label": "B",
          "content": "Reduce the visibility timeout of the queue to automatically remove the faulty message."
        },
        {
          "label": "C",
          "content": "Configure a new SQS FIFO queue as a dead-letter queue to isolate the faulty messages."
        },
        {
          "label": "D",
          "content": "Configure a new SQS standard queue as a dead-letter queue to isolate the faulty messages."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. Configure a new SQS standard queue as a dead-letter queue to isolate the faulty messages.</p><p>This solution will meet the requirements for the following reasons:</p><p>Continued Processing of Subsequent Messages:</p><p>By implementing a dead-letter queue (DLQ), the system can move the problematic message out of the main processing queue after a defined number of failed processing attempts.</p><p>Once the faulty message is moved to the DLQ, the system will automatically continue processing the next messages in the queue, preventing the blockage that's currently occurring.</p><p>This ensures business continuity while still preserving the problematic message for analysis.</p><p>Analysis of Faulty Messages:</p><p>The dead-letter queue preserves the faulty messages in a separate queue where they can be examined without affecting the main processing flow.</p><p>This allows the solutions architect to analyze the problematic order message in detail to understand what's causing the error.</p><p>The original message content and metadata are maintained, providing valuable diagnostic information.</p><p>Proper Error Handling:</p><p>Using a DLQ is a standard pattern for handling poison messages (messages that cannot be processed successfully) in event-driven architectures.</p><p>It provides a systematic way to deal with errors rather than letting them repeatedly block the system.</p><p>The other options have significant limitations:</p><p>A. Increasing the backend processing timeout to 30 seconds would not solve the problem. If the message is causing an error, giving it more processing time won't help - it will still fail and continue to block subsequent messages.</p><p>B. Reducing the visibility timeout would not automatically remove the faulty message. It would only make the message visible again sooner, causing it to be picked up for processing again, likely resulting in the same error. This could create an infinite loop of failed processing attempts.</p><p>Option D provides the most effective solution by isolating the problematic message while allowing the system to continue processing other orders. It also preserves the faulty message for analysis, which is crucial for identifying and fixing the underlying issue.</p><p>Sources</p><p>Amazon SQS error handling and problematic messages - Amazon Simple Queue Service (https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/best-practices-error-handling.html)</p><p>Using dead-letter queues in Amazon SQS - Amazon Simple Queue Service (https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html)</p><p>Amazon SQS best practices - Amazon Simple Queue Service (https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-best-practices.html)</p><p>Regarding SQS dead-letter queue (DLQ) | AWS re:Post (https://repost.aws/questions/QU6S7Kzhe4T3yWAQbyuHufow/regarding-sqs-dead-letter-queue-dlq)</p><p>Infinite retries due to exceeded SQS visibility timeout | AWS re:Post(https://repost.aws/questions/QUOGzGulGSS1-wtgDIj22USw/infinite-retries-due-to-exceeded-sqs-visibility-timeout) </p><p>Capturing problematic messages in Amazon SQS - Amazon Simple Queue Service (https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/capturing-problematic-messages.html)</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "cad6a302894643838f069df251a7423f",
      "questionNumber": 277,
      "type": "multiple",
      "content": "<p>Question #277</p><p>A company has automated the nightly retraining of its machine learning models by using AWS Step Functions. The workflow consists of multiple steps that use AWS Lambda. Each step can fail for various reasons, and any failure causes a failure of the overall workflow. A review reveals that the retraining has failed multiple nights in a row without the company noticing the failure. A solutions architect needs to improve the workflow so that notifications are sent for all types of failures in the retraining process.</p><p>Which combination of steps should the solutions architect take to meet these requirements? (Choose three.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an Amazon Simple Notification Service (Amazon SNS) topic with a subscription of type &quot;Email&quot; that targets the team&#39;s mailing list."
        },
        {
          "label": "B",
          "content": "Create a task named &quot;Email&quot; that forwards the input arguments to the SNS topic."
        },
        {
          "label": "C",
          "content": "Add a Catch field to all Task, Map, and Parallel states that have a statement of &quot;ErrorEquals&quot;: [&quot;States.ALL&quot;] and &quot;Next&quot;: &quot;Email&quot;."
        },
        {
          "label": "D",
          "content": "Add a new email address to Amazon Simple Email Service (Amazon SES). Verify the email address."
        },
        {
          "label": "E",
          "content": "Create a task named &quot;Email&quot; that forwards the input arguments to the SES email address."
        },
        {
          "label": "F",
          "content": "Add a Catch field to all Task, Map, and Parallel states that have a statement of &quot;ErrorEquals&quot;: [&quot;States.Runtime&quot;] and &quot;Next&quot;: &quot;Email&quot;."
        }
      ],
      "correctAnswer": "ABC",
      "explanation": "<p>The correct combination of steps is ABC. To ensure that the team is notified of any failures in the retraining process, the architect should create an SNS topic and subscribe the team's email to it (A). Then, a task named \"Email\" should be created within the Step Functions workflow to publish messages to the SNS topic when a failure occurs (B). To catch all types of failures, a Catch block should be added to all states with a condition that listens for any error (\"States.ALL\") and then transitions to the \"Email\" task (C). This setup will capture any failure, regardless of its type, and trigger the notification process.</p><p> </p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "31c965e4f01343e59de57b9c52df864e",
      "questionNumber": 278,
      "type": "single",
      "content": "<p>Question #278</p><p>A company plans to deploy a new private intranet service on Amazon EC2 instances inside a VPC. An AWS Site-to-Site VPN connects the VPC to the company's on-premises network. The new service must communicate with existing on-premises services. The on-premises services are accessible through the use of hostnames that reside in the company.example DNS zone. This DNS zone is wholly hosted on premises and is available only on the company's private network.</p><p><br></p><p>A solutions architect must ensure that the new service can resolve hostnames on the company.example domain to integrate with existing services.</p><p><br></p><p>Which solution meets these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an empty private zone in Amazon Route 53 for company.example. Add an additional NS record to the company&#39;s on-premises company.example zone that points to the authoritative name servers for the new private zone in Route 53."
        },
        {
          "label": "B",
          "content": "Turn on DNS hostnames for the VPC. Configure a new outbound endpoint with Amazon Route 53 Resolver. Create a Resolver rule to forward requests for company.example to the on-premises name servers."
        },
        {
          "label": "C",
          "content": "Turn on DNS hostnames for the VPC. Configure a new inbound resolver endpoint with Amazon Route 53 Resolver. Configure the on-premises DNS server to forward requests for company.example to the new resolver."
        },
        {
          "label": "D",
          "content": "Use AWS Systems Manager to configure a run document that will install a hosts file that contains any required hostnames. Use an Amazon EventBridge rule to run the document when an instance is entering the running state."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>The correct answer is B. By enabling DNS hostnames for the VPC and configuring a new outbound endpoint with Amazon Route 53 Resolver, the architect can create a Resolver rule that forwards DNS queries for the company.example domain to the on-premises DNS server. This setup allows the EC2 instances within the VPC to resolve the hostnames of the on-premises services, thus integrating with the existing services.</p><p>The requirement is for the EC2 instances in the VPC to resolve hostnames in the company.example domain, which is hosted on-premises. Since the DNS zone is only available on the company's private network, we need to forward DNS queries for company.example to the on-premises DNS servers. &nbsp;</p><p> Why Option B is Correct: &nbsp;</p><p>1. Turn on DNS hostnames for the VPC – Ensures that instances in the VPC can resolve DNS hostnames. &nbsp;</p><p>2. Configure an outbound endpoint with Route 53 Resolver – Allows the VPC to send DNS queries to external (on-premises) DNS servers. &nbsp;</p><p>3. Create a Resolver rule to forward requests for company.example to on-premises name servers – This ensures that any DNS queries for company.example are forwarded to the on-premises DNS servers, while all other queries go to the default AWS resolver. &nbsp;</p><p> Why Other Options Are Incorrect: &nbsp;</p><p>- A: Creating an empty private zone in Route 53 and modifying the on-premises DNS to point to it would break resolution since the zone is hosted on-premises. &nbsp;</p><p>- C: Configuring an inbound resolver would allow on-premises systems to query AWS DNS, but we need the opposite (VPC to query on-premises DNS). &nbsp;</p><p>- D: Using a hosts file is not scalable and would require manual updates whenever hostnames change. &nbsp;</p><p> Conclusion: &nbsp;</p><p>Option B is the correct solution because it forwards DNS queries for company.example to the on-premises DNS servers while allowing all other queries to resolve normally. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "90b685956f5e40ac97f06d9e2e5d9b19",
      "questionNumber": 279,
      "type": "single",
      "content": "<p>Question #279</p><p>A company uses AWS CloudFormation to deploy applications within multiple VPCs that are all attached to a transit gateway. Each VPC that sends traffic to the public internet must send the traffic through a shared services VPC. Each subnet within a VPC uses the default VPC route table, and the traffic is routed to the transit gateway. The transit gateway uses its default route table for any VPC attachment.</p><p><br></p><p>A security audit reveals that an Amazon EC2 instance that is deployed within a VPC can communicate with an EC2 instance that is deployed in any of the company's other VPCs. A solutions architect needs to limit the traffic between the VPCs. Each VPC must be able to communicate only with a pre-defined, limited set of authorized VPCs.</p><p><br></p><p>What should the solutions architect do to meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Update the network ACL of each subnet within a VPC to allow outbound traffic only to the authorized VPCs. Remove all deny rules except the default deny rule."
        },
        {
          "label": "B",
          "content": "Update all the security groups that are used within a VPC to deny outbound traffic to security groups that are used within the unauthorized VPCs."
        },
        {
          "label": "C",
          "content": "Create a dedicated transit gateway route table for each VPC attachment. Route traffic only to the authorized VPCs."
        },
        {
          "label": "D",
          "content": "Update the main route table of each VPC to route traffic only to the authorized VPCs through the transit gateway."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>The correct answer is C. To restrict traffic between VPCs and ensure that each VPC can only communicate with a specific set of authorized VPCs, the solutions architect should create a dedicated transit gateway route table for each VPC attachment. By doing so, the architect can define explicit routes within these dedicated route tables to only allow traffic to the authorized VPCs. This approach provides granular control over the routing of traffic between VPCs and ensures that the communication is limited as per the security requirements.</p><p>The issue is that all VPCs are attached to the transit gateway and using its default route table, which allows unrestricted communication between all attached VPCs. To restrict traffic between VPCs while still allowing communication only with predefined authorized VPCs, the best approach is to:</p><p>1. Create dedicated transit gateway route tables for each VPC attachment instead of using the default route table.</p><p>2. Configure explicit routes in each transit gateway route table to allow traffic only to the authorized VPCs.</p><p>This ensures that VPCs can communicate only with the explicitly allowed VPCs while still maintaining the shared internet egress through the shared services VPC.</p><p> Why Not the Other Options?</p><p>- A. Network ACLs are stateless and apply broadly to subnets, making them inefficient for fine-grained VPC-to-VPC control. They also don’t scale well for dynamic environments.</p><p>- B. Security Groups are instance-level controls and would require manual updates for every instance, which is impractical.</p><p>- D. Updating VPC route tables alone won’t restrict traffic at the transit gateway level, as the transit gateway’s default route table still allows all VPCs to communicate.</p><p> Key Takeaway:</p><p>Transit Gateway route tables provide the most scalable and centralized way to control inter-VPC routing while maintaining the required connectivity. </p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "eed222a6764f43e18dfd5c17d480db16",
      "questionNumber": 280,
      "type": "single",
      "content": "<p>Question #280</p><p>A company has a Windows-based desktop application that is packaged and deployed to the users' Windows machines. The company recently acquired another company that has employees who primarily use machines with a Linux operating system. The acquiring company has decided to migrate and rehost the Windows-based desktop application to AWS.</p><p><br></p><p>All employees must be authenticated before they use the application. The acquiring company uses Active Directory on premises but wants a simplified way to manage access to the application on AWS for all the employees.</p><p><br></p><p>Which solution will rehost the application on AWS with the LEAST development effort?</p>",
      "options": [
        {
          "label": "A",
          "content": "Set up and provision an Amazon Workspaces virtual desktop for every employee. Implement authentication by using Amazon Cognito identity pools. Instruct employees to run the application from their provisioned Workspaces virtual desktops."
        },
        {
          "label": "B",
          "content": "Create an Auto Scaling group of Windows-based Amazon EC2 instances. Join each EC2 instance to the company&rsquo;s Active Directory domain. Implement authentication by using the Active Directory that is running on premises. Instruct employees to run the application by using a Windows remote desktop."
        },
        {
          "label": "C",
          "content": "Use an Amazon AppStream 2.0 image builder to create an image that includes the application and the required configurations. Provision an AppStream 2.0 On-Demand fleet with dynamic Fleet Auto Scaling policies for running the image. Implement authentication by using AppStream 2.0 user pools. Instruct the employees to access the application by starting browser-based AppStream 2.0 streaming sessions."
        },
        {
          "label": "D",
          "content": "Refactor and containerize the application to run as a web-based application. Run the application in Amazon Elastic Container Service (Amazon ECS) on AWS Fargate with step scaling policies. Implement authentication by using Amazon Cognito user pools. Instruct the employees to run the application from their browsers."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>The correct answer is C. Amazon AppStream 2.0 allows for the streaming of desktop applications through a web browser, which requires minimal to no changes to the existing application. By creating an AppStream 2.0 image with the application and configuring it to run on an On-Demand fleet with Auto Scaling, the company can provide access to the application across different operating systems with the least development effort. Additionally, using AppStream 2.0 user pools for authentication simplifies access management for the AWS environment.</p><p> Option C: Amazon AppStream 2.0</p><p>- Pros: </p><p> &nbsp;- Streams the Windows application to any device (including Linux) via a browser.</p><p> &nbsp;- No need to refactor the application.</p><p> &nbsp;- AppStream 2.0 User Pools provide simple authentication (or can integrate with Active Directory).</p><p> &nbsp;- Dynamic scaling ensures cost efficiency.</p><p>- Cons: None significant for this use case.</p><p>- Effort: Least (no refactoring, minimal setup, supports Linux users seamlessly).</p><p> Option D: Refactor to Web App + ECS/Fargate + Cognito</p><p>- Pros: Modern approach, scalable.</p><p>- Cons: </p><p> &nbsp;- Requires refactoring the Windows desktop app into a web app (high development effort).</p><p> &nbsp;- Not a rehosting solution (this is re-architecting).</p><p>- Effort: Highest (complete rewrite of the application).</p><p> Conclusion</p><p>Option C (Amazon AppStream 2.0) is the best choice because:</p><p>- It rehosts the Windows app without modification.</p><p>- Supports Linux users via browser streaming.</p><p>- Provides simple authentication (User Pools or AD integration).</p><p>- Requires the least development effort.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "d72923c43fa7439a8683ce1287cfccfc",
      "questionNumber": 281,
      "type": "single",
      "content": "<p>Question #281</p><p>A company is collecting a large amount of data from a fleet of IoT devices. Data is stored as Optimized Row Columnar (ORC) files in the Hadoop Distributed File System (HDFS) on a persistent Amazon EMR cluster. The company's data analytics team queries the data by using SQL in Apache Presto deployed on the same EMR cluster. Queries scan large amounts of data, always run for less than 15 minutes, and run only between 5 PM and 10 PM.</p><p><br></p><p>The company is concerned about the high cost associated with the current solution. A solutions architect must propose the most cost-effective solution that will allow SQL data queries.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Store data in Amazon S3. Use Amazon Redshift Spectrum to query data."
        },
        {
          "label": "B",
          "content": "Store data in Amazon S3. Use the AWS Glue Data Catalog and Amazon Athena to query data."
        },
        {
          "label": "C",
          "content": "Store data in EMR File System (EMRFS). Use Presto in Amazon EMR to query data."
        },
        {
          "label": "D",
          "content": "Store data in Amazon Redshift. Use Amazon Redshift to query data."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Store data in Amazon S3. Use the AWS Glue Data Catalog and Amazon Athena to query data.</p><p>This solution will be the most cost-effective for the given scenario for the following reasons:</p><p>Serverless Architecture: Amazon Athena is a serverless query service, meaning you don't pay for any idle resources. Since the queries only run between 5 PM and 10 PM, a serverless solution eliminates the cost of maintaining infrastructure during the 19 hours per day when no queries are running.</p><p>Pay-Per-Query Pricing: With Athena, you only pay for the data scanned during query execution. This is ideal for the intermittent query pattern described (only 5 hours per day), as you're not paying for unused capacity.</p><p>Optimized for ORC Files: Athena works efficiently with columnar file formats like ORC (which the company is already using). ORC files in S3 can be queried directly by Athena, providing good performance while minimizing the amount of data scanned, which further reduces costs.</p><p>AWS Glue Data Catalog Integration: The AWS Glue Data Catalog provides a persistent metadata store that makes the data easily discoverable and queryable without requiring constant cluster resources.</p><p>SQL Interface: Athena provides a standard SQL interface, which meets the requirement for SQL data queries and would be familiar to users currently using Presto.</p><p>The other options have significant cost disadvantages:</p><p>A. Amazon Redshift Spectrum with data in S3 would require maintaining a Redshift cluster even during the 19 hours per day when no queries are running, which would be less cost-effective than the serverless Athena approach.</p><p>D. Storing data in Amazon Redshift would be the most expensive option because:</p><p>It would require provisioning Redshift cluster capacity to handle the large data volumes</p><p>The cluster would be idle for 19 hours per day</p><p>There would be additional costs for loading data from IoT devices into Redshift</p><p>Redshift is optimized for complex analytical queries rather than simple queries that run for less than 15 minutes</p><p>The combination of S3 storage (which is very cost-effective for large data volumes) and Athena's serverless query capability provides the most economical solution for the described workload pattern, while still meeting the requirement for SQL-based data analysis.</p><p>Sources</p><p>Amazon Redshift Spectrum overview - Amazon Redshift (https://docs.aws.amazon.com/redshift/latest/dg/c-spectrum-overview.html)</p><p>When should I use Athena? - Amazon Athena (https://docs.aws.amazon.com/athena/latest/ug/when-should-i-use-ate.html)</p><p>In-place querying - Storage Best Practices for Data and Analytics Applications (https://docs.aws.amazon.com/whitepapers/latest/building-data-lakes/in-place-querying.html)</p><p>Querying your data lake - Amazon Redshift (https://docs.aws.amazon.com/redshift/latest/gsg/data-lake.html)</p><p>Architecture patterns to optimize Amazon Redshift performance at scale | AWS Big Data Blog(https://aws.amazon.com/cn/blogs/big-data/architecture-patterns-to-optimize-amazon-redshift-performance-at-scale/) </p><p>Amazon Redshift deep dive - Data Warehousing on AWS (https://docs.aws.amazon.com/whitepapers/latest/data-warehousing-on-aws/amazon-redshift-deep-dive.html)</p><p>Accelerate Amazon Redshift Data Lake queries with AWS Glue Data Catalog Column Statistics | AWS Big Data Blog (https://aws.amazon.com/cn/blogs/big-data/accelerate-amazon-redshift-data-lake-queries-with-column-level-statistics/)</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "34db74aa8fb2416fb87b296c6043f438",
      "questionNumber": 282,
      "type": "single",
      "content": "<p>Question #282</p><p>A large company recently experienced an unexpected increase in Amazon RDS and Amazon DynamoDB costs. The company needs to increase visibility into details of AWS Billing and Cost Management. There are various accounts associated with AWS Organizations, including many development and production accounts. There is no consistent tagging strategy across the organization, but there are guidelines in place that require all infrastructure to be deployed using AWS CloudFormation with consistent tagging. Management requires cost center numbers and project ID numbers for all existing and future DynamoDB tables and RDS instances.</p><p><br></p><p>Which strategy should the solutions architect provide to meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use Tag Editor to tag existing resources. Create cost allocation tags to define the cost center and project ID and allow 24 hours for tags to propagate to existing resources."
        },
        {
          "label": "B",
          "content": "Use an AWS Config rule to alert the finance team of untagged resources. Create a centralized AWS Lambda based solution to tag untagged RDS databases and DynamoDB resources every hour using a cross-account role."
        },
        {
          "label": "C",
          "content": "Use Tag Editor to tag existing resources. Create cost allocation tags to define the cost center and project ID. Use SCPs to restrict resource creation that do not have the cost center and project ID on the resource."
        },
        {
          "label": "D",
          "content": "Create cost allocation tags to define the cost center and project ID and allow 24 hours for tags to propagate to existing resources. Update existing federated roles to restrict privileges to provision resources that do not include the cost center and project ID on the resource."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>The correct answer is C. The solutions architect should use the Tag Editor to apply tags to existing resources, ensuring that cost allocation tags for the cost center and project ID are in place. Additionally, creating SCPs (Service Control Policies) within AWS Organizations can enforce tagging requirements for all resources created moving forward, ensuring that new DynamoDB tables and RDS instances cannot be provisioned without the necessary tags. This approach not only addresses the immediate need to tag existing resources but also prevents future un-tagged resources from being created.</p><p> Requirements Summary:</p><p>1. Increase visibility into AWS Billing and Cost Management (requires proper tagging).</p><p>2. No consistent tagging strategy, but CloudFormation mandates consistent tagging for new deployments.</p><p>3. Management requires cost center and project ID tags for all existing and future RDS and DynamoDB resources.</p><p>4. Multiple accounts under AWS Organizations (centralized control needed).</p><p> Analysis of Options:</p><p> Option A:</p><p>- Tag Editor can manually tag existing resources (partially meets the requirement).</p><p>- Cost allocation tags help in billing visibility.</p><p>- But: Waiting 24 hours for tag propagation doesn’t enforce future compliance. No mechanism prevents untagged resource creation.</p><p> Option B:</p><p>- AWS Config rule can detect untagged resources (reactive, not proactive).</p><p>- Lambda-based tagging solution (complex, requires maintenance, and may lag in real-time enforcement).</p><p>- Does not prevent untagged resource creation, only reacts afterward.</p><p> Option C:</p><p>- Tag Editor ensures existing resources are tagged.</p><p>- Cost allocation tags enable billing visibility.</p><p>- SCPs (Service Control Policies) enforce compliance proactively by blocking resource creation without required tags.</p><p>- CloudFormation ensures new deployments comply with tagging guidelines.</p><p>- Best meets all requirements: Existing tagging, billing visibility, and future enforcement.</p><p> Option D:</p><p>- Cost allocation tags help with billing.</p><p>- Federated role updates restrict untagged resource creation, but this is less scalable and harder to manage than SCPs.</p><p>- No mention of tagging existing resources (only propagation, which doesn’t guarantee coverage).</p><p> Why Option C is Best:</p><p>- Proactive enforcement via SCPs (blocks non-compliant resource creation).</p><p>- Tag Editor covers existing resources.</p><p>- CloudFormation ensures future deployments comply (since SCPs enforce tagging).</p><p>- Cost allocation tags provide billing visibility.</p><p> Conclusion:</p><p>Option C is the most comprehensive and scalable solution, addressing both existing and future tagging requirements while enforcing compliance via SCPs.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "5626670137eb424fad74fa125210998a",
      "questionNumber": 283,
      "type": "multiple",
      "content": "<p>Question #283</p><p>A company wants to send data from its on-premises systems to Amazon S3 buckets. The company created the S3 buckets in three different accounts. The company must send the data privately without the data traveling across the internet. The company has no existing dedicated connectivity to AWS.</p><p>Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Establish a networking account in the AWS Cloud. Create a private VPC in the networking account. Set up an AWS Direct Connect connection with a private VIF between the on-premises environment and the private VPC."
        },
        {
          "label": "B",
          "content": "Establish a networking account in the AWS Cloud. Create a private VPC in the networking account. Set up an AWS Direct Connect connection with a public VIF between the on-premises environment and the private VPC."
        },
        {
          "label": "C",
          "content": "Create an Amazon S3 interface endpoint in the networking account."
        },
        {
          "label": "D",
          "content": "Create an Amazon S3 gateway endpoint in the networking account."
        },
        {
          "label": "E",
          "content": "Establish a networking account in the AWS Cloud. Create a private VPC in the networking account. Peer VPCs from the &nbsp;accounts that host the S3 buckets with the VPC in the network account</span></p>"
        }
      ],
      "correctAnswer": "AC",
      "explanation": "<p>The combination of A and C meets the requirements for private data transfer to S3 buckets across three different accounts:</p><p><br></p><p>Option A - Establishes the foundational connectivity:</p><p><br></p><p>Private VIF with Direct Connect: Creates a dedicated private connection from on-premises to AWS without using the internet</p><p>Networking account with private VPC: Provides a centralized hub for managing connectivity across multiple accounts</p><p>Private VIF (not public VIF): Ensures traffic remains private and doesn't traverse public networks</p><p>Option C - Enables private S3 access:</p><p><br></p><p>S3 interface endpoint: Creates a private connection to S3 service within the VPC using AWS PrivateLink</p><p>Cross-account access: Interface endpoints can be configured to access S3 buckets in different AWS accounts</p><p>Private connectivity: Traffic from on-premises → Direct Connect → VPC → S3 interface endpoint → S3 buckets remains entirely private</p><p>Why this combination works:</p><p><br></p><p>No internet traversal: Data flows through Direct Connect private VIF to VPC interface endpoint</p><p>Multi-account support: Interface endpoints support cross-account S3 bucket access</p><p>Centralized architecture: Single networking account manages connectivity for all three target accounts</p><p>Why other options don't work:</p><p><br></p><p>Option B (Public VIF): Would expose traffic to public AWS network, not meeting \"private\" requirement</p><p>Option D (Gateway endpoint): Only works within the same account and cannot access S3 buckets in other accounts</p><p>Reference Documents: AWS Direct Connect virtual interfaces - AWS Direct Connect &nbsp;(https://docs.aws.amazon.com/directconnect/latest/UserGuide/Welcome.html)</p><p><br></p><p>Virtual interfaces (VIF) - AWS Direct Connect for Amazon Connect (https://docs.aws.amazon.com/whitepapers/latest/aws-direct-connect-for-amazon-connect/virtual-interfaces-vif.html)</p><p><br></p><p>Sources</p><p>Hybrid network connections - Hybrid Connectivity （https://docs.aws.amazon.com/whitepapers/latest/hybrid-connectivity/hybrid-network-connections.html）</p><p><br></p><p>What is AWS Direct Connect? - AWS Direct Connect （https://docs.aws.amazon.com/directconnect/latest/UserGuide/Welcome.html）</p><p><br></p><p>Public VIF vs Private VIF for accessing S3 for multi-accounts | AWS re:Post （https://repost.aws/questions/QUNetathjRThKw2IMPFB9sCg/public-vif-vs-private-vif-for-accessing-s3-for-multi-accounts）</p><p><br></p><p>Virtual interfaces (VIF) - AWS Direct Connect for Amazon Connect (https://docs.aws.amazon.com/whitepapers/latest/aws-direct-connect-for-amazon-connect/virtual-interfaces-vif.html)</p><p><br></p><p>Best practices for connecting your Veeam on-premises environment to Amazon S3 | AWS Storage Blog (https://aws.amazon.com/blogs/storage/best-practices-for-connecting-your-veeam-on-premises-environment-to-amazon-s3/)</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "d1b73beaf3894913aa8b3b557d40f0fd",
      "questionNumber": 284,
      "type": "single",
      "content": "<p>Question #284</p><p>A company operates quick-service restaurants. The restaurants follow a predictable model with high sales traffic for 4 hours daily. Sales traffic is lower outside of those peak hours.</p><p><br></p><p>The point of sale and management platform is deployed in the AWS Cloud and has a backend that is based on Amazon DynamoDB. The database table uses provisioned throughput mode with 100,000 RCUs and 80,000 WCUs to match known peak resource consumption.</p><p><br></p><p>The company wants to reduce its DynamoDB cost and minimize the operational overhead for the IT staff.</p><p><br></p><p>Which solution meets these requirements MOST cost-effectively?</p>",
      "options": [
        {
          "label": "A",
          "content": "Reduce the provisioned RCUs and WCUs."
        },
        {
          "label": "B",
          "content": "Change the DynamoDB table to use on-demand capacity."
        },
        {
          "label": "C",
          "content": "Enable Dynamo DB auto scaling for the table."
        },
        {
          "label": "D",
          "content": "Purchase 1-year reserved capacity that is sufficient to cover the peak load for 4 hours each day."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Enable DynamoDB auto scaling for the table.</p><p>This solution will meet the requirements most cost-effectively for the following reasons:</p><p>Predictable Traffic Pattern: The restaurant has a predictable model with high sales traffic for 4 hours daily and lower traffic outside those peak hours. This is an ideal scenario for DynamoDB auto scaling, which can automatically adjust capacity based on actual usage patterns.</p><p>Cost Optimization: With auto scaling, the provisioned capacity will automatically scale up during the 4-hour peak periods and scale down during the remaining 20 hours of lower traffic. This means you'll only pay for the capacity you actually need at any given time, rather than constantly paying for peak capacity (100,000 RCUs and 80,000 WCUs) throughout the entire day.</p><p>Minimal Operational Overhead: Once configured, auto scaling requires minimal intervention from the IT staff. The system will automatically adjust capacity based on defined utilization targets, eliminating the need for manual capacity adjustments throughout the day.</p><p>Balance of Performance and Cost: Auto scaling ensures that the application maintains performance during peak hours while optimizing costs during off-peak hours.</p><p>The other options have significant limitations:</p><p>A. Reducing the provisioned RCUs and WCUs would lower costs but could lead to throttling during peak hours, potentially affecting customer experience. This approach doesn't address the variable nature of the workload.</p><p>B. Changing to on-demand capacity could work but would likely be more expensive for this use case. On-demand pricing is generally higher than provisioned capacity pricing for predictable workloads with high utilization during specific periods.</p><p>D. Purchasing 1-year reserved capacity for peak load would be wasteful since the peak load only occurs for 4 hours each day. You would be paying for unused capacity during the remaining 20 hours, which is not cost-effective.</p><p>DynamoDB auto scaling provides the optimal balance between cost savings and operational simplicity for this specific use case with predictable traffic patterns. It automatically adjusts capacity to match the restaurant's daily traffic patterns while minimizing both costs and operational overhead.</p><p>Sources</p><p>Reduce latency and cost in read-heavy applications using Amazon DynamoDB Accelerator | AWS Database Blog （https://aws.amazon.com/cn/blogs/database/reduce-latency-and-cost-in-read-heavy-applications-using-amazon-dynamodb-accelerator/）</p><p>Choose the right throughput strategy for Amazon DynamoDB applications | AWS Database Blog （https://aws.amazon.com/cn/blogs/database/choose-the-right-throughput-strategy-for-amazon-dynamodb-applications/）</p><p>Amazon DynamoDB Reserved Capacity – Fast NoSQL Key-Value Database – Amazon Web Services （https://aws.amazon.com/cn/dynamodb/reserved-capacity/）</p><p>Evaluate your DynamoDB table's capacity mode - Amazon DynamoDB （https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/CostOptimization_TableCapacityMode.html）</p><p>Minimize DynamoDB costs | AWS re:Post （https://repost.aws/knowledge-center/dynamodb-minimize-costs）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "66bf56065abd4c32bcf266360744373d",
      "questionNumber": 285,
      "type": "single",
      "content": "<p>Question #285</p><p>A company hosts a blog post application on AWS using Amazon API Gateway, Amazon DynamoDB, and AWS Lambda. The application currently does not use API keys to authorize requests. The API model is as follows:<br><br></p><p>GET /posts/{postId}: to get post details</p><p>GET /users/{userId}: to get user details</p><p>GET /comments/{commentId}: to get comments details<br><br></p><p>The company has noticed users are actively discussing topics in the comments section, and the company wants to increase user engagement by making the comments appear in real time.<br><br></p><p>Which design should be used to reduce comment latency and improve user experience?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use edge-optimized API with Amazon CloudFront to cache API responses."
        },
        {
          "label": "B",
          "content": "Modify the blog application code to request GET/comments/{commentId} every 10 seconds."
        },
        {
          "label": "C",
          "content": "Use AWS AppSync and leverage WebSockets to deliver comments."
        },
        {
          "label": "D",
          "content": "Change the concurrency limit of the Lambda functions to lower the API response time."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Use AWS AppSync and leverage WebSockets to deliver comments.</p><p>This solution will best reduce comment latency and improve user experience for the following reasons:</p><p>Real-time Communication: WebSockets provide a persistent, bidirectional connection between the client and server, allowing comments to be pushed to users instantly as they are created. This is the only solution that enables true real-time updates without requiring users to take any action.</p><p>Reduced Latency: With WebSockets, new comments are delivered to users immediately when they're posted, eliminating the delay that would occur with traditional request-response patterns. This directly addresses the requirement to \"reduce comment latency.\"</p><p>Enhanced User Experience: Real-time comment updates create a more engaging and interactive experience, as users can see conversations develop naturally without having to refresh the page or wait for periodic updates. This directly addresses the requirement to \"improve user experience.\"</p><p>Efficient Resource Usage: WebSockets are more efficient than polling approaches, as they only transmit data when there's new information, rather than making repeated requests that often return no new data.</p><p>Scalability: AWS AppSync is designed to handle large numbers of concurrent WebSocket connections, making it suitable for a growing blog application with active user discussions.</p><p>The other options have significant limitations:</p><p>A. Using edge-optimized API with CloudFront to cache API responses would actually be counterproductive for real-time comments. Caching would delay the delivery of new comments, as users might see cached (older) versions of the comments section rather than the latest updates.</p><p>B. Polling the comments endpoint every 10 seconds creates unnecessary load on the backend and still introduces a delay of up to 10 seconds before new comments appear. This approach is inefficient and provides a suboptimal user experience compared to real-time updates.</p><p>D. Changing the concurrency limit of Lambda functions might improve the overall throughput of the API but doesn't address the fundamental issue of delivering comments in real time. It would still rely on clients repeatedly requesting data rather than receiving updates as they occur.</p><p>AWS AppSync with WebSockets is the only solution that fundamentally changes the communication pattern to enable true real-time delivery of comments, which is essential for creating the engaging user experience the company is looking for.</p><p>Sources</p><p>Amazon API Gateway - Serverless Applications Lens （https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/amazon-api-gateway.html）</p><p>Amazon ElastiCache vs API gateway edge-optimized endpoint | AWS re:Post （https://repost.aws/questions/QU4xZPFTZ3TASRtyDteJBM7Q/amazon-elasticache-vs-api-gateway-edge-optimized-endpoint）</p><p>Scale lambdas to tens of thousands concurrent runnings | AWS re:Post （https://repost.aws/questions/QUjv5PAE_dQ56EXvEpkJ1WTQ/scale-lambdas-to-tens-of-thousands-concurrent-runnings）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "46a5bb2f4df34a2ebb4930064a87b440",
      "questionNumber": 286,
      "type": "single",
      "content": "<p>Question #286</p><p>A company manages hundreds of AWS accounts centrally in an organization in AWS Organizations. The company recently started to allow product teams to create and manage their own S3 access points in their accounts. The S3 access points can be accessed only within VPCs, not on the internet.<br><br></p><p>What is the MOST operationally efficient way to enforce this requirement?</p>",
      "options": [
        {
          "label": "A",
          "content": "Set the S3 access point resource policy to deny the s3:CreateAccessPoint action unless the s3:AccessPointNetworkOrigin condition key evaluates to VPC."
        },
        {
          "label": "B",
          "content": "Create an SCP at the root level in the organization to deny the s3:CreateAccessPoint action unless the s3:AccessPointNetworkOrigin condition key evaluates to VPC."
        },
        {
          "label": "C",
          "content": "Use AWS CloudFormation StackSets to create a new IAM policy in each AWS account that allows the s3:CreateAccessPoint action only if the s3:AccessPointNetworkOrigin condition key evaluates to VPC."
        },
        {
          "label": "D",
          "content": "Set the S3 bucket policy to deny the s3:CreateAccessPoint action unless the s3:AccessPointNetworkOrigin condition key evaluates to VPC."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Create an SCP at the root level in the organization to deny the s3:CreateAccessPoint action unless the s3:AccessPointNetworkOrigin condition key evaluates to VPC.</p><p>This solution is the MOST operationally efficient way to enforce the requirement that S3 access points can be accessed only within VPCs for the following reasons:</p><p>Centralized Management: With hundreds of AWS accounts to manage, implementing a single Service Control Policy (SCP) at the organization's root level provides a centralized approach to enforce this requirement across all accounts. This eliminates the need to configure and maintain separate policies in each individual account.</p><p>Preventative Control: SCPs act as guardrails that cannot be overridden by account administrators. By denying the creation of access points unless they're VPC-only, this approach prevents non-compliant access points from being created in the first place, rather than detecting them after creation.</p><p>Minimal Maintenance: Once the SCP is deployed at the root level, it automatically applies to all existing and future accounts in the organization without requiring additional configuration or deployment steps as new accounts are added.</p><p>Consistent Enforcement: The SCP ensures uniform enforcement of the VPC-only requirement across all accounts, eliminating potential gaps or inconsistencies that might occur with decentralized approaches.</p><p>The other options have significant limitations:</p><p>A. Setting the S3 access point resource policy would need to be done for each access point individually after it's created. This is not operationally efficient for hundreds of accounts and doesn't prevent the initial creation of non-compliant access points.</p><p>C. Using AWS CloudFormation StackSets to create IAM policies in each account would require more complex setup and ongoing maintenance. While this approach could work, it's less efficient than using an SCP because:</p><p>It requires deploying and maintaining StackSets</p><p>IAM policies can potentially be modified by account administrators</p><p>It adds unnecessary complexity compared to the simpler SCP approach</p><p>Option B provides the most straightforward, centralized, and efficient way to enforce the requirement across the entire organization with minimal operational overhead, making it the best choice for this scenario.</p><p>Sources</p><p>Creating access points restricted to a virtual private cloud - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-points-vpc.html）</p><p>Configuring IAM policies for using access points for directory buckets - Amazon Simple Storage Service（https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-points-directory-buckets-policies.html） </p><p>Access control in Amazon S3 - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-management.html）</p><p>SEC03-BP08 Share resources securely within your organization - AWS Well-Architected Framework (2023-04-10) （https://docs.aws.amazon.com/wellarchitected/2023-04-10/framework/sec_permissions_share_securely.html）</p><p>Controlling access from VPC endpoints with bucket policies - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies-vpc-endpoint.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "b6c40a390f63448ba0d6802dc73e1fb7",
      "questionNumber": 287,
      "type": "single",
      "content": "<p>Question #287</p><p>A solutions architect must update an application environment within AWS Elastic Beanstalk using a blue/green deployment methodology. The solutions architect creates an environment that is identical to the existing application environment and deploys the application to the new environment.<br><br></p><p>What should be done next to complete the update?</p>",
      "options": [
        {
          "label": "A",
          "content": "Redirect to the new environment using Amazon Route 53."
        },
        {
          "label": "B",
          "content": "Select the Swap Environment URLs option."
        },
        {
          "label": "C",
          "content": "Replace the Auto Scaling launch configuration."
        },
        {
          "label": "D",
          "content": "Update the DNS records to point to the green environment."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Select the Swap Environment URLs option.</p><p>This is the correct next step in completing a blue/green deployment in AWS Elastic Beanstalk for the following reasons:</p><p>Seamless Transition: The \"Swap Environment URLs\" feature is specifically designed for blue/green deployments in Elastic Beanstalk. It allows for an instantaneous switch of the CNAME records between the two environments, directing traffic from the old (blue) environment to the new (green) environment without downtime.</p><p>Built-in Functionality: This is a native feature of Elastic Beanstalk that's purpose-built for this exact scenario. It's the standard and recommended approach for completing blue/green deployments within the Elastic Beanstalk service.</p><p>Minimal DNS Propagation: When you swap environment URLs in Elastic Beanstalk, the change happens almost immediately because it's simply swapping the CNAME records within AWS's DNS infrastructure. This results in minimal or no downtime for users.</p><p>Easy Rollback: If any issues are discovered after the swap, you can easily roll back by performing another swap to return traffic to the original environment.</p><p>The other options have significant limitations:</p><p>A. Redirecting using Amazon Route 53 would require additional configuration and wouldn't be the most direct method when using Elastic Beanstalk's built-in functionality. While Route 53 can be used for more complex routing scenarios, it's unnecessary for a standard Elastic Beanstalk blue/green deployment.</p><p>C. Replacing the Auto Scaling launch configuration wouldn't redirect traffic to the new environment. Launch configurations define how new instances are created within an Auto Scaling group, but they don't affect routing of user traffic.</p><p>D. Manually updating DNS records to point to the green environment would be more complex and time-consuming than using the built-in \"Swap Environment URLs\" feature. It would also likely result in longer propagation times, potentially causing inconsistent user experiences during the transition.</p><p>The \"Swap Environment URLs\" option is the most efficient and reliable method to complete a blue/green deployment in AWS Elastic Beanstalk after creating and deploying to a new environment.</p><p>Sources</p><p>Swap the Environment of an Elastic Beanstalk Application - Blue/Green Deployments on AWS （https://docs.aws.amazon.com/whitepapers/latest/blue-green-deployments/swap-the-environment-of-an-elastic-beanstalk-application.html）</p><p>Blue/Green deployments with Elastic Beanstalk - AWS Elastic Beanstalk （https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html）</p><p>Blue/Green deployment | AWS re:Post （https://repost.aws/questions/QU39OiY-naQZ--BOKDcRkzTw/blue-green-deployment）</p><p>Seamless Production Deployment with Elastic Beanstalk | .NET on AWS Blog （https://aws.amazon.com/cn/blogs/dotnet/seamless-production-deployment-with-elastic-beanstalk/）</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "ae1d3226990e4fe895ae831122b180a0",
      "questionNumber": 288,
      "type": "single",
      "content": "<p>Question #288</p><p>A company is building an image service on the web that will allow users to upload and search random photos. At peak usage, up to 10,000 users worldwide will upload their images. They will then overlay text on the uploaded images, which will then be published on the company website.</p><p>Which design should a solutions architect implement?</p>",
      "options": [
        {
          "label": "A",
          "content": "Store the uploaded images in Amazon Elastic File System (Amazon EFS). Send application log information about each image to Amazon CloudWatch Logs. Create a fleet of Amazon EC2 instances that use CloudWatch Logs to determine which images need to be processed. Place processed images in another directory in Amazon EFS. Enable Amazon CloudFront and configure the origin to be the one of the EC2 instances in the fleet."
        },
        {
          "label": "B",
          "content": "Store the uploaded images in an Amazon S3 bucket and configure an S3 bucket event notification to send a message to Amazon Simple Notification Service (Amazon SNS). Create a fleet of Amazon EC2 instances behind an Application Load Balancer (ALB) to pull messages from Amazon SNS to process the images and place them in Amazon Elastic File System (Amazon EFS). Use Amazon CloudWatch metrics for the SNS message volume to scale out EC2 instances. Enable Amazon CloudFront and configure the origin to be the ALB in front of the EC2 instances."
        },
        {
          "label": "C",
          "content": "Store the uploaded images in an Amazon S3 bucket and configure an S3 bucket event notification to send a message to the Amazon Simple Queue Service (Amazon SQS) queue. Create a fleet of Amazon EC2 instances to pull messages from the SQS queue to process the images and place them in another S3 bucket. Use Amazon CloudWatch metrics for queue depth to scale out EC2 instances. Enable Amazon CloudFront and configure the origin to be the S3 bucket that contains the processed images."
        },
        {
          "label": "D",
          "content": "Store the uploaded images on a shared Amazon Elastic Block Store (Amazon EBS) volume mounted to a fleet of Amazon EC2 Spot instances. Create an Amazon DynamoDB table that contains information about each uploaded image and whether it has been processed. Use an Amazon EventBridge rule to scale out EC2 instances. Enable Amazon CloudFront and configure the origin to reference an Elastic Load Balancer in front of the fleet of EC2 instances."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>The correct answer is C. This solution involves using Amazon S3 for storing uploaded images, which is a scalable and durable storage solution. By configuring S3 bucket event notifications to send messages to an SQS queue, the system can decouple image processing from the upload process. A fleet of EC2 instances can then process the images as they are received from the SQS queue. Using CloudWatch metrics to monitor the SQS queue depth allows for automatic scaling of EC2 instances to handle the workload. Finally, enabling Amazon CloudFront with the S3 bucket of processed images as the origin ensures fast content delivery to users worldwide.</p><p>1. Uploaded images stored in Amazon S3: &nbsp;</p><p> &nbsp; - S3 is highly scalable, durable, and cost-effective for storing large volumes of images. &nbsp;</p><p> &nbsp; - It supports event notifications when new objects are uploaded, triggering downstream processing.</p><p>2. S3 event notification → Amazon SQS: &nbsp;</p><p> &nbsp; - SQS decouples the upload event from processing, ensuring reliability and fault tolerance. &nbsp;</p><p> &nbsp; - If processing fails, messages remain in the queue for retry.</p><p>3. EC2 instances process messages from SQS: &nbsp;</p><p> &nbsp; - Workers pull messages from SQS, process images (overlaying text), and store results in another S3 bucket. &nbsp;</p><p> &nbsp; - Auto-scaling based on queue depth (CloudWatch metrics) ensures efficient scaling during peak loads.</p><p>4. Processed images in another S3 bucket + CloudFront: &nbsp;</p><p> &nbsp; - S3 is ideal for serving static content (processed images). &nbsp;</p><p> &nbsp; - CloudFront (CDN) improves global access speed by caching images at edge locations. &nbsp;</p><p> Why the other options are suboptimal:</p><p>- A: Uses EFS for storage, which is expensive for large-scale object storage (better for shared file systems). Also, relying on CloudWatch Logs for processing logic is inefficient. &nbsp;</p><p>- B: Uses SNS (pub/sub) instead of SQS (queue), which lacks message retention if workers fail. Also, EFS is unnecessary for processed images (S3 is better). &nbsp;</p><p>- D: Uses shared EBS, which doesn’t scale well globally and introduces single-point failure risks. Spot Instances are risky for critical workloads, and DynamoDB adds unnecessary complexity.</p><p> Key AWS Services in C:</p><p>✅ Amazon S3 (storage) &nbsp;</p><p>✅ SQS (decoupled processing) &nbsp;</p><p>✅ EC2 + Auto Scaling (processing) &nbsp;</p><p>✅ CloudFront (fast global delivery) &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "ce95ce5ba7a6468094f47acf22682578",
      "questionNumber": 289,
      "type": "single",
      "content": "<p>Question #289</p><p>A company has deployed its database on an Amazon RDS for MySQL DB instance in the us-east-1 Region. The company needs to make its data available to customers in Europe. The customers in Europe must have access to the same data as customers in the United States (US) and will not tolerate high application latency or stale data. The customers in Europe and the customers in the US need to write to the database. Both groups of customers need to see updates from the other group in real time.<br><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an Amazon Aurora MySQL replica of the RDS for MySQL DB instance. Pause application writes to the RDS DB instance. Promote the Aurora Replica to a standalone DB cluster. Reconfigure the application to use the Aurora database and resume writes. Add eu-west-1 as a secondary Region to the DB cluster. Enable write forwarding on the DB cluster. Deploy the application in eu-west-1. Configure the application to use the Aurora MySQL endpoint in eu-west-1."
        },
        {
          "label": "B",
          "content": "Add a cross-Region replica in eu-west-1 for the RDS for MySQL DB instance. Configure the replica to replicate write queries back to the primary DB instance. Deploy the application in eu-west-1. Configure the application to use the RDS for MySQL endpoint in eu-west-1."
        },
        {
          "label": "C",
          "content": "Copy the most recent snapshot from the RDS for MySQL DB instance to eu-west-1. Create a new RDS for MySQL DB instance in eu-west-1 from the snapshot. Configure MySQL logical replication from us-east-1 to eu-west-1. Enable write forwarding on the DB cluster. Deploy the application in eu-west-1. Configure the application to use the RDS for MySQL endpoint in eu-west-1."
        },
        {
          "label": "D",
          "content": "Convert the RDS for MySQL DB instance to an Amazon Aurora MySQL DB cluster. Add eu-west-1 as a secondary Region to the DB cluster. Enable write forwarding on the DB cluster. Deploy the application in eu-west-1. Configure the application to use the Aurora MySQL endpoint in eu-west-1."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p style=\"text-align: start;\">Option A:</p><p style=\"text-align: start;\">Migrate from RDS MySQL to Aurora: First, create an Amazon Aurora MySQL replica for the existing RDS for MySQL instance in us-east-1 (leveraging the compatibility between Aurora and MySQL). After pausing write operations to the original RDS instance, promote the Aurora replica to a standalone cluster—this is the standard zero-data-loss process for migrating RDS MySQL to Aurora.</p><p style=\"text-align: start;\">Configure cross-Region Global Database: Add eu-west-1 as a secondary region to the Aurora cluster and enable the \"write forwarding\" feature. Applications in the European region can write to the local Aurora endpoint, and requests will be automatically forwarded to the primary region (us-east-1) for processing. Data is synchronized back to the secondary region in real time, ensuring both European and American customers can see each other's updates instantly. Meanwhile, the local endpoint significantly reduces access latency for European customers.</p><p style=\"text-align: start;\">Option B: Cross-Region replicas of RDS for MySQL are read-only and cannot accept write requests, nor do they support forwarding write queries back to the primary instance. This fails to meet the requirement of \"European customers being able to write.\"</p><p style=\"text-align: start;\">Option C: RDS for MySQL does not support the \"write forwarding\" feature, and logical replication only enables one-way synchronization. It cannot achieve bidirectional writing and real-time updates between European and American customers.</p><p style=\"text-align: start;\">Option D: RDS for MySQL cannot be directly converted to an Aurora cluster. The migration requires the process of \"creating an Aurora replica + promoting it to a cluster,\" so the first step of this option does not align with the actual usage logic of AWS services.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "c31b22c8db2645c68fec50e1ab736dd6",
      "questionNumber": 290,
      "type": "single",
      "content": "<p>Question #290</p><p>A company is serving files to its customers through an SFTP server that is accessible over the internet. The SFTP server is running on a single Amazon EC2 instance with an Elastic IP address attached. Customers connect to the SFTP server through its Elastic IP address and use SSH for authentication. The EC2 instance also has an attached security group that allows access from all customer IP addresses. </p><p><br></p><p>A solutions architect must implement a solution to improve availability, minimize the complexity of infrastructure management, and minimize the disruption to customers who access files. The solution must not change the way customers connect.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Disassociate the Elastic IP address from the EC2 instance. Create an Amazon S3 bucket to be used for SFTP file hosting. Create an AWS Transfer Family server. Configure the Transfer Family server with a publicly accessible endpoint. Associate the SFTP Elastic IP address with the new endpoint. Point the Transfer Family server to the S3 bucket. Sync all files from the SFTP server to the S3 bucket."
        },
        {
          "label": "B",
          "content": "Disassociate the Elastic IP address from the EC2 instance. Create an Amazon S3 bucket to be used for SFTP file hosting. Create an AWS Transfer Family server. Configure the Transfer Family server with a VPC-hosted, internet-facing endpoint. Associate the SFTP Elastic IP address with the new endpoint. Attach the security group with customer IP addresses to the new endpoint. Point the Transfer Family server to the S3 bucket. Sync all files from the SFTP server to the S3 bucket."
        },
        {
          "label": "C",
          "content": "Disassociate the Elastic IP address from the EC2 instance. Create a new Amazon Elastic File System (Amazon EFS) file system to be used for SFTP file hosting. Create an AWS Fargate task definition to run an SFTP server. Specify the EFS file system as a mount in the task definition. Create a Fargate service by using the task definition, and place a Network Load Balancer (NLB) in front of the service. When configuring the service, attach the security group with customer IP addresses to the tasks that run the SFTP server. Associate the Elastic IP address with the NLB. Sync all files from the SFTP server to the S3 bucket."
        },
        {
          "label": "D",
          "content": "Disassociate the Elastic IP address from the EC2 instance. Create a multi-attach Amazon Elastic Block Store (Amazon EBS) volume to be used for SFTP file hosting. Create a Network Load Balancer (NLB) with the Elastic IP address attached. Create an Auto Scaling group with EC2 instances that run an SFTP server. Define in the Auto Scaling group that instances that are launched should attach the new multi-attach EBS volume. Configure the Auto Scaling group to automatically add instances behind the NLB. Configure the Auto Scaling group to use the security group that allows customer IP addresses for the EC2 instances that the Auto Scaling group launches. Sync all files from the SFTP server to the new multi-attach EBS volume."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>The correct answer is B. By disassociating the Elastic IP from the EC2 instance and creating an AWS Transfer Family server with a VPC-hosted, internet-facing endpoint, the architect can maintain the same IP address for customer connections while improving availability and reducing management complexity. The security group can be attached to the new endpoint to maintain the existing access controls. Pointing the Transfer Family server to an S3 bucket allows for scalable file storage and synchronization from the existing SFTP server.</p><p> Requirements:</p><p>1. Improve availability: The current single EC2 instance is a single point of failure. The solution must provide high availability.</p><p>2. Minimize infrastructure management complexity: Moving to a managed service (like AWS Transfer Family) reduces operational overhead.</p><p>3. Minimize disruption to customers: Customers must continue connecting the same way (using the same Elastic IP and SSH authentication).</p><p>4. No change to customer connection method: The Elastic IP and security group rules must remain intact.</p><p> Why Option B?</p><p>- AWS Transfer Family is a managed SFTP service that provides high availability and scalability without managing servers.</p><p>- By configuring a VPC-hosted, internet-facing endpoint, the Transfer Family server can reuse the existing Elastic IP and security group (allowing customer IPs), ensuring no changes for customers.</p><p>- Amazon S3 is a durable and highly available storage backend for the files.</p><p>- The solution syncs files from the old SFTP server to S3, ensuring continuity.</p><p> Why Not Other Options?</p><p>- Option A: Uses a publicly accessible endpoint for Transfer Family, which doesn’t allow attaching the existing security group (required for customer IP restrictions).</p><p>- Option C: Uses Fargate + EFS + NLB, which introduces unnecessary complexity (managing containers, load balancers, and file sync) compared to the managed AWS Transfer Family.</p><p>- Option D: Uses multi-attach EBS + Auto Scaling + NLB, which is overly complex and doesn’t leverage managed services effectively.</p><p> Key Points:</p><p>- AWS Transfer Family is the best-managed solution for SFTP with built-in HA.</p><p>- Reusing the Elastic IP and security group ensures no customer disruption.</p><p>- S3 is the most scalable and durable storage option.</p><p>Thus, B is the correct answer. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "3992c45fb4d44dee98199288e5a1d80d",
      "questionNumber": 291,
      "type": "single",
      "content": "<p>Question #291</p><p>A company ingests and processes streaming market data. The data rate is constant. A nightly process that calculates aggregate statistics takes 4 hours to complete. The statistical analysis is not critical to the business, and data points are processed during the next iteration if a particular run fails.</p><p><br></p><p>The current architecture uses a pool of Amazon EC2 Reserved Instances with 1-year reservations. These EC2 instances run full time to ingest and store the streaming data in attached Amazon Elastic Block Store (Amazon EBS) volumes. A scheduled script launches EC2 On-Demand Instances each night to perform the nightly processing. The instances access the stored data from NFS shares on the ingestion servers. The script terminates the instances when the processing is complete.</p><p><br></p><p>The Reserved Instance reservations are expiring. The company needs to determine whether to purchase new reservations or implement a new design.</p><p><br></p><p>Which solution will meet these requirements MOST cost-effectively?</p>",
      "options": [
        {
          "label": "A",
          "content": "Update the ingestion process to use Amazon Kinesis Data Firehose to save data to Amazon S3. Use a scheduled script to launch a fleet of EC2 On-Demand Instances each night to perform the batch processing of the S3 data. Configure the script to terminate the instances when the processing is complete."
        },
        {
          "label": "B",
          "content": "Update the ingestion process to use Amazon Kinesis Data Firehose to save data to Amazon S3. Use AWS Batch with Spot Instances to perform nightly processing with a maximum Spot price that is 50% of the On-Demand price."
        },
        {
          "label": "C",
          "content": "Update the ingestion process to use a fleet of EC2 Reserved Instances with 3-year reservations behind a Network Load Balancer. Use AWS Batch with Spot Instances to perform nightly processing with a maximum Spot price that is 50% of the On-Demand price."
        },
        {
          "label": "D",
          "content": "Update the ingestion process to use Amazon Kinesis Data Firehose to save data to Amazon Redshift. Use Amazon EventBridge to schedule an AWS Lambda function to run nightly to query Amazon Redshift to generate the daily statistics."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>The correct answer is B. Using Amazon Kinesis Data Firehose to save data to Amazon S3 is a cost-effective way to handle streaming data. By using AWS Batch with Spot Instances for the nightly processing, the company can take advantage of lower Spot Instance prices while maintaining the flexibility to handle the processing if it fails. Setting a maximum Spot price that is 50% of the On-Demand price provides a buffer against market price fluctuations. This approach is more cost-effective than maintaining a fleet of Reserved Instances or using only On-Demand Instances, as it optimizes for both storage and compute costs.</p><p>Update the ingestion process to use Amazon Kinesis Data Firehose to save data to Amazon S3. Use AWS Batch with Spot Instances to perform nightly processing with a maximum Spot price that is 50% of the On-Demand price. &nbsp;</p><p> Why? &nbsp;</p><p>1. Kinesis Data Firehose + S3: &nbsp;</p><p> &nbsp; - Replacing the EC2 ingestion servers with Kinesis Data Firehose eliminates the need for Reserved Instances, reducing costs significantly. &nbsp;</p><p> &nbsp; - Firehose automatically streams data to Amazon S3, which is highly durable, scalable, and cost-efficient for storage. &nbsp;</p><p>2. AWS Batch with Spot Instances: &nbsp;</p><p> &nbsp; - Since the nightly processing is non-critical and can be retried, Spot Instances (at 50% of On-Demand price) are ideal for cost savings. &nbsp;</p><p> &nbsp; - AWS Batch simplifies job scheduling and automatically manages Spot Instance fleets. &nbsp;</p><p>3. Cost Optimization: &nbsp;</p><p> &nbsp; - No upfront Reserved Instance commitments. &nbsp;</p><p> &nbsp; - Spot Instances provide the lowest compute cost for batch workloads. &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- Option A: Uses On-Demand Instances (more expensive than Spot). &nbsp;</p><p>- Option C: Still relies on Reserved Instances (unnecessary cost since Firehose is serverless). &nbsp;</p><p>- Option D: Amazon Redshift is overkill for this use case (expensive for just nightly batch processing). &nbsp;</p><p> Conclusion: &nbsp;</p><p>Option B is the most cost-effective, leveraging serverless ingestion (Firehose + S3) and cheap compute (Spot Instances via AWS Batch). &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "386885b95ddf4b8daecacebbb14f765b",
      "questionNumber": 292,
      "type": "single",
      "content": "<p>Question #292</p><p>A company needs to migrate an on-premises SFTP site to AWS. The SFTP site currently runs on a Linux VM. Uploaded files are made available to downstream applications through an NFS share. </p><p><br></p><p>As part of the migration to AWS, a solutions architect must implement high availability. The solution must provide external vendors with a set of static public IP addresses that the vendors can allow. The company has set up an AWS Direct Connect connection between its on-premises data center and its VPC.</p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an AWS Transfer Family server. Configure an internet-facing VPC endpoint for the Transfer Family server. Specify an Elastic IP address for each subnet. Configure the Transfer Family server to place files into an Amazon Elastic File System (Amazon EFS) file system that is deployed across multiple Availability Zones. Modify the configuration on the downstream applications that access the existing NFS share to mount the EFS endpoint instead."
        },
        {
          "label": "B",
          "content": "Create an AWS Transfer Family server. Configure a publicly accessible endpoint for the Transfer Family server. Configure the Transfer Family server to place files into an Amazon Elastic File System (Amazon EFS) file system that is deployed across multiple Availability Zones. Modify the configuration on the downstream applications that access the existing NFS share to mount the EFS endpoint instead."
        },
        {
          "label": "C",
          "content": "Use AWS Application Migration Service to migrate the existing Linux VM to an Amazon EC2 instance. Assign an Elastic IP address to the EC2 instance. Mount an Amazon Elastic File System (Amazon EFS) file system to the EC2 instance. Configure the SFTP server to place files in the EFS file system. Modify the configuration on the downstream applications that access the existing NFS share to mount the EFS endpoint instead."
        },
        {
          "label": "D",
          "content": "Use AWS Application Migration Service to migrate the existing Linux VM to an AWS Transfer Family server. Configure a publicly accessible endpoint for the Transfer Family server. Configure the Transfer Family server to place files into an Amazon FSx for Lustre file system that is deployed across multiple Availability Zones. Modify the configuration on the downstream applications that access the existing NFS share to mount the FSx for Lustre endpoint instead."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>The correct answer is A. By creating an AWS Transfer Family server and configuring it with an internet-facing VPC endpoint, the company can provide a secure and scalable SFTP solution. Assigning an Elastic IP address to each subnet ensures that external vendors have a set of static public IP addresses to allow. Using Amazon EFS for file storage provides high availability and is accessible across multiple Availability Zones. This approach requires minimal operational overhead as it leverages managed services and does not require managing the underlying EC2 instances or network infrastructure.</p><p>The question outlines the following requirements: &nbsp;</p><p>1. Migrate an on-premises SFTP site to AWS with high availability. &nbsp;</p><p>2. Provide external vendors with static public IP addresses (Elastic IPs) for allowlisting. &nbsp;</p><p>3. Use AWS Direct Connect for connectivity between on-premises and AWS. &nbsp;</p><p>4. Replace the NFS share with a highly available AWS storage solution accessible by downstream applications. &nbsp;</p><p>5. Minimize operational overhead. &nbsp;</p><p> Why Option A is Correct: &nbsp;</p><p>- AWS Transfer Family is a managed SFTP service that eliminates the need to manage EC2 instances. &nbsp;</p><p>- Internet-facing VPC endpoint allows external vendors to connect via static Elastic IPs (required for allowlisting). &nbsp;</p><p>- Amazon EFS is a fully managed, multi-AZ file system that replaces the NFS share and provides high availability. &nbsp;</p><p>- Least operational overhead because AWS manages the SFTP server (Transfer Family) and EFS, reducing maintenance. &nbsp;</p><p> Why Other Options Are Incorrect: &nbsp;</p><p>- Option B: Lacks Elastic IPs, which are required for vendors to allowlist static IPs. &nbsp;</p><p>- Option C: Uses EC2 instead of AWS Transfer Family, increasing operational overhead (manual patching, scaling, HA setup). &nbsp;</p><p>- Option D: Uses FSx for Lustre, which is not a direct replacement for NFS (EFS is better suited). Also, migrating a VM to Transfer Family is not a standard approach. &nbsp;</p><p> Conclusion: &nbsp;</p><p>Option A is the best solution because it uses AWS Transfer Family (managed SFTP) with Elastic IPs and EFS (highly available NFS replacement), meeting all requirements with minimal operational effort. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "74ebc85d78c54c1a9736d569adc248ec",
      "questionNumber": 293,
      "type": "single",
      "content": "<p>Question #293</p><p>A solutions architect has an operational workload deployed on Amazon EC2 instances in an Auto Scaling group. The VPC architecture spans two Availability Zones (AZ) with a subnet in each that the Auto Scaling group is targeting. The VPC is connected to an on-premises environment and connectivity cannot be interrupted. The maximum size of the Auto Scaling group is 20 instances in service. The VPC IPv4 addressing is as follows:</p><p><br></p><p>VPC CIDR: 10.0.0.0/23</p><p><br></p><p>AZ1 subnet CIDR: 10.0.0.0/24</p><p><br></p><p>AZ2 subnet CIDR: 10.0.1.0/24</p><p><br></p><p>Since deployment, a third AZ has become available in the Region. The solutions architect wants to adopt the new AZ without adding additional IPv4 address space and without service downtime.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Update the Auto Scaling group to use the AZ2 subnet only. Delete and re-create the AZ1 subnet using half the previous address space. Adjust the Auto Scaling group to also use the new AZ1 subnet. When the instances are healthy, adjust the Auto Scaling group to use the AZ1 subnet only. Remove the current AZ2 subnet. Create a new AZ2 subnet using the second half of the address space from the original AZ1 subnet. Create a new AZ3 subnet using half the original AZ2 subnet address space, then update the Auto Scaling group to target all three new subnets."
        },
        {
          "label": "B",
          "content": "Terminate the EC2 instances in the AZ1 subnet. Delete and re-create the AZ1 subnet using half the address space. Update the Auto Scaling group to use this new subnet. Repeat this for the second AZ. Define a new subnet in AZ3, then update the Auto Scaling group to target all three new subnets."
        },
        {
          "label": "C",
          "content": "Create a new VPC with the same IPv4 address space and define three subnets, with one for each AZ. Update the existing Auto Scaling group to target the new subnets in the new VPC."
        },
        {
          "label": "D",
          "content": "Update the Auto Scaling group to use the AZ2 subnet only. Update the AZ1 subnet to have half the previous address space. Adjust the Auto Scaling group to also use the AZ1 subnet again. When the instances are healthy, adjust the Auto Scaling group to use the AZ1 subnet only. Update the current AZ2 subnet and assign the second half of the address space from the original AZ1 subnet. Create a new AZ3 subnet using half the original AZ2 subnet address space, then update the Auto Scaling group to target all three new subnets."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>The correct answer is A. This solution allows the architect to adopt the new AZ without needing additional IPv4 address space or causing downtime. By first using the AZ2 subnet, modifying the AZ1 subnet to half its size, and then reintroducing it, the architect can create space for a new subnet in the AZ3. This approach maintains connectivity and service availability while efficiently utilizing the existing address space to accommodate the new AZ.</p><p>The requirements are:</p><p>1. Adopt a third AZ without adding additional IPv4 address space.</p><p>2. No service downtime (connectivity cannot be interrupted).</p><p>3. Maximize address space usage within the existing VPC CIDR (`10.0.0.0/23`).</p><p> Why A is correct:</p><p>- Step 1: Shift all instances to AZ2 (using `10.0.1.0/24`) to free up AZ1 (`10.0.0.0/24`).</p><p>- Step 2: Delete and re-create AZ1 subnet with half the original space (`10.0.0.0/25`), then adjust the Auto Scaling group to include it again.</p><p>- Step 3: Once instances are healthy, shift all traffic to AZ1 (`10.0.0.0/25`) and free up AZ2 (`10.0.1.0/24`).</p><p>- Step 4: Reconfigure AZ2 subnet to use the second half of the original AZ1 space (`10.0.0.128/25`).</p><p>- Step 5: Create a new AZ3 subnet using half of the original AZ2 space (`10.0.1.0/25`).</p><p>- Step 6: Update the Auto Scaling group to use all three subnets (`10.0.0.0/25`, `10.0.0.128/25`, and `10.0.1.0/25`).</p><p>This approach:</p><p>- Avoids downtime by gradually migrating instances.</p><p>- Uses the same VPC CIDR without expansion.</p><p>- Evenly distributes addresses across three AZs.</p><p> Why D is incorrect:</p><p>- It suggests updating the AZ1 subnet to half its size without deleting it first, which is not possible in AWS. Subnet CIDRs cannot be modified after creation; they must be deleted and recreated.</p><p> Why B and C are incorrect:</p><p>- B: Terminating instances causes downtime.</p><p>- C: Creating a new VPC would require reconfiguring connectivity (VPN/Direct Connect), causing interruption.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "a53ca2ad295349e880198e68d46a7721",
      "questionNumber": 294,
      "type": "single",
      "content": "<p>Question #294</p><p>A company uses an organization in AWS Organizations to manage the company's AWS accounts. The company uses AWS CloudFormation to deploy all infrastructure. A finance team wants to build a chargeback model. The finance team asked each business unit to tag resources by using a predefined list of project values.</p><p><br></p><p>When the finance team used the AWS Cost and Usage Report in AWS Cost Explorer and filtered based on project, the team noticed noncompliant project values. The company wants to enforce the use of project tags for new resources.</p><p><br></p><p>Which solution will meet these requirements with the LEAST effort?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a tag policy that contains the allowed project tag values in the organization&#39;s management account. Create an SCP that denies the cloudformation:CreateStack API operation unless a project tag is added. Attach the SCP to each OU."
        },
        {
          "label": "B",
          "content": "Create a tag policy that contains the allowed project tag values in each OU. Create an SCP that denies the cloudformation:CreateStack API operation unless a project tag is added. Attach the SCP to each OU."
        },
        {
          "label": "C",
          "content": "Create a tag policy that contains the allowed project tag values in the AWS management account. Create an IAM policy that denies the cloudformation:CreateStack API operation unless a project tag is added. Assign the policy to each user."
        },
        {
          "label": "D",
          "content": "Use AWS Service Catalog to manage the CloudFormation stacks as products. Use a TagOptions library to control project tag values. Share the portfolio with all OUs that are in the organization."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>The correct answer is A. By creating a tag policy in the organization's management account, the company can define the allowed project tag values at the organizational level. Then, creating an SCP (Service Control Policy) that denies the creation of CloudFormation stacks without a project tag ensures compliance across all accounts within the organization. Attaching the SCP to each OU (Organizational Unit) simplifies the enforcement process and requires minimal effort compared to creating policies in each individual account or OU.</p><p>The requirements are: &nbsp;</p><p>1. Enforce the use of project tags for new resources. &nbsp;</p><p>2. Ensure only predefined project tag values are used. &nbsp;</p><p>3. Minimize effort in implementation. &nbsp;</p><p>Option A is the best solution because: &nbsp;</p><p>- Tag Policy (AWS Organizations feature) enforces allowed tag values across all accounts in the organization. &nbsp;</p><p>- Service Control Policy (SCP) prevents CloudFormation stack creation unless the required `project` tag is included. &nbsp;</p><p>- SCP applies to all OUs, ensuring consistent enforcement without per-account or per-user configuration. &nbsp;</p><p> Why not the other options? &nbsp;</p><p>- B: Tag policies are set at the organization level, not per OU. This approach is redundant and less efficient. &nbsp;</p><p>- C: IAM policies require manual assignment to each user, which is more effort than using SCPs. &nbsp;</p><p>- D: AWS Service Catalog is a valid approach but requires more setup effort (creating portfolios, products, and sharing them) compared to tag policies + SCPs. &nbsp;</p><p> Conclusion: &nbsp;</p><p>A is the most efficient solution because it leverages AWS Organizations features (Tag Policies + SCPs) to enforce tagging compliance at scale. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "44f010e322b14b63a96637ad1248f908",
      "questionNumber": 295,
      "type": "single",
      "content": "<p>Question #295</p><p>An application is deployed on Amazon EC2 instances that run in an Auto Scaling group. The Auto Scaling group configuration uses only one type of instance. <br><br>CPU and memory utilization metrics show that the instances are underutilized. A solutions architect needs to implement a solution to permanently reduce the EC2 cost and increase the utilization.<br><br></p><p>Which solution will meet these requirements with the LEAST number of configuration changes in the future?</p>",
      "options": [
        {
          "label": "A",
          "content": "List instance types that have properties that are similar to the properties that the current instances have. Modify the Auto Scaling group&#39;s launch template configuration to use multiple instance types from the list."
        },
        {
          "label": "B",
          "content": "Use the information about the application&#39;s CPU and memory utilization to select an instance type that matches the requirements. Modify the Auto Scaling group&#39;s configuration by adding the new instance type. Remove the current instance type from the configuration."
        },
        {
          "label": "C",
          "content": "Use the information about the application&#39;s CPU and memory utilization to specify CPU and memory requirements in a new revision of the Auto Scaling group&#39;s launch template. Remove the current instance type from the configuration."
        },
        {
          "label": "D",
          "content": "Create a script that selects the appropriate instance types from the AWS Price List Bulk API. Use the selected instance types to create a new revision of the Auto Scaling group&#39;s launch template."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Use the information about the application's CPU and memory utilization to specify CPU and memory requirements in a new revision of the Auto Scaling group's launch template. Remove the current instance type from the configuration.</p><p>This solution will meet the requirements with the LEAST number of configuration changes in the future for the following reasons:</p><p>Automatic Instance Selection: By specifying CPU and memory requirements rather than specific instance types, you allow AWS to automatically select the most cost-effective instances that meet your application's needs. This approach leverages AWS's built-in instance selection capabilities.</p><p>Future-Proofing: As AWS introduces new instance types with better price-performance ratios, your Auto Scaling group will automatically be able to use these new instances without requiring any configuration changes. The system will continuously select the most cost-effective option that meets your specified requirements.</p><p>Adaptability to Price Changes: When instance pricing changes over time, the Auto Scaling group will automatically adjust to use the most cost-effective instances that meet your CPU and memory requirements without requiring manual updates.</p><p>Right-Sizing Based on Actual Usage: Using the actual CPU and memory utilization data ensures that you're selecting instance sizes that match your application's real needs, addressing the underutilization issue directly.</p><p>Minimal Maintenance: Once configured, this solution requires virtually no ongoing maintenance or updates to continue optimizing costs as AWS's instance offerings evolve.</p><p>Option D (creating a script that selects instance types from the AWS Price List Bulk API) would require significantly more configuration changes in the future because:</p><p>The script would need to be regularly updated and maintained as AWS introduces new instance types or changes pricing.</p><p>Each time the script identifies better instance options, you would need to create a new revision of the launch template.</p><p>This approach introduces additional complexity and potential points of failure with the custom script.</p><p>It requires ongoing monitoring and manual intervention to ensure the script continues to function correctly.</p><p>By specifying CPU and memory requirements directly in the launch template, you create a more automated, self-maintaining solution that will continue to optimize costs with minimal future configuration changes, making Option C the best answer.</p><p>Sources</p><p>EC2 Instance upgrade or auto scale? | AWS re:Post （https://repost.aws/questions/QUKa8gFPuzTs2kC31vM5OiKA/ec2-instance-upgrade-or-auto-scale）</p><p>Migrate your Auto Scaling groups to launch templates - Amazon EC2 Auto Scaling （https://docs.aws.amazon.com/autoscaling/ec2/userguide/migrate-to-launch-templates.html）</p><p>Autoscaling group cannot update capacity despite sufficient service quota being available. | AWS re:Post （https://repost.aws/questions/QUUx0Nl_XBTdiHpXthkeJvgA/autoscaling-group-cannot-update-capacity-despite-sufficient-service-quota-being-available）</p><p>COST09-BP03 Supply resources dynamically - Cost Optimization Pillar （https://docs.aws.amazon.com/wellarchitected/latest/cost-optimization-pillar/cost_manage_demand_resources_dynamic.html）</p><p>Use multiple launch templates - Amazon EC2 Auto Scaling （https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-mixed-instances-groups-launch-template-overrides.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "ca285e31483a453cb4271219f6020c3f",
      "questionNumber": 296,
      "type": "single",
      "content": "Question #296<p>A company implements a containerized application by using Amazon Elastic Container Service (Amazon ECS) and Amazon API Gateway. The application data is stored in Amazon Aurora databases and Amazon DynamoDB databases. The company automates infrastructure provisioning by using AWS CloudFormation. The company automates application deployment by using AWS CodePipeline.<br><br></p><p>A solutions architect needs to implement a disaster recovery (DR) strategy that meets an RPO of 2 hours and an RTO of 4 hours.<br><br></p><p>Which solution will meet these requirements MOST cost-effectively?</p>",
      "options": [
        {
          "label": "A",
          "content": "Set up an Aurora global database and DynamoDB global tables to replicate the databases to a secondary AWS Region. In the primary Region and in the secondary Region, configure an API Gateway API with a Regional endpoint. Implement Amazon CloudFront with origin failover to route traffic to the secondary Region during a DR scenario."
        },
        {
          "label": "B",
          "content": "Use AWS Database Migration Service (AWS DMS), Amazon EventBridge, and AWS Lambda to replicate the Aurora databases to a secondary AWS Region. Use DynamoDB Streams, EventBridge, and Lambda to replicate the DynamoDB databases to the secondary Region. In the primary Region and in the secondary Region, configure an API Gateway API with a Regional endpoint. Implement Amazon Route 53 failover routing to switch traffic from the primary Region to the secondary Region."
        },
        {
          "label": "C",
          "content": "Use AWS Backup to create backups of the Aurora databases and the DynamoDB databases in a secondary AWS Region. In the primary Region and in the secondary Region, configure an API Gateway API with a Regional endpoint. Implement Amazon Route 53 failover routing to switch traffic from the primary Region to the secondary Region."
        },
        {
          "label": "D",
          "content": "Set up an Aurora global database and DynamoDB global tables to replicate the databases to a secondary AWS Region. In the primary Region and in the secondary Region, configure an API Gateway API with a Regional endpoint. Implement Amazon Route 53 failover routing to switch traffic from the primary Region to the secondary Region."
        }
      ],
      "correctAnswer": "C",
      "explanation": "Correct C. https://docs.aws.amazon.com/aws-backup/latest/devguide/cross-region-backup.html",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "9c403eae24844e7e9dfdd96f25a2bf15",
      "questionNumber": 297,
      "type": "single",
      "content": "<p>Question #297</p><p>A company has a complex web application that leverages Amazon CloudFront for global scalability and performance. Over time, users report that the web application is slowing down. <br><br>The company's operations team reports that the CloudFront cache hit ratio has been dropping steadily. The cache metrics report indicates that query strings on some URLs are inconsistently ordered and are specified sometimes in mixed-case letters and sometimes in lowercase letters.<br><br></p><p>Which set of actions should the solutions architect take to increase the cache hit ratio as quickly as possible?</p>",
      "options": [
        {
          "label": "A",
          "content": "Deploy a Lambda@Edge function to sort parameters by name and force them to be lowercase. Select the CloudFront viewer request trigger to invoke the function."
        },
        {
          "label": "B",
          "content": "Update the CloudFront distribution to disable caching based on query string parameters."
        },
        {
          "label": "C",
          "content": "Deploy a reverse proxy after the load balancer to post-process the emitted URLs in the application to force the URL strings to be lowercase."
        },
        {
          "label": "D",
          "content": "Update the CloudFront distribution to specify casing-insensitive query string processing."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Deploy a Lambda@Edge function to sort parameters by name and force them to be lowercase. Select the CloudFront viewer request trigger to invoke the function.</p><p>This solution will increase the cache hit ratio as quickly as possible for the following reasons:</p><p>Root Cause Targeting: The problem is specifically identified as inconsistently ordered query strings and mixed-case letters in the URLs. Lambda@Edge with a viewer request trigger directly addresses both of these issues by normalizing the query parameters before CloudFront processes the request.</p><p>Complete Solution: This approach solves both problems simultaneously:</p><p>It sorts the parameters by name, ensuring consistent ordering regardless of how they're submitted</p><p>It forces all parameters to lowercase, eliminating case sensitivity issues</p><p>Preserves Functionality: Unlike disabling query string caching altogether (option B), this solution maintains the ability to cache based on query parameters, which is likely important for the application's functionality.</p><p>Immediate Effect: Once deployed, the Lambda@Edge function will immediately begin normalizing all incoming requests, quickly improving the cache hit ratio without requiring changes to the application code.</p><p>Flexibility: If additional normalization is needed in the future, the Lambda function can be updated to handle new requirements.</p><p>The other options have significant limitations:</p><p>B. Updating the CloudFront distribution to disable caching based on query string parameters would likely break application functionality if those parameters are meaningful for content differentiation. This is a blunt approach that doesn't address the actual problem and could negatively impact the application.</p><p>D. There is no built-in CloudFront setting for \"casing-insensitive query string processing.\" CloudFront treats query strings as case-sensitive by default, and there isn't a simple configuration option to change this behavior. This option doesn't exist as described and wouldn't address the ordering issue even if it did.</p><p>Lambda@Edge is specifically designed for this type of request manipulation and is the most appropriate solution for normalizing query strings before they reach the CloudFront cache. By implementing option A, the company can quickly improve the cache hit ratio while maintaining all the functionality of their web application.</p><p>Sources</p><p>Increase the proportion of requests that are served directly from the CloudFront caches (cache hit ratio) - Amazon CloudFront （https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cache-hit-ratio.html）</p><p>Lambda@Edge example functions - Amazon CloudFront （https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-examples.html）</p><p>Cloudfront CDN is not working with Videos | AWS re:Post （https://repost.aws/questions/QUZrZsoVf9SwuBr3loEgCk-Q/cloudfront-cdn-is-not-working-with-videos）</p><p>CloudFront will miss cache if request is from CURL | AWS re:Post （https://repost.aws/questions/QUqHQyupGdTgqf6vAqGle1Lg/cloudfront-will-miss-cache-if-request-is-from-curl）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "1293fbac672f4515841d9ee48f3a6bae",
      "questionNumber": 298,
      "type": "single",
      "content": "<p>Question #298</p><p>A company runs an ecommerce application in a single AWS Region. The application uses a five-node Amazon Aurora MySQL DB cluster to store information about customers and their recent orders. The DB cluster experiences a large number of write transactions throughout the day. <br><br>The company needs to replicate the data in the Aurora database to another Region to meet disaster recovery requirements. The company has an RPO of 1 hour.</p><p><br>Which solution will meet these requirements with the LOWEST cost?</p>",
      "options": [
        {
          "label": "A",
          "content": "Modify the Aurora database to be an Aurora global database. Create a second Aurora database in another Region."
        },
        {
          "label": "B",
          "content": "Enable the Backtrack feature for the Aurora database. Create an AWS Lambda function that runs daily to copy the snapshots of the database to a backup Region."
        },
        {
          "label": "C",
          "content": "Use AWS Database Migration Service (AWS DMS). Create a DMS change data capture (CDC) task that replicates the ongoing changes from the Aurora database to an Amazon S3 bucket in another Region."
        },
        {
          "label": "D",
          "content": "Turn off automated Aurora backups. Configure Aurora backups with a backup frequency of 1 hour. Specify another Region as the destination Region. Select the Aurora database as the resource assignment."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Use AWS Database Migration Service (AWS DMS). Create a DMS change data capture (CDC) task that replicates the ongoing changes from the Aurora database to an Amazon S3 bucket in another Region.</p><p>This solution will meet the RPO requirement of 1 hour with the LOWEST cost for the following reasons:</p><p>Cost-Effectiveness: Among the options presented, AWS DMS with CDC to S3 is the most cost-effective solution for cross-region replication. S3 storage is significantly less expensive than maintaining a full Aurora database replica in another region. You only pay for:</p><p>The DMS replication instance (which can be sized appropriately)</p><p>S3 storage costs (which are much lower than Aurora storage)</p><p>Data transfer costs (which would be incurred with any solution)</p><p>RPO Compliance: AWS DMS with CDC can easily meet the 1-hour RPO requirement. CDC captures changes continuously and can be configured to replicate these changes to the target S3 bucket at frequencies well under 1 hour.</p><p>Appropriate for Write-Heavy Workloads: The scenario mentions \"a large number of write transactions throughout the day.\" DMS with CDC is designed specifically to efficiently capture and replicate ongoing changes, making it suitable for write-intensive workloads.</p><p>The other options have significant cost disadvantages:</p><p>A. Aurora Global Database would provide the lowest RPO (typically less than 1 second), but at a much higher cost. It requires maintaining a full secondary Aurora cluster in the destination region, including compute instances and storage. This is overkill for an RPO requirement of 1 hour and would be significantly more expensive than the DMS to S3 approach.</p><p>B. Using Aurora Backtrack with Lambda to copy snapshots daily would not meet the 1-hour RPO requirement. Daily snapshots would result in an RPO of up to 24 hours, which exceeds the specified requirement. Even if modified to run hourly, this approach would be less efficient and potentially more costly than DMS with CDC for a database with frequent write transactions.</p><p>While the DMS to S3 solution may require additional steps during recovery (restoring from S3 to a new Aurora cluster), this trade-off is acceptable given the significant cost savings and the ability to meet the 1-hour RPO requirement. The company can restore from S3 to a new Aurora cluster in the DR region only when needed, avoiding the ongoing costs of maintaining a full secondary database.</p><p>Sources</p><p>Comparing Amazon Aurora Replicas, Aurora cross-Region Replicas, and Aurora global databases - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/aurora-replication-options/introduction.html）</p><p>Disaster recovery options in the cloud - Disaster Recovery of Workloads on AWS: Recovery in the Cloud （https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-options-in-the-cloud.html）</p><p>Automated Backup and Restore Solution for Cross-Region Aurora MySQL Databases | AWS re:Post （https://repost.aws/questions/QUhC-a3j4USkKFB7-_MCi--A/automated-backup-and-restore-solution-for-cross-region-aurora-mysql-databases）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "aaf8e7aad148408bae683bc0e6bb0330",
      "questionNumber": 299,
      "type": "single",
      "content": "<p>Question #299</p><p>A company's solutions architect is evaluating an AWS workload that was deployed several years ago. The application tier is stateless and runs on a single large Amazon EC2 instance that was launched from an AMI. The application stores data in a MySQL database that runs on a single EC2 instance. <br><br>The CPU utilization on the application server EC2 instance often reaches 100% and causes the application to stop responding. The company manually installs patches on the instances. Patching has caused downtime in the past. The company needs to make the application highly available. <br><br>Which solution will meet these requirements with the LEAST development effort?</p>",
      "options": [
        {
          "label": "A",
          "content": "Move the application tier to AWS Lambda functions in the existing VPC. Create an Application Load Balancer to distribute traffic across the Lambda functions. Use Amazon GuardDuty to scan the Lambda functions. Migrate the database to Amazon DocumentDB (with MongoDB compatibility)."
        },
        {
          "label": "B",
          "content": "Change the EC2 instance type to a smaller Graviton powered instance type. Use the existing AMI to create a launch template for an Auto Scaling group. Create an Application Load Balancer to distribute traffic across the instances in the Auto Scaling group. Set the Auto Scaling group to scale based on CPU utilization. Migrate the database to Amazon DynamoDB."
        },
        {
          "label": "C",
          "content": "Move the application tier to containers by using Docker. Run the containers on Amazon Elastic Container Service (Amazon ECS) with EC2 instances. Create an Application Load Balancer to distribute traffic across the ECS cluster. Configure the ECS cluster to scale based on CPU utilization. Migrate the database to Amazon Neptune."
        },
        {
          "label": "D",
          "content": "Create a new AMI that is configured with AWS Systems Manager Agent (SSM Agent). Use the new AMI to create a launch template for an Auto Scaling group. Use smaller instances in the Auto Scaling group. Create an Application Load Balancer to distribute traffic across the instances in the Auto Scaling group. Set the Auto Scaling group to scale based on CPU utilization. Migrate the database to Amazon Aurora MySQL."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Option D is the correct answer because it involves the least development effort by creating a new AMI with the SSM Agent for automated patching and setting up an Auto Scaling group with smaller instances for high availability. Additionally, migrating the database to Amazon Aurora MySQL is less effort than the other database migration options since it's a compatible choice with the existing MySQL database.</p><p> Key Requirements:</p><p>1. High Availability: The current setup has a single EC2 instance for both the application and database, which is a single point of failure.</p><p>2. Performance Issues: The application server CPU often reaches 100%, causing downtime.</p><p>3. Patching Issues: Manual patching causes downtime.</p><p>4. Least Development Effort: The solution should minimize code changes or rearchitecting.</p><p> Why Option D is Best:</p><p>- Auto Scaling Group (ASG): Replaces the single large EC2 instance with multiple smaller instances behind a load balancer, improving scalability and availability.</p><p>- AWS Systems Manager (SSM): Automates patching without downtime (replacing manual patching).</p><p>- Amazon Aurora MySQL: Provides a highly available, scalable, and managed database solution (replacing the single MySQL instance). Aurora is MySQL-compatible, minimizing migration effort.</p><p>- Least Development Effort: Uses AMI-based deployment (no need to refactor the app for containers or serverless).</p><p> Why Other Options Are Less Ideal:</p><p>- A (Lambda + DocumentDB): Requires rewriting the app for serverless and migrating to a non-relational database (high development effort).</p><p>- B (Graviton + DynamoDB): Switching to Graviton may require AMI/application changes, and DynamoDB is non-relational (may not fit the data model).</p><p>- C (ECS + Neptune): Containerization requires significant rework, and Neptune is a graph database (not a drop-in replacement for MySQL).</p><p> Summary:</p><p>Option D meets all requirements with minimal changes by leveraging Auto Scaling, SSM, and Aurora MySQL. </p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "aaef0bba648a447f91c8e42b1a026908",
      "questionNumber": 300,
      "type": "single",
      "content": "<p>Question #300</p><p>A company is planning to migrate several applications to AWS. The company does not have a good understanding of its entire application estate. The estate consists of a mixture of physical machines and VMs. <br><br>One application that the company will migrate has many dependencies that are sensitive to latency. The company is unsure what all the dependencies are. However, the company knows that the low-latency communications use a custom IP-based protocol that runs on port 1000. The company wants to migrate the application and these dependencies together to move all the low-latency interfaces to AWS at the same time. <br><br>The company has installed the AWS Application Discovery Agent and has been collecting data for several months. </p><p><br></p><p>What should the company do to identify the dependencies that need to be migrated in the same phase as the application?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use AWS Migration Hub and select the servers that host the application. Visualize the network graph to find servers that interact with the application. Turn on data exploration in Amazon Athena. Query the data that is transferred between the servers to identify the servers that communicate on port 1000. Return to Migration Hub. Create a move group that is based on the findings from the Athena queries."
        },
        {
          "label": "B",
          "content": "Use AWS Application Migration Service and select the servers that host the application. Visualize the network graph to find servers that interact with the application. Configure Application Migration Service to launch test instances for all the servers that interact with the application. Perform acceptance tests on the test instances. If no issues are identified, create a move group that is based on the tested servers."
        },
        {
          "label": "C",
          "content": "Use AWS Migration Hub and select the servers that host the application. Turn on data exploration in Network Access Analyzer. Use the Network Access Analyzer console to select the servers that host the application. Select a Network Access Scope of port 1000 and note the matching servers. Return to Migration Hub. Create a move group that is based on the findings from Network Access Analyzer."
        },
        {
          "label": "D",
          "content": "Use AWS Migration Hub and select the servers that host the application. Push the Amazon CloudWatch agent to the identified servers by using the AWS Application Discovery Agent. Export the CloudWatch logs that the agents collect to Amazon S3. Use Amazon Athena to query the logs to find servers that communicate on port 1000. Return to Migration Hub. Create a move group that is based on the findings from the Athena queries."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>The correct answer is: &nbsp;</p><p>A. Use AWS Migration Hub and select the servers that host the application. Visualize the network graph to find servers that interact with the application. Turn on data exploration in Amazon Athena. Query the data that is transferred between the servers to identify the servers that communicate on port 1000. Return to Migration Hub. Create a move group that is based on the findings from the Athena queries. &nbsp;</p><p> Explanation: &nbsp;</p><p>1. AWS Migration Hub is the central tool for tracking migrations, and it integrates with AWS Application Discovery Service (which has been collecting data for months). &nbsp;</p><p>2. The network graph visualization in Migration Hub helps identify dependencies by showing server interactions. &nbsp;</p><p>3. Amazon Athena can query the collected discovery data to filter servers communicating on port 1000 (the custom protocol). &nbsp;</p><p>4. Finally, a move group can be created in Migration Hub to migrate the application and its identified dependencies together. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- B: AWS Application Migration Service (MGN) is for rehosting (lift-and-shift), not dependency discovery. &nbsp;</p><p>- C: Network Access Analyzer checks security policies, not dependency mapping. &nbsp;</p><p>- D: CloudWatch logs are not the right tool for discovering network dependencies (Athena queries on Discovery data are more efficient). &nbsp;</p><p>Thus, Option A is the best approach.</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": true,
      "noteContent": "Network Access Analyzer checks security policies, not dependency mapping."
    },
    {
      "id": "d2a2b0c6ff844d848e94160c84bae1fc",
      "questionNumber": 301,
      "type": "single",
      "content": "Question #301<p>A company is building an application that will run on an AWS Lambda function. Hundreds of customers will use the application. The company wants to give each customer a quota of requests for a specific time period. The quotas must match customer usage patterns. Some customers must receive a higher quota for a shorter time period. </p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an Amazon API Gateway REST API with a proxy integration to invoke the Lambda function. For each customer, configure an API Gateway usage plan that includes an appropriate request quota. Create an API key from the usage plan for each user that the customer needs."
        },
        {
          "label": "B",
          "content": "Create an Amazon API Gateway HTTP API with a proxy integration to invoke the Lambda function. For each customer configure an API Gateway usage plan that includes an appropriate request quota Configure route-level throttling for each usage plan. Create an API Key from the usage plan for each user that the customer needs."
        },
        {
          "label": "C",
          "content": "Create a Lambda function alias for each customer. Include a concurrency limit with an appropriate request quota. Create a Lambda function URL for each function alias. Share the Lambda function URL for each alias with the relevant customer."
        },
        {
          "label": "D",
          "content": "Create an Application Load Balancer (ALB) in a VPC. Configure the Lambda function as a target for the ALB. Configure an AWS WAF web ACL for the ALB. For each customer configure a rule-based rule that includes an appropriate request quota."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Option A is the correct answer because it allows for the creation of individual usage plans for each customer with specific request quotas through Amazon API Gateway. This approach provides a scalable and flexible way to manage customer-specific quotas for a Lambda function.</p><p>The requirements are: &nbsp;</p><p>1. Quota per customer – Each customer should have a specific request quota for a given time period. &nbsp;</p><p>2. Flexible quotas – Some customers need higher quotas for shorter periods, meaning quotas should be customizable per customer. &nbsp;</p><p>3. Hundreds of customers – The solution must scale efficiently. &nbsp;</p><p>Why Option A is correct? &nbsp;</p><p>- API Gateway Usage Plans + API Keys allow you to define request quotas per customer. &nbsp;</p><p>- Each usage plan can specify: &nbsp;</p><p> &nbsp;- Throttling limits (requests per second) &nbsp;</p><p> &nbsp;- Quota limits (total requests per day/week/month) &nbsp;</p><p>- API Keys are assigned to customers, enforcing their respective usage plan. &nbsp;</p><p>- REST API with proxy integration is a valid way to invoke Lambda while managing quotas. &nbsp;</p><p>Why other options are incorrect? &nbsp;</p><p>- B: HTTP API does not support usage plans or API keys (only REST API does). &nbsp;</p><p>- C: Lambda concurrency limits are not per-customer quotas; they limit concurrent executions, not request counts over time. &nbsp;</p><p>- D: AWS WAF rate-based rules are not per-customer quotas; they apply globally and lack the granularity of API Gateway usage plans. &nbsp;</p><p> Final Answer: &nbsp;</p><p>✅ A. Create an Amazon API Gateway REST API with a proxy integration to invoke the Lambda function. For each customer, configure an API Gateway usage plan that includes an appropriate request quota. Create an API key from the usage plan for each user that the customer needs.</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "de1aba3e25c94cae98e4db76bea50570",
      "questionNumber": 302,
      "type": "single",
      "content": "<p>Question #302</p><p>A company is planning to migrate its on-premises VMware cluster of 120 VMs to AWS. The VMs have many different operating systems and many custom software packages installed. The company also has an on-premises NFS server that is 10 TB in size. The company has set up a 10 Gbps AWS Direct Connect connection to AWS for the migration. <br><br>Which solution will complete the migration to AWS in the LEAST amount of time?</p>",
      "options": [
        {
          "label": "A",
          "content": "Export the on-premises VMs and copy them to an Amazon S3 bucket. Use VM Import/Export to create AMIs from the VM images that are stored in Amazon S3. Order an AWS Snowball Edge device. Copy the NFS server data to the device. Restore the NFS server data to an Amazon EC2 instance that has NFS configured."
        },
        {
          "label": "B",
          "content": "Configure AWS Application Migration Service with a connection to the VMware cluster. Create a replication job for the VMs. Create an Amazon Elastic File System (Amazon EFS) file system. Configure AWS DataSync to copy the NFS server data to the EFS file system over the Direct Connect connection."
        },
        {
          "label": "C",
          "content": "Recreate the VMs on AWS as Amazon EC2 instances. Install all the required software packages. Create an Amazon FSx for Lustre file system. Configure AWS DataSync to copy the NFS server data to the FSx for Lustre file system over the Direct Connect connection."
        },
        {
          "label": "D",
          "content": "Order two AWS Snowball Edge devices. Copy the VMs and the NFS server data to the devices. Run VM Import/Export after the data from the devices is loaded to an Amazon S3 bucket. Create an Amazon Elastic File System (Amazon EFS) file system. Copy the NFS server data from Amazon S3 to the EFS file system."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Option B is the correct answer because it provides a fast migration strategy by using AWS Application Migration Service for the VMware cluster and AWS DataSync for the NFS server data over the 10 Gbps Direct Connect connection. This approach minimizes the migration time without the need for physical transportation of data or recreating VMs and software packages on AWS.</p><p>The requirements are: &nbsp;</p><p>1. Migrate 120 VMs (with diverse OS and custom software) quickly. &nbsp;</p><p>2. Migrate a 10 TB NFS server efficiently. &nbsp;</p><p>3. Use the existing 10 Gbps Direct Connect connection for the migration. &nbsp;</p><p> Why Option B is the Best? &nbsp;</p><p>- AWS Application Migration Service (MGN) is the fastest way to lift-and-shift VMware VMs to AWS with minimal downtime. &nbsp;</p><p> &nbsp;- It replicates VMs directly from VMware to AWS over Direct Connect. &nbsp;</p><p> &nbsp;- No manual reinstallation of software is needed. &nbsp;</p><p>- EFS + DataSync is efficient for migrating the NFS server: &nbsp;</p><p> &nbsp;- DataSync optimizes transfer over Direct Connect, speeding up the 10 TB migration. &nbsp;</p><p> &nbsp;- EFS is a managed NFS-compatible service, eliminating the need to manage an EC2-based NFS server. &nbsp;</p><p> Why Other Options Are Worse? &nbsp;</p><p>- A: &nbsp;</p><p> &nbsp;- VM Import/Export + Snowball is slow (manual exports, Snowball shipping delays). &nbsp;</p><p> &nbsp;- Snowball is unnecessary since Direct Connect is already available. &nbsp;</p><p>- C: &nbsp;</p><p> &nbsp;- Manually recreating VMs is time-consuming (installing software again). &nbsp;</p><p> &nbsp;- FSx for Lustre is overkill (optimized for HPC, not general NFS workloads). &nbsp;</p><p>- D: &nbsp;</p><p> &nbsp;- Snowball is unnecessary (Direct Connect is faster for 10 TB). &nbsp;</p><p> &nbsp;- Manual VM Import/Export + EFS copy adds extra steps, increasing migration time. &nbsp;</p><p> Final Answer: &nbsp;</p><p>✅ B. Configure AWS Application Migration Service with a connection to the VMware cluster. Create a replication job for the VMs. Create an Amazon Elastic File System (Amazon EFS) file system. Configure AWS DataSync to copy the NFS server data to the EFS file system over the Direct Connect connection.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "24a1a6566e3a43ec8756224ee136d369",
      "questionNumber": 303,
      "type": "single",
      "content": "<p>Question #303</p><p>An online survey company runs its application in the AWS Cloud. The application is distributed and consists of microservices that run in an automatically scaled Amazon Elastic Container Service (Amazon ECS) cluster. The ECS cluster is a target for an Application Load Balancer (ALB). The ALB is a custom origin for an Amazon CloudFront distribution. <br><br>The company has a survey that contains sensitive data. The sensitive data must be encrypted when it moves through the application. The application's data-handling microservice is the only microservice that should be able to decrypt the data. <br><br>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a symmetric AWS Key Management Service (AWS KMS) key that is dedicated to the data-handling microservice. Create a field-level encryption profile and a configuration. Associate the KMS key and the configuration with the CloudFront cache behavior."
        },
        {
          "label": "B",
          "content": "Create an RSA key pair that is dedicated to the data-handling microservice. Upload the public key to the CloudFront distribution. Create a field-level encryption profile and a configuration. Add the configuration to the CloudFront cache behavior."
        },
        {
          "label": "C",
          "content": "Create a symmetric AWS Key Management Service (AWS KMS) key that is dedicated to the data-handling microservice. Create a Lambda@Edge function. Program the function to use the KMS key to encrypt the sensitive data."
        },
        {
          "label": "D",
          "content": "Create an RSA key pair that is dedicated to the data-handling microservice. Create a Lambda@Edge function. Program the function to use the private key of the RSA key pair to encrypt the sensitive data."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Option B is the correct answer because it involves using an RSA key pair for field-level encryption with Amazon CloudFront, which allows the data-handling microservice to have the corresponding private key for decryption. This ensures that only the specific microservice can decrypt the sensitive data as required.</p><p>The requirements are: &nbsp;</p><p>1. Encrypt sensitive survey data in transit through the application. &nbsp;</p><p>2. Only the data-handling microservice should decrypt it (no other microservices). &nbsp;</p><p>3. The architecture involves CloudFront → ALB → ECS microservices. &nbsp;</p><p> Why Option B is Correct? &nbsp;</p><p>- CloudFront Field-Level Encryption (FLE) is the best solution for encrypting specific fields (e.g., survey responses) before they reach the ALB/ECS. &nbsp;</p><p> &nbsp;- RSA Key Pair ensures only the data-handling microservice (with the private key) can decrypt the data. &nbsp;</p><p> &nbsp;- Public key is uploaded to CloudFront, which encrypts sensitive fields before caching/forwarding requests. &nbsp;</p><p> &nbsp;- Private key remains with the data-handling microservice, ensuring no other component can decrypt the data. &nbsp;</p><p> Why Other Options Are Incorrect? &nbsp;</p><p>- A: &nbsp;</p><p> &nbsp;- AWS KMS symmetric keys are not suitable for CloudFront FLE (FLE requires asymmetric keys). &nbsp;</p><p> &nbsp;- KMS keys would allow any authorized service to decrypt, violating the \"only data-handling microservice\" requirement. &nbsp;</p><p>- C: &nbsp;</p><p> &nbsp;- Lambda@Edge + KMS symmetric key does not ensure end-to-end encryption (data is decrypted at CloudFront, then re-encrypted). &nbsp;</p><p> &nbsp;- KMS symmetric keys do not restrict decryption to a single microservice. &nbsp;</p><p>- D: &nbsp;</p><p> &nbsp;- Lambda@Edge + RSA private key encryption is backwards (private keys should decrypt, not encrypt). &nbsp;</p><p> &nbsp;- CloudFront FLE is a managed solution and more secure than custom Lambda@Edge encryption. &nbsp;</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "98fcbae2e9aa4ec49548e764fff63b22",
      "questionNumber": 304,
      "type": "single",
      "content": "<p>Question #304</p><p>A solutions architect is determining the DNS strategy for an existing VPC. The VPC is provisioned to use the 10.24.34.0/24 CIDR block. The VPC also uses Amazon Route 53 Resolver for DNS. New requirements mandate that DNS queries must use private hosted zones. Additionally, instances that have public IP addresses must receive corresponding public hostnames. <br><br>Which solution will meet these requirements to ensure that the domain names are correctly resolved within the VPC?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a private hosted zone. Activate the enableDnsSupport attribute and the enableDnsHostnames attribute for the VPC. Update the VPC DHCP options set to include domain-name-servers=10.24.34.2."
        },
        {
          "label": "B",
          "content": "Create a private hosted zone Associate the private hosted zone with the VPC. Activate the enableDnsSupport attribute and the enableDnsHostnames attribute for the VPC. Create a new VPC DHCP options set, and configure domain-name-servers=AmazonProvidedDNS. Associate the new DHCP options set with the VPC."
        },
        {
          "label": "C",
          "content": "Deactivate the enableDnsSupport attribute for the VPC. Activate the enableDnsHostnames attribute for the VPC. Create a new VPC DHCP options set, and configure domain-name-servers=10.24.34.2. Associate the new DHCP options set with the VPC."
        },
        {
          "label": "D",
          "content": "Create a private hosted zone. Associate the private hosted zone with the VPC. Activate the enableDnsSupport attribute for the VPC. Deactivate the enableDnsHostnames attribute for the VPC. Update the VPC DHCP options set to include domain-name-servers=AmazonProvidedDNS."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Option B is the correct answer because it involves creating a private hosted zone and associating it with the VPC while also enabling both DNS support and DNS hostnames attributes. Configuring the DHCP options set to use AmazonProvidedDNS ensures that the domain names are correctly resolved for both private and public instances within the VPC.</p><p>The requirements are: &nbsp;</p><p>1. DNS queries must use private hosted zones (for internal domain resolution). &nbsp;</p><p>2. Instances with public IPs must receive public hostnames (for public DNS resolution). &nbsp;</p><p>3. The VPC already uses Amazon Route 53 Resolver for DNS. &nbsp;</p><p> Why Option B is Correct? &nbsp;</p><p>- Private hosted zone associated with the VPC → Ensures internal DNS queries resolve via Route 53. &nbsp;</p><p>- enableDnsSupport = true → Allows DNS resolution within the VPC (required for private hosted zones). &nbsp;</p><p>- enableDnsHostnames = true → Ensures instances with public IPs get public DNS hostnames. &nbsp;</p><p>- DHCP options set with `domain-name-servers=AmazonProvidedDNS` → Forces instances to use AWS's DNS (Route 53 Resolver). &nbsp;</p><p> Why Other Options Are Incorrect? &nbsp;</p><p>- A: &nbsp;</p><p> &nbsp;- Uses a custom DNS server (10.24.34.2) instead of `AmazonProvidedDNS`, breaking integration with Route 53 Resolver. &nbsp;</p><p>- C: &nbsp;</p><p> &nbsp;- Disables `enableDnsSupport`, which prevents private hosted zone resolution. &nbsp;</p><p> &nbsp;- Uses a custom DNS server (10.24.34.2) instead of AWS DNS. &nbsp;</p><p>- D: &nbsp;</p><p> &nbsp;- Disables `enableDnsHostnames`, preventing public DNS hostnames for instances with public IPs. &nbsp;</p><p> Final Answer: &nbsp;</p><p>✅ B. Create a private hosted zone. Associate the private hosted zone with the VPC. Activate the `enableDnsSupport` attribute and the `enableDnsHostnames` attribute for the VPC. Create a new VPC DHCP options set, and configure `domain-name-servers=AmazonProvidedDNS`. Associate the new DHCP options set with the VPC.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "040667ef15fd4a8e908aee8a87317825",
      "questionNumber": 305,
      "type": "single",
      "content": "<p>Question #305</p><p>A data analytics company has an Amazon Redshift cluster that consists of several reserved nodes. The cluster is experiencing unexpected bursts of usage because a team of employees is compiling a deep audit analysis report. The queries to generate the report are complex read queries and are CPU intensive. <br><br>Business requirements dictate that the cluster must be able to service read and write queries at all times. A solutions architect must devise a solution that accommodates the bursts of usage. <br><br>Which solution meets these requirements MOST cost-effectively?</p>",
      "options": [
        {
          "label": "A",
          "content": "Provision an Amazon EMR cluster Offload the complex data processing tasks."
        },
        {
          "label": "B",
          "content": "Deploy an AWS Lambda function to add capacity to the Amazon Redshift cluster by using a classic resize operation when the cluster&rsquo;s CPU metrics in Amazon CloudWatch reach 80%."
        },
        {
          "label": "C",
          "content": "Deploy an AWS Lambda function to add capacity to the Amazon Redshift cluster by using an elastic resize operation when the cluster&rsquo;s CPU metrics in Amazon CloudWatch reach 80%."
        },
        {
          "label": "D",
          "content": "Turn on the Concurrency Scaling feature for the Amazon Redshift cluster."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. Turn on the Concurrency Scaling feature for the Amazon Redshift cluster.</p><p>This solution meets the requirements most cost-effectively for the following reasons:</p><p>Designed for burst workloads: Concurrency Scaling is specifically designed to handle unexpected bursts of usage, which is exactly what the company is experiencing with the deep audit analysis report.</p><p>Optimized for read queries: Concurrency Scaling is particularly effective for read-heavy workloads, and the scenario specifically mentions that the queries are \"complex read queries and are CPU intensive.\"</p><p>Maintains service continuity: When enabled, Concurrency Scaling automatically adds additional cluster capacity when needed, ensuring that both read and write queries can be serviced at all times, which is a stated business requirement.</p><p>Cost-effectiveness:</p><p>Amazon Redshift provides up to one hour of free Concurrency Scaling credits per day, which may be sufficient for the burst usage described.</p><p>You only pay for what you use beyond the free credits, and the scaling clusters are automatically turned off when not needed.</p><p>There are no upfront costs or commitments required.</p><p>Simplicity: Turning on Concurrency Scaling is a simple configuration change that doesn't require building and maintaining additional infrastructure or monitoring systems.</p><p>Option C (using Lambda to add capacity through elastic resize operations) is less optimal because:</p><p>Elastic resize operations take time to complete and may cause a brief interruption in service.</p><p>The solution would require building and maintaining a Lambda function and CloudWatch alarms.</p><p>Once resized, the cluster would remain at the larger size (and higher cost) until another resize operation is performed, even if the burst workload has completed.</p><p>There would be a delay between reaching the CPU threshold and completing the resize operation, during which performance could be degraded.</p><p>Concurrency Scaling provides a more elegant, purpose-built solution for this exact scenario - handling burst workloads while maintaining service levels for all queries, with minimal operational overhead and cost-effective pricing.</p><p>Sources</p><p>Concurrency scaling - Amazon Redshift （https://docs.aws.amazon.com/redshift/latest/dg/concurrency-scaling.html）</p><p>Scale read and write workloads with Amazon Redshift | AWS Big Data Blog （https://aws.amazon.com/cn/blogs/big-data/scale-read-and-write-workloads-with-amazon-redshift/）</p><p>Amazon Redshift Concurrency Scaling - Amazon Web Services （https://aws.amazon.com/cn/redshift/features/concurrency-scaling/）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "b8abda73866246a2ae5e4b90db85dc21",
      "questionNumber": 306,
      "type": "multiple",
      "content": "<p>Question #306</p><p>A research center is migrating to the AWS Cloud and has moved its on-premises 1 PB object storage to an Amazon S3 bucket. One hundred scientists are using this object storage to store their work-related documents. Each scientist has a personal folder on the object store. All the scientists are members of a single IAM user group. </p><p><br></p><p>The research center's compliance officer is worried that scientists will be able to access each other's work. The research center has a strict obligation to report on which scientist accesses which documents. The team that is responsible for these reports has little AWS experience and wants a ready-to-use solution that minimizes operational overhead. </p><p><br></p><p>Which combination of actions should a solutions architect take to meet these requirements? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an identity policy that grants the user read and write access. Add a condition that specifies that the S3 paths must be prefixed with $(aws:username). Apply the policy on the scientists&rsquo; IAM user group."
        },
        {
          "label": "B",
          "content": "Configure a trail with AWS CloudTrail to capture all object-level events in the S3 bucket. Store the trail output in another S3 bucket. Use Amazon Athena to query the logs and generate reports."
        },
        {
          "label": "C",
          "content": "Enable S3 server access logging. Configure another S3 bucket as the target for log delivery. Use Amazon Athena to query the logs and generate reports."
        },
        {
          "label": "D",
          "content": "Create an S3 bucket policy that grants read and write access to users in the scientists&rsquo; IAM user group."
        },
        {
          "label": "E",
          "content": "Configure a trail with AWS CloudTrail to capture all object-level events in the S3 bucket and write the events to Amazon CloudWatch. Use the Amazon Athena CloudWatch connector to query the logs and generate reports."
        }
      ],
      "correctAnswer": "AB",
      "explanation": "<p>The requirements are: &nbsp;</p><p>1. Prevent scientists from accessing each other’s folders (data isolation). &nbsp;</p><p>2. Track and report who accessed which documents (audit compliance). &nbsp;</p><p>3. Minimize operational overhead (simple, ready-to-use solution). &nbsp;</p><p> Why Option A is Correct? &nbsp;</p><p>- IAM policy with `${aws:username}` prefix ensures each scientist can only access their own folder. &nbsp;</p><p>- Applied at the IAM group level, reducing management overhead. &nbsp;</p><p>- No need for complex S3 bucket policies—access control is handled via IAM. &nbsp;</p><p> Why Option B is Correct? &nbsp;</p><p>- CloudTrail logs all S3 object-level API calls, including `GetObject`, `PutObject`, etc. &nbsp;</p><p>- Stored in another S3 bucket for security. &nbsp;</p><p>- Amazon Athena can query logs directly (no need for manual processing). &nbsp;</p><p>- Fully managed solution (minimal operational effort). &nbsp;</p><p> Why Other Options Are Incorrect? &nbsp;</p><p>- C: &nbsp;</p><p> &nbsp;- S3 server access logs lack user identity (only show IP addresses), making compliance reporting difficult. &nbsp;</p><p>- D: &nbsp;</p><p> &nbsp;- A bucket policy alone cannot enforce per-user folder access (would require complex conditions). &nbsp;</p><p>- E: &nbsp;</p><p> &nbsp;- CloudWatch is unnecessary—CloudTrail logs can be queried directly via Athena without extra steps. &nbsp;</p><p> Final Answer: &nbsp;</p><p>✅ A. Create an identity policy that grants the user read and write access. Add a condition that specifies that the S3 paths must be prefixed with `${aws:username}`. Apply the policy on the scientists’ IAM user group. &nbsp;</p><p>✅ B. Configure a trail with AWS CloudTrail to capture all object-level events in the S3 bucket. Store the trail output in another S3 bucket. Use Amazon Athena to query the logs and generate reports.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "ef0eaf3fea8c46ea93698c9206163ed4",
      "questionNumber": 307,
      "type": "single",
      "content": "<p>Question #307</p><p>A company uses AWS Organizations to manage a multi-account structure. The company has hundreds of AWS accounts and expects the number of accounts to increase. The company is building a new application that uses Docker images. The company will push the Docker images to Amazon Elastic Container Registry (Amazon ECR). Only accounts that are within the company’s organization should have access to the images. </p><p><br></p><p>The company has a CI/CD process that runs frequently. The company wants to retain all the tagged images. However, the company wants to retain only the five most recent untagged images. </p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a private repository in Amazon ECR. Create a permissions policy for the repository that allows only required ECR operations. Include a condition to allow the ECR operations if the value of the aws:PrincipalOrgID condition key is equal to the ID of the company&rsquo;s organization. Add a lifecycle rule to the ECR repository that deletes all untagged images over the count of five."
        },
        {
          "label": "B",
          "content": "Create a public repository in Amazon ECR. Create an IAM role in the ECR account. Set permissions so that any account can assume the role if the value of the aws:PrincipalOrgID condition key is equal to the ID of the company&rsquo;s organization. Add a lifecycle rule to the ECR repository that deletes all untagged images over the count of five."
        },
        {
          "label": "C",
          "content": "Create a private repository in Amazon ECR. Create a permissions policy for the repository that includes only required ECR operations. Include a condition to allow the ECR operations for all account IDs in the organization. Schedule a daily Amazon EventBridge rule to invoke an AWS Lambda function that deletes all untagged images over the count of five."
        },
        {
          "label": "D",
          "content": "Create a public repository in Amazon ECR. Configure Amazon ECR to use an interface VPC endpoint with an endpoint policy that includes the required permissions for images that the company needs to pull. Include a condition to allow the ECR operations for all account IDs in the company&rsquo;s organization. Schedule a daily Amazon EventBridge rule to invoke an AWS Lambda function that deletes all untagged images over the count of five."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Option A is the correct answer because it sets up a private repository in Amazon ECR with a permissions policy that restricts access to only those accounts within the organization, ensuring security. Additionally, the lifecycle rule helps manage untagged images efficiently, reducing operational overhead.</p><p>The requirements are: &nbsp;</p><p>1. Restrict ECR access to accounts within the organization (security). &nbsp;</p><p>2. Retain all tagged images (compliance/rollback needs). &nbsp;</p><p>3. Keep only the 5 most recent untagged images (storage optimization). &nbsp;</p><p>4. Minimize operational overhead (automated, no manual cleanup). &nbsp;</p><p> Why Option A is Correct? &nbsp;</p><p>- Private ECR repository ensures only authorized users can access images. &nbsp;</p><p>- Repository policy with `aws:PrincipalOrgID` restricts access to organization members. &nbsp;</p><p>- Lifecycle rule automatically deletes untagged images beyond the 5 most recent (no manual intervention). &nbsp;</p><p>- Least operational overhead (fully managed by AWS, no Lambda/EventBridge needed). &nbsp;</p><p> Why Other Options Are Incorrect? &nbsp;</p><p>- B: &nbsp;</p><p> &nbsp;- Public repositories are insecure (anyone can pull images, violating isolation). &nbsp;</p><p> &nbsp;- IAM role assumption adds unnecessary complexity. &nbsp;</p><p>- C: &nbsp;</p><p> &nbsp;- Manual cleanup via Lambda/EventBridge increases operational overhead (lifecycle rules are simpler). &nbsp;</p><p> &nbsp;- Hardcoding account IDs is not scalable (vs. `aws:PrincipalOrgID`). &nbsp;</p><p>- D: &nbsp;</p><p> &nbsp;- Public repositories are insecure (violates access control requirements). &nbsp;</p><p> &nbsp;- VPC endpoints are irrelevant (access control should be policy-based, not network-based). &nbsp;</p><p> &nbsp;- Lambda/EventBridge cleanup is more complex than lifecycle rules. &nbsp;</p><p> Final Answer: &nbsp;</p><p>✅ A. Create a private repository in Amazon ECR. Create a permissions policy for the repository that allows only required ECR operations. Include a condition to allow the ECR operations if the value of the `aws:PrincipalOrgID` condition key is equal to the ID of the company’s organization. Add a lifecycle rule to the ECR repository that deletes all untagged images over the count of five.</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "1564c638b4e547cea04f077458a26554",
      "questionNumber": 308,
      "type": "single",
      "content": "<p>Question #308</p><p>A solutions architect is reviewing a company's process for taking snapshots of Amazon RDS DB instances. The company takes automatic snapshots every day and retains the snapshots for 7 days. </p><p><br></p><p>The solutions architect needs to recommend a solution that takes snapshots every 6 hours and retains the snapshots for 30 days. The company uses AWS Organizations to manage all of its AWS accounts. The company needs a consolidated view of the health of the RDS snapshots. </p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Turn on the cross-account management feature in AWS Backup. Create a backup plan that specifies the frequency and retention requirements. Add a tag to the DB instances. Apply the backup plan by using tags. Use AWS Backup to monitor the status of the backups."
        },
        {
          "label": "B",
          "content": "Turn on the cross-account management feature in Amazon RDS. Create a snapshot global policy that specifies the frequency and retention requirements. Use the RDS console in the management account to monitor the status of the backups."
        },
        {
          "label": "C",
          "content": "Turn on the cross-account management feature in AWS CloudFormation. From the management account, deploy a CloudFormation stack set that contains a backup plan from AWS Backup that specifies the frequency and retention requirements. Create an AWS Lambda function in the management account to monitor the status of the backups. Create an Amazon EventBridge rule in each account to run the Lambda function on a schedule."
        },
        {
          "label": "D",
          "content": "Configure AWS Backup in each account. Create an Amazon Data Lifecycle Manager lifecycle policy that specifies the frequency and retention requirements. Specify the DB instances as the target resource. Use the Amazon Data Lifecycle Manager console in each member account to monitor the status of the backups."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Option A is the correct answer because it utilizes AWS Backup's cross-account management feature to create and apply a backup plan with the desired frequency and retention settings. This approach allows for a consolidated view of the backup status across all accounts with minimal operational overhead.</p><p>The requirements are: &nbsp;</p><p>1. Take RDS snapshots every 6 hours (higher frequency than daily). &nbsp;</p><p>2. Retain snapshots for 30 days (longer retention than 7 days). &nbsp;</p><p>3. Consolidated view of backup health across all accounts (using AWS Organizations). &nbsp;</p><p>4. Minimize operational overhead (automated, no manual scripting). &nbsp;</p><p> Why Option A is Correct? &nbsp;</p><p>- AWS Backup is the managed service designed for centralized backup management. &nbsp;</p><p>- Cross-account management allows backups to be managed from a central account (aligned with AWS Organizations). &nbsp;</p><p>- Backup plan easily configures 6-hour snapshots + 30-day retention. &nbsp;</p><p>- Tag-based application ensures only relevant RDS instances are included. &nbsp;</p><p>- AWS Backup Dashboard provides a consolidated view of backup status across accounts. &nbsp;</p><p> Why Other Options Are Incorrect? &nbsp;</p><p>- B: &nbsp;</p><p> &nbsp;- Amazon RDS does not natively support cross-account snapshot policies (AWS Backup is required for centralized management). &nbsp;</p><p> &nbsp;- No built-in way to enforce 6-hour snapshots (only manual or AWS Backup can do this). &nbsp;</p><p>- C: &nbsp;</p><p> &nbsp;- Overly complex (CloudFormation stack sets + Lambda + EventBridge introduce unnecessary overhead). &nbsp;</p><p> &nbsp;- AWS Backup already provides monitoring—no need for custom Lambda functions. &nbsp;</p><p>- D: &nbsp;</p><p> &nbsp;- Amazon Data Lifecycle Manager (DLM) is for EBS snapshots, not RDS (AWS Backup is the correct service for RDS). &nbsp;</p><p> &nbsp;- Requires manual setup in each account, violating the \"consolidated view\" requirement. &nbsp;</p><p> Final Answer: &nbsp;</p><p>✅ A. Turn on the cross-account management feature in AWS Backup. Create a backup plan that specifies the frequency and retention requirements. Add a tag to the DB instances. Apply the backup plan by using tags. Use AWS Backup to monitor the status of the backups.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "3f9f28110fc94b05b6b361d0462658ee",
      "questionNumber": 309,
      "type": "multiple",
      "content": "<p>Question #309</p><p>A company is using AWS Organizations with a multi-account architecture. The company's current security configuration for the account architecture includes SCPs, resource-based policies, identity-based policies, trust policies, and session policies. A solutions architect needs to allow an IAM user in Account A to assume a role in Account B. Which combination of steps must the solutions architect take to meet this requirement? (Choose three.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Configure the SCP for Account A to allow the action."
        },
        {
          "label": "B",
          "content": "Configure the resource-based policies to allow the action."
        },
        {
          "label": "C",
          "content": "Configure the identity-based policy on the user in Account A to allow the action."
        },
        {
          "label": "D",
          "content": "Configure the identity-based policy on the user in Account B to allow the action."
        },
        {
          "label": "E",
          "content": "Configure the trust policy on the target role in Account B to allow the action."
        },
        {
          "label": "F",
          "content": "Configure the session policy to allow the action and to be passed programmatically by the GetSessionToken API operation."
        }
      ],
      "correctAnswer": "ACE",
      "explanation": "<p>Based on the information provided, the correct combination of steps that the solutions architect must take to allow an IAM user in Account A to assume a role in Account B are:</p><p>A. Configure the SCP for Account A to allow the action. </p><p>C. Configure the identity-based policy on the user in Account A to allow the action. </p><p>E. Configure the trust policy on the target role in Account B to allow the action.</p><p>Here's why these three steps are necessary:</p><p>Configure the SCP for Account A to allow the action (A):</p><p>Service Control Policies (SCPs) in AWS Organizations set the maximum permissions boundary for accounts.</p><p>If there are restrictive SCPs in place, they could block the sts:AssumeRole action even if other policies allow it.</p><p>The SCP must not deny the ability for users in Account A to perform the sts:AssumeRole action.</p><p>Configure the identity-based policy on the user in Account A to allow the action (C):</p><p>The IAM user in Account A needs explicit permission to assume the specific role in Account B.</p><p>This is done by attaching an identity-based policy to the user (or to a group the user belongs to) that includes the sts:AssumeRole permission for the specific role ARN in Account B.</p><p>Without this permission, the user cannot initiate the role assumption process.</p><p>Configure the trust policy on the target role in Account B to allow the action (E):</p><p>The role in Account B must trust the IAM user from Account A to assume it.</p><p>This is configured in the role's trust policy, which specifies which principals (in this case, the user from Account A) are allowed to assume the role.</p><p>Without this trust relationship, the role will reject assumption attempts even if the user has permission to try.</p><p>The other options are not required:</p><p>B. Configure the resource-based policies to allow the action - While a trust policy is a type of resource-based policy, this option is referring to other resource-based policies, which are not necessary for cross-account role assumption.</p><p>D. Configure the identity-based policy on the user in Account B to allow the action - This is incorrect because there is no user in Account B involved in this scenario. The requirement is for a user in Account A to assume a role in Account B.</p><p>In summary, cross-account role assumption requires coordination between policies in both accounts: the permission to assume the role (in Account A) and the trust relationship allowing that assumption (in Account B), all within the boundaries set by any applicable SCPs.</p><p>Sources</p><p>How to allow an IAM user in Account A to assume a role in Account B in an AWS organization | AWS re:Post （https://repost.aws/questions/QU0DuguIVGSAO8tK80hP_E7w/how-to-allow-an-iam-user-in-account-a-to-assume-a-role-in-account-b-in-an-aws-organization）</p><p>Four ways to grant cross-account access in AWS | AWS Security Blog （https://aws.amazon.com/blogs/security/four-ways-to-grant-cross-account-access-in-aws/）</p><p>Effectively implementing resource controls policies in a multi-account environment | AWS Security Blog （https://aws.amazon.com/blogs/security/effectively-implementing-resource-controls-policies-in-a-multi-account-environment/）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "92f226fc36c347f3bcaad5c22e4dcecb",
      "questionNumber": 310,
      "type": "single",
      "content": "<p>Question #310</p><p>A company wants to use Amazon S3 to back up its on-premises file storage solution. The company’s on-premises file storage solution supports NFS, and the company wants its new solution to support NFS. The company wants to archive the backup files after 5 days. If the company needs archived files for disaster recovery, the company is willing to wait a few days for the retrieval of those files. </p><p><br></p><p>Which solution meets these requirements MOST cost-effectively?</p>",
      "options": [
        {
          "label": "A",
          "content": "Deploy an AWS Storage Gateway file gateway that is associated with an S3 bucket. Move the files from the on-premises file storage solution to the file gateway. Create an S3 Lifecycle rule to move the files to S3 Standard-Infrequent Access (S3 Standard-IA) after 5 days."
        },
        {
          "label": "B",
          "content": "Deploy an AWS Storage Gateway volume gateway that is associated with an S3 bucket. Move the files from the on-premises file storage solution to the volume gateway. Create an S3 Lifecycle rule to move the files to S3 Glacier Deep Archive after 5 days."
        },
        {
          "label": "C",
          "content": "Deploy an AWS Storage Gateway tape gateway that is associated with an S3 bucket. Move the files from the on-premises file storage solution to the tape gateway. Create an S3 Lifecycle rule to move the files to S3 Standard-Infrequent Access (S3 Standard-IA) after 5 days."
        },
        {
          "label": "D",
          "content": "Deploy an AWS Storage Gateway file gateway that is associated with an S3 bucket. Move the files from the on-premises file storage solution to the file gateway. Create an S3 Lifecycle rule to move the files to S3 Glacier Deep Archive after 5 days."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Option D is the correct answer because it uses AWS Storage Gateway's file gateway, which supports NFS, and the S3 Lifecycle rule to move files to S3 Glacier Deep Archive after 5 days. This option is cost-effective for long-term archiving and retrieval that can tolerate a few days of delay, as it provides the lowest storage cost for archiving data that is infrequently accessed.</p><p>The requirements are: &nbsp;</p><p>1. Support NFS (since the on-premises file storage uses NFS). &nbsp;</p><p>2. Archive backups after 5 days (with a willingness to wait days for retrieval if needed). &nbsp;</p><p>3. Cost-effective solution (prioritizing lower storage costs for archived data). &nbsp;</p><p> Why Option D is Correct? &nbsp;</p><p>- Storage Gateway File Gateway provides NFS support, matching the on-premises file storage solution. &nbsp;</p><p>- S3 Glacier Deep Archive is the most cost-effective storage class for long-term backups that are rarely accessed (retrieval times of 12+ hours are acceptable per the requirement). &nbsp;</p><p>- Lifecycle rule automates the transition from S3 Standard to Glacier Deep Archive after 5 days. &nbsp;</p><p> Why Other Options Are Incorrect? &nbsp;</p><p>- A: &nbsp;</p><p> &nbsp;- Uses S3 Standard-IA, which is not cost-effective for long-term archiving (higher cost than Glacier Deep Archive). &nbsp;</p><p>- B: &nbsp;</p><p> &nbsp;- Volume Gateway is for block storage (iSCSI), not file storage (NFS)—does not meet the NFS requirement. &nbsp;</p><p>- C: &nbsp;</p><p> &nbsp;- Tape Gateway is for backup to virtual tapes, not file storage (NFS)—does not meet the NFS requirement. &nbsp;</p><p> &nbsp;- S3 Standard-IA is again not cost-effective for archiving. &nbsp;</p><p> Final Answer: &nbsp;</p><p>✅ D. Deploy an AWS Storage Gateway file gateway that is associated with an S3 bucket. Move the files from the on-premises file storage solution to the file gateway. Create an S3 Lifecycle rule to move the files to S3 Glacier Deep Archive after 5 days.</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "2f5e69af80fb4721a96fe7ffaff32386",
      "questionNumber": 311,
      "type": "single",
      "content": "<p>Question #311</p><p>A company runs its application on Amazon EC2 instances and AWS Lambda functions. The EC2 instances experience a continuous and stable load. The Lambda functions experience a varied and unpredictable load. The application includes a caching layer that uses an Amazon MemoryDB for Redis cluster. <br><br>A solutions architect must recommend a solution to minimize the company's overall monthly costs. <br><br>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Purchase an EC2 instance Savings Plan to cover the EC2 instances. Purchase a Compute Savings Plan for Lambda to cover the minimum expected consumption of the Lambda functions. Purchase reserved nodes to cover the MemoryDB cache nodes."
        },
        {
          "label": "B",
          "content": "Purchase a Compute Savings Plan to cover the EC2 instances. Purchase Lambda reserved concurrency to cover the expected Lambda usage. Purchase reserved nodes to cover the MemoryDB cache nodes."
        },
        {
          "label": "C",
          "content": "Purchase a Compute Savings Plan to cover the entire expected cost of the EC2 instances, Lambda functions, and MemoryDB cache nodes."
        },
        {
          "label": "D",
          "content": "Purchase a Compute Savings Plan to cover the EC2 instances and the MemoryDB cache nodes. Purchase Lambda reserved concurrency to cover the expected Lambda usage."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Purchase an EC2 instance Savings Plan to cover the EC2 instances. Purchase a Compute Savings Plan for Lambda to cover the minimum expected consumption of the Lambda functions. Purchase reserved nodes to cover the MemoryDB cache nodes.</p><p>This solution will minimize the company's overall monthly costs for the following reasons:</p><p>EC2 Instance Savings Plan for EC2 instances with continuous and stable load:</p><p>EC2 Instance Savings Plans provide the deepest discounts (up to 72%) for EC2 instances</p><p>Since the EC2 instances experience a \"continuous and stable load,\" this makes them perfect candidates for EC2 Instance Savings Plans</p><p>These plans are instance-specific and ideal for predictable workloads</p><p>Compute Savings Plan for Lambda functions with varied and unpredictable load:</p><p>Compute Savings Plans offer flexibility across compute services including Lambda</p><p>For the \"varied and unpredictable\" Lambda workload, a Compute Savings Plan is ideal because:</p><p>It only commits to the minimum expected consumption level</p><p>Any usage above the commitment is charged at on-demand rates</p><p>It provides flexibility without over-committing to capacity that might not be used</p><p>Reserved nodes for MemoryDB cache:</p><p>MemoryDB reserved nodes provide significant discounts for long-term commitments</p><p>Since caching layers typically have stable requirements, reserved nodes are appropriate</p><p>Option B is less cost-effective because:</p><p>Using a Compute Savings Plan for EC2 instances would provide less savings than an EC2 Instance Savings Plan for the stable EC2 workload.</p><p>Lambda reserved concurrency is not a cost-saving mechanism but rather a capacity reservation feature:</p><p>It guarantees that a specific number of execution environments will be available</p><p>It doesn't provide any discount on Lambda execution costs</p><p>For unpredictable workloads, this could lead to paying for reserved capacity that isn't used</p><p>The combination in option A aligns each service with the most appropriate cost-saving mechanism based on its usage pattern:</p><p>Stable workloads (EC2 and MemoryDB) with service-specific commitments for maximum savings</p><p>Variable workloads (Lambda) with flexible commitments that cover only the minimum expected usage</p><p>This approach provides the optimal balance of cost savings and flexibility for the described workload characteristics.</p><p>Sources</p><p>Decide on Savings Plans or Reserved Instances for EC2 instances | AWS re:Post （https://ap-southeast-2.console.aws.amazon.com/console/home?nc2=h_ct&region=ap-southeast-2&src=header-signin#）</p><p>My boss asking me on cost optimization in AWS | AWS re:Post （https://repost.aws/questions/QU3MV37BYRR6ePI6WmUgOW0g/my-boss-asking-me-on-cost-optimization-in-aws）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "81ba7a7c9a074cd7a96f2fe99159ee48",
      "questionNumber": 312,
      "type": "single",
      "content": "<p>Question #312</p><p>A company is launching a new online game on Amazon EC2 instances. The game must be available globally. The company plans to run the game in three AWS Regions us-east-1, eu-west-1, and ap-southeast-1. The game's leaderboards, player inventory, and event status must be available across Regions. <br><br>A solutions architect must design a solution that will give any Region the ability to scale to handle the load of all Regions. Additionally, users must automatically connect to the Region that provides the least latency. </p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an EC2 Spot Fleet. Attach the Spot Fleet to a Network Load Balancer (NLB) in each Region. Create an AWS Global Accelerator IP address that points to the NLB. Create an Amazon Route 53 latency-based routing entry for the Global Accelerator IP address. Save the game metadata to an Amazon RDS for MySQL DB instance in each Region. Set up a read replica in the other Regions."
        },
        {
          "label": "B",
          "content": "Create an Auto Scaling group for the EC2 instances. Attach the Auto Scaling group to a Network Load Balancer (NLB) in each Region. For each Region, create an Amazon Route 53 entry that uses geoproximity routing and points to the NLB in that Region. Save the game metadata to MySQL databases on EC2 instances in each Region. Set up replication between the database EC2 instances in each Region."
        },
        {
          "label": "C",
          "content": "Create an Auto Scaling group for the EC2 instances. Attach the Auto Scaling group to a Network Load Balancer (NLB) in each Region. For each Region, create an Amazon Route 53 entry that uses latency-based routing and points to the NLB in that Region. Save the game metadata to an Amazon DynamoDB global table."
        },
        {
          "label": "D",
          "content": "Use EC2 Global View. Deploy the EC2 instances to each Region. Attach the instances to a Network Load Balancer (NLB). Deploy a DNS server on an EC2 instance in each Region. Set up custom logic on each DNS server to redirect the user to the Region that provides the lowest latency. Save the game metadata to an Amazon Aurora global database."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Option C is the correct answer because it leverages Amazon DynamoDB global tables to maintain game metadata across Regions, which simplifies the data management and replication process. Additionally, using Amazon Route 53 latency-based routing ensures that users connect to the Region with the least latency, and attaching the Auto Scaling group to an NLB in each Region allows for scaling to handle variable loads with minimal operational overhead.</p><p>The requirements are: &nbsp;</p><p>1. Global availability (game must run in multiple Regions). &nbsp;</p><p>2. Low-latency connections (users must automatically connect to the nearest Region). &nbsp;</p><p>3. Cross-Region data consistency (leaderboards, inventory, and event status must sync globally). &nbsp;</p><p>4. Scalability (any Region should handle the load of all Regions). &nbsp;</p><p>5. Minimal operational overhead (fully managed services preferred). &nbsp;</p><p> Why Option C is Correct? &nbsp;</p><p>- Auto Scaling + NLB ensures scalability within each Region. &nbsp;</p><p>- Route 53 latency-based routing automatically directs users to the closest Region (lowest latency). &nbsp;</p><p>- DynamoDB Global Tables provide multi-Region replication with low-latency reads/writes, meeting the cross-Region data requirement. &nbsp;</p><p>- Fully managed services (no manual replication or DNS logic required). &nbsp;</p><p> Why Other Options Are Incorrect? &nbsp;</p><p>- A: &nbsp;</p><p> &nbsp;- Global Accelerator + RDS read replicas adds unnecessary complexity (Global Accelerator is for fixed endpoints, not dynamic scaling). &nbsp;</p><p> &nbsp;- RDS cross-Region replication has higher latency than DynamoDB Global Tables. &nbsp;</p><p>- B: &nbsp;</p><p> &nbsp;- Geoproximity routing is less precise than latency-based routing. &nbsp;</p><p> &nbsp;- Manual MySQL replication increases operational overhead (vs. DynamoDB Global Tables). &nbsp;</p><p>- D: &nbsp;</p><p> &nbsp;- EC2 Global View is for monitoring, not load balancing. &nbsp;</p><p> &nbsp;- Custom DNS logic violates the \"least operational overhead\" requirement. &nbsp;</p><p> &nbsp;- Aurora Global Database is overkill (DynamoDB is better for leaderboards/inventory). &nbsp;</p><p> Final Answer: &nbsp;</p><p>✅ C. Create an Auto Scaling group for the EC2 instances. Attach the Auto Scaling group to a Network Load Balancer (NLB) in each Region. For each Region, create an Amazon Route 53 entry that uses latency-based routing and points to the NLB in that Region. Save the game metadata to an Amazon DynamoDB global table.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "5c0445361b374e1bbd788b3776c89058",
      "questionNumber": 313,
      "type": "multiple",
      "content": "<p>Question #313</p><p>A company is deploying a third-party firewall appliance solution from AWS Marketplace to monitor and protect traffic that leaves the company's AWS environments. The company wants to deploy this appliance into a shared services VPC and route all outbound internet-bound traffic through the appliances. </p><p><br></p><p>A solutions architect needs to recommend a deployment method that prioritizes reliability and minimizes failover time between firewall appliances within a single AWS Region. The company has set up routing from the shared services VPC to other VPCs. </p><p><br></p><p>Which steps should the solutions architect recommend to meet these requirements? (Choose three.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Deploy two firewall appliances into the shared services VPC, each in a separate Availability Zone."
        },
        {
          "label": "B",
          "content": "Create a new Network Load Balancer in the shared services VPC. Create a new target group, and attach it to the new Network Load Balancer. Add each of the firewall appliance instances to the target group."
        },
        {
          "label": "C",
          "content": "Create a new Gateway Load Balancer in the shared services VPC. Create a new target group, and attach it to the new Gateway Load Balancer. Add each of the firewall appliance instances to the target group."
        },
        {
          "label": "D",
          "content": "Create a VPC interface endpoint. Add a route to the route table in the shared services VPC. Designate the new endpoint as the next hop for traffic that enters the shared services VPC from other VPCs."
        },
        {
          "label": "E",
          "content": "Deploy two firewall appliances into the shared services VPC, each in the same Availability Zone."
        },
        {
          "label": "F",
          "content": "Create a VPC Gateway Load Balancer endpoint. Add a route to the route table in the shared services VPC. Designate the new endpoint as the next hop for traffic that enters the shared services VPC from other VPCs."
        }
      ],
      "correctAnswer": "ACF",
      "explanation": "<p>The requirements are: &nbsp;</p><p>1. High reliability (minimized failover time for firewall appliances). &nbsp;</p><p>2. Centralized outbound traffic inspection (all internet-bound traffic must pass through the firewall). &nbsp;</p><p>3. Deployment in a shared services VPC with routing from other VPCs. &nbsp;</p><p> Why These Options Are Correct? &nbsp;</p><p>- A. Deploy two firewall appliances in separate AZs → Ensures high availability (if one AZ fails, the other remains operational). &nbsp;</p><p>- C. Use a Gateway Load Balancer (GWLB) → GWLB is designed for third-party virtual appliances (like firewalls) and provides scalability + seamless failover. &nbsp;</p><p>- F. Create a VPC GWLB endpoint + update route tables → Directs all traffic through the firewall appliances (GWLB endpoint acts as the next hop). &nbsp;</p><p> Why Other Options Are Incorrect? &nbsp;</p><p>- B. Network Load Balancer (NLB) → NLB is not optimized for firewall appliances (GWLB is purpose-built for this). &nbsp;</p><p>- D. VPC interface endpoint → Used for AWS services (e.g., S3, DynamoDB), not for routing traffic through firewalls. &nbsp;</p><p>- E. Deploy both firewalls in the same AZ → Single point of failure (violates reliability requirements). &nbsp;</p><p> Final Answer: &nbsp;</p><p>✅ A. Deploy two firewall appliances into the shared services VPC, each in a separate Availability Zone. &nbsp;</p><p>✅ C. Create a new Gateway Load Balancer in the shared services VPC. Create a new target group, and attach it to the new Gateway Load Balancer. Add each of the firewall appliance instances to the target group. &nbsp;</p><p>✅ F. Create a VPC Gateway Load Balancer endpoint. Add a route to the route table in the shared services VPC. Designate the new endpoint as the next hop for traffic that enters the shared services VPC from other VPCs.</p><p>参考：https://docs.aws.amazon.com/elasticloadbalancing/latest/gateway/getting-started.html，<br>https://docs.aws.amazon.com/vpc/latest/privatelink/create-gateway-load-balancer-endpoint-service.html<br>https://aws.amazon.com/blogs/networking-and-content-delivery/introducing-aws-gateway-load-balancer-supported-architecture-patterns/</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "09033d1ef37c454f84e62f8c22e41d86",
      "questionNumber": 314,
      "type": "multiple",
      "content": "<p>Question #314</p><p>A solutions architect needs to migrate an on-premises legacy application to AWS. The application runs on two servers behind a load balancer. The application requires a license file that is associated with the MAC address of the server's network adapter. It takes the software vendor 12 hours to send new license files. The application also uses configuration files with a static IP address to access a database server; host names are not supported. Given these requirements, which combination of steps should be taken to implement highly available architecture for the application servers in AWS? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a pool of Elastic Network Interfaces (ENIs). Request license files from the vendor for the pool, and store the license files in Amazon S3. Create a bootstrap automation script to download a license file and attach the corresponding ENI to an Amazon EC2 instance."
        },
        {
          "label": "B",
          "content": "Create a pool of ENIs. Request license files from the vendor for the pool, store the license files on an Amazon EC2 instance. Create an AMI from the instance and use this AMI for all future EC2 instances."
        },
        {
          "label": "C",
          "content": "Create a bootstrap automation script to request a new license file from the vendor. When the response is received, apply the license file to an Amazon EC2 instance."
        },
        {
          "label": "D",
          "content": "Edit the bootstrap automation script to read the database server IP address from the AWS Systems Manager Parameter Store, and inject the value into the local configuration files."
        },
        {
          "label": "E",
          "content": "Edit an Amazon EC2 instance to include the database server IP address in the configuration files and recreate the AMI to use for all future EC2 instances."
        }
      ],
      "correctAnswer": "AD",
      "explanation": "<p>To implement a highly available architecture while addressing the licensing and configuration requirements, the architect should create a pool of ENIs with associated license files and store them in Amazon S3 (Option A), and use a bootstrap script to attach the correct ENI to the EC2 instance. Additionally, the database server IP address should be dynamically injected into the configuration files using the AWS Systems Manager Parameter Store (Option D), eliminating the need for manual updates or AMI recreation.</p><p>The requirements are: &nbsp;</p><p>1. Legacy application requires MAC-bound licensing (must persist across instance replacements). &nbsp;</p><p>2. Configuration files use static IPs (must be dynamically injected since hostnames are not supported). &nbsp;</p><p>3. High availability (must survive instance failures without manual intervention). &nbsp;</p><p> Why Option A is Correct? &nbsp;</p><p>- ENI Pool with Pre-Licensed MACs: &nbsp;</p><p> &nbsp;- Elastic Network Interfaces (ENIs) retain MAC addresses even when detached. &nbsp;</p><p> &nbsp;- Pre-request licenses for the ENIs and store them in S3 (scalable, durable storage). &nbsp;</p><p> &nbsp;- Bootstrap script automatically attaches the correct ENI and applies the license. &nbsp;</p><p>- Ensures HA: If an instance fails, a new one can attach the same ENI (preserving licensing). &nbsp;</p><p> Why Option D is Correct? &nbsp;</p><p>- AWS Systems Manager (SSM) Parameter Store for dynamic IP injection: &nbsp;</p><p> &nbsp;- Store the database IP in Parameter Store (managed, secure). &nbsp;</p><p> &nbsp;- Bootstrap script fetches the IP and updates config files at launch. &nbsp;</p><p>- Eliminates hardcoded IPs in AMIs, enabling flexibility and easier updates. &nbsp;</p><p> Why Other Options Are Incorrect? &nbsp;</p><p>- B: &nbsp;</p><p> &nbsp;- Storing licenses on an EC2 instance (not S3) is not scalable or durable. &nbsp;</p><p> &nbsp;- AMI-based licensing is inflexible (requires rebuilding AMIs for new licenses). &nbsp;</p><p>- C: &nbsp;</p><p> &nbsp;- Requesting new licenses on-demand violates the 12-hour vendor delay (breaks HA during failover). &nbsp;</p><p>- E: &nbsp;</p><p> &nbsp;- Hardcoding IPs in AMIs is inflexible (IP changes require AMI rebuilds). &nbsp;</p><p> Final Answer: &nbsp;</p><p>✅ A. Create a pool of ENIs. Request license files from the vendor for the pool, and store the license files in Amazon S3. Create a bootstrap automation script to download a license file and attach the corresponding ENI to an Amazon EC2 instance. &nbsp;</p><p>✅ D. Edit the bootstrap automation script to read the database server IP address from the AWS Systems Manager Parameter Store, and inject the value into the local configuration files.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "f856779ba344414c863910fc096d818e",
      "questionNumber": 315,
      "type": "single",
      "content": "<p>Question #315</p><p>A company runs its sales reporting application in an AWS Region in the United States. The application uses an Amazon API Gateway Regional API and AWS Lambda functions to generate on-demand reports from data in an Amazon RDS for MySQL database. The frontend of the application is hosted on Amazon S3 and is accessed by users through an Amazon CloudFront distribution. The company is using Amazon Route 53 as the DNS service for the domain. Route 53 is configured with a simple routing policy to route traffic to the API Gateway API. </p><p><br></p><p>In the next 6 months, the company plans to expand operations to Europe. More than 90% of the database traffic is read-only traffic. The company has already deployed an API Gateway API and Lambda functions in the new Region. A solutions architect must design a solution that minimizes latency for users who download reports. </p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use an AWS Database Migration Service (AWS DMS) task with full load to replicate the primary database in the original Region to the database in the new Region. Change the Route 53 record to latency-based routing to connect to the API Gateway API."
        },
        {
          "label": "B",
          "content": "Use an AWS Database Migration Service (AWS DMS) task with full load plus change data capture (CDC) to replicate the primary database in the original Region to the database in the new Region. Change the Route 53 record to geolocation routing to connect to the API Gateway API."
        },
        {
          "label": "C",
          "content": "Configure a cross-Region read replica for the RDS database in the new Region. Change the Route 53 record to latency-based routing to connect to the API Gateway API."
        },
        {
          "label": "D",
          "content": "Configure a cross-Region read replica for the RDS database in the new Region. Change the Route 53 record to geolocation routing to connect to the API Gateway API."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Options C:</p><p style=\"text-align: start;\"><span style=\"color: rgb(31, 35, 41); background-color: rgba(0, 0, 0, 0); font-size: 16px;\"><strong>Cross-Region read replica</strong></span>: Configure a cross-Region read replica for the RDS database in the European Region, which is suitable for the 90% read-only traffic scenario. The read replica synchronizes data from the primary database, allowing Lambda/API Gateway in the European Region to access the local database directly, significantly reducing latency at the database layer.</p><p style=\"text-align: start;\"><span style=\"color: rgb(31, 35, 41); background-color: rgba(0, 0, 0, 0); font-size: 16px;\"><strong>Latency-based routing</strong></span>: Route 53's latency-based routing policy automatically selects the API Gateway endpoint with the lowest latency for users, directly matching the core requirement of \"minimizing latency\"—it is more precise than geolocation routing (which only divides traffic by region).</p><p style=\"text-align: start;\">Options A and B: AWS DMS is a data migration tool, not the optimal solution for RDS read-only traffic. Cross-Region read replicas are RDS's native capability for read-only scenarios, which is more efficient;</p><p style=\"text-align: start;\">Option D: Geolocation routing only distributes traffic by the user's region and cannot precisely match \"minimum latency\" (latency may vary between endpoints in the same region), so it is less suitable than latency-based routing.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "3d0dee68910e481296098d8030a79e3c",
      "questionNumber": 316,
      "type": "single",
      "content": "<p>Question #316</p><p>A software company needs to create short-lived test environments to test pull requests as part of its development process. Each test environment consists of a single Amazon EC2 instance that is in an Auto Scaling group. </p><p><br></p><p>The test environments must be able to communicate with a central server to report test results. The central server is located in an on-premises data center. A solutions architect must implement a solution so that the company can create and delete test environments without any manual intervention. The company has created a transit gateway with a VPN attachment to the on-premises network. </p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an AWS CloudFormation template that contains a transit gateway attachment and related routing configurations. Create a CloudFormation stack set that includes this template. Use CloudFormation StackSets to deploy a new stack for each VPC in the account. Deploy a new VPC for each test environment."
        },
        {
          "label": "B",
          "content": "Create a single VPC for the test environments. Include a transit gateway attachment and related routing configurations. Use AWS CloudFormation to deploy all test environments into the VPC."
        },
        {
          "label": "C",
          "content": "Create a new OU in AWS Organizations for testing. Create an AWS CloudFormation template that contains a VPC, necessary networking resources, a transit gateway attachment, and related routing configurations. Create a CloudFormation stack set that includes this template. Use CloudFormation StackSets for deployments into each account under the testing OU. Create a new account for each test environment."
        },
        {
          "label": "D",
          "content": "Convert the test environment EC2 instances into Docker images. Use AWS CloudFormation to configure an Amazon Elastic Kubernetes Service (Amazon EKS) cluster in a new VPC, create a transit gateway attachment, and create related routing configurations. Use Kubernetes to manage the deployment and lifecycle of the test environments."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Option B is the correct answer because it simplifies the setup by using a single VPC for all test environments, which reduces the complexity and operational overhead. By including a transit gateway attachment and related routing configurations within the VPC, test environments can communicate with the central server without the need for creating new VPCs or managing multiple accounts.</p><p>The requirements are: &nbsp;</p><p>1. Automated creation/deletion of test environments (no manual intervention). &nbsp;</p><p>2. Each test environment is a single EC2 instance in an Auto Scaling group. &nbsp;</p><p>3. Test environments must communicate with an on-premises central server (via transit gateway VPN). &nbsp;</p><p>4. Minimize operational overhead (simple, scalable solution). &nbsp;</p><p> Why Option B is Correct? &nbsp;</p><p>- Single VPC for all test environments reduces complexity (no need for multiple VPCs or accounts). &nbsp;</p><p>- Transit gateway attachment in the VPC enables on-premises connectivity once (shared by all test environments). &nbsp;</p><p>- CloudFormation automates deployment of EC2 instances (Auto Scaling) and networking. &nbsp;</p><p>- Least operational overhead: No need for StackSets, multi-account setups, or container orchestration. &nbsp;</p><p> Why Other Options Are Incorrect? &nbsp;</p><p>- A: &nbsp;</p><p> &nbsp;- Deploying a new VPC per test environment is overly complex (transit gateway attachments and routing must be repeated). &nbsp;</p><p> &nbsp;- StackSets add unnecessary management overhead (single VPC is simpler). &nbsp;</p><p>- C: &nbsp;</p><p> &nbsp;- Creating new accounts per test environment is excessive (violates \"least operational overhead\"). &nbsp;</p><p> &nbsp;- StackSets + multi-account OU introduces unnecessary complexity. &nbsp;</p><p>- D: &nbsp;</p><p> &nbsp;- Containerizing EC2 instances + EKS is overkill for single-instance test environments. &nbsp;</p><p> &nbsp;- Adds Kubernetes management overhead when simple Auto Scaling suffices. &nbsp;</p><p> Final Answer: &nbsp;</p><p>✅ B. Create a single VPC for the test environments. Include a transit gateway attachment and related routing configurations. Use AWS CloudFormation to deploy all test environments into the VPC.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "315274204d7f4416b737dc34474e0761",
      "questionNumber": 317,
      "type": "multiple",
      "content": "<p>Question #317</p><p>A company is deploying a new API to AWS. The API uses Amazon API Gateway with a Regional API endpoint and an AWS Lambda function for hosting. The API retrieves data from an external vendor API, stores data in an Amazon DynamoDB global table, and retrieves data from the DynamoDB global table. The API key for the vendor's API is stored in AWS Secrets Manager and is encrypted with a customer managed key in AWS Key Management Service (AWS KMS). The company has deployed its own API into a single AWS Region. A solutions architect needs to change the API components of the company’s API to ensure that the components can run across multiple Regions in an active-active configuration. Which combination of changes will meet this requirement with the LEAST operational overhead? (Choose three.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Deploy the API to multiple Regions. Configure Amazon Route 53 with custom domain names that route traffic to each Regional API endpoint. Implement a Route 53 multivalue answer routing policy."
        },
        {
          "label": "B",
          "content": "Create a new KMS multi-Region customer managed key. Create a new KMS customer managed replica key in each in-scope Region."
        },
        {
          "label": "C",
          "content": "Replicate the existing Secrets Manager secret to other Regions. For each in-scope Region&#39;s replicated secret, select the appropriate KMS key."
        },
        {
          "label": "D",
          "content": "Create a new AWS managed KMS key in each in-scope Region. Convert an existing key to a multi-Region key. Use the multi-Region key in other Regions."
        },
        {
          "label": "E",
          "content": "Create a new Secrets Manager secret in each in-scope Region. Copy the secret value from the existing Region to the new secret in each in-scope Region."
        },
        {
          "label": "F",
          "content": "Modify the deployment process for the Lambda function to repeat the deployment across in-scope Regions. Turn on the multi-Region option for the existing API. Select the Lambda function that is deployed in each Region as the backend for the multi-Region API."
        }
      ],
      "correctAnswer": "ABC",
      "explanation": "<p>To deploy the API across multiple Regions with an active-active configuration and minimal operational overhead, the architect should deploy the API to multiple Regions (Option A), create a new KMS multi-Region key (Option B), and replicate the Secrets Manager secret to other Regions (Option C). This approach ensures that the encryption keys and secrets are available in each Region without the need to manage multiple secrets or keys.</p><p>The requirements are: &nbsp;</p><p>1. Deploy the API in an active-active configuration across multiple Regions (high availability). &nbsp;</p><p>2. Minimize operational overhead (use managed services where possible). &nbsp;</p><p>3. Ensure cross-Region data consistency (DynamoDB global table is already set up). &nbsp;</p><p> Why Option A is Correct? &nbsp;</p><p>- Route 53 with multivalue answer routing distributes traffic across Regional API endpoints, enabling active-active failover. &nbsp;</p><p>- Custom domain names ensure a consistent endpoint for users. &nbsp;</p><p> Why Option B is Correct? &nbsp;</p><p>- KMS multi-Region keys allow Secrets Manager to decrypt secrets in other Regions without re-encrypting. &nbsp;</p><p>- Replica keys sync automatically, reducing manual key management. &nbsp;</p><p> Why Option C is Correct? &nbsp;</p><p>- Secrets Manager cross-Region replication ensures the vendor API key is available in all Regions. &nbsp;</p><p>- Automatically uses the correct KMS key per Region (aligned with Option B). &nbsp;</p><p> Why Other Options Are Incorrect? &nbsp;</p><p>- D: &nbsp;</p><p> &nbsp;- AWS-managed KMS keys cannot be converted to multi-Region keys (only customer-managed keys support this). &nbsp;</p><p>- E: &nbsp;</p><p> &nbsp;- Manually copying secrets increases overhead (Secrets Manager replication is automated). &nbsp;</p><p>- F: &nbsp;</p><p> &nbsp;- API Gateway \"multi-Region\" option does not exist—this is a distractor (Route 53 handles routing). &nbsp;</p><p> Final Answer: &nbsp;</p><p>✅ A. Deploy the API to multiple Regions. Configure Amazon Route 53 with custom domain names that route traffic to each Regional API endpoint. Implement a Route 53 multivalue answer routing policy. &nbsp;</p><p>✅ B. Create a new KMS multi-Region customer managed key. Create a new KMS customer managed replica key in each in-scope Region. &nbsp;</p><p>✅ C. Replicate the existing Secrets Manager secret to other Regions. For each in-scope Region's replicated secret, select the appropriate KMS key.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "6b04f815ee904e1ebab0000711259b9b",
      "questionNumber": 318,
      "type": "single",
      "content": "<p>Question #318</p><p>An online retail company hosts its stateful web-based application and MySQL database in an on-premises data center on a single server. The company wants to increase its customer base by conducting more marketing campaigns and promotions. In preparation, the company wants to migrate its application and database to AWS to increase the reliability of its architecture. </p><p><br></p><p>Which solution should provide the HIGHEST level of reliability?</p>",
      "options": [
        {
          "label": "A",
          "content": "Migrate the database to an Amazon RDS MySQL Multi-AZ DB instance. Deploy the application in an Auto Scaling group on Amazon EC2 instances behind an Application Load Balancer. Store sessions in Amazon Neptune."
        },
        {
          "label": "B",
          "content": "Migrate the database to Amazon Aurora MySQL. Deploy the application in an Auto Scaling group on Amazon EC2 instances behind an Application Load Balancer. Store sessions in an Amazon ElastiCache for Redis replication group."
        },
        {
          "label": "C",
          "content": "Migrate the database to Amazon DocumentDB (with MongoDB compatibility). Deploy the application in an Auto Scaling group on Amazon EC2 instances behind a Network Load Balancer. Store sessions in Amazon Kinesis Data Firehose."
        },
        {
          "label": "D",
          "content": "Migrate the database to an Amazon RDS MariaDB Multi-AZ DB instance. Deploy the application in an Auto Scaling group on Amazon EC2 instances behind an Application Load Balancer. Store sessions in Amazon ElastiCache for Memcached."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Migrate the database to Amazon Aurora MySQL. Deploy the application in an Auto Scaling group on Amazon EC2 instances behind an Application Load Balancer. Store sessions in an Amazon ElastiCache for Redis replication group.</p><p>This solution provides the highest level of reliability for the following reasons:</p><p>Database Layer: Amazon Aurora MySQL offers superior reliability compared to standard RDS MySQL Multi-AZ:</p><p>Aurora's storage layer is distributed across multiple Availability Zones by default</p><p>It provides faster automated failover (typically under 30 seconds)</p><p>Aurora has a self-healing storage system that automatically repairs disk failures</p><p>It can have up to 15 read replicas for higher availability and read scaling</p><p>Application Layer: The Auto Scaling group with EC2 instances behind an Application Load Balancer provides:</p><p>Automatic replacement of unhealthy instances</p><p>Distribution of traffic across multiple Availability Zones</p><p>Ability to scale out during high-demand periods (like marketing campaigns and promotions)</p><p>Fault tolerance against instance and Availability Zone failures</p><p>Session Management: Amazon ElastiCache for Redis replication group is purpose-built for session storage:</p><p>Redis is widely used and optimized for session caching</p><p>A replication group provides automatic failover capabilities</p><p>It offers multi-AZ deployment for high availability</p><p>Redis provides in-memory performance with persistence options</p><p>Integration and Ecosystem: This architecture uses services that are designed to work together seamlessly:</p><p>Aurora MySQL is compatible with MySQL, making migration straightforward</p><p>ElastiCache for Redis has extensive integration with common web frameworks for session handling</p><p>The entire stack can be deployed across multiple Availability Zones for maximum reliability</p><p>Option A is less reliable because:</p><p>Standard RDS MySQL Multi-AZ, while reliable, doesn't offer the same level of durability and availability as Aurora</p><p>Amazon Neptune is a graph database service that isn't optimized for session storage (it's designed for highly connected data)</p><p>Using Neptune for session storage would be an unusual architecture choice that could introduce unnecessary complexity</p><p>The architecture in option B represents AWS best practices for highly available web applications with stateful requirements. It provides redundancy at every layer (database, application, and session storage) and can handle the increased load from marketing campaigns and promotions while maintaining high reliability.</p><p>Sources</p><p>Data tier (Amazon Aurora and Amazon ElastiCache) - Best Practices for WordPress on AWS （https://docs.aws.amazon.com/whitepapers/latest/best-practices-wordpress/data-tier-amazon-aurora-and-amazon-elasticache.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": true,
      "noteContent": "Aurora&amp;#039;s storage layer is distributed across multiple Availability Zones by default by default by default"
    },
    {
      "id": "ea6c17b9406840049e1c201deaf68001",
      "questionNumber": 319,
      "type": "single",
      "content": "<p>Question #319</p><p>A company’s solutions architect needs to provide secure Remote Desktop connectivity to users for Amazon EC2 Windows instances that are hosted in a VPC. The solution must integrate centralized user management with the company's on-premises Active Directory. Connectivity to the VPC is through the internet. The company has hardware that can be used to establish an AWS Site-to-Site VPN connection. <br><br>Which solution will meet these requirements MOST cost-effectively?</p>",
      "options": [
        {
          "label": "A",
          "content": "Deploy a managed Active Directory by using AWS Directory Service for Microsoft Active Directory. Establish a trust with the on-premises Active Directory. Deploy an EC2 instance as a bastion host in the VPC. Ensure that the EC2 instance is joined to the domain. Use the bastion host to access the target instances through RDP."
        },
        {
          "label": "B",
          "content": "Configure AWS IAM Identity Center (AWS Single Sign-On) to integrate with the on-premises Active Directory by using the AWS Directory Service for Microsoft Active Directory AD Connector. Configure permission sets against user groups for access to AWS Systems Manager. Use Systems Manager Fleet Manager to access the target instances through RDP."
        },
        {
          "label": "C",
          "content": "Implement a VPN between the on-premises environment and the target VPC. Ensure that the target instances are joined to the on-premises Active Directory domain over the VPN connection. Configure RDP access through the VPN. Connect from the company&rsquo;s network to the target instances."
        },
        {
          "label": "D",
          "content": "Deploy a managed Active Directory by using AWS Directory Service for Microsoft Active Directory. Establish a trust with the on-premises Active Directory. Deploy a Remote Desktop Gateway on AWS by using an AWS Quick Start. Ensure that the Remote Desktop Gateway is joined to the domain. Use the Remote Desktop Gateway to access the target instances through RDP."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>The requirements are: &nbsp;</p><p>1. Secure RDP access to EC2 Windows instances (over the internet). &nbsp;</p><p>2. Centralized user management via on-premises Active Directory. &nbsp;</p><p>3. Cost-effective solution (avoid unnecessary infrastructure). &nbsp;</p><p> Why Option B is Correct? &nbsp;</p><p>- AWS IAM Identity Center (SSO) + AD Connector integrates with on-premises AD without deploying a full managed AD. &nbsp;</p><p>- Systems Manager Session Manager (via Fleet Manager) provides secure RDP without exposing instances to the internet. &nbsp;</p><p>- No bastion host/VPN needed (reduces cost and complexity). &nbsp;</p><p> Why Other Options Are Incorrect? &nbsp;</p><p>- A: &nbsp;</p><p> &nbsp;- Bastion host adds cost/management overhead (requires public IP, security hardening). &nbsp;</p><p> &nbsp;- AWS Managed AD is expensive if only used for RDP access. &nbsp;</p><p>- C: &nbsp;</p><p> &nbsp;- VPN is not cost-effective for individual user RDP access (better for site-to-site connectivity). &nbsp;</p><p> &nbsp;- Exposes instances to on-premises network risks. &nbsp;</p><p>- D: &nbsp;</p><p> &nbsp;- Remote Desktop Gateway is overkill (adds unnecessary EC2/licensing costs). &nbsp;</p><p> &nbsp;- AWS Managed AD is expensive for this use case. &nbsp;</p><p> Final Answer: &nbsp;</p><p>✅ B. Configure AWS IAM Identity Center (AWS Single Sign-On) to integrate with the on-premises Active Directory by using the AWS Directory Service for Microsoft Active Directory AD Connector. Configure permission sets against user groups for access to AWS Systems Manager. Use Systems Manager Fleet Manager to access the target instances through RDP.</p><p>https://aws.amazon.com/blogs/mt/console-based-access-to-windows-instances-using-aws-systems-manager-fleet-manager/</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "53996820046949b0baac6684d2954103",
      "questionNumber": 320,
      "type": "single",
      "content": "<p>Question #320</p><p>A company's compliance audit reveals that some Amazon Elastic Block Store (Amazon EBS) volumes that were created in an AWS account were not encrypted. A solutions architect must implement a solution to encrypt all new EBS volumes at rest. <br><br>Which solution will meet this requirement with the LEAST effort?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an Amazon EventBridge rule to detect the creation of unencrypted EBS volumes. Invoke an AWS Lambda function to delete noncompliant volumes."
        },
        {
          "label": "B",
          "content": "Use AWS Audit Manager with data encryption."
        },
        {
          "label": "C",
          "content": "Create an AWS Config rule to detect the creation of a new EBS volume. Encrypt the volume by using AWS Systems Manager Automation."
        },
        {
          "label": "D",
          "content": "Turn on EBS encryption by default in all AWS Regions."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. Turn on EBS encryption by default in all AWS Regions.</p><p>This solution will meet the requirement to encrypt all new EBS volumes at rest with the least effort for the following reasons:</p><p>Built-in functionality: Enabling EBS encryption by default is a native AWS feature that doesn't require any custom code, additional services, or complex configurations.</p><p>Simple implementation: It's a one-time setting change per region that can be quickly enabled through the EC2 console, AWS CLI, or API.</p><p>Proactive approach: Once enabled, all new EBS volumes created in the region will be automatically encrypted at creation time, regardless of how they are created (console, CLI, API, or through services that create EBS volumes).</p><p>No ongoing maintenance: Unlike the other options, this solution doesn't require monitoring, updating code, or managing additional AWS services.</p><p>Comprehensive coverage: This setting applies to all EBS volumes created in the region, ensuring complete compliance with the encryption requirement.</p><p>The other options require significantly more effort:</p><p>A. Creating an EventBridge rule with Lambda would require writing code, setting up IAM permissions, testing the function, and monitoring its operation. It's also reactive rather than preventive, as it would delete volumes after they're created unencrypted.</p><p>B. AWS Audit Manager is primarily for compliance reporting and doesn't directly enforce encryption. It would help identify unencrypted volumes but wouldn't prevent their creation.</p><p>C. Creating an AWS Config rule with Systems Manager Automation would require setting up multiple services, creating automation documents, and ongoing maintenance. It's more complex and resource-intensive than simply enabling encryption by default.</p><p>Turning on EBS encryption by default is the most straightforward and effective solution to ensure all new EBS volumes are encrypted at rest, making it the option that requires the least effort to implement.</p><p>Sources</p><p>SEC08-BP02 Enforce encryption at rest - AWS Well-Architected Framework （https://docs.aws.amazon.com/wellarchitected/2023-10-03/framework/sec_protect_data_rest_encrypt.html）</p><p>Data at Rest Encryption with Amazon EBS - AWS Key Management Service Best Practices （https://docs.aws.amazon.com/whitepapers/latest/kms-best-practices/data-at-rest-encryption-with-amazon-ebs.html）</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "6ceaaf4748e3409088e76a848f1a9e87",
      "questionNumber": 321,
      "type": "single",
      "content": "<p>Question #321</p><p>A research company is running daily simulations in the AWS Cloud to meet high demand. The simulations run on several hundred Amazon EC2 instances that are based on Amazon Linux 2. Occasionally, a simulation gets stuck and requires a cloud operations engineer to solve the problem by connecting to an EC2 instance through SSH. </p><p><br></p><p>Company policy states that no EC2 instance can use the same SSH key and that all connections must be logged in AWS CloudTrail. </p><p><br></p><p>How can a solutions architect meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Launch new EC2 instances, and generate an individual SSH key for each instance. Store the SSH key in AWS Secrets Manager. Create a new IAM policy, and attach it to the engineers&rsquo; IAM role with an Allow statement for the GetSecretValue action. Instruct the engineers to fetch the SSH key from Secrets Manager when they connect through any SSH client."
        },
        {
          "label": "B",
          "content": "Create an AWS Systems Manager document to run commands on EC2 instances to set a new unique SSH key. Create a new IAM policy, and attach it to the engineers&rsquo; IAM role with an Allow statement to run Systems Manager documents. Instruct the engineers to run the document to set an SSH key and to connect through any SSH client."
        },
        {
          "label": "C",
          "content": "Launch new EC2 instances without setting up any SSH key for the instances. Set up EC2 Instance Connect on each instance. Create a new IAM policy, and attach it to the engineers&rsquo; IAM role with an Allow statement for the SendSSHPublicKey action. Instruct the engineers to connect to the instance by using a browser-based SSH client from the EC2 console."
        },
        {
          "label": "D",
          "content": "Set up AWS Secrets Manager to store the EC2 SSH key. Create a new AWS Lambda function to create a new SSH key and to call AWS Systems Manager Session Manager to set the SSH key on the EC2 instance. Configure Secrets Manager to use the Lambda function for automatic rotation once daily. Instruct the engineers to fetch the SSH key from Secrets Manager when they connect through any SSH client."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Option C is the correct answer because it uses EC2 Instance Connect, which allows engineers to connect to their instances via SSH without the need to manage SSH keys. Instance Connect generates a new SSH key pair for each connection, and all connections are logged in CloudTrail, meeting both company policy requirements with minimal effort.</p><p>The requirements are: &nbsp;</p><p>1. No EC2 instance can use the same SSH key (unique keys per instance). &nbsp;</p><p>2. All SSH connections must be logged in CloudTrail (auditability). &nbsp;</p><p>3. Engineers must be able to SSH into instances for troubleshooting. &nbsp;</p><p> Why Option C is Correct? &nbsp;</p><p>- EC2 Instance Connect automatically generates ephemeral SSH keys (unique per session). &nbsp;</p><p>- No long-term SSH keys stored (complies with company policy). &nbsp;</p><p>- CloudTrail logs all `SendSSHPublicKey` API calls (meets audit requirements). &nbsp;</p><p>- Browser-based SSH client from the EC2 console simplifies access for engineers. &nbsp;</p><p> Why Other Options Are Incorrect? &nbsp;</p><p>- A: &nbsp;</p><p> &nbsp;- Storing SSH keys in Secrets Manager violates the \"no same key\" policy (keys must be unique per instance). &nbsp;</p><p> &nbsp;- Manual key management is error-prone and unscalable for hundreds of instances. &nbsp;</p><p>- B: &nbsp;</p><p> &nbsp;- Systems Manager documents cannot enforce unique SSH keys per instance (manual setup required). &nbsp;</p><p> &nbsp;- No CloudTrail logging for SSH connections (only logs Systems Manager API calls). &nbsp;</p><p>- D: &nbsp;</p><p> &nbsp;- Daily key rotation is insufficient (keys must be unique per instance, not just rotated). &nbsp;</p><p> &nbsp;- Overly complex (Lambda + Secrets Manager + Session Manager for SSH is unnecessary). &nbsp;</p><p> Final Answer: &nbsp;</p><p>✅ C. Launch new EC2 instances without setting up any SSH key for the instances. Set up EC2 Instance Connect on each instance. Create a new IAM policy, and attach it to the engineers’ IAM role with an Allow statement for the `SendSSHPublicKey` action. Instruct the engineers to connect to the instance by using a browser-based SSH client from the EC2 console.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "3c7fd08e03574d3d8a0e12cdd74cc4e2",
      "questionNumber": 322,
      "type": "single",
      "content": "<p>Question #322</p><p>A company is migrating mobile banking applications to run on Amazon EC2 instances in a VPC. Backend service applications run in an on-premises data center. The data center has an AWS Direct Connect connection into AWS. The applications that run in the VPC need to resolve DNS requests to an on-premises Active Directory domain that runs in the data center. </p><p><br></p><p>Which solution will meet these requirements with the LEAST administrative overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Provision a set of EC2 instances across two Availability Zones in the VPC as caching DNS servers to resolve DNS queries from the application servers within the VPC."
        },
        {
          "label": "B",
          "content": "Provision an Amazon Route 53 private hosted zone. Configure NS records that point to on-premises DNS servers."
        },
        {
          "label": "C",
          "content": "Create DNS endpoints by using Amazon Route 53 Resolver. Add conditional forwarding rules to resolve DNS namespaces between the on-premises data center and the VPC."
        },
        {
          "label": "D",
          "content": "Provision a new Active Directory domain controller in the VPC with a bidirectional trust between this new domain and the on-premises Active Directory domain."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Create DNS endpoints by using Amazon Route 53 Resolver. Add conditional forwarding rules to resolve DNS namespaces between the on-premises data center and the VPC.</p><p>This solution will meet the requirements with the least administrative overhead for the following reasons:</p><p>Fully managed service: Route 53 Resolver is a fully managed AWS service that eliminates the need to provision, configure, and maintain your own DNS servers. AWS handles the underlying infrastructure, scaling, and high availability.</p><p>Purpose-built for hybrid DNS resolution: Route 53 Resolver with conditional forwarding rules is specifically designed to handle DNS resolution between AWS and on-premises environments, making it ideal for this banking application scenario.</p><p>Simple configuration: Setting up Route 53 Resolver endpoints and forwarding rules is straightforward through the AWS Console, CLI, or API. Once configured, it requires minimal ongoing management.</p><p>High availability by default: Route 53 Resolver endpoints are automatically deployed across multiple Availability Zones for high availability without additional configuration.</p><p>Integration with Direct Connect: Route 53 Resolver works seamlessly with Direct Connect connections, utilizing the existing network path to forward DNS queries to on-premises DNS servers.</p><p>The other options would require significantly more administrative overhead:</p><p>A. Provisioning EC2 instances as caching DNS servers would require:</p><p>Launching and maintaining multiple EC2 instances across AZs</p><p>Installing and configuring DNS server software</p><p>Managing security patches and updates</p><p>Implementing monitoring and failover mechanisms</p><p>Scaling the solution as needed</p><p>B. Using a Route 53 private hosted zone with NS records pointing to on-premises DNS servers has limitations:</p><p>NS records in private hosted zones don't function the same way as in public zones</p><p>This approach doesn't properly handle recursive DNS resolution for Active Directory</p><p>It would require additional configuration and potentially custom DNS solutions</p><p>Route 53 Resolver with conditional forwarding rules provides a clean, managed solution that's specifically designed for hybrid DNS resolution scenarios like this one, making it the option with the least administrative overhead.</p><p>Sources</p><p>Set up DNS resolution for hybrid networks in a single-account AWS environment - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/set-up-dns-resolution-for-hybrid-networks-in-a-single-account-aws-environment.html）</p><p>Active Directory Domain Services integration with Amazon Route 53 | Networking & Content Delivery （https://aws.amazon.com/cn/blogs/networking-and-content-delivery/active-directory-domain-services-integration-with-amazon-route-53/）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "248c7f45405a4bda86ec37091c905749",
      "questionNumber": 323,
      "type": "single",
      "content": "<p>Question #323</p><p>A company processes environmental data. The company has set up sensors to provide a continuous stream of data from different areas in a city. The data is available in JSON format. <br><br>The company wants to use an AWS solution to send the data to a database that does not require fixed schemas for storage. The data must be sent in real time. <br><br>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use Amazon Kinesis Data Firehose to send the data to Amazon Redshift."
        },
        {
          "label": "B",
          "content": "Use Amazon Kinesis Data Streams to send the data to Amazon DynamoDB."
        },
        {
          "label": "C",
          "content": "Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to send the data to Amazon Aurora."
        },
        {
          "label": "D",
          "content": "Use Amazon Kinesis Data Firehose to send the data to Amazon Keyspaces (for Apache Cassandra)."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Use Amazon Kinesis Data Streams to send the data to Amazon DynamoDB.</p><p>This solution will meet the requirements for the following reasons:</p><p>Real-time data processing: Amazon Kinesis Data Streams is designed to collect and process large streams of data records in real time. It can handle the continuous stream of sensor data and make it available for processing within milliseconds of being received.</p><p>Schema-less database storage: Amazon DynamoDB is a NoSQL database service that does not require fixed schemas. This makes it ideal for storing JSON data from sensors that might have varying structures or attributes over time. Each item in DynamoDB can have a different set of attributes, allowing for flexible data modeling.</p><p>JSON format support: DynamoDB natively supports JSON data, making it straightforward to store the sensor data in its original format without transformation.</p><p>Scalability: Both Kinesis Data Streams and DynamoDB are fully managed services that can scale to handle the high volume of data from multiple sensors across different areas in the city.</p><p>Let's examine why the other options don't fully meet the requirements:</p><p>A. Kinesis Data Firehose to Amazon Redshift - Amazon Redshift is a columnar database that requires a predefined schema. While Firehose provides real-time data delivery, Redshift doesn't meet the requirement for schema-less storage.</p><p>C. Amazon MSK to Amazon Aurora - While Amazon MSK provides real-time streaming capabilities, Amazon Aurora (whether PostgreSQL or MySQL compatible) requires defined schemas for tables, which doesn't meet the schema-less requirement.</p><p>D. Kinesis Data Firehose to Amazon Keyspaces - While Amazon Keyspaces is a NoSQL database that could potentially handle schema-less data, it's optimized for Apache Cassandra workloads and typically requires more schema definition than DynamoDB. Additionally, Keyspaces is less commonly used for this type of IoT sensor data compared to DynamoDB.</p><p>The combination of Kinesis Data Streams and DynamoDB is a well-established pattern for real-time processing of schema-less data in AWS, making it the most appropriate solution for the company's environmental data processing requirements.</p><p>Sources</p><p>Serverless Stream-Based Processing for Real-Time Insights | AWS Architecture Blog （https://aws.amazon.com/cn/blogs/architecture/serverless-stream-based-processing-for-real-time-insights/）</p><p>Amazon Redshift data ingestion options | AWS Big Data Blog（https://aws.amazon.com/cn/blogs/big-data/amazon-redshift-data-ingestion-options/） </p><p>Working with streaming data on AWS - Build Modern Data Streaming Architectures on AWS （https://docs.aws.amazon.com/whitepapers/latest/build-modern-data-streaming-analytics-architectures/working-with-streaming-data-on-aws.html）</p><p>Architecture patterns to optimize Amazon Redshift performance at scale | AWS Big Data Blog （https://aws.amazon.com/cn/blogs/big-data/architecture-patterns-to-optimize-amazon-redshift-performance-at-scale/）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "1a8d905880764d3c8574897f9bbcce97",
      "questionNumber": 324,
      "type": "single",
      "content": "<p>Question #324</p><p>A company is migrating a legacy application from an on-premises data center to AWS. The application uses MongoDB as a key-value database. According to the company's technical guidelines, all Amazon EC2 instances must be hosted in a private subnet without an internet connection. In addition, all connectivity between applications and databases must be encrypted. The database must be able to scale based on demand. </p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create new Amazon DocumentDB (with MongoDB compatibility) tables for the application with Provisioned IOPS volumes. Use the instance endpoint to connect to Amazon DocumentDB."
        },
        {
          "label": "B",
          "content": "Create new Amazon DynamoDB tables for the application with on-demand capacity. Use a gateway VPC endpoint for DynamoDB to connect to the DynamoDB tables."
        },
        {
          "label": "C",
          "content": "Create new Amazon DynamoDB tables for the application with on-demand capacity. Use an interface VPC endpoint for DynamoDB to connect to the DynamoDB tables."
        },
        {
          "label": "D",
          "content": "Create new Amazon DocumentDB (with MongoDB compatibility) tables for the application with Provisioned IOPS volumes. Use the cluster endpoint to connect to Amazon DocumentDB."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p style=\"text-align: start;\"><span style=\"color: rgb(31, 35, 41); background-color: rgba(0, 0, 0, 0); font-size: 16px;\"><strong>Option B</strong></span>:<span style=\"color: rgb(31, 35, 41); background-color: rgba(0, 0, 0, 0); font-size: 16px;\">DynamoDB</span> is AWS's native key-value database, and its \"on-demand capacity mode\" supports \"on-demand scaling\" of the database;</p><li style=\"text-align: start;\">The <span style=\"color: rgb(31, 35, 41); background-color: rgba(0, 0, 0, 0); font-size: 16px;\">gateway VPC endpoint</span> is the dedicated VPC access method for DynamoDB, which is suitable for scenarios where \"private subnets have no internet connection\" (traffic is transmitted via the VPC internal network, no public network required);</li><li style=\"text-align: start;\">DynamoDB connections are encrypted by default, meeting the requirement of \"encrypted connections between applications and databases.\"</li><p style=\"text-align: start;\">Option A: DocumentDB (MongoDB-compatible) is not configured with a VPC endpoint; private subnets without public internet cannot access its instance endpoint, failing to meet network requirements;</p><p style=\"text-align: start;\">Option C: The VPC endpoint type corresponding to DynamoDB is gateway-type; interface VPC endpoints are not applicable to DynamoDB, making this configuration invalid;</p><p style=\"text-align: start;\">Option D: DocumentDB is not configured with a VPC endpoint; private subnets cannot access its cluster endpoint, failing to meet network requirements.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "d509702d12c64333b57ecb6b4ce143a0",
      "questionNumber": 325,
      "type": "single",
      "content": "<p>Question #325</p><p>A company is running an application on Amazon EC2 instances in the AWS Cloud. The application is using a MongoDB database with a replica set as its data tier. The MongoDB database is installed on systems in the company’s on-premises data center and is accessible through an AWS Direct Connect connection to the data center environment.</p><p><br></p><p> A solutions architect must migrate the on-premises MongoDB database to Amazon DocumentDB (with MongoDB compatibility). </p><p><br></p><p>Which strategy should the solutions architect choose to perform this migration?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a fleet of EC2 instances. Install MongoDB Community Edition on the EC2 instances, and create a database. Configure continuous synchronous replication with the database that is running in the on-premises data center."
        },
        {
          "label": "B",
          "content": "Create an AWS Database Migration Service (AWS DMS) replication instance. Create a source endpoint for the on-premises MongoDB database by using change data capture (CDC). Create a target endpoint for the Amazon DocumentDB database. Create and run a DMS migration task."
        },
        {
          "label": "C",
          "content": "Create a data migration pipeline by using AWS Data Pipeline. Define data nodes for the on-premises MongoDB database and the Amazon DocumentDB database. Create a scheduled task to run the data pipeline."
        },
        {
          "label": "D",
          "content": "Create a source endpoint for the on-premises MongoDB database by using AWS Glue crawlers. Configure continuous asynchronous replication between the MongoDB database and the Amazon DocumentDB database."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Option B is the correct answer. AWS Database Migration Service (AWS DMS) is designed for migrating databases to AWS with minimal downtime. Using CDC with AWS DMS allows for continuous data replication from the source MongoDB database to the target Amazon DocumentDB, which is compatible with MongoDB.</p><p>AWS Database Migration Service (AWS DMS) is the recommended service for migrating databases to AWS, including migrations from MongoDB to Amazon DocumentDB (with MongoDB compatibility). Here’s why: &nbsp;</p><p>- AWS DMS supports Change Data Capture (CDC), which allows continuous replication of changes from the source (on-premises MongoDB) to the target (Amazon DocumentDB) with minimal downtime. &nbsp;</p><p>- It handles schema conversion (if needed) and ensures data consistency. &nbsp;</p><p>- The steps involve: &nbsp;</p><p> &nbsp;1. Creating a DMS replication instance. &nbsp;</p><p> &nbsp;2. Defining a source endpoint for the on-premises MongoDB. &nbsp;</p><p> &nbsp;3. Defining a target endpoint for Amazon DocumentDB. &nbsp;</p><p> &nbsp;4. Creating and running a migration task (initial load + CDC for ongoing changes). &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A: Running MongoDB on EC2 with synchronous replication is complex, costly, and not a managed solution like Amazon DocumentDB. &nbsp;</p><p>- C: AWS Data Pipeline is not designed for real-time or CDC-based database migrations; it’s better for batch data processing. &nbsp;</p><p>- D: AWS Glue crawlers are for metadata discovery (not database migration), and asynchronous replication is not a standard feature for this use case. &nbsp;</p><p>Thus, B is the correct and most efficient approach. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "6cdd3bd4a5304f288c3a156d6051ec97",
      "questionNumber": 326,
      "type": "single",
      "content": "<p>Question #326</p><p>A company is rearchitecting its applications to run on AWS. The company’s infrastructure includes multiple Amazon EC2 instances. The company's development team needs different levels of access. The company wants to implement a policy that requires all Windows EC2 instances to be joined to an Active Directory domain on AWS. The company also wants to implement enhanced security processes such as multifactor authentication (MFA). The company wants to use managed AWS services wherever possible. </p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an AWS Directory Service for Microsoft Active Directory implementation. Launch an Amazon Workspace. Connect to and use the Workspace for domain security configuration tasks."
        },
        {
          "label": "B",
          "content": "Create an AWS Directory Service for Microsoft Active Directory implementation. Launch an EC2 instance. Connect to and use the EC2 instance for domain security configuration tasks."
        },
        {
          "label": "C",
          "content": "Create an AWS Directory Service Simple AD implementation. Launch an EC2 instance. Connect to and use the EC2 instance for domain security configuration tasks."
        },
        {
          "label": "D",
          "content": "Create an AWS Directory Service Simple AD implementation. Launch an Amazon Workspace. Connect to and use the Workspace for domain security configuration tasks."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p style=\"text-align: start;\"><span style=\"color: rgb(31, 35, 41); background-color: rgba(0, 0, 0, 0); font-size: 16px;\">Option B</span>:<span style=\"color: rgb(31, 35, 41); background-color: rgba(0, 0, 0, 0); font-size: 16px;\">AWS Directory Service for Microsoft Active Directory</span> is a managed Microsoft AD service by AWS. It supports full Active Directory features (including joining Windows EC2 instances to the domain) and is compatible with enhanced security processes like MFA, which meets the requirements of \"managed services\" and \"enhanced security\";Launching an EC2 instance and using it to perform domain security configuration tasks is a common and feasible method for managing an Active Directory domain.</p><p style=\"text-align: start;\">Option A: Amazon WorkSpaces is a virtual desktop service, primarily used for end-user office work. It is not a tool for domain security configuration management, so it does not meet the requirements;</p><p style=\"text-align: start;\">Options C and D: AWS Directory Service Simple AD is a simplified AD service with limited functions. It does not support enhanced security processes like MFA, so it cannot meet the \"enhanced security\" requirement.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "5e176a78f8d541b189121c8df1105f6f",
      "questionNumber": 327,
      "type": "single",
      "content": "<p>Question #327</p><p>A company wants to migrate its on-premises application to AWS. The database for the application stores structured product data and temporary user session data. The company needs to decouple the product data from the user session data. The company also needs to implement replication in another AWS Region for disaster recovery.<br><br>Which solution will meet these requirements with the HIGHEST performance?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an Amazon RDS DB instance with separate schemas to host the product data and the user session data. Configure a read replica for the DB instance in another Region."
        },
        {
          "label": "B",
          "content": "Create an Amazon RDS DB instance to host the product data. Configure a read replica for the DB instance in another Region. Create a global datastore in Amazon ElastiCache for Memcached to host the user session data."
        },
        {
          "label": "C",
          "content": "Create two Amazon DynamoDB global tables. Use one global table to host the product data. Use the other global table to host the user session data. Use DynamoDB Accelerator (DAX) for caching."
        },
        {
          "label": "D",
          "content": "Create an Amazon RDS DB instance to host the product data. Configure a read replica for the DB instance in another Region. Create an Amazon DynamoDB global table to host the user session data."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p><br></p><p>To meet the requirements of \"decoupling structured product data and temporary user session data + cross-Region disaster recovery + highest performance,\" the suitability of each component is as follows:</p><p style=\"text-align: start;\">Option D: </p><ol><li style=\"text-align: start;\"><span style=\"color: rgb(31, 35, 41); background-color: rgba(0, 0, 0, 0); font-size: 16px;\"><strong>Structured product data</strong></span>:Amazon RDS is a relational database that is suitable for storing structured data and handling complex query scenarios. Configuring a cross-Region read replica meets disaster recovery needs while ensuring the processing performance of structured data.</li><li style=\"text-align: start;\"><span style=\"color: rgb(31, 35, 41); background-color: rgba(0, 0, 0, 0); font-size: 16px;\"><strong>Temporary user session data</strong></span>:Amazon DynamoDB global tables are NoSQL databases that are suitable for high-concurrency read/write scenarios of temporary session data. Global tables support automatic cross-Region synchronization, which naturally meets disaster recovery needs, while decoupling from product data stored in RDS.</li><li style=\"text-align: start;\">Performance aspect: The query performance of RDS for structured data, the high-concurrency read/write performance of DynamoDB for session data, and the cross-Region synchronization efficiency of global tables together ensure the highest performance of the solution.</li></ol><p style=\"text-align: start;\">Option A: Storing two types of data in different schemas within the same RDS instance does not achieve data decoupling, which fails to meet the core requirement;</p><p style=\"text-align: start;\">Option B: ElastiCache for Memcached is primarily a cache. Although session data stored in the cache has fast read/write speeds, the persistence/disaster recovery capability of the cache is weaker than that of DynamoDB global tables, and the stability of session data is insufficient;</p><p style=\"text-align: start;\">Option C: DynamoDB is a NoSQL database, which is not suitable for complex query scenarios of structured product data, and its performance in processing structured data is lower than that of RDS.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "e1093c90958646a89ec8c420ebfc5233",
      "questionNumber": 328,
      "type": "single",
      "content": "<p>Question #328</p><p>A company orchestrates a multi-account structure on AWS by using AWS Control Tower. The company is using AWS Organizations, AWS Config, and AWS Trusted Advisor. The company has a specific OU for development accounts that developers use to experiment on AWS. The company has hundreds of developers, and each developer has an individual development account. </p><p><br></p><p>The company wants to optimize costs in these development accounts. Amazon EC2 instances and Amazon RDS instances in these accounts must be burstable. The company wants to disallow the use of other services that are not relevant. </p><p><br></p><p>What should a solutions architect recommend to meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a custom SCP in AWS Organizations to allow the deployment of only burstable instances and to disallow services that are not relevant. Apply the SCP to the development OU."
        },
        {
          "label": "B",
          "content": "Create a custom detective control (guardrail) in AWS Control Tower. Configure the control (guardrail) to allow the deployment of only burstable instances and to disallow services that are not relevant. Apply the control (guardrail) to the development OU."
        },
        {
          "label": "C",
          "content": "Create a custom preventive control (guardrail) in AWS Control Tower. Configure the control (guardrail) to allow the deployment of only burstable instances and to disallow services that are not relevant. Apply the control (guardrail) to the development OU."
        },
        {
          "label": "D",
          "content": "Create an AWS Config rule in the AWS Control Tower account. Configure the AWS Config rule to allow the deployment of only burstable instances and to disallow services that are not relevant. Deploy the AWS Config rule to the development OU by using AWS CloudFormation StackSets."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p><span style=\"color: rgb(31, 35, 41); background-color: rgba(0, 0, 0, 0); font-size: 16px;\">Option C</span>:</p><p>The preventive guardrail in AWS Control Tower is a native compliance management tool for its multi-account architecture. It can be configured to allow only burstable instances and disallow irrelevant services; after applying this guardrail to the development OU, it can directly block the creation of non-compliant resources/services, which fully aligns with the Control Tower multi-account orchestration scenario.</p><li style=\"text-align: start;\">Option A: Although SCP (Service Control Policy) can achieve similar control, the company orchestrates multi-accounts through Control Tower—using Control Tower's preventive guardrail directly is more consistent with its existing management framework;</li><li style=\"text-align: start;\">Option B: A detective guardrail can only identify non-compliant operations but cannot block the use of irrelevant services, failing to meet the core requirement of \"disallow\";</li><li style=\"text-align: start;\">Option D: AWS Config rules are primarily for detection and cannot block the creation of non-compliant resources/services, so they cannot fulfill the \"disallow use\" requirement.</li>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "da9059fbec4c4236960ed20c389c27d0",
      "questionNumber": 329,
      "type": "single",
      "content": "<p>Question #329</p><p>A financial services company runs a complex, multi-tier application on Amazon EC2 instances and AWS Lambda functions. The application stores temporary data in Amazon S3. The S3 objects are valid for only 45 minutes and are deleted after 24 hours. </p><p><br></p><p>The company deploys each version of the application by launching an AWS CloudFormation stack. The stack creates all resources that are required to run the application. When the company deploys and validates a new application version.</p><p><br></p><p>The company deletes the CloudFormation stack of the old version. The company recently tried to delete the CloudFormation stack of an old application version, but the operation failed. An analysis shows that CloudFormation failed to delete an existing S3 bucket. A solutions architectneeds to resolve this issue without making major changes to the application's architecture. <br><br>Which solution meets these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Implement a Lambda function that deletes all files from a given S3 bucket. Integrate this Lambda function as a custom resource into the CloudFormation stack. Ensure that the custom resource has a DependsOn attribute that points to the S3 bucket&#39;s resource."
        },
        {
          "label": "B",
          "content": "Modify the CloudFormation template to provision an Amazon Elastic File System (Amazon EFS) file system to store the temporary files there instead of in Amazon S3. Configure the Lambda functions to run in the same VPC as the file system. Mount the file system to the EC2 instances and Lambda functions."
        },
        {
          "label": "C",
          "content": "Modify the CloudFormation stack to create an S3 Lifecycle rule that expires all objects 45 minutes after creation. Add a DependsOn attribute that points to the S3 bucket&rsquo;s resource."
        },
        {
          "label": "D",
          "content": "Modify the CloudFormation stack to attach a DeletionPolicy attribute with a value of Delete to the S3 bucket."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Option A is the correct answer. By implementing a Lambda function that empties the S3 bucket before deletion, the architect can ensure that the S3 bucket gets deleted as part of the CloudFormation stack deletion process. The DependsOn attribute ensures the order of operations, allowing the bucket to be emptied before attempting to delete it.</p><p>The issue is that CloudFormation fails to delete an S3 bucket because it contains objects, and CloudFormation cannot delete non-empty buckets by default. The solution must: &nbsp;</p><p>1. Ensure the bucket is empty before deletion (without major architectural changes). &nbsp;</p><p>2. Integrate seamlessly with CloudFormation stack deletion. &nbsp;</p><p>Option A achieves this by: &nbsp;</p><p>- Using a Lambda-backed custom resource to programmatically delete all objects in the bucket. &nbsp;</p><p>- Adding a `DependsOn` attribute to ensure the Lambda runs before CloudFormation attempts to delete the bucket. &nbsp;</p><p>- This is a minimal change and follows CloudFormation best practices for cleanup. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- B: Switching to EFS is a major architectural change (against the requirements). &nbsp;</p><p>- C: A lifecycle rule does not help—it only deletes objects after 45 minutes, but CloudFormation deletion is immediate. &nbsp;</p><p>- D: `DeletionPolicy: Delete` does not work for non-empty S3 buckets (default behavior already tries to delete the bucket). &nbsp;</p><p> Key Takeaway: &nbsp;</p><p>A custom resource (Lambda) to empty the bucket before deletion is the cleanest solution. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "3001f60b45104e1c97af31cec81672c5",
      "questionNumber": 330,
      "type": "single",
      "content": "<p>Question #330</p><p>A company has developed a mobile game. The backend for the game runs on several virtual machines located in an on-premises data center. The business logic is exposed using a REST API with multiple functions. Player session data is stored in central file storage. Backend services use different API keys for throttling and to distinguish between live and test traffic. <br><br>The load on the game backend varies throughout the day. During peak hours, the server capacity is not sufficient. There are also latency issues when fetching player session data. Management has asked a solutions architect to present a cloud architecture that can handle the game’s varying load and provide low-latency data access. The API model should not be changed. <br><br>Which solution meets these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Implement the REST API using a Network Load Balancer (NLB). Run the business logic on an Amazon EC2 instance behind the NLB. Store player session data in Amazon Aurora Serverless."
        },
        {
          "label": "B",
          "content": "Implement the REST API using an Application Load Balancer (ALB). Run the business logic in AWS Lambda. Store player session data in Amazon DynamoDB with on-demand capacity."
        },
        {
          "label": "C",
          "content": "Implement the REST API using Amazon API Gateway. Run the business logic in AWS Lambda. Store player session data in Amazon DynamoDB with on-demand capacity."
        },
        {
          "label": "D",
          "content": "Implement the REST API using AWS AppSync. Run the business logic in AWS Lambda. Store player session data in Amazon Aurora Serverless."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Option C is the correct answer. Using Amazon API Gateway in combination with AWS Lambda allows for a serverless architecture that can scale automatically to handle varying loads. DynamoDB with on-demand capacity provides low-latency access to player session data and scales well with the game's needs.</p><p>The requirements are: &nbsp;</p><p>1. Handle variable load (scaling during peak hours). &nbsp;</p><p>2. Low-latency data access (for player session data). &nbsp;</p><p>3. No changes to the API model (REST API must remain the same). &nbsp;</p><p>Option C meets all these requirements: &nbsp;</p><p>- Amazon API Gateway provides a fully managed REST API solution with automatic scaling and throttling (supports API keys). &nbsp;</p><p>- AWS Lambda runs the business logic, scaling automatically with demand. &nbsp;</p><p>- Amazon DynamoDB (on-demand) offers low-latency access to session data and scales seamlessly. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A: &nbsp;</p><p> &nbsp;- EC2 + NLB does not auto-scale (still requires manual capacity management). &nbsp;</p><p> &nbsp;- Aurora Serverless is not optimized for low-latency session storage (better for structured data, not transient session data). &nbsp;</p><p>- B: &nbsp;</p><p> &nbsp;- ALB + Lambda works, but API Gateway is better suited for REST APIs (built-in features like API keys, caching, and throttling). &nbsp;</p><p>- D: &nbsp;</p><p> &nbsp;- AWS AppSync is for GraphQL, not REST (violates the \"no API changes\" requirement). &nbsp;</p><p> Key Takeaway: &nbsp;</p><p>API Gateway + Lambda + DynamoDB (on-demand) is the best combination for scalable, low-latency REST APIs with variable load. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "a03f9343ea55426eb916c4048f6e31ea",
      "questionNumber": 331,
      "type": "single",
      "content": "<p>Question #331</p><p>A company is migrating an application to the AWS Cloud. The application runs in an on-premises data center and writes thousands of images into a mounted NFS file system each night. After the company migrates the application, the company will host the application on an Amazon EC2 instance with a mounted Amazon Elastic File System (Amazon EFS) file system. </p><p><br></p><p>The company has established an AWS Direct Connect connection to AWS. Before the migration cutover, a solutions architect must build a process that will replicate the newly created on-premises images to the EFS file system. </p><p><br></p><p>What is the MOST operationally efficient way to replicate the images?</p>",
      "options": [
        {
          "label": "A",
          "content": "Configure a periodic process to run the aws s3 sync command from the on-premises file system to Amazon S3. Configure an AWS Lambda function to process event notifications from Amazon S3 and copy the images from Amazon S3 to the EFS file system."
        },
        {
          "label": "B",
          "content": "Deploy an AWS Storage Gateway file gateway with an NFS mount point. Mount the file gateway file system on the on-premises server. Configure a process to periodically copy the images to the mount point."
        },
        {
          "label": "C",
          "content": "Deploy an AWS DataSync agent to an on-premises server that has access to the NFS file system. Send data over the Direct Connect connection to an S3 bucket by using a public VIF. Configure an AWS Lambda function to process event notifications from Amazon S3 and copy the images from Amazon S3 to the EFS file system."
        },
        {
          "label": "D",
          "content": "Deploy an AWS DataSync agent to an on-premises server that has access to the NFS file system. Send data over the Direct Connect connection to an AWS PrivateLink interface VPC endpoint for Amazon EFS by using a private VIF. Configure a DataSync scheduled task to send the images to the EFS file system every 24 hours."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Option D is the correct answer. Using AWS DataSync with a scheduled task to directly replicate data to Amazon EFS over a Direct Connect connection is the most operationally efficient solution. It leverages the Direct Connect for a secure and fast connection and automates the replication process without the need for additional Lambda functions or S3 intermediation.</p><p>The requirement is to efficiently replicate thousands of images from an on-premises NFS file system to Amazon EFS using AWS Direct Connect. The most operationally efficient solution should: &nbsp;</p><p>1. Leverage AWS DataSync (optimized for high-performance, scheduled data transfers). &nbsp;</p><p>2. Use Direct Connect (private VIF) for secure, low-latency transfer. &nbsp;</p><p>3. Avoid unnecessary steps (e.g., intermediate S3 storage + Lambda). &nbsp;</p><p>Option D meets all these criteria: &nbsp;</p><p>- AWS DataSync agent is installed on-premises to access the NFS file system. &nbsp;</p><p>- Direct Connect (private VIF) ensures secure, high-throughput transfer. &nbsp;</p><p>- PrivateLink for EFS allows direct, private connectivity to EFS. &nbsp;</p><p>- Scheduled task automates nightly syncs without manual intervention. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A: &nbsp;</p><p> &nbsp;- S3 + Lambda adds complexity (extra steps, slower than direct EFS sync). &nbsp;</p><p> &nbsp;- S3 sync is not optimized for large-scale file transfers compared to DataSync. &nbsp;</p><p>- B: &nbsp;</p><p> &nbsp;- Storage Gateway (File Gateway) is not ideal for nightly bulk syncs (better for continuous access, not batch transfers). &nbsp;</p><p>- C: &nbsp;</p><p> &nbsp;- S3 as an intermediate step is unnecessary (DataSync can write directly to EFS). &nbsp;</p><p> &nbsp;- Public VIF is less secure than private VIF for internal data transfers. &nbsp;</p><p> Key Takeaway: &nbsp;</p><p>AWS DataSync + Direct Connect (private VIF) + EFS PrivateLink is the most efficient and secure way to automate nightly image replication. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "30fcaaf443314a15bcf4020d2db6a590",
      "questionNumber": 332,
      "type": "single",
      "content": "<p>Question #332</p><p>A company recently migrated a web application from an on-premises data center to the AWS Cloud. The web application infrastructure consists of an Amazon CloudFront distribution that routes to an Application Load Balancer (ALB), with Amazon Elastic Container Service (Amazon ECS) to process requests. A recent security audit revealed that the web application is accessible by using both CloudFront and ALB endpoints. However, the company requires that the web application must be accessible only by using the CloudFront endpoint. </p><p><br></p><p>Which solution will meet this requirement with the LEAST amount of effort?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a new security group and attach it to the CloudFront distribution. Update the ALB security group ingress to allow access only from the CloudFront security group."
        },
        {
          "label": "B",
          "content": "Update ALB security group ingress to allow access only from the com.amazonaws.global.cloudfront.origin-facing CloudFront managed prefix list."
        },
        {
          "label": "C",
          "content": "Create a com.amazonaws.region.elasticloadbalancing VPC interface endpoint for Elastic Load Balancing. Update the ALB scheme from internet-facing to internal."
        },
        {
          "label": "D",
          "content": "Extract CloudFront IPs from the AWS provided ip-ranges.json document. Update ALB security group ingress to allow access only from CloudFront IPs."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Update ALB security group ingress to allow access only from the com.amazonaws.global.cloudfront.origin-facing CloudFront managed prefix list.</p><p>This solution will meet the requirement with the least amount of effort for the following reasons:</p><p>AWS-managed solution: The CloudFront managed prefix list (com.amazonaws.global.cloudfront.origin-facing) is maintained by AWS and contains all the IP addresses that CloudFront uses to route requests to origins. This means you don't need to manually track or update IP ranges.</p><p>Simple implementation: Updating the ALB security group to reference this managed prefix list requires just a single configuration change in the security group's inbound rules.</p><p>Automatic updates: When AWS adds or changes CloudFront IP ranges, the managed prefix list is automatically updated, ensuring your security configuration remains effective without any manual intervention.</p><p>No additional resources: This approach doesn't require creating any new AWS resources, just modifying an existing security group.</p><p>Option A (creating a new security group for CloudFront) is not feasible because:</p><p>CloudFront is a global service and doesn't run within your VPC, so you cannot attach a security group to a CloudFront distribution.</p><p>CloudFront uses a wide range of IP addresses that change over time, making it impractical to manually maintain a list of these IPs in a security group.</p><p>This approach would require ongoing maintenance to keep the IP ranges up to date, increasing operational overhead.</p><p>By using the AWS-managed prefix list in the ALB security group, you ensure that only requests coming through CloudFront can reach your ALB, effectively restricting direct access to the ALB endpoint. This meets the security requirement that the web application must be accessible only through the CloudFront endpoint, and it does so with minimal effort and maintenance.</p><p>Sources</p><p>ALB security group with cloudFront origin-facing prefix list ingress setting blocks target group health check for eks fargate | AWS re:Post （https://repost.aws/questions/QUHLS6hJytSXOTIIUVL9UFnQ/alb-security-group-with-cloudfront-origin-facing-prefix-list-ingress-setting-blocks-target-group-health-check-for-eks-fargate）</p><p>Limit access to your origins using the AWS-managed prefix list for Amazon CloudFront | Networking & Content Delivery （https://aws.amazon.com/cn/blogs/networking-and-content-delivery/limit-access-to-your-origins-using-the-aws-managed-prefix-list-for-amazon-cloudfront/）</p><p>Restrict access to Application Load Balancers - Amazon CloudFront （https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/restrict-access-to-load-balancer.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "81dc0df32a67430b91b45df41ffd0df3",
      "questionNumber": 333,
      "type": "single",
      "content": "Question #333<p>A company hosts a community forum site using an Application Load Balancer (ALB) and a Docker application hosted in an Amazon ECS cluster. The site data is stored in Amazon RDS for MySQL and the container image is stored in ECR. The company needs to provide their customers with a disaster recovery SLA with an RTO of no more than 24 hours and RPO of no more than 8 hours. <br><br>Which of the following solutions is the MOST cost-effective way to meet the requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use AWS CloudFormation to deploy identical ALB, EC2, ECS and RDS resources in two regions. Schedule RDS snapshots every 8 hours. Use RDS multi-region replication to update the secondary region&#39;s copy of the database. In the event of a failure, restore from the latest snapshot, and use an Amazon Route 53 DNS failover policy to automatically redirect customers to the ALB in the secondary region."
        },
        {
          "label": "B",
          "content": "Store the Docker image in ECR in two regions. Schedule RDS snapshots every 8 hours with snapshots copied to the secondary region. In the event of a failure, use AWS CloudFormation to deploy the ALB, EC2, ECS and RDS resources in the secondary region, restore from the latest snapshot, and update the DNS record to point to the ALB in the secondary region."
        },
        {
          "label": "C",
          "content": "Use AWS CloudFormation to deploy identical ALB, EC2, ECS, and RDS resources in a secondary region. Schedule hourly RDS MySQL backups to Amazon S3 and use cross-region replication to replicate data to a bucket in the secondary region. In the event of a failure, import the latest Docker image to Amazon ECR in the secondary region, deploy to the EC2 instance, restore the latest MySQL backup, and update the DNS record to point to the ALB in the secondary region."
        },
        {
          "label": "D",
          "content": "Deploy a pilot light environment in a secondary region with an ALB and a minimal resource EC2 deployment for Docker in an AWS Auto Scaling group with a scaling policy to increase instance size and number of nodes. Create a cross-region read replica of the RDS data. In the event of a failure, promote the replica to primary, and update the DNS record to point to the ALB in the secondary region."
        }
      ],
      "correctAnswer": "B",
      "explanation": "Option B is the correct answer. This solution involves storing the Docker image in ECR across two regions and taking RDS snapshots every 8 hours, which aligns with the RPO requirement. In the event of a failure, the company can deploy necessary resources in the secondary region using AWS CloudFormation and restore from the latest RDS snapshot, which meets the RTO requirement. This approach is cost-effective as it does not maintain a hot standby environment continuously.",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "e187f286e5c54871a806a19350359b22",
      "questionNumber": 334,
      "type": "single",
      "content": "<p>Question #334</p><p>A company is migrating its infrastructure to the AWS Cloud. The company must comply with a variety of regulatory standards for different projects. The company needs a multi-account environment. </p><p><br></p><p>A solutions architect needs to prepare the baseline infrastructure. The solution must provide a consistent baseline of management and security, but it must allow flexibility for different compliance requirements within various AWS accounts. The solution also needs to integrate with the existing on-premises Active Directory Federation Services (AD FS) server. </p><p><br></p><p>Which solution meets these requirements with the LEAST amount of operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an organization in AWS Organizations. Create a single SCP for least privilege access across all accounts. Create a single OU for all accounts. Configure an IAM identity provider for federation with the on-premises AD FS server. Configure a central logging account with a defined process for log generating services to send log events to the central account. Enable AWS Config in the central account with conformance packs for all accounts."
        },
        {
          "label": "B",
          "content": "Create an organization in AWS Organizations. Enable AWS Control Tower on the organization. Review included controls (guardrails) for SCPs. Check AWS Config for areas that require additions. Add OUs as necessary. Connect AWS IAM Identity Center (AWS Single Sign-On) to the on-premises AD FS server."
        },
        {
          "label": "C",
          "content": "Create an organization in AWS Organizations. Create SCPs for least privilege access. Create an OU structure, and use it to group AWS accounts. Connect AWS IAM Identity Center (AWS Single Sign-On) to the on-premises AD FS server. Configure a central logging account with a defined process for log generating services to send log events to the central account. Enable AWS Config in the central account with aggregators and conformance packs."
        },
        {
          "label": "D",
          "content": "Create an organization in AWS Organizations. Enable AWS Control Tower on the organization. Review included controls (guardrails) for SCPs. Check AWS Config for areas that require additions. Configure an IAM identity provider for federation with the on-premises AD FS server."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>The best solution that meets the requirements with the LEAST operational overhead is: &nbsp;</p><p> Option B &nbsp;</p><p>Create an organization in AWS Organizations. Enable AWS Control Tower on the organization. Review included controls (guardrails) for SCPs. Check AWS Config for areas that require additions. Add OUs as necessary. Connect AWS IAM Identity Center (AWS Single Sign-On) to the on-premises AD FS server. &nbsp;</p><p> Why Option B? &nbsp;</p><p>1. AWS Control Tower – Provides a pre-configured baseline for security, compliance, and governance across multiple accounts with automated guardrails (SCPs) and centralized logging. This reduces manual setup and operational overhead. &nbsp;</p><p>2. IAM Identity Center (AWS SSO) – Simplifies federation with on-premises AD FS, enabling seamless single sign-on (SSO) for users. &nbsp;</p><p>3. Organizational Units (OUs) – Allows grouping accounts by compliance requirements while maintaining a consistent baseline. &nbsp;</p><p>4. Least Operational Overhead – Control Tower automates many best practices (SCPs, logging, identity management), reducing manual configurations. &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- Option A: Requires manual setup of SCPs, logging, and AWS Config, increasing operational overhead. &nbsp;</p><p>- Option C: Involves manual SCP and AWS Config management, which is less efficient than Control Tower’s automation. &nbsp;</p><p>- Option D: Uses IAM Identity Provider instead of IAM Identity Center (AWS SSO), which is less streamlined for AD FS integration. &nbsp;</p><p> Conclusion &nbsp;</p><p>Option B is the best choice because it leverages AWS Control Tower for automated governance and IAM Identity Center (AWS SSO) for seamless AD FS integration, minimizing operational effort while ensuring compliance flexibility. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "608fb8610d4e43be92add75e80b3c713",
      "questionNumber": 335,
      "type": "multiple",
      "content": "<p>Question #335</p><p>An online magazine will launch its latest edition this month. This edition will be the first to be distributed globally. The magazine's dynamic website currently uses an Application Load Balancer (ALB) in front of the web tier, a fleet of Amazon EC2 instances for web and application servers, and Amazon Aurora MySQL. Portions of the website include static content and almost all traffic is read-only. </p><p><br></p><p>The magazine is expecting a significant spike in internet traffic when the new edition is launched. Optimal performance is a top priority for the week following the launch. </p><p><br></p><p>Which combination of steps should a solutions architect take to reduce system response times for a global audience? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Use logical cross-Region replication to replicate the Aurora MySQL database to a secondary Region. Replace the web servers with Amazon S3. Deploy S3 buckets in cross-Region replication mode."
        },
        {
          "label": "B",
          "content": "Ensure the web and application tiers are each in Auto Scaling groups. Introduce an AWS Direct Connect connection. Deploy the web and application tiers in Regions across the world."
        },
        {
          "label": "C",
          "content": "Migrate the database from Amazon Aurora to Amazon RDS for MySQL. Ensure all three of the application tiers &ndash; web, application, and database &ndash; are in private subnets."
        },
        {
          "label": "D",
          "content": "Use an Aurora global database for physical cross-Region replication. Use Amazon S3 with cross-Region replication for static content and resources. Deploy the web and application tiers in Regions across the world."
        },
        {
          "label": "E",
          "content": "Introduce Amazon Route 53 with latency-based routing and Amazon CloudFront distributions. Ensure the web and application tiers are each in Auto Scaling groups."
        }
      ],
      "correctAnswer": "DE",
      "explanation": "<p>Option D is correct because using an Aurora global database provides cross-Region replication with low latency, which is suitable for read-only traffic. Option E is also correct as Amazon CloudFront can cache static content close to users worldwide, and Route 53 with latency-based routing can direct users to the nearest web tier, reducing response times. </p><p>Option D &nbsp;</p><p>Use an Aurora global database for physical cross-Region replication. Use Amazon S3 with cross-Region replication for static content and resources. Deploy the web and application tiers in Regions across the world. &nbsp;</p><p> Option E &nbsp;</p><p>Introduce Amazon Route 53 with latency-based routing and Amazon CloudFront distributions. Ensure the web and application tiers are each in Auto Scaling groups. &nbsp;</p><p> Why Options D & E? &nbsp;</p><p> Option D (Global Database & Multi-Region Deployment) &nbsp;</p><p>- Aurora Global Database – Provides low-latency reads in multiple regions with physical replication (faster than logical replication). &nbsp;</p><p>- S3 Cross-Region Replication (CRR) – Ensures static content is available globally with minimal latency. &nbsp;</p><p>- Multi-Region Deployment – Places web and app tiers closer to users, reducing response times. &nbsp;</p><p> Option E (CDN & Auto Scaling) &nbsp;</p><p>- Amazon CloudFront – Caches static and dynamic content at edge locations, reducing latency for global users. &nbsp;</p><p>- Route 53 (Latency-Based Routing) – Directs users to the nearest available endpoint for the best performance. &nbsp;</p><p>- Auto Scaling – Ensures the web and app tiers scale dynamically to handle traffic spikes. &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- Option A: Replacing web servers with S3 is impractical (S3 is for static content, not dynamic web apps). &nbsp;</p><p>- Option B: Direct Connect is unnecessary for global performance (it’s for private network connectivity, not latency reduction). &nbsp;</p><p>- Option C: Migrating to RDS MySQL is a downgrade (Aurora is better for scalability and global replication). &nbsp;</p><p> Conclusion &nbsp;</p><p>D + E is the best combination because: &nbsp;</p><p>✔ Aurora Global Database + S3 CRR ensures low-latency data access worldwide. &nbsp;</p><p>✔ CloudFront + Route 53 optimizes content delivery and routing. &nbsp;</p><p>✔ Auto Scaling maintains performance during traffic spikes. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "0baf1dd989a247fb9760530bff6fc5da",
      "questionNumber": 336,
      "type": "single",
      "content": "<p>Question #336</p><p>An online gaming company needs to optimize the cost of its workloads on AWS. The company uses a dedicated account to host the production environment for its online gaming application and an analytics application. <br><br>Amazon EC2 instances host the gaming application and must always be available. The EC2 instances run all year. The analytics application uses data that is stored in Amazon S3. The analytics application can be interrupted and resumed without issue. <br><br>Which solution will meet these requirements MOST cost-effectively?</p>",
      "options": [
        {
          "label": "A",
          "content": "Purchase an EC2 Instance Savings Plan for the online gaming application instances. Use On-Demand Instances for the analytics application."
        },
        {
          "label": "B",
          "content": "Purchase an EC2 Instance Savings Plan for the online gaming application instances. Use Spot Instances for the analytics application."
        },
        {
          "label": "C",
          "content": "Use Spot Instances for the online gaming application and the analytics application. Set up a catalog in AWS Service Catalog to provision services at a discount."
        },
        {
          "label": "D",
          "content": "Use On-Demand Instances for the online gaming application. Use Spot Instances for the analytics application. Set up a catalog in AWS Service Catalog to provision services at a discount."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Purchase an EC2 Instance Savings Plan for the online gaming application instances. Use Spot Instances for the analytics application.</p><p>This solution will meet the requirements most cost-effectively for the following reasons:</p><p>For the online gaming application:</p><p>The gaming application must always be available and runs continuously throughout the year</p><p>EC2 Instance Savings Plan is ideal for this workload as it provides significant discounts (up to 72% compared to On-Demand pricing) in exchange for a commitment to a consistent amount of compute usage</p><p>This type of Savings Plan is instance-specific, allowing the company to commit to the exact instance types needed for the gaming application</p><p>Since the application runs all year, the company can maximize the utilization of their Savings Plan commitment</p><p>For the analytics application:</p><p>The key characteristic of this application is that it \"can be interrupted and resumed without issue\"</p><p>Spot Instances are perfect for this use case as they offer the deepest discounts (up to 90% off On-Demand prices) but can be interrupted with short notice</p><p>Since the analytics application is designed to handle interruptions, it can take advantage of Spot Instance pricing without compromising functionality</p><p>The data for the analytics application is stored in Amazon S3, so even if instances are terminated, no data is lost</p><p>Option A (using On-Demand Instances for the analytics application) would be significantly more expensive than using Spot Instances. On-Demand Instances provide flexibility without upfront payment or long-term commitment, but they come at a premium price compared to Spot Instances. Since the analytics application can handle interruptions, there's no operational benefit to paying the higher On-Demand prices.</p><p>The combination of EC2 Instance Savings Plan for the always-on gaming application and Spot Instances for the interruptible analytics application provides the optimal balance of reliability and cost-effectiveness. This approach ensures the gaming application remains continuously available while minimizing costs for the analytics workload that can tolerate interruptions.</p><p>Sources</p><p>Select the best pricing model - Cost Optimization Pillar （https://docs.aws.amazon.com/wellarchitected/latest/cost-optimization-pillar/select-the-best-pricing-model.html）</p><p>Cost-effective resources - Games Industry Lens（https://docs.aws.amazon.com/wellarchitected/latest/games-industry-lens/games-cost-cost-eff-resc.html） </p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "0ec34022d19c4545b8b2c3b82bdd5f7b",
      "questionNumber": 337,
      "type": "multiple",
      "content": "<p>Question #337</p><p>A company runs applications in hundreds of production AWS accounts. The company uses AWS Organizations with all features enabled and has a centralized backup operation that uses AWS Backup. The company is concerned about ransomware attacks. </p><p><br></p><p>To address this concern, the company has created a new policy that all backups must be resilient to breaches of privileged-user credentials in any production account. </p><p><br></p><p>Which combination of steps will meet this new requirement? (Choose three.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Implement cross-account backup with AWS Backup vaults in designated non-production accounts."
        },
        {
          "label": "B",
          "content": "Add an SCP that restricts the modification of AWS Backup vaults."
        },
        {
          "label": "C",
          "content": "Implement AWS Backup Vault Lock in compliance mode."
        },
        {
          "label": "D",
          "content": "Implement least privilege access for the IAM service role that is assigned to AWS Backup."
        },
        {
          "label": "E",
          "content": "Configure the backup frequency, lifecycle, and retention period to ensure that at least one backup always exists in the cold tier."
        },
        {
          "label": "F",
          "content": "<p><span style=\"color: rgb(80, 80, 80);\">Configure AWS Backup to write all backups to an Amazon S3 bucket in a designated non-production account. Ensure that the S3 bucket has S3 Object Lock enabled.</span></p>"
        }
      ],
      "correctAnswer": "ABD",
      "explanation": "<p style=\"text-align: start;\"><span style=\"color: rgb(0, 0, 0); background-color: rgba(0, 0, 0, 0); font-size: 16px;\"><strong>Option A</strong></span>: Implement cross-account backup to AWS Backup vaults in designated non-production accounts. This isolates backup resources from production accounts—even if privileged-user credentials in production accounts are compromised, the attacker cannot access backup vaults in non-production accounts, securing backups at the resource level.</p><p style=\"text-align: start;\"><span style=\"color: rgb(0, 0, 0); background-color: rgba(0, 0, 0, 0); font-size: 16px;\"><strong>Option B</strong></span>: Add an SCP to restrict modifications to AWS Backup vaults. Leveraging the permission management capabilities of AWS Organizations, this blocks privileged users (or compromised credentials) in production accounts from tampering with or deleting backup vaults, enhancing backup integrity protection.</p><p style=\"text-align: start;\"><span style=\"color: rgb(0, 0, 0); background-color: rgba(0, 0, 0, 0); font-size: 16px;\"><strong>Option D</strong></span>: Implement least privilege access for the IAM service role assigned to AWS Backup. This role retains only the necessary permissions for completing backup tasks (no excessive backup deletion/modification permissions), reducing the risk of abuse (e.g., manipulation via compromised privileged-user credentials in production accounts).</p><ul><li style=\"text-align: start;\">Option C: AWS Backup Vault Lock in compliance mode is mainly used to prevent backups in vaults from being deleted or modified. However, cross-account backup has already isolated vaults to non-production accounts, so privileged users in production accounts cannot access them—this configuration is therefore unnecessary;</li><li style=\"text-align: start;\">Option E: Configuring backup frequency, lifecycle, etc., only optimizes storage costs and is irrelevant to the security requirement of \"resisting privileged-user credential breaches\";</li><li style=\"text-align: start;\">Option F: AWS Backup stores backups in dedicated Backup vaults and cannot write directly to S3 buckets. This configuration does not align with AWS Backup's usage logic and cannot meet the requirement.</li></ul>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "815a643391264df2bbc3c614e9ab4291",
      "questionNumber": 338,
      "type": "multiple",
      "content": "<p>Question #338</p><p>A company needs to aggregate Amazon CloudWatch logs from its AWS accounts into one central logging account. The collected logs must remain in the AWS Region of creation. The central logging account will then process the logs, normalize the logs into standard output format, and stream the output logs to a security tool for more processing. </p><p><br></p><p>A solutions architect must design a solution that can handle a large volume of logging data that needs to be ingested. Less logging will occur outside normal business hours than during normal business hours. The logging solution must scale with the anticipated load. The solutions architect has decided to use an AWS Control Tower design to handle the multi-account logging process. </p><p><br></p><p>Which combination of steps should the solutions architect take to meet the requirements? (Choose three.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a destination Amazon Kinesis data stream in the central logging account."
        },
        {
          "label": "B",
          "content": "Create a destination Amazon Simple Queue Service (Amazon SQS) queue in the central logging account."
        },
        {
          "label": "C",
          "content": "Create an IAM role that grants Amazon CloudWatch Logs the permission to add data to the Amazon Kinesis data stream. Create a trust policy. Specify the trust policy in the IAM role. In each member account, create a subscription filter for each log group to send data to the Kinesis data stream."
        },
        {
          "label": "D",
          "content": "Create an IAM role that grants Amazon CloudWatch Logs the permission to add data to the Amazon Simple Queue Service (Amazon SQS) queue. Create a trust policy. Specify the trust policy in the IAM role. In each member account, create a single subscription filter for all log groups to send data to the SQS queue."
        },
        {
          "label": "E",
          "content": "Create an AWS Lambda function. Program the Lambda function to normalize the logs in the central logging account and to write the logs to the security tool."
        },
        {
          "label": "F",
          "content": "Create an AWS Lambda function. Program the Lambda function to normalize the logs in the member accounts and to write the logs to the security tool."
        }
      ],
      "correctAnswer": "ACE",
      "explanation": "<p>The correct combination of steps to meet the requirements is: &nbsp;</p><p> Option A &nbsp;</p><p>✅ Create a destination Amazon Kinesis data stream in the central logging account. &nbsp;</p><p>- Kinesis Data Streams is ideal for high-volume, real-time log ingestion and can scale automatically with varying loads (business hours vs. off-hours). &nbsp;</p><p> Option C &nbsp;</p><p>✅ Create an IAM role that grants Amazon CloudWatch Logs the permission to add data to the Kinesis data stream. Create a trust policy. Specify the trust policy in the IAM role. In each member account, create a subscription filter for each log group to send data to the Kinesis data stream. &nbsp;</p><p>- IAM Role & Trust Policy: Ensures secure cross-account log streaming. &nbsp;</p><p>- Subscription Filters: Allow per-log-group forwarding to Kinesis (required for regional log retention). &nbsp;</p><p> Option E &nbsp;</p><p>✅ Create an AWS Lambda function. Program the Lambda function to normalize the logs in the central logging account and to write the logs to the security tool. &nbsp;</p><p>- Lambda can process, normalize, and forward logs to the security tool efficiently. &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- Option B (SQS): Not suitable for high-volume log streaming (Kinesis is better for real-time, scalable ingestion). &nbsp;</p><p>- Option D (SQS + Single Subscription Filter): SQS is not optimal for logs, and a single subscription filter cannot handle per-log-group forwarding. &nbsp;</p><p>- Option F (Lambda in Member Accounts): Violates the requirement to process logs in the central account. &nbsp;</p><p> Summary of Correct Steps &nbsp;</p><p>1. Kinesis Data Stream (A) → Handles scalable log ingestion. &nbsp;</p><p>2. IAM Role + Subscription Filters (C) → Securely forwards logs from member accounts. &nbsp;</p><p>3. Lambda in Central Account (E) → Normalizes and forwards logs to the security tool. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "3cc8f79791b14f51b73e290f9cf04021",
      "questionNumber": 339,
      "type": "multiple",
      "content": "<p>Question #339</p><p>A company is migrating a legacy application from an on-premises data center to AWS. The application consists of a single application server and a Microsoft SQL Server database server. Each server is deployed on a VMware VM that consumes 500 TB of data across multiple attached volumes. </p><p><br></p><p>The company has established a 10 Gbps AWS Direct Connect connection from the closest AWS Region to its on-premises data center. The Direct Connect connection is not currently in use by other services. </p><p><br></p><p>Which combination of steps should a solutions architect take to migrate the application with the LEAST amount of downtime? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Use anAWS Server Migration Service (AWS SMS) replication job to migrate the database server VM to AWS."
        },
        {
          "label": "B",
          "content": "Use VM Import/Export to import the application server VM."
        },
        {
          "label": "C",
          "content": "Export the VM images to an AWS Snowball Edge Storage Optimized device."
        },
        {
          "label": "D",
          "content": "Use an AWS Server Migration Service (AWS SMS) replication job to migrate the application server VM to AWS."
        },
        {
          "label": "E",
          "content": "Use an AWS Database Migration Service (AWS DMS) replication instance to migrate the database to an Amazon RDS DB instance."
        }
      ],
      "correctAnswer": "DE",
      "explanation": "<p>Based on the information provided, the correct combination of steps to migrate the application with the least amount of downtime is:</p><p>D. Use an AWS Server Migration Service (AWS SMS) replication job to migrate the application server VM to AWS. E. Use an AWS Database Migration Service (AWS DMS) replication instance to migrate the database to an Amazon RDS DB instance.</p><p>Here's why this combination provides the least downtime:</p><p>AWS Server Migration Service (AWS SMS) for the application server:</p><p>AWS SMS is designed specifically for migrating VMware VMs to AWS</p><p>It supports incremental replication, which means it can transfer the initial data and then only sync the changes</p><p>This approach minimizes downtime because the application can continue running during the replication process</p><p>SMS automatically handles the conversion of the VM to an Amazon EC2 instance, preserving the operating system, applications, and configurations</p><p>AWS Database Migration Service (AWS DMS) for the database:</p><p>AWS DMS is purpose-built for database migrations with minimal downtime</p><p>It supports Microsoft SQL Server as a source and Amazon RDS as a target</p><p>DMS can perform continuous data replication using Change Data Capture (CDC), which means the source database can remain operational during migration</p><p>It captures ongoing changes to the source database and applies them to the target RDS instance, keeping both in sync</p><p>The combination of these services leverages the 10 Gbps Direct Connect connection efficiently by:</p><p>Allowing both migrations to occur simultaneously</p><p>Supporting incremental and continuous replication to minimize the final cutover time</p><p>Enabling testing of the migrated environment before the final switch</p><p>The migration process would typically follow these steps:</p><p>Set up AWS SMS to replicate the application server VM to AWS</p><p>Configure AWS DMS to perform the initial load of the SQL Server database to Amazon RDS</p><p>Enable ongoing replication in DMS to capture changes during the migration</p><p>Test the migrated application with the new RDS database</p><p>Schedule a brief maintenance window for the final cutover</p><p>Redirect traffic to the new AWS environment</p><p>This approach allows the company to migrate both the application server and database with minimal disruption to their operations, making it the optimal choice for achieving the least amount of downtime during the migration.</p><p>Sources</p><p>AWS Migration and Transfer category icon Migration and transfer - Overview of Amazon Web Services （https://docs.aws.amazon.com/whitepapers/latest/aws-overview/migration-services.html）</p><p>Tools for heterogeneous database migrations - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-sql-server/heterogeneous-migration-tools.html）</p><p>Migrate legacy database | AWS re:Post （https://repost.aws/questions/QUTC4STLNPTNeCYjbQMSErcA/migrate-legacy-database）</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "1dc1c9f955c84bee94de361c094a655c",
      "questionNumber": 340,
      "type": "multiple",
      "content": "<p>Question #340</p><p>A company operates a fleet of servers on premises and operates a fleet of Amazon EC2 instances in its organization in AWS Organizations. The company's AWS accounts contain hundreds of VPCs. The company wants to connect its AWS accounts to its on-premises network. AWS Site-to-Site VPN connections are already established to a single AWS account. The company wants to control which VPCs can communicate with other VPCs. </p><p><br></p><p>Which combination of steps will achieve this level of control with the LEAST operational effort? (Choose three.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a transit gateway in an AWS account. Share the transit gateway across accounts by using AWS Resource Access Manager (AWS RAM)."
        },
        {
          "label": "B",
          "content": "Configure attachments to all VPCs and VPNs."
        },
        {
          "label": "C",
          "content": "Setup transit gateway route tables. Associate the VPCs and VPNs with the route tables."
        },
        {
          "label": "D",
          "content": "Configure VPC peering between the VPCs."
        },
        {
          "label": "E",
          "content": "Configure attachments between the VPCs and VPNs."
        },
        {
          "label": "F",
          "content": "Setup route tables on the VPCs and VPNs."
        }
      ],
      "correctAnswer": "ABC",
      "explanation": "<p>Based on the information provided, the correct combination of steps to achieve the required level of control with the least operational effort is:</p><p>A. Create a transit gateway in an AWS account. Share the transit gateway across accounts by using AWS Resource Access Manager (AWS RAM). B. Configure attachments to all VPCs and VPNs. C. Setup transit gateway route tables. Associate the VPCs and VPNs with the route tables.</p><p>Here's why this combination is the most effective solution:</p><p>Create a transit gateway in an AWS account. Share the transit gateway across accounts by using AWS Resource Access Manager (AWS RAM):</p><p>Transit Gateway acts as a network transit hub that connects VPCs and on-premises networks</p><p>Using AWS RAM to share the transit gateway across accounts eliminates the need to create and manage separate transit gateways in each account</p><p>This centralized approach significantly reduces operational overhead compared to managing hundreds of individual connections</p><p>The transit gateway can be managed from a single account while being accessible to all accounts in the organization</p><p>Configure attachments to all VPCs and VPNs:</p><p>Transit Gateway attachments connect the transit gateway to VPCs and the existing Site-to-Site VPN connections</p><p>This creates a hub-and-spoke network architecture where all traffic flows through the transit gateway</p><p>Each VPC only needs a single attachment to the transit gateway, rather than multiple connections to other VPCs</p><p>The existing VPN connections can be attached to the transit gateway, leveraging the already established connections to the on-premises network</p><p>Setup transit gateway route tables. Associate the VPCs and VPNs with the route tables:</p><p>Transit Gateway route tables provide granular control over which VPCs can communicate with each other</p><p>By creating multiple route tables and associating specific VPCs and VPNs with them, the company can implement network segmentation</p><p>This allows for precise control of traffic flows between VPCs and between VPCs and the on-premises network</p><p>Route propagation can be configured to automatically update routes when new resources are added</p><p>This combination provides a scalable, centrally managed solution that can handle hundreds of VPCs across multiple accounts while maintaining fine-grained control over network traffic. It leverages the existing VPN connections and provides a framework that can easily accommodate future growth with minimal operational overhead.</p><p>Sources</p><p>Transit gateway attachment configuration | AWS re:Post （https://repost.aws/questions/QUvX0QyO3pRQWkBQspwcgVKg/transit-gateway-attachment-configuration）</p><p>Enable communication from VPC A to on-premise through VPC B | AWS re:Post （https://repost.aws/questions/QUousDSAqlR4-KscOJXcEA_w/enable-communication-from-vpc-a-to-on-premise-through-vpc-b）</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "71ce224ed1c544389c68cd1922cfde5b",
      "questionNumber": 341,
      "type": "single",
      "content": "<p>Question #341</p><p>A company needs to optimize the cost of its application on AWS. The application uses AWS Lambda functions and Amazon Elastic Container Service (Amazon ECS) containers that run on AWS Fargate. The application is write-heavy and stores data in an Amazon Aurora MySQL database. <br><br>The load on the application is not consistent. The application experiences long periods of no usage, followed by sudden and significant increases and decreases in traffic. The database runs on a memory optimized DB instance that cannot handle the load. <br><br>A solutions architect must design a solution that can scale to handle the changes in traffic.</p><p><br></p><p>Which solution will meet these requirements MOST cost-effectively?</p>",
      "options": [
        {
          "label": "A",
          "content": "Add additional read replicas to the database. Purchase Instance Savings Plans and RDS Reserved Instances."
        },
        {
          "label": "B",
          "content": "Migrate the database to an Aurora DB cluster that has multiple writer instances. Purchase Instance Savings Plans."
        },
        {
          "label": "C",
          "content": "Migrate the database to an Aurora global database. Purchase Compute Savings Plans and RDS Reserved instances."
        },
        {
          "label": "D",
          "content": "Migrate the database to Aurora Serverless v1. Purchase Compute Savings Plans."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Option D is the correct answer. Aurora Serverless v1 is designed to automatically scale with the application's load, making it a cost-effective choice for workloads with unpredictable traffic. Compute Savings Plans can provide additional cost savings for compute resources.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "6e4ec199c02e480c86f6f06188ff94e3",
      "questionNumber": 342,
      "type": "single",
      "content": "<p>Question #342</p><p>A company migrated an application to the AWS Cloud. The application runs on two Amazon EC2 instances behind an Application Load Balancer (ALB). <br><br>Application data is stored in a MySQL database that runs on an additional EC2 instance. The application's use of the database is read-heavy. <br><br>The application loads static content from Amazon Elastic Block Store (Amazon EBS) volumes that are attached to each EC2 instance. The static content is updated frequently and must be copied to each EBS volume. <br><br>The load on the application changes throughout the day. During peak hours, the application cannot handle all the incoming requests. Trace data shows that the database cannot handle the read load during peak hours.</p><p><br></p><p>Which solution will improve the reliability of the application?</p>",
      "options": [
        {
          "label": "A",
          "content": "Migrate the application to a set of AWS Lambda functions. Set the Lambda functions as targets for the ALB. Create a new single EBS volume for the static content. Configure the Lambda functions to read from the new EBS volume. Migrate the database to an Amazon RDS for MySQL Multi-AZ DB cluster."
        },
        {
          "label": "B",
          "content": "Migrate the application to a set of AWS Step Functions state machines. Set the state machines as targets for the ALB. Create an Amazon Elastic File System (Amazon EFS) file system for the static content. Configure the state machines to read from the EFS file system. Migrate the database to Amazon Aurora MySQL Serverless v2 with a reader DB instance."
        },
        {
          "label": "C",
          "content": "Containerize the application. Migrate the application to an Amazon Elastic Container Service (Amazon ECS) cluster. Use the AWS Fargate launch type for the tasks that host the application. Create a new single EBS volume for the static content. Mount the new EBS volume on the ECS cluster. Configure AWS Application Auto Scaling on the ECS cluster. Set the ECS service as a target for the ALB. Migrate the database to an Amazon RDS for MySQL Multi-AZ DB cluster."
        },
        {
          "label": "D",
          "content": "Containerize the application. Migrate the application to an Amazon Elastic Container Service (Amazon ECS) cluster. Use the AWS Fargate launch type for the tasks that host the application. Create an Amazon Elastic File System (Amazon EFS) file system for the static content. Mount the EFS file system to each container. Configure AWS Application Auto Scaling on the ECS cluster. Set the ECS service as a target for the ALB. Migrate the database to Amazon Aurora MySQL Serverless v2 with a reader DB instance."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>The correct answer is D. Here's the detailed reasoning:</p><p> Key Issues in the Current Setup:</p><p>1. Database Read Bottleneck: The MySQL database cannot handle read-heavy workloads during peak hours.</p><p>2. Static Content Management: Static content is stored on individual EBS volumes, which must be manually copied to each EC2 instance, causing inefficiency.</p><p>3. Scalability: The application cannot handle peak traffic due to fixed EC2 capacity.</p><p> Solution Analysis:</p><p>- Option A: </p><p> &nbsp;- Migrating to Lambda is not ideal for applications requiring persistent storage (static content on EBS). </p><p> &nbsp;- A single EBS volume cannot be shared across Lambda functions (they are stateless).</p><p> &nbsp;- RDS Multi-AZ improves availability but does not solve the read scalability issue.</p><p>- Option B: </p><p> &nbsp;- AWS Step Functions is not suitable for hosting a web application (it’s an orchestration service, not a compute service). </p><p> &nbsp;- While EFS solves the static content issue and Aurora Serverless v2 with a reader helps with read scaling, the compute solution is incorrect.</p><p>- Option C: </p><p> &nbsp;- Containerizing with ECS Fargate is a good approach for scalability. </p><p> &nbsp;- However, using a single EBS volume is problematic because EBS cannot be mounted to multiple tasks simultaneously. </p><p> &nbsp;- RDS Multi-AZ does not address read scaling.</p><p>- Option D (Correct Answer):</p><p> &nbsp;- Containerization with ECS Fargate: Enables automatic scaling of compute resources.</p><p> &nbsp;- Amazon EFS for Static Content: Provides a shared file system accessible by all containers, eliminating the need to manually sync content.</p><p> &nbsp;- Aurora MySQL Serverless v2 with a Reader: </p><p> &nbsp; &nbsp;- Aurora Serverless v2 automatically scales database capacity.</p><p> &nbsp; &nbsp;- A reader instance offloads read traffic, solving the read-heavy bottleneck.</p><p> &nbsp;- Application Auto Scaling: Dynamically adjusts the number of ECS tasks based on demand.</p><p> Why Option D is Best:</p><p>- Solves database read scalability with Aurora Serverless v2 + reader.</p><p>- Solves static content management with EFS (shared across containers).</p><p>- Solves compute scalability with ECS Fargate + Auto Scaling.</p><p>- Maintains high availability and reliability.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "66d35abc872a4e74882889d2e3067479",
      "questionNumber": 343,
      "type": "single",
      "content": "<p>Question #343</p><p>A solutions architect wants to make sure that only AWS users or roles with suitable permissions can access a new Amazon API Gateway endpoint. The solutions architect wants an end-to-end view of each request to analyze the latency of the request and create service maps.</p><p><br></p><p>How can the solutions architect design the API Gateway access control and perform request inspections?</p>",
      "options": [
        {
          "label": "A",
          "content": "For the API Gateway method, set the authorization to AWS_IAM. Then, give the IAM user or role execute-api:Invoke permission on the REST API resource. Enable the API caller to sign requests with AWS Signature when accessing the endpoint. Use AWS X-Ray to trace and analyze user requests to API Gateway."
        },
        {
          "label": "B",
          "content": "For the API Gateway resource, set CORS to enabled and only return the company&#39;s domain in Access-Control-Allow-Origin headers. Then, give the IAM user or role execute-api:Invoke permission on the REST API resource. Use Amazon CloudWatch to trace and analyze user requests to API Gateway."
        },
        {
          "label": "C",
          "content": "Create an AWS Lambda function as the custom authorizer, ask the API client to pass the key and secret when making the call, and then use Lambda to validate the key/secret pair against the IAM system. Use AWS X-Ray to trace and analyze user requests to API Gateway."
        },
        {
          "label": "D",
          "content": "Create a client certificate for API Gateway. Distribute the certificate to the AWS users and roles that need to access the endpoint. Enable the API caller to pass the client certificate when accessing the endpoint. Use Amazon CloudWatch to trace and analyze user requests to API Gateway."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Based on the requirements to ensure only AWS users or roles with suitable permissions can access an API Gateway endpoint and to have an end-to-end view of each request for latency analysis and service mapping, the answer is:</p><p>A. For the API Gateway method, set the authorization to AWS_IAM. Then, give the IAM user or role execute-api:Invoke permission on the REST API resource. Enable the API caller to sign requests with AWS Signature when accessing the endpoint. Use AWS X-Ray to trace and analyze user requests to API Gateway.</p><p>This is the correct solution because:</p><p>Setting the API Gateway method authorization to AWS_IAM ensures that only authenticated AWS users or roles can access the API. This directly addresses the requirement that \"only AWS users or roles with suitable permissions can access the new API Gateway endpoint.\"</p><p>Granting the execute-api:Invoke permission to specific IAM users or roles provides fine-grained access control, allowing you to determine exactly who can invoke the API.</p><p>AWS Signature (Sigv4) signing of requests provides a secure authentication mechanism that verifies the identity of the requester and ensures the integrity of the request.</p><p>AWS X-Ray provides end-to-end tracing capabilities that allow you to:</p><p>Track requests as they flow through your application</p><p>Analyze latency at each step of the request processing</p><p>Create service maps that visualize the relationships between API Gateway and other services</p><p>Identify bottlenecks and troubleshoot performance issues</p><p>Option B is incorrect because:</p><p>CORS (Cross-Origin Resource Sharing) is primarily designed to control which domains can access your API from a web browser. While it's useful for browser-based security, it doesn't provide the IAM-based authentication and authorization required in the scenario.</p><p>Simply returning the company's domain in Access-Control-Allow-Origin headers doesn't restrict API access to specific AWS users or roles.</p><p>Amazon CloudWatch alone doesn't provide the comprehensive end-to-end tracing and service mapping capabilities that X-Ray does. While CloudWatch offers metrics and logs for API Gateway, it doesn't create the service maps needed to visualize request flows across services.</p><p>The combination of AWS_IAM authorization and X-Ray tracing in Option A provides both the security controls and the observability features required by the solutions architect.</p><p>Sources</p><p>CloudWatch integration with X-Ray - AWS X-Ray （https://docs.aws.amazon.com/xray/latest/devguide/xray-services-cloudwatch.html）</p><p>Security design principles - Security Overview of Amazon API Gateway（https://docs.aws.amazon.com/whitepapers/latest/security-overview-amazon-api-gateway/security-design-principles.html） </p><p>Securing Your AWS Infrastructure: A Zero Trust Approach | AWS re:Post （https://repost.aws/articles/ARDWfm5wKhROyIzu8gghrHKw/securing-your-aws-infrastructure-a-zero-trust-approach）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "c49951699b5949859640b76b0cc1dacd",
      "questionNumber": 344,
      "type": "single",
      "content": "<p>Question #344</p><p>A company is using AWS CodePipeline for the CI/CD of an application to an Amazon EC2 Auto Scaling group. All AWS resources are defined in AWS CloudFormation templates. The application artifacts are stored in an Amazon S3 bucket and deployed to the Auto Scaling group using instance user data scripts. As the application has become more complex, recent resource changes in the CloudFormation templates have caused unplanned downtime.</p><p><br></p><p>How should a solutions architect improve the CI/CD pipeline to reduce the likelihood that changes in the templates will cause downtime?</p>",
      "options": [
        {
          "label": "A",
          "content": "Adapt the deployment scripts to detect and report CloudFormation error conditions when performing deployments. Write test plans for a testing team to run in a non-production environment before approving the change for production."
        },
        {
          "label": "B",
          "content": "Implement automated testing using AWS CodeBuild in a test environment. Use CloudFormation change sets to evaluate changes before deployment. Use AWS CodeDeploy to leverage blue/green deployment patterns to allow evaluations and the ability to revert changes, if needed."
        },
        {
          "label": "C",
          "content": "Use plugins for the integrated development environment (IDE) to check the templates for errors, and use the AWS CLI to validate that the templates are correct. Adapt the deployment code to check for error conditions andgenerate notifications on errors. Deploy to a test environment and run a manual test plan before approving the change for production."
        },
        {
          "label": "D",
          "content": "Use AWS CodeDeploy and a blue/green deployment pattern with CloudFormation to replace the user data deployment scripts. Have the operators log in to running instances and go through a manual test plan to verify the application is running as expected."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>The correct answer is B. Here's the detailed reasoning:</p><p> Key Issues in the Current Setup:</p><p>1. Unplanned Downtime: Recent CloudFormation changes caused outages, indicating insufficient testing and risk mitigation.</p><p>2. Manual Deployment via User Data Scripts: Reliance on instance user data scripts is error-prone and lacks proper deployment controls.</p><p>3. Lack of Automated Testing & Safe Deployment Strategies: No structured way to validate changes before production.</p><p> Solution Analysis:</p><p>- Option A: </p><p> &nbsp;- While error detection and manual testing help, they are reactive and slow (manual testing introduces delays). </p><p> &nbsp;- Does not address the root cause (lack of automated validation and safe deployment mechanisms).</p><p>- Option B (Correct Answer):</p><p> &nbsp;- Automated Testing (CodeBuild): Ensures changes are validated in a test environment before production.</p><p> &nbsp;- CloudFormation Change Sets: Allows previewing changes before applying them, reducing unintended impacts.</p><p> &nbsp;- CodeDeploy with Blue/Green: </p><p> &nbsp; &nbsp;- Eliminates dependency on user data scripts.</p><p> &nbsp; &nbsp;- Enables zero-downtime deployments by shifting traffic only after validation.</p><p> &nbsp; &nbsp;- Provides a rollback mechanism if issues arise.</p><p>- Option C: </p><p> &nbsp;- IDE plugins and CLI validation are helpful but static checks only (they don’t test runtime behavior). </p><p> &nbsp;- Manual testing in a staging environment is better than nothing but not scalable compared to automation.</p><p>- Option D: </p><p> &nbsp;- Blue/Green via CodeDeploy is good, but manual testing is inefficient and error-prone. </p><p> &nbsp;- Logging into instances for verification is not a best practice in cloud-native CI/CD.</p><p> Why Option B is Best:</p><p>- Prevents Downtime by validating changes before production (change sets + automated testing).</p><p>- Eliminates Risky User Data Scripts by using CodeDeploy’s controlled deployment strategies.</p><p>- Enables Fast Rollback with blue/green deployments if issues occur.</p><p>- Follows AWS Best Practices for CI/CD (automation, testing, and safe deployments).</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "08f41e773240422ebd8fb36272259d44",
      "questionNumber": 345,
      "type": "single",
      "content": "<p>Question #345</p><p>A North American company with headquarters on the East Coast is deploying a new web application running on Amazon EC2 in the us-east-1 Region. The application should dynamically scale to meet user demand and maintain resiliency. Additionally, the application must have disaster recovery capabilities in an active-passive configuration with the us-west-1 Region.</p><p><br></p><p>Which steps should a solutions architect take after creating a VPC in the us-east-1 Region?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a VPC in the us-west-1 Region. Use inter-Region VPC peering to connect both VPCs. Deploy an Application Load Balancer (ALB) spanning multiple Availability Zones (AZs) to the VPC in the us-east-1 Region. Deploy EC2 instances across multiple AZs in each Region as part of an Auto Scaling group spanning both VPCs and served by the ALB."
        },
        {
          "label": "B",
          "content": "Deploy an Application Load Balancer (ALB) spanning multiple Availability Zones (AZs) to the VPC in the us-east-1 Region. Deploy EC2 instances across multiple AZs as part of an Auto Scaling group served by the ALB. Deploy the same solution to the us-west-1 Region. Create an Amazon Route 53 record set with a failover routing policy and health checks enabled to provide high availability across both Regions."
        },
        {
          "label": "C",
          "content": "Create a VPC in the us-west-1 Region. Use inter-Region VPC peering to connect both VPCs. Deploy an Application Load Balancer (ALB) that spans both VPCs. Deploy EC2 instances across multiple Availability Zones as part of an Auto Scaling group in each VPC served by the ALB. Create an Amazon Route 53 record that points to the ALB."
        },
        {
          "label": "D",
          "content": "Deploy an Application Load Balancer (ALB) spanning multiple Availability Zones (AZs) to the VPC in the us-east-1 Region. Deploy EC2 instances across multiple AZs as part of an Auto Scaling group served by the ALB. Deploy the same solution to the us-west-1 Region. Create separate Amazon Route 53 records in each Region that point to the ALB in the Region. Use Route 53 health checks to provide high availability across both Regions."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>The correct answer is B. &nbsp;</p><p> Explanation: &nbsp;</p><p>The question requires a solution that provides: &nbsp;</p><p>1. Dynamic scaling and resiliency in the primary Region (us-east-1). &nbsp;</p><p>2. Disaster recovery (DR) in an active-passive configuration with us-west-1. &nbsp;</p><p>Option B meets these requirements by: &nbsp;</p><p>- Deploying an ALB across multiple AZs in us-east-1 for high availability. &nbsp;</p><p>- Using Auto Scaling groups to dynamically scale EC2 instances. &nbsp;</p><p>- Replicating the infrastructure in us-west-1 (passive standby). &nbsp;</p><p>- Using Route 53 with failover routing and health checks to automatically redirect traffic to the secondary Region if the primary fails. &nbsp;</p><p> Why not the other options? &nbsp;</p><p>- A: Incorrect because an ALB cannot span multiple Regions, and inter-Region VPC peering is not required for an active-passive setup. &nbsp;</p><p>- C: Incorrect because an ALB cannot span multiple VPCs/Regions, and Route 53 failover routing is missing. &nbsp;</p><p>- D: Incorrect because it suggests using separate Route 53 records without a failover policy, which does not ensure automatic failover. &nbsp;</p><p>Best Practice: &nbsp;</p><p>- Active-Passive DR requires Route 53 failover routing with health checks. &nbsp;</p><p>- ALB and Auto Scaling ensure scalability and resiliency within a Region. &nbsp;</p><p>- Inter-Region VPC peering is unnecessary for this scenario since the secondary Region is passive. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "a3315ad3085547e995afd77455ea3c7b",
      "questionNumber": 346,
      "type": "single",
      "content": "<p>Question #346</p><p>A company has a legacy application that runs on multiple .NET Framework components. The components share the same Microsoft SQL Server database and communicate with each other asynchronously by using Microsoft Message Queuing (MSMQ). <br><br>The company is starting a migration to containerized .NET Core components and wants to refactor the application to run on AWS. The .NET Core components require complex orchestration. The company must have full control over networking and host configuration. The application's database model is strongly relational.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Host the INET Core components on AWS App Runner. Host the database on Amazon RDS for SQL Server. Use Amazon EventBridge for asynchronous messaging."
        },
        {
          "label": "B",
          "content": "Host the .NET Core components on Amazon Elastic Container Service (Amazon ECS) with the AWS Fargate launch type. Host the database on Amazon DynamoDB. Use Amazon Simple Notification Service (Amazon SNS) for asynchronous messaging."
        },
        {
          "label": "C",
          "content": "Host the .NET Core components on AWS Elastic Beanstalk. Host the database on Amazon Aurora PostgreSQL Serverless v2. Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) for asynchronous messaging."
        },
        {
          "label": "D",
          "content": "Host the .NET Core components on Amazon Elastic Container Service (Amazon ECS) with the Amazon EC2 launch type. Host the database on Amazon Aurora MySQL Serverless v2. Use Amazon Simple Queue Service (Amazon SQS) for asynchronous messaging."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Option D is the correct answer. Amazon ECS with the EC2 launch type provides full control over networking and host configuration, which is a requirement mentioned. Amazon Aurora MySQL Serverless v2 supports strongly relational database models, and Amazon SQS can be used for asynchronous messaging, replacing MSMQ.The question outlines the following requirements: &nbsp;</p><p>1. Containerized .NET Core components with complex orchestration. &nbsp;</p><p>2. Full control over networking and host configuration (implies EC2 launch type is preferred over serverless). &nbsp;</p><p>3. Strongly relational database model (requires a relational database, not NoSQL). &nbsp;</p><p>4. Asynchronous messaging (replacing MSMQ). &nbsp;</p><p>Option D meets all these requirements: &nbsp;</p><p>- Amazon ECS with EC2 launch type → Provides full control over networking and host configuration (unlike Fargate). &nbsp;</p><p>- Amazon Aurora MySQL Serverless v2 → A fully managed relational database compatible with SQL Server migrations. &nbsp;</p><p>- Amazon SQS → A managed queue service that replaces MSMQ for asynchronous messaging. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A: AWS App Runner is serverless and doesn’t allow full control over networking/host configuration. Amazon EventBridge is for event-driven architectures, not direct MSMQ replacement. &nbsp;</p><p>- B: DynamoDB is NoSQL (not relational). Fargate removes host-level control. SNS is pub/sub, not a direct MSMQ replacement. &nbsp;</p><p>- C: Elastic Beanstalk is PaaS (less control than ECS). Aurora PostgreSQL is fine, but Amazon MSK is overkill for simple async messaging (SQS is better). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- ECS with EC2 launch type is ideal when fine-grained control over infrastructure is needed. &nbsp;</p><p>- Aurora MySQL/PostgreSQL is a good replacement for SQL Server in migrations. &nbsp;</p><p>- SQS is the AWS equivalent of MSMQ for decoupled messaging. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "ac2989a7126242758bfd83fe86da983c",
      "questionNumber": 347,
      "type": "single",
      "content": "<p>Question #347</p><p>A solutions architect has launched multiple Amazon EC2 instances in a placement group within a single Availability Zone. Because of additional load on the system, the solutions architect attempts to add new instances to the placement group. However, the solutions architect receives an insufficient capacity error.</p><p><br></p><p>What should the solutions architect do to troubleshoot this issue?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use a spread placement group. Set a minimum of eight instances for each Availability Zone."
        },
        {
          "label": "B",
          "content": "Stop and start all the instances in the placement group. Try the launch again."
        },
        {
          "label": "C",
          "content": "Create a new placement group. Merge the new placement group with the original placement group."
        },
        {
          "label": "D",
          "content": "Launch the additional instances as Dedicated Hosts in the placement groups."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the scenario where a solutions architect is experiencing an insufficient capacity error when trying to add new EC2 instances to an existing placement group, the correct answer is:</p><p>B. Stop and start all the instances in the placement group. Try the launch again.</p><p>Here's why this is the correct approach:</p><p>When you create a placement group and launch instances in it, AWS attempts to place all instances close together on the underlying hardware to optimize network performance. However, as time passes and you try to add more instances, AWS might not have enough capacity available in that specific hardware section to accommodate the new instances while maintaining the placement group's requirements.</p><p>Stopping and starting all existing instances in the placement group forces AWS to re-evaluate the placement of these instances. When you restart them, AWS will attempt to find a section of hardware that can accommodate all your existing instances plus the new ones you want to add. This essentially gives AWS a chance to reorganize the placement group on hardware that has enough capacity for all your instances.</p><p>The other options are incorrect for the following reasons:</p><p>A. Use a spread placement group. Set a minimum of eight instances for each Availability Zone.</p><p>This doesn't address the current issue with the existing placement group</p><p>Changing to a spread placement group would require creating a new group and migrating instances</p><p>Spread placement groups are designed for distributing instances across separate hardware (reducing correlated failures), not for solving capacity issues</p><p>There's no setting for \"minimum instances\" in placement groups</p><p>C. Create a new placement group. Merge the new placement group with the original placement group.</p><p>AWS doesn't support merging placement groups</p><p>This approach wouldn't solve the underlying capacity issue</p><p>D. Launch the additional instances as Dedicated Hosts in the placement groups.</p><p>Dedicated Hosts cannot be used with placement groups</p><p>Placement groups and Dedicated Hosts are separate features with different purposes</p><p>The stop-and-start approach is a documented AWS best practice for addressing insufficient capacity errors in placement groups, as it gives AWS the opportunity to reallocate resources in a way that can accommodate all your instances together.</p><p>Sources</p><p>Placement strategies for your placement groups - Amazon Elastic Compute Cloud（https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-strategies.html） </p><p>Why is my Auto Scaling group not launching new instances when the load increases? | AWS re:Post （https://repost.aws/questions/QUQsrSasV_Tyyqw_lB8W9Lfw/why-is-my-auto-scaling-group-not-launching-new-instances-when-the-load-increases）</p><p>Troubleshoot Amazon EC2 instance launch issues - Amazon Elastic Compute Cloud （https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/troubleshooting-launch.html）</p><p>Placement Groups | AWS re:Post （https://repost.aws/questions/QUV59mDpbkTNO3ZEGu1oRwmQ/placement-groups）</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "c3337ad4b703482ebad711ee42b237f6",
      "questionNumber": 348,
      "type": "multiple",
      "content": "<p>Question #348</p><p>A company has used infrastructure as code (IaC) to provision a set of two Amazon EC2 instances. The instances have remained the same for several years. The company's business has grown rapidly in the past few months. In response, the company’s operations team has implemented an Auto Scaling group to manage the sudden increases in traffic. Company policy requires a monthly installation of security updates on all operating systems that are running.</p><p><br></p><p>The most recent security update required a reboot. As a result, the Auto Scaling group terminated the instances and replaced them with new, unpatched instances.</p><p><br></p><p>Which combination of steps should a solutions architect recommend to avoid a recurrence of this issue? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Modify the Auto Scaling group by setting the Update policy to target the oldest launch configuration for replacement."
        },
        {
          "label": "B",
          "content": "Create a new Auto Scaling group before the next patch maintenance. During the maintenance window, patch both groups and reboot the instances."
        },
        {
          "label": "C",
          "content": "Create an Elastic Load Balancer in front of the Auto Scaling group. Configure monitoring to ensure that target group health checks return healthy after the Auto Scaling group replaces the terminated instances."
        },
        {
          "label": "D",
          "content": "Create automation scripts to patch an AMI, update the launch configuration, and invoke an Auto Scaling instance refresh."
        },
        {
          "label": "E",
          "content": "Create an Elastic Load Balancer in front of the Auto Scaling group. Configure termination protection on the instances."
        }
      ],
      "correctAnswer": "AD",
      "explanation": "<p>The core issue is that Auto Scaling replaces manually patched instances with unpatched instances from the original launch configuration/launch template. To prevent this, we need to: &nbsp;</p><p>1. Ensure new instances launch with the latest patches (Option D). &nbsp;</p><p> &nbsp; - Automate AMI patching → Create a golden AMI with security updates. &nbsp;</p><p> &nbsp; - Update the launch configuration/template → Point to the new AMI. &nbsp;</p><p> &nbsp; - Trigger an Auto Scaling instance refresh → Gracefully replace old instances with patched ones. &nbsp;</p><p>2. Prioritize replacement of outdated instances (Option A). &nbsp;</p><p> &nbsp; - Auto Scaling can be configured to first terminate instances using the oldest launch template (ensuring newer, patched versions persist). &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- B (Dual ASGs): Overly complex for this use case; instance refresh (Option D) is simpler. &nbsp;</p><p>- C (ELB + Health Checks): Doesn’t solve the patching issue—only ensures healthy instances. &nbsp;</p><p>- E (Termination Protection): Breaks Auto Scaling by preventing instance replacement. &nbsp;</p><p> Best Practices: &nbsp;</p><p>- Instance Refresh is the AWS-recommended way to roll out updates to Auto Scaling groups. &nbsp;</p><p>- AMI lifecycle management ensures new instances are pre-patched. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "f4993c849bf6402db622803118bde324",
      "questionNumber": 349,
      "type": "single",
      "content": "<p>Question #349</p><p>A team of data scientists is using Amazon SageMaker instances and SageMaker APIs to train machine learning (ML) models. The SageMaker instances are deployed in a VPC that does not have access to or from the internet. Datasets for ML model training are stored in an Amazon S3 bucket. Interface VPC endpoints provide access to Amazon S3 and the SageMaker APIs.</p><p><br></p><p>Occasionally, the data scientists require access to the Python Package Index (PyPI) repository to update Python packages that they use as part of their workflow. A solutions architect must provide access to the PyPI repository while ensuring that the SageMaker instances remain isolated from the internet.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an AWS CodeCommit repository for each package that the data scientists need to access. Configure code synchronization between the PyPI repository and the CodeCommit repository. Create a VPC endpoint for CodeCommit."
        },
        {
          "label": "B",
          "content": "Create a NAT gateway in the VPC. Configure VPC routes to allow access to the internet with a network ACL that allows access to only the PyPI repository endpoint."
        },
        {
          "label": "C",
          "content": "Create a NAT instance in the VPC. Configure VPC routes to allow access to the internet. Configure SageMaker notebook instance firewall rules that allow access to only the PyPI repository endpoint."
        },
        {
          "label": "D",
          "content": "Create an AWS CodeArtifact domain and repository. Add an external connection for public:pypi to the CodeArtifact repository. Configure the Python client to use the CodeArtifact repository. Create a VPC endpoint for CodeArtifact."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>The requirements are: &nbsp;</p><p>1. SageMaker instances must remain isolated from the internet (no direct internet access). &nbsp;</p><p>2. Data scientists need access to PyPI (Python Package Index) for package updates. &nbsp;</p><p>3. Current setup uses VPC endpoints for S3 and SageMaker APIs (private connectivity). &nbsp;</p><p>Option D meets these requirements by: &nbsp;</p><p>- Using AWS CodeArtifact as a private PyPI proxy (no internet access needed). &nbsp;</p><p>- Configuring an external connection to public PyPI (so CodeArtifact can fetch packages). &nbsp;</p><p>- Using a VPC endpoint for CodeArtifact (keeps traffic within AWS, no internet exposure). &nbsp;</p><p>- Configuring the Python client to use CodeArtifact instead of PyPI (seamless integration). &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A: AWS CodeCommit is for Git repositories, not PyPI package hosting (does not solve the package dependency issue). &nbsp;</p><p>- B & C: Both use NAT gateway/NAT instance, which allows internet access (violates the requirement to keep SageMaker instances isolated). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- CodeArtifact is the AWS-managed solution for private package repositories. &nbsp;</p><p>- VPC endpoints ensure secure, private AWS service access without internet exposure. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "b4748625d17a40dba6ade0721e5cdca7",
      "questionNumber": 350,
      "type": "single",
      "content": "<p>Question #350</p><p>A solutions architect works for a government agency that has strict disaster recovery requirements. All Amazon Elastic Block Store (Amazon EBS) snapshots are required to be saved in at least two additional AWS Regions. The agency also is required to maintain the lowest possible operational overhead. <br><br>Which solution meets these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Configure a policy in Amazon Data Lifecycle Manager (Amazon DLM) to run once daily to copy the EBS snapshots to the additional Regions."
        },
        {
          "label": "B",
          "content": "Use Amazon EventBridge to schedule an AWS Lambda function to copy the EBS snapshots to the additional Regions."
        },
        {
          "label": "C",
          "content": "Setup AWS Backup to create the EBS snapshots. Configure Amazon S3 Cross-Region Replication to copy the EBS snapshots to the additional Regions."
        },
        {
          "label": "D",
          "content": "Schedule Amazon EC2 Image Builder to run once daily to create an AMI and copy the AMI to the additional Regions."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Based on the government agency's requirements to save EBS snapshots in at least two additional AWS Regions with the lowest possible operational overhead, the correct answer is:</p><p>A. Configure a policy in Amazon Data Lifecycle Manager (Amazon DLM) to run once daily to copy the EBS snapshots to the additional Regions.</p><p>Here's why this is the best solution:</p><p>Amazon Data Lifecycle Manager (DLM) is specifically designed to automate the creation, retention, and deletion of EBS snapshots. It has built-in functionality to copy snapshots across regions, which directly addresses the requirement to save snapshots in multiple regions.</p><p>DLM provides the lowest operational overhead among the options because:</p><p>It's a fully managed service that requires minimal setup and maintenance</p><p>It uses policies that can be configured once and then run automatically</p><p>It doesn't require any custom code or additional infrastructure</p><p>It handles scheduling, copying, and retention management automatically</p><p>DLM policies can be configured to run daily, which meets the implied frequency requirement, and can specify multiple destination regions for the copies.</p><p>The other options have significant drawbacks:</p><p>B. Using Amazon EventBridge with AWS Lambda would require:</p><p>Writing and maintaining custom code for the Lambda function</p><p>Setting up and managing EventBridge rules</p><p>Handling error cases and retries This creates much higher operational overhead compared to DLM.</p><p>C. AWS Backup with S3 Cross-Region Replication is incorrect because:</p><p>EBS snapshots are not stored in S3 buckets in a way that can be directly managed by S3 Cross-Region Replication</p><p>S3 Cross-Region Replication applies to S3 objects, not EBS snapshots</p><p>This would require additional custom processes to work, increasing operational overhead</p><p>D. EC2 Image Builder is designed for creating and managing AMIs (Amazon Machine Images), not for managing EBS snapshots directly. Using it would:</p><p>Create unnecessary AMIs when only snapshots are required</p><p>Involve more complex processes than needed</p><p>Result in higher costs and operational overhead</p><p>Amazon Data Lifecycle Manager provides the most direct, purpose-built solution for automatically copying EBS snapshots to multiple regions with minimal operational overhead, making it the best choice for meeting the government agency's requirements.</p><p>Sources</p><p>Amazon DLM for snapshot automation | AWS re:Post （https://repost.aws/questions/QU7IIT89-2TVeEXmsR4z9lxQ/amazon-dlm-for-snapshot-automation）</p><p>Creating EBS volume backups with AMIs and EBS snapshots - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/backup-recovery/new-ebs-volume-backups.html）</p><p>Resilience in Amazon EC2 - Amazon Elastic Compute Cloud （https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/disaster-recovery-resiliency.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "5d490b1a47fc4dcfae622f45f3923083",
      "questionNumber": 351,
      "type": "single",
      "content": "<p>Question #351</p><p>A company has a project that is launching Amazon EC2 instances that are larger than required. The project's account cannot be part of the company's organization in AWS Organizations due to policy restrictions to keep this activity outside of corporate IT. The company wants to allow only the launch of t3.small EC2 instances by developers in the project's account. These EC2 instances must be restricted to the us-east-2 Region. </p><p><br></p><p>What should a solutions architect do to meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a new developer account. Move all EC2 instances, users, and assets into us-east-2. Add the account to the company&#39;s organization in AWS Organizations. Enforce a tagging policy that denotes Region affinity."
        },
        {
          "label": "B",
          "content": "Create an SCP that denies the launch of all EC2 instances except t3.small EC2 instances in us-east-2. Attach the SCP to the project&#39;s account."
        },
        {
          "label": "C",
          "content": "Create and purchase a t3.small EC2 Reserved Instance for each developer in us-east-2. Assign each developer a specific EC2 instance with their name as the tag."
        },
        {
          "label": "D",
          "content": "Create an IAM policy than allows the launch of only t3.small EC2 instances in us-east-2. Attach the policy to the roles and groups that the developers use in the project&#39;s account."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Based on the requirement to allow only the launch of t3.small EC2 instances in the us-east-2 Region for developers in a project account that cannot be part of the company's AWS Organizations, the correct answer is:</p><p>D. Create an IAM policy than allows the launch of only t3.small EC2 instances in us-east-2. Attach the policy to the roles and groups that the developers use in the project's account.</p><p>This is the correct solution because:</p><p>IAM policies provide precise control over what actions users can perform in AWS. By creating a policy that specifically allows only t3.small instances in us-east-2, you can enforce the restriction at the permission level.</p><p>The policy can be attached to IAM roles and groups that developers use, ensuring consistent application of the restriction across all developer accounts.</p><p>IAM policies are preventative controls that block unauthorized actions before they occur, rather than detective controls that identify violations after the fact.</p><p>This approach doesn't require any additional cost or resource provisioning - it's purely a permission-based solution.</p><p>Option C (Creating and purchasing t3.small EC2 Reserved Instances for each developer) is incorrect for several reasons:</p><p>Reserved Instances are primarily a billing construct for cost savings, not a security control. Purchasing Reserved Instances doesn't prevent users from launching other instance types.</p><p>Assigning specific EC2 instances to developers with name tags doesn't restrict them from launching additional instances of any size or in any region.</p><p>This approach would require ongoing management overhead to purchase and assign Reserved Instances for each developer.</p><p>It would incur unnecessary costs by requiring the company to purchase Reserved Instances for all developers, even if they're not always running instances.</p><p>The IAM policy approach directly addresses the requirement to restrict instance types and regions with minimal operational overhead and no additional cost. It's a preventative control that enforces the restriction at the permission level, making it the most effective solution for this scenario.</p><p>Sources</p><p>Example policies to control access to the Amazon EC2 console - Amazon Elastic Compute Cloud </p><p>Amazon EC2: Allows full EC2 access within a specific Region, programmatically and in the console - AWS Identity and Access Management </p><p>AWS global condition context keys - AWS Identity and Access Management </p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "f0bee05820774522962dfb3e10ddcfff",
      "questionNumber": 352,
      "type": "single",
      "content": "<p>Question #352</p><p>A scientific company needs to process text and image data from an Amazon S3 bucket. The data is collected from several radar stations during a live, time-critical phase of a deep space mission. The radar stations upload the data to the source S3 bucket. The data is prefixed by radar station identification number. <br><br>The company created a destination S3 bucket in a second account. Data must be copied from the source S3 bucket to the destination S3 bucket to meet a compliance objective. This replication occurs through the use of an S3 replication rule to cover all objects in the source S3 bucket. <br><br>One specific radar station is identified as having the most accurate data. Data replication at this radar station must be monitored for completion within 30 minutes after the radar station uploads the objects to the source S3 bucket.</p><p><br></p><p>What should a solutions architect do to meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Setup an AWS DataSync agent to replicate the prefixed data from the source S3 bucket to the destination S3 bucket. Select to use all available bandwidth on the task, and monitor the task to ensure that it is in the TRANSFERRING status. Create an Amazon EventBridge rule to initiate an alert if this status changes."
        },
        {
          "label": "B",
          "content": "In the second account, create another S3 bucket to receive data from the radar station with the most accurate data. Set up a new replication rule for this new S3 bucket to separate the replication from the other radar stations. Monitor the maximum replication time to the destination. Create an Amazon EventBridge rule to initiate an alert when the time exceeds the desired threshold."
        },
        {
          "label": "C",
          "content": "Enable Amazon S3 Transfer Acceleration on the source S3 bucket, and configure the radar station with the most accurate data to use the new endpoint. Monitor the S3 destination bucket&#39;s TotalRequestLatency metric. Create an Amazon EventBridge rule to initiate an alert if this status changes."
        },
        {
          "label": "D",
          "content": "Create a new S3 replication rule on the source S3 bucket that filters for the keys that use the prefix of the radar station with the most accurate data. Enable S3 Replication Time Control (S3 RTC). Monitor the maximum replication time to the destination. Create an Amazon EventBridge rule to initiate an alert when the time exceeds the desired threshold."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Option D is the correct answer. By creating a specific S3 replication rule that filters for the prefix of the radar station with the most accurate data and enabling S3 Replication Time Control, the architect can ensure that the data is replicated within the required 30-minute window. Setting up an Amazon EventBridge rule to monitor and alert on the replication time exceeds the threshold provides the necessary monitoring for compliance.</p><p>The requirements are: &nbsp;</p><p>1. Replicate data from a source S3 bucket to a destination bucket in another account (already configured). &nbsp;</p><p>2. Ensure data from one specific radar station (identified by prefix) replicates within 30 minutes. &nbsp;</p><p>3. Monitor replication time and trigger alerts if the SLA is breached. &nbsp;</p><p>Option D meets these requirements by: &nbsp;</p><p>- Creating a separate replication rule for the high-priority radar station (filtering by prefix). &nbsp;</p><p>- Enabling S3 Replication Time Control (RTC) → Guarantees replication within 15 minutes (well under the 30-minute requirement). &nbsp;</p><p>- Monitoring replication time and triggering EventBridge alerts if delays occur. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A: AWS DataSync is unnecessary for S3-to-S3 replication (native S3 replication is more efficient). Monitoring `TRANSFERRING` status doesn’t ensure timing compliance. &nbsp;</p><p>- B: Creating a separate bucket adds complexity without solving the monitoring requirement. &nbsp;</p><p>- C: S3 Transfer Acceleration improves upload speed to S3 but doesn’t help with replication timing. &nbsp;</p><p> Best Practices: &nbsp;</p><p>- S3 Replication Time Control (RTC) is the only way to guarantee replication within a defined timeframe (99.99% of objects within 15 minutes). &nbsp;</p><p>- Prefix-based filtering allows granular control over replication rules. &nbsp;</p><p>- EventBridge alerts can track replication metrics (e.g., `MaximumReplicationTime`). &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "bc1d9dd07caf46c6b26be8ffad7a73ec",
      "questionNumber": 353,
      "type": "multiple",
      "content": "<p>Question #353</p><p>A company wants to migrate its on-premises data center to the AWS Cloud. This includes thousands of virtualized Linux and Microsoft Windows servers, SAN storage, Java and PHP applications with MySQL, and Oracle databases. There are many dependent services hosted either in the same data center or externally. The technical documentation is incomplete and outdated. A solutions architect needs to understand the current environment and estimate the cloud resource costs after the migration. </p><p><br></p><p>Which tools or services should the solutions architect use to plan the cloud migration? (Choose three.)</p>",
      "options": [
        {
          "label": "A",
          "content": "AWS Application Discovery Service"
        },
        {
          "label": "B",
          "content": "AWS SMS"
        },
        {
          "label": "C",
          "content": "AWS X-Ray"
        },
        {
          "label": "D",
          "content": "AWS Cloud Adoption Readiness Tool (CART)"
        },
        {
          "label": "E",
          "content": "Amazon Inspector"
        },
        {
          "label": "F",
          "content": "AWS Migration Hub"
        }
      ],
      "correctAnswer": "ADF",
      "explanation": "<p>Based on the scenario where a company needs to migrate thousands of virtualized servers, applications, and databases from an on-premises data center to AWS with incomplete documentation, the three most appropriate tools or services for planning the cloud migration are:</p><p>A. AWS Application Discovery Service D. AWS Cloud Adoption Readiness Tool (CART) F. AWS Migration Hub</p><p>Here's why these three options are the correct choices:</p><p>AWS Application Discovery Service (Option A):</p><p>This service automatically discovers on-premises servers, applications, and their dependencies</p><p>It collects detailed information about server configurations, utilization metrics, and network connections</p><p>It's specifically designed to help understand the current environment when documentation is incomplete or outdated</p><p>The collected data helps in right-sizing AWS resources and estimating costs based on actual usage patterns</p><p>This addresses the requirement to \"understand the current environment\" despite incomplete documentation</p><p>AWS Cloud Adoption Readiness Tool (CART) (Option D):</p><p>CART helps assess an organization's readiness for cloud adoption across business, technical, and financial dimensions</p><p>It provides a framework for identifying gaps in the migration planning process</p><p>It helps estimate cloud resource costs by analyzing the financial aspects of cloud adoption</p><p>This addresses the requirement to \"estimate the cloud resource costs after the migration\"</p><p>AWS Migration Hub (Option F):</p><p>Migration Hub serves as a central console for planning and tracking application migrations</p><p>It integrates with Application Discovery Service to organize discovered data into applications</p><p>It provides a unified view of migration status across multiple applications and servers</p><p>It helps plan migration waves and strategies based on dependencies identified during discovery</p><p>This addresses the need to coordinate a complex migration with \"many dependent services\"</p><p>The other options are not as suitable for this specific scenario:</p><p>B. AWS SMS (Server Migration Service) - While useful for the actual migration of virtual machines, it's not primarily a discovery or cost estimation tool.</p><p>C. AWS X-Ray - This is a distributed tracing service for applications already running on AWS, not a migration planning tool.</p><p>E. Amazon Inspector - This is a security assessment service for applications deployed on AWS, not a tool for understanding on-premises environments or estimating migration costs.</p><p>The combination of Application Discovery Service, Cloud Adoption Readiness Tool, and Migration Hub provides a comprehensive approach to understanding the current environment, planning the migration strategy, and estimating cloud resource costs, which are the key requirements specified in the scenario.</p><p>Sources</p><p>Discovery, planning, and recommendation migration tools - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-tools/discovery.html）</p><p>Tools for migrating to the AWS Cloud - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-tools/introduction.html）</p><p>Windows migration process - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-microsoft-workloads-aws/overview-windows-migration-process.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "901bb383e49a4b0ca207d0018c0b75b3",
      "questionNumber": 354,
      "type": "single",
      "content": "<p>Question #354</p><p>A solutions architect is reviewing an application's resilience before launch. The application runs on an Amazon EC2 instance that is deployed in a private subnet of a VPC. The EC2 instance is provisioned by an Auto Scaling group that has a minimum capacity of 1 and a maximum capacity of 1. The application stores data on an Amazon RDS for MySQL DB instance. The VPC has subnets configured in three Availability Zones and is configured with a single NAT gateway.</p><p><br></p><p>The solutions architect needs to recommend a solution to ensure that the application will operate across multiple Availability Zones.</p><p><br></p><p>Which solution will meet this requirement?</p>",
      "options": [
        {
          "label": "A",
          "content": "Deploy an additional NAT gateway in the other Availability Zones. Update the route tables with appropriate routes. Modify the RDS for MySQL DB instance to a Multi-AZ configuration. Configure the Auto Scaling group to launch the instances across Availability Zones. Set the minimum capacity and maximum capacity of the Auto Scaling group to 3."
        },
        {
          "label": "B",
          "content": "Replace the NAT gateway with a virtual private gateway. Replace the RDS for MySQL DB instance with an Amazon Aurora MySQL DB cluster. Configure the Auto Scaling group to launch instances across all subnets in the VPC. Set the minimum capacity and maximum capacity of the Auto Scaling group to 3."
        },
        {
          "label": "C",
          "content": "Replace the NAT gateway with a NAT instance. Migrate the RDS for MySQL DB instance to an RDS for PostgreSQL DB instance. Launch a new EC2 instance in the other Availability Zones."
        },
        {
          "label": "D",
          "content": "Deploy an additional NAT gateway in the other Availability Zones. Update the route tables with appropriate routes. Modify the RDS for MySQL DB instance to turn on automatic backups and retain the backups for 7 days. Configure the Auto Scaling group to launch instances across all subnets in the VPC. Keep the minimum capacity and the maximum capacity of the Auto Scaling group at 1."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Option A is the correct answer. Deploying an additional NAT gateway in other Availability Zones and updating the route tables ensures that the application can access the internet from those zones. Modifying the RDS for MySQL DB instance to a Multi-AZ configuration provides database resilience across zones. Configuring the Auto Scaling group to launch instances across Availability Zones and setting the minimum and maximum capacity to 3 ensures that the application can continue to operate even if one Availability Zone is down.</p><p>The requirement is to ensure the application operates across multiple Availability Zones (AZs) for high availability. The current setup has: &nbsp;</p><p>- Single EC2 instance (Auto Scaling min/max = 1) in one private subnet. &nbsp;</p><p>- Single NAT gateway (a single point of failure). &nbsp;</p><p>- Single RDS MySQL instance (not Multi-AZ). &nbsp;</p><p>Option A addresses all these issues by: &nbsp;</p><p>1. Adding NAT gateways in other AZs → Eliminates the NAT gateway bottleneck. &nbsp;</p><p>2. Converting RDS MySQL to Multi-AZ → Automatic failover to a standby in another AZ. &nbsp;</p><p>3. Configuring Auto Scaling to span AZs (min/max = 3) → Ensures instances run in multiple AZs. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- B: &nbsp;</p><p> &nbsp;- Virtual private gateway is for VPN/Direct Connect (not needed here). &nbsp;</p><p> &nbsp;- Aurora MySQL is good but doesn’t solve the NAT gateway issue. &nbsp;</p><p> &nbsp;- Auto Scaling min/max = 3 is correct, but the rest is unnecessary. &nbsp;</p><p>- C: &nbsp;</p><p> &nbsp;- NAT instance is less reliable than NAT gateway. &nbsp;</p><p> &nbsp;- PostgreSQL migration is irrelevant to resilience. &nbsp;</p><p>- D: &nbsp;</p><p> &nbsp;- Auto Scaling min/max = 1 means only one instance runs (no multi-AZ redundancy). &nbsp;</p><p> &nbsp;- Backups don’t provide high availability (only disaster recovery). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- Multi-AZ RDS ensures database failover. &nbsp;</p><p>- Multi-AZ NAT gateways prevent network bottlenecks. &nbsp;</p><p>- Auto Scaling across AZs ensures compute redundancy. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "6c157b40fe9d4fa1949a9b3015b530b8",
      "questionNumber": 355,
      "type": "single",
      "content": "<p>Question #355</p><p>A company is planning to migrate its on-premises transaction-processing application to AWS. The application runs inside Docker containers that are hosted on VMs in the company's data center. The Docker containers have shared storage where the application records transaction data. <br><br>The transactions are time sensitive. The volume of transactions inside the application is unpredictable. The company must implement a low latency storage solution that will automatically scale throughput to meet increased demand. The company cannot develop the application further and cannot continue to administer the Docker hosting environment.</p><p><br></p><p>How should the company migrate the application to AWS to meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Migrate the containers that run the application to Amazon Elastic Kubernetes Service (Amazon EKS). Use Amazon S3 to store the transaction data that the containers share."
        },
        {
          "label": "B",
          "content": "Migrate the containers that run the application to AWS Fargate for Amazon Elastic Container Service (Amazon ECS). Create an Amazon Elastic File System (Amazon EFS) file system. Create a Fargate task definition. Add a volume to the task definition to point to the EFS file system."
        },
        {
          "label": "C",
          "content": "Migrate the containers that run the application to AWS Fargate for Amazon Elastic Container Service (Amazon ECS). Create an Amazon Elastic Block Store (Amazon EBS) volume. Create a Fargate task definition. Attach the EBS volume to each running task."
        },
        {
          "label": "D",
          "content": "Launch Amazon EC2 instances. Install Docker on the EC2 instances. Migrate the containers to the EC2 instances. Create an Amazon Elastic File System (Amazon EFS) file system. Add a mount point to the EC2 instances for the EFS file system."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Option B is the correct answer. Migrating the application to AWS Fargate for Amazon ECS allows the company to run containers without managing the underlying infrastructure. Using Amazon EFS provides a shared file system that can be accessed by multiple tasks running the application, and it can automatically scale to meet the throughput demands. Fargate's serverless model aligns with the company's requirement to avoid further development and administration of the Docker environment.</p><p>The requirements are: &nbsp;</p><p>1. Migrate Docker containers without managing the hosting environment → AWS Fargate (serverless containers). &nbsp;</p><p>2. Shared storage for transaction data → Amazon EFS (scalable, low-latency shared file storage). &nbsp;</p><p>3. Automatic throughput scaling → EFS automatically scales IOPS based on demand. &nbsp;</p><p>4. No application changes → EFS provides a POSIX-compliant file system (compatible with existing storage). &nbsp;</p><p>Option B meets all these requirements by: &nbsp;</p><p>- Using Fargate for ECS (no Docker host management). &nbsp;</p><p>- Configuring EFS as shared storage (attached via task definition). &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A: Amazon S3 is not a file system (cannot be mounted directly by containers). &nbsp;</p><p>- C: EBS volumes cannot be shared across multiple containers (no shared storage). &nbsp;</p><p>- D: EC2 instances require Docker management (violates the \"no administration\" requirement). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- Fargate + EFS is the standard for serverless containers with shared storage. &nbsp;</p><p>- EFS scales automatically and provides low-latency access for transactional workloads. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "8535ad044f6341a08c43427d76d94575",
      "questionNumber": 356,
      "type": "single",
      "content": "<p>Question #356</p><p>A company is planning to migrate to the AWS Cloud. The company hosts many applications on Windows servers and Linux servers. Some of the servers are physical, and some of the servers are virtual. The company uses several types of databases in its on-premises environment. The company does not have an accurate inventory of its on-premises servers and applications. <br><br>The company wants to rightsize its resources during migration. A solutions architect needs to obtain information about the network connections and the application relationships. The solutions architect must assess the company’s current environment and develop a migration plan.</p><p><br></p><p>Which solution will provide the solutions architect with the required information to develop the migration plan?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use Migration Evaluator to request an evaluation of the environment from AWS. Use the AWS Application Discovery Service Agentless Collector to import the details into a Migration Evaluator Quick Insights report."
        },
        {
          "label": "B",
          "content": "Use AWS Migration Hub and install the AWS Application Discovery Agent on the servers. Deploy the Migration Hub Strategy Recommendations application data collector. Generate a report by using Migration Hub Strategy Recommendations."
        },
        {
          "label": "C",
          "content": "Use AWS Migration Hub and run the AWS Application Discovery Service Agentless Collector on the servers. Group the servers and databases by using AWS Application Migration Service. Generate a report by using Migration Hub Strategy Recommendations."
        },
        {
          "label": "D",
          "content": "Use the AWS Migration Hub import tool to load the details of the company&rsquo;son-premises environment. Generate a report by using Migration Hub Strategy Recommendations."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Option B is the correct answer. Installing the AWS Application Discovery Agent on the servers and using AWS Migration Hub will allow the architect to collect detailed information about the on-premises environment, including network connections and application relationships. This data can then be used to generate a report with Migration Hub Strategy Recommendations, which will assist in developing an informed migration plan.</p><p>The company needs to: &nbsp;</p><p>1. Discover and inventory on-premises servers (physical/virtual, Windows/Linux). &nbsp;</p><p>2. Analyze application dependencies (network connections, relationships). &nbsp;</p><p>3. Rightsize resources for migration planning. &nbsp;</p><p>Option B meets these requirements by: &nbsp;</p><p>- Using AWS Migration Hub (central tracking for migration). &nbsp;</p><p>- Installing AWS Application Discovery Agents (collects detailed server configs, performance data, and network dependencies). &nbsp;</p><p>- Deploying Migration Hub Strategy Recommendations (analyzes data to suggest optimal AWS sizing and migration strategies). &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A: Migration Evaluator (formerly TSO Logic) provides high-level cost estimates but lacks dependency mapping. The Agentless Collector only works for VMware (not physical servers). &nbsp;</p><p>- C: The Agentless Collector is limited to VMware (cannot inventory physical servers or non-VMware environments). &nbsp;</p><p>- D: The import tool requires manual data entry (no automated discovery). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- Application Discovery Service (ADS) with agents is the most comprehensive way to map dependencies for heterogeneous environments (physical/virtual, Windows/Linux). &nbsp;</p><p>- Migration Hub Strategy Recommendations uses ADS data to suggest rightsizing (e.g., EC2 instance types, RDS options). &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "7db8b20adc9a44219172283375b38fc1",
      "questionNumber": 357,
      "type": "single",
      "content": "<p>Question #357</p><p>A financial services company sells its software-as-a-service (SaaS) platform for application compliance to large global banks. The SaaS platform runs on AWS and uses multiple AWS accounts that are managed in an organization in AWS Organizations. The SaaS platform uses many AWS resources globally. <br><br>For regulatory compliance, all API calls to AWS resources must be audited, tracked for changes, and stored in a durable and secure data store.</p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a new AWS CloudTrail trail. Use an existing Amazon S3 bucket in the organization&#39;s management account to store the logs. Deploy the trail to all AWS Regions. Enable MFA delete and encryption on the S3 bucket."
        },
        {
          "label": "B",
          "content": "Create a new AWS CloudTrail trail in each member account of the organization. Create new Amazon S3 buckets to store the logs. Deploy the trail to all AWS Regions. Enable MFA delete and encryption on the S3 buckets."
        },
        {
          "label": "C",
          "content": "Create a new AWS CloudTrail trail in the organization&#39;s management account. Create a new Amazon S3 bucket with versioning turned on to store the logs. Deploy the trail for all accounts in the organization. Enable MFA delete and encryption on the S3 bucket."
        },
        {
          "label": "D",
          "content": "Create a new AWS CloudTrail trail in the organization&#39;s management account. Create a new Amazon S3 bucket to store the logs. Configure Amazon Simple Notification Service (Amazon SNS) to send log-file delivery notifications to an external management system that will track the logs. Enable MFA delete and encryption on the S3 bucket."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Option C is the correct answer. By creating a new AWS CloudTrail trail in the organization's management account and configuring it to apply to all accounts in the organization, the solution provides a centralized auditing mechanism with minimal operational overhead. Using a single S3 bucket with versioning ensures that logs are durable and can't be accidentally deleted, and MFA delete and encryption provide additional security.</p><p>The requirements are: &nbsp;</p><p>1. Audit and track all API calls across multiple AWS accounts in an organization. &nbsp;</p><p>2. Store logs durably and securely (with encryption and MFA delete protection). &nbsp;</p><p>3. Minimize operational overhead (centralized management). &nbsp;</p><p>Option C meets these requirements by: &nbsp;</p><p>- Creating a single CloudTrail trail in the management account (applies to all member accounts via AWS Organizations). &nbsp;</p><p>- Storing logs in a centralized S3 bucket (with versioning for durability). &nbsp;</p><p>- Enabling MFA delete and encryption for security. &nbsp;</p><p>- Deploying the trail to all accounts automatically (no per-account setup). &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A: Uses an existing S3 bucket, which may not have versioning enabled (a compliance risk). &nbsp;</p><p>- B: Per-account trails and buckets increase operational overhead (violates the \"LEAST operational overhead\" requirement). &nbsp;</p><p>- D: SNS notifications add unnecessary complexity (tracking changes is already handled by CloudTrail + S3). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- Organization-wide CloudTrail trails reduce management complexity. &nbsp;</p><p>- S3 versioning + MFA delete ensures log integrity and prevents tampering. &nbsp;</p><p>- KMS encryption meets regulatory requirements for data security. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "df4ace549f9a4986b6d9d7bbad181ca2",
      "questionNumber": 358,
      "type": "single",
      "content": "<p>Question #358</p><p>A company is deploying a distributed in-memory database on a fleet of Amazon EC2 instances. The fleet consists of a primary node and eight worker nodes. The primary node is responsible for monitoring cluster health, accepting user requests, distributing user requests to worker nodes, and sending an aggregate response back to a client. Worker nodes communicate with each other to replicate data partitions. <br><br>The company requires the lowest possible networking latency to achieve maximum performance.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Launch memory optimized EC2 instances in a partition placement group."
        },
        {
          "label": "B",
          "content": "Launch compute optimized EC2 instances in a partition placement group."
        },
        {
          "label": "C",
          "content": "Launch memory optimized EC2 instances in a cluster placement group."
        },
        {
          "label": "D",
          "content": "Launch compute optimized EC2 instances in a spread placement group."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the requirement for a distributed in-memory database with the lowest possible networking latency, the correct answer is:</p><p>C. Launch memory optimized EC2 instances in a cluster placement group.</p><p>Here's why this is the best solution:</p><p>Memory Optimized Instances:</p><p>A distributed in-memory database, as the name suggests, primarily relies on memory for data storage and processing</p><p>Memory optimized instances (like the R-series) are specifically designed with high memory-to-CPU ratios to support memory-intensive applications</p><p>These instances provide the necessary RAM capacity for storing the database entirely in memory, which is essential for optimal performance</p><p>Cluster Placement Group:</p><p>Cluster placement groups are specifically designed to provide the lowest possible network latency and highest packet-per-second performance between instances</p><p>They pack instances close together within a single Availability Zone on high-bandwidth, low-latency hardware</p><p>This placement strategy is ideal for applications that benefit from low-latency node-to-node communication, such as distributed databases where nodes need to communicate frequently for data replication</p><p>The other options are less suitable for the following reasons:</p><p>A. Memory optimized EC2 instances in a partition placement group:</p><p>While memory optimized instances are appropriate, partition placement groups are designed to spread instances across distinct hardware partitions</p><p>Partition groups help reduce the impact of hardware failures but don't provide the same level of network performance as cluster placement groups</p><p>This would not deliver the \"lowest possible networking latency\" required</p><p>B. Compute optimized EC2 instances in a partition placement group:</p><p>Compute optimized instances prioritize CPU performance over memory, which is not ideal for an in-memory database</p><p>Partition placement groups, as mentioned above, don't provide the lowest possible network latency</p><p>D. Compute optimized EC2 instances in a spread placement group:</p><p>Compute optimized instances are not optimal for memory-intensive workloads</p><p>Spread placement groups place instances on distinct hardware to maximize availability, but this comes at the cost of higher network latency between instances</p><p>This option would provide the highest network latency among all the choices</p><p>The combination of memory optimized instances (for the in-memory database requirements) and a cluster placement group (for the lowest possible networking latency) makes option C the optimal solution for achieving maximum performance in this distributed database scenario.</p><p>Sources</p><p>PERF04-BP06 Choose your workload's location based on network requirements - AWS Well-Architected Framework （https://docs.aws.amazon.com/wellarchitected/2023-10-03/framework/perf_networking_choose_workload_location_network_requirements.html）</p><p>Placement strategies for your placement groups - Amazon Elastic Compute Cloud （https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-strategies.html）</p><p>Improve network latency for Linux based EC2 instances - Amazon Elastic Compute Cloud （https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ena-improve-network-latency-linux.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "7baa191bed9941c0ac1a36e843b3365f",
      "questionNumber": 359,
      "type": "single",
      "content": "<p>Question #359</p><p>A company maintains information on premises in approximately 1 million .csv files that are hosted on a VM. The data initially is 10 TB in size and grows at a rate of 1 TB each week. The company needs to automate backups of the data to the AWS Cloud. <br><br>Backups of the data must occur daily. The company needs a solution that applies custom filters to back up only a subset of the data that is located in designated source directories. The company has set up an AWS Direct Connect connection.</p><p><br></p><p>Which solution will meet the backup requirements with the LEAST operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use the Amazon S3 CopyObject API operation with multipart upload to copy the existing data to Amazon S3. Use the CopyObject API operation to replicate new data to Amazon S3 daily."
        },
        {
          "label": "B",
          "content": "Create a backup plan in AWS Backup to back up the data to Amazon S3. Schedule the backup plan to run daily."
        },
        {
          "label": "C",
          "content": "Install the AWS DataSync agent as a VM that runs on the on-premises hypervisor. Configure a DataSync task to replicate the data to Amazon S3 daily."
        },
        {
          "label": "D",
          "content": "Use an AWS Snowball Edge device for the initial backup. Use AWS DataSync for incremental backups to Amazon S3 daily."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the requirement to automate backups of approximately 1 million CSV files (10 TB initially, growing at 1 TB weekly) from on-premises to AWS Cloud with custom filters for specific directories, the solution with the least operational overhead is:</p><p>C. Install the AWS DataSync agent as a VM that runs on the on-premises hypervisor. Configure a DataSync task to replicate the data to Amazon S3 daily.</p><p>Here's why this is the best solution:</p><p>AWS DataSync is specifically designed for transferring large amounts of data between on-premises environments and AWS Cloud. It's purpose-built for this exact use case.</p><p>DataSync provides built-in filtering capabilities that allow you to include or exclude specific directories, which directly addresses the requirement to \"apply custom filters to back up only a subset of the data that is located in designated source directories.\"</p><p>DataSync can efficiently utilize the existing Direct Connect connection, optimizing transfer speeds and providing secure data transfer.</p><p>The DataSync agent runs as a VM on your existing hypervisor, making deployment straightforward with minimal infrastructure changes.</p><p>DataSync tasks can be easily scheduled to run daily, meeting the requirement for daily backups.</p><p>DataSync handles the complexity of transferring millions of small files efficiently, which is important given the 1 million CSV files in this scenario.</p><p>DataSync provides detailed monitoring, logging, and verification of transferred data, reducing the operational overhead of manually checking if backups were successful.</p><p>Option B (AWS Backup) is not the optimal choice because:</p><p>AWS Backup is primarily designed for backing up AWS resources, not for transferring data from on-premises to AWS.</p><p>AWS Backup would require additional components like AWS Storage Gateway to handle on-premises data, increasing the operational overhead.</p><p>AWS Backup has more limited filtering capabilities compared to DataSync, making it harder to back up only specific directories.</p><p>For large-scale data transfer from on-premises to AWS, AWS Backup would be less efficient than DataSync, which is specifically optimized for this purpose.</p><p>AWS DataSync provides the most streamlined, purpose-built solution for this specific use case with the least operational overhead, making option C the correct answer.</p><p>Sources</p><p>AWS DataSync for Backup solution | AWS re:Post （https://repost.aws/questions/QUK80Pmh1YRj60N53GDtl7UA/aws-datasync-for-backup-solution）</p><p>REL09-BP01 Identify and back up all data that needs to be backed up, or reproduce the data from sources - AWS Well-Architected Framework （https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_backing_up_data_identified_backups_data.html）</p><p>Choosing what AWS DataSync transfers - AWS DataSync （https://docs.aws.amazon.com/datasync/latest/userguide/task-options.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "90f03026ee1e48f88bc0b3cf11c8ca0b",
      "questionNumber": 360,
      "type": "single",
      "content": "<p>Question #360</p><p>A financial services company has an asset management product that thousands of customers use around the world. The customers provide feedback about the product through surveys. The company is building a new analytical solution that runs on Amazon EMR to analyze the data from these surveys. The following user personas need to access the analytical solution to perform different actions:</p><p><br></p><p>- Administrator: Provisions the EMR cluster for the analytics team based on the team’s requirements</p><p>- Data engineer: Runs ETL scripts to process, transform, and enrich the datasets</p><p>- Data analyst: Runs SQL and Hive queries on the data</p><p><br></p><p>A solutions architect must ensure that all the user personas have least privilege access to only the resources that they need. The user personas must be able to launch only applications that are approved and authorized. The solution also must ensure tagging for all resources that the user personas create.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create IAM roles for each user persona. Attach identity-based policies to define which actions the user who assumes the role can perform. Create an AWS Config rule to check for noncompliant resources. Configure the rule to notify the administrator to remediate the noncompliant resources."
        },
        {
          "label": "B",
          "content": "Setup Kerberos-based authentication for EMR clusters upon launch. Specify a Kerberos security configuration along with cluster-specific Kerberos options."
        },
        {
          "label": "C",
          "content": "Use AWS Service Catalog to control the Amazon EMR versions available for deployment, the cluster configuration, and the permissions for each user persona."
        },
        {
          "label": "D",
          "content": "Launch the EMR cluster by using AWS CloudFormation, Attach resource-based policies to the EMR cluster during cluster creation. Create an AWS Config rule to check for noncompliant clusters and noncompliant Amazon S3 buckets. Configure the rule to notify the administrator to remediate the noncompliant resources."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Option C is the correct answer. AWS Service Catalog allows for the creation of a catalog of approved resources and applications that users can launch. It enables the definition of specific permissions for eachuser persona, ensuring that they have least privilege access and can only launch authorized applications. It also supports resource tagging, which helps in tracking the resources created by each user.</p><p>The requirements are: &nbsp;</p><p>1. Least privilege access for each user persona (Administrator, Data Engineer, Data Analyst). &nbsp;</p><p>2. Restrict launch to approved/authorized applications (controlled EMR versions/configurations). &nbsp;</p><p>3. Enforce tagging for all created resources. &nbsp;</p><p>Option C meets these requirements by: &nbsp;</p><p>- Using AWS Service Catalog to: &nbsp;</p><p> &nbsp;- Define approved EMR configurations (versions, cluster setups). &nbsp;</p><p> &nbsp;- Assign IAM permissions per persona (e.g., Data Analysts can only run queries, not modify clusters). &nbsp;</p><p> &nbsp;- Enforce tagging policies (via Service Catalog TagOptions). &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A: IAM roles + AWS Config lacks centralized control over approved EMR configurations. AWS Config only detects noncompliance (doesn’t prevent it). &nbsp;</p><p>- B: Kerberos authentication secures access but doesn’t address least privilege or approved application launches. &nbsp;</p><p>- D: CloudFormation + AWS Config enforces resource policies but doesn’t simplify user access control like Service Catalog. &nbsp;</p><p> Best Practices: &nbsp;</p><p>- Service Catalog is the standard for pre-approved, governed AWS resource deployments. &nbsp;</p><p>- Tag enforcement ensures compliance and cost tracking. &nbsp;</p><p>- Least privilege is achieved by mapping IAM permissions to Service Catalog portfolios. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "23b15a75a1ab45eeaa64c133b80e5cf9",
      "questionNumber": 361,
      "type": "single",
      "content": "<p>Question #361</p><p>A software as a service (SaaS) company uses AWS to host a service that is powered by AWS PrivateLink. The service consists of proprietary software that runs on three Amazon EC2 instances behind a Network Load Balancer (NLB). The instances are in private subnets in multiple Availability Zones in the eu-west-2 Region. All the company's customers are in eu-west-2. <br><br>However, the company now acquires a new customer in the us-east-1 Region. The company creates a new VPC and new subnets in us-east-1. The company establishes inter-Region VPC peering between the VPCs in the two Regions.</p><p><br></p><p>The company wants to give the new customer access to the SaaS service, but the company does not want to immediately deploy new EC2 resources in us-east-1.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Configure a PrivateLink endpoint service in us-east-1 to use the existing NLB that is in eu-west-2. Grant specific AWS accounts access to connect to the SaaS service."
        },
        {
          "label": "B",
          "content": "Create an NLB in us-east-1. Create an IP target group that uses the IP addresses of the company&#39;s instances in eu-west-2 that host the SaaS service. Configure a PrivateLink endpoint service that uses the NLB that is in us-east-1. Grant specific AWS accounts access to connect to the SaaS service."
        },
        {
          "label": "C",
          "content": "Create an Application Load Balancer (ALB) in front of the EC2 instances in eu-west-2. Create an NLB in us-east-1. Associate the NLB that is in us-east-1 with an ALB target group that uses the ALB that is in eu-west-2. Configure a PrivateLink endpoint service that uses the NLB that is in us-east-1. Grant specific AWS accounts access to connect to the SaaS service."
        },
        {
          "label": "D",
          "content": "Use AWS Resource Access Manager (AWS RAM) to share the EC2 instances that are in eu-west-2. In us-east-1, create an NLB and an instance target group that includes the shared EC2 instances from eu-west-2. Configure a PrivateLink endpoint service that uses the NLB that is in us-east-1. Grant specific AWS accounts access to connect to the SaaS service."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Option B is the correct answer. By creating a new NLB in us-east-1 and using the IP addresses of the instances in eu-west-2 as the target group, the company can establish a PrivateLink endpoint service that allows the new customer to access the SaaS service without deploying new EC2 resources in us-east-1. This solution leverages inter-Region VPC peering to maintain the connection between the regions.</p><p>The requirements are: &nbsp;</p><p>1. Provide access to the SaaS service (hosted in eu-west-2) to a new customer in us-east-1. &nbsp;</p><p>2. Avoid deploying new EC2 instances in us-east-1 (must reuse existing eu-west-2 instances). &nbsp;</p><p>3. Use AWS PrivateLink (secure, private connectivity). &nbsp;</p><p>Option B meets these requirements by: &nbsp;</p><p>- Creating an NLB in us-east-1 (required for PrivateLink endpoint services). &nbsp;</p><p>- Configuring an IP target group pointing to the private IPs of the eu-west-2 instances (traffic routes via inter-Region VPC peering). &nbsp;</p><p>- Setting up a PrivateLink endpoint service in us-east-1 using the new NLB (customer connects securely via PrivateLink). &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A: PrivateLink endpoint services cannot reference NLBs in another Region (must be in the same Region). &nbsp;</p><p>- C: ALB cannot be used with PrivateLink (only NLBs are supported). &nbsp;</p><p>- D: AWS RAM cannot share EC2 instances across Regions (only within the same Region). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- Inter-Region VPC peering enables private connectivity across Regions. &nbsp;</p><p>- PrivateLink + NLB provides secure, scalable access without exposing resources to the public internet. &nbsp;</p><p>- IP target groups allow cross-Region routing (unlike instance/ALB target groups). &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "7ffc4798af494020a5c48828d0153b34",
      "questionNumber": 362,
      "type": "single",
      "content": "<p>Question #362</p><p>A company needs to monitor a growing number of Amazon S3 buckets across two AWS Regions. The company also needs to track the percentage of objects that are encrypted in Amazon S3. The company needs a dashboard to display this information for internal compliance teams.</p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a new S3 Storage Lens dashboard in each Region to track bucket and encryption metrics. Aggregate data from both Region dashboards into a single dashboard in Amazon QuickSight for the compliance teams."
        },
        {
          "label": "B",
          "content": "Deploy an AWS Lambda function in each Region to list the number of buckets and the encryption status of objects. Store this data in Amazon S3. Use Amazon Athena queries to display the data on a custom dashboard in Amazon QuickSight for the compliance teams."
        },
        {
          "label": "C",
          "content": "Use the S3 Storage Lens default dashboard to track bucket and encryption metrics. Give the compliance teams access to the dashboard directly in the S3 console."
        },
        {
          "label": "D",
          "content": "Create an Amazon EventBridge rule to detect AWS CloudTrail events for S3 object creation. Configure the rule to invoke an AWS Lambda function to record encryption metrics in Amazon DynamoDB. Use Amazon QuickSight to display the metrics in a dashboard for the compliance teams."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the requirement to monitor S3 buckets across two AWS Regions and track the percentage of encrypted objects with the least operational overhead, the correct answer is:</p><p>C. Use the S3 Storage Lens default dashboard to track bucket and encryption metrics. Give the compliance teams access to the dashboard directly in the S3 console.</p><p>Here's why this is the best solution with the least operational overhead:</p><p>S3 Storage Lens default dashboard is specifically designed to provide organization-wide visibility into object storage usage and activity across all AWS Regions from a single console.</p><p>The default dashboard automatically includes metrics for tracking bucket counts, storage usage, and encryption status across all regions without any additional configuration.</p><p>S3 Storage Lens provides built-in encryption metrics that show the percentage of objects that are encrypted, directly addressing the company's requirement.</p><p>Using the default dashboard eliminates the need to create and maintain separate dashboards for each region, significantly reducing operational overhead.</p><p>Compliance teams can be given direct access to the S3 Storage Lens dashboard through IAM permissions, allowing them to view the necessary metrics without requiring additional tools or data aggregation.</p><p>Option A (Creating new S3 Storage Lens dashboards in each Region and aggregating in QuickSight) would involve much higher operational overhead because:</p><p>It requires creating and maintaining multiple S3 Storage Lens dashboards (one for each region).</p><p>It requires setting up and configuring Amazon QuickSight as an additional service.</p><p>It necessitates building data pipelines to aggregate data from multiple dashboards into QuickSight.</p><p>It would require ongoing maintenance of both the individual dashboards and the QuickSight dashboard.</p><p>It adds complexity with no additional benefit, since the default dashboard already provides cross-region visibility.</p><p>The S3 Storage Lens default dashboard is specifically designed to address multi-region monitoring use cases like this one, making it the solution with the least operational overhead while still meeting all the requirements.</p><p>Sources</p><p>Assessing your storage activity and usage with Amazon S3 Storage Lens - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage_lens.html）</p><p>AWS S3 Advanced metrics and recommendations | AWS re:Post （https://repost.aws/questions/QUP1Werf6jRd2WfjVI32chMg/aws-s3-advanced-metrics-and-recommendations）</p><p>Viewing S3 Storage Lens metrics on the dashboards - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage_lens_view_metrics_dashboard.html）</p><p>Cutting-Edge Innovation, Yet No Simple Way to See Your S3 Storage Size? | AWS re:Post （https://repost.aws/questions/QUw4nuXCbTSN6D-jdt90c1uQ/cutting-edge-innovation-yet-no-simple-way-to-see-your-s3-storage-size）</p><p>Amazon S3 Features – Storage Lens – AWS （https://aws.amazon.com/s3/storage-lens/）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "c4be4af432d84e5d81cfa606e68e0262",
      "questionNumber": 363,
      "type": "single",
      "content": "<p>Question #363</p><p>A company’s CISO has asked a solutions architect to re-engineer the company's current CI/CD practices to make sure patch deployments to its application can happen as quickly as possible with minimal downtime if vulnerabilities are discovered. The company must also be able to quickly roll back a change in case of errors.</p><p><br></p><p>The web application is deployed in a fleet of Amazon EC2 instances behind an Application Load Balancer. The company is currently using GitHub to host the application source code, and has configured an AWS CodeBuild project to build the application. The company also intends to use AWS CodePipeline to trigger builds from GitHub commits using the existing CodeBuild project.</p><p><br></p><p>What CI/CD configuration meets all of the requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Configure CodePipeline with a deploy stage using AWS CodeDeploy configured for in-place deployment. Monitor the newly deployed code, and, if there are any issues, push another code update."
        },
        {
          "label": "B",
          "content": "Configure CodePipeline with a deploy stage using AWS CodeDeploy configured for blue/green deployments. Monitor the newly deployed code, and, if there are any issues, trigger a manual rollback using CodeDeploy."
        },
        {
          "label": "C",
          "content": "Configure CodePipeline with a deploy stage using AWS CloudFormation to create a pipeline for test and production stacks. Monitor the newly deployed code, and, if there are any issues, push another code update."
        },
        {
          "label": "D",
          "content": "Configure the CodePipeline with a deploy stage using AWS OpsWorks and in-place deployments. Monitor the newly deployed code, and, if there are any issues, push another code update."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Option B is the correct answer. Configuring AWS CodePipeline with AWS CodeDeploy for blue/green deployments allows for minimal downtime and quick rollbacks. This approach maintains two production environments, with one serving as a standby for the other. If issues are detected in the new deployment, the company can quickly switch back to the previous version, ensuring high availability and fault tolerance.</p><p>The requirements are: &nbsp;</p><p>1. Fast patch deployments with minimal downtime → Blue/Green deployment (avoids downtime by shifting traffic to new instances). &nbsp;</p><p>2. Quick rollback capability → CodeDeploy’s built-in rollback feature (reverts to the last known good version). &nbsp;</p><p>3. CI/CD pipeline integration → CodePipeline + CodeDeploy (supports GitHub, CodeBuild, and automated deployments). &nbsp;</p><p>Option B meets these requirements by: &nbsp;</p><p>- Using CodeDeploy with Blue/Green deployments (minimizes downtime and enables instant rollback). &nbsp;</p><p>- Monitoring + manual rollback (if issues arise, revert with a single action). &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A: In-place deployments cause downtime and lack automated rollback. &nbsp;</p><p>- C: CloudFormation alone doesn’t handle Blue/Green deployments for EC2 fleets (CodeDeploy is better suited). &nbsp;</p><p>- D: OpsWorks in-place deployments also cause downtime and lack native rollback. &nbsp;</p><p> Best Practices: &nbsp;</p><p>- Blue/Green deployments are ideal for zero-downtime patching. &nbsp;</p><p>- CodeDeploy rollback ensures quick recovery from failed deployments. &nbsp;</p><p>- CodePipeline orchestrates the workflow (GitHub → CodeBuild → CodeDeploy). &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "b68a748d71f440958e7815d8ed93121c",
      "questionNumber": 364,
      "type": "single",
      "content": "Question #364<p>A company is managing many AWS accounts by using an organization in AWS Organizations. Different business units in the company run applications on Amazon EC2 instances. All the EC2 instances must have a BusinessUnit tag so that the company can track the cost for each business unit.</p><p><br></p><p>A recent audit revealed that some instances were missing this tag. The company manually added the missing tag to the instances.</p><p><br></p><p>What should a solutions architect do to enforce the tagging requirement in the future?</p>",
      "options": [
        {
          "label": "A",
          "content": "Enable tag policies in the organization. Create a tag policy for the BusinessUnit tag. Ensure that compliance with tag key capitalization is turned off. Implement the tag policy for the ec2:instance resource type. Attach the tag policy to the root of the organization."
        },
        {
          "label": "B",
          "content": "Enable tag policies in the organization. Create a tag policy for the BusinessUnit tag. Ensure that compliance with tag key capitalization is turned on. Implement the tag policy for the ec2:instance resource type. Attach the tag policy to the organization&#39;s management account."
        },
        {
          "label": "C",
          "content": "Create an SCP and attach the SCP to the root of the organization. Include the following statement in the SCP:<br><img src=\"images/sap/469452f57411714d6501e80f446339c9485bd0b4.png\" >"
        },
        {
          "label": "D",
          "content": "Create an SCP and attach the SCP to the organization&rsquo;s management account. Include the following statement in the SCP:<br><img src=\"images/sap/e693695d6e2105fa1df768ec41953da226df7003.png\" >"
        }
      ],
      "correctAnswer": "C",
      "explanation": "Answer is C. <p>To enforce the tagging requirement for EC2 instances across multiple AWS accounts in an organization, a solutions architect should implement the following measures:</p><p><br></p><p>1. Create a tag policy:</p><p>   - Use AWS Organizations to create a tag policy that defines the required \"BusinessUnit\" tag for EC2 instances.</p><p>   - Specify the tag key, allowed values, and enforcement rules in the policy.</p><p>2. Attach the tag policy:</p><p>   - Attach the created tag policy to the root of the organization or to specific Organizational Units (OUs) that contain the accounts running EC2 instances.</p><p>3. Enable tag policy enforcement:</p><p>   - In the tag policy, enable enforcement for EC2 instances to prevent the creation or modification of instances without the required tag.</p><p>4. Implement proactive controls:</p><p>   - Use Service Control Policies (SCPs) to deny the creation of EC2 instances without the required tag.</p><p>   - This adds an extra layer of protection at the organization level.</p><p>5. Set up compliance monitoring:</p><p>   - Use AWS Config to create rules that check for compliance with the tagging policy.</p><p>   - Configure AWS Config to send notifications when non-compliant resources are detected.</p><p>6. Automate tag addition:</p><p>   - Implement AWS Lambda functions triggered by CloudTrail events to automatically add the required tag if it's missing when an instance is created or modified.</p><p>7. Educate and communicate:</p><p>   - Inform all teams about the tagging requirements and provide clear guidelines on how to properly tag resources.</p><p>8. Regular audits:</p><p>   - Schedule periodic reviews of resource tagging to ensure ongoing compliance.</p><p>9. Consider using AWS Resource Groups Tagging API:</p><p>   - This can help in managing tags across multiple accounts and services more efficiently.</p><p><br></p><p>By implementing these measures, the company can ensure that all EC2 instances have the required \"BusinessUnit\" tag, enabling accurate cost tracking for each business unit. Remember to test these policies in a non-production environment before applying them organization-wide, and always follow the principle of least privilege when setting up permissions and policies.</p><p>Sources</p><p>[1] [Best practices for managing organizational units (OUs) with AWS Organizations - AWS Organizations] (https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_ous_best_practices.html)</p><p>[3] [Best practices for a multi-account environment - AWS Organizations] (https://docs.aws.amazon.com/organizations/latest/userguide/orgs_best-practices.html)</p><p>[6] [Implementing a tagging strategy for detailed cost and usage data - AWS Prescriptive Guidance] (https://docs.aws.amazon.com/prescriptive-guidance/latest/cost-allocation-tagging/introduction.html)</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "a43f2a7190134b7aacf6282a1bbde289",
      "questionNumber": 365,
      "type": "single",
      "content": "<p>Question #365</p><p>A company is running a workload that consists of thousands of Amazon EC2 instances. The workload is running in a VPC that contains several public subnets and private subnets. The public subnets have a route for 0.0.0.0/0 to an existing internet gateway. The private subnets have a route for 0.0.0.0/0 to an existing NAT gateway.</p><p><br></p><p>A solutions architect needs to migrate the entire fleet of EC2 instances to use IPv6. The EC2 instances that are in private subnets must not be accessible from the public internet.</p><p><br></p><p>What should the solutions architect do to meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Update the existing VPC, and associate a custom IPv6 CIDR block with the VPC and all subnets. Update all the VPC route tables, and add a route for ::/0 to the internet gateway."
        },
        {
          "label": "B",
          "content": "Update the existing VPC, and associate an Amazon-provided IPv6 CIDR block with the VPC and all subnets. Update the VPC route tables for all private subnets, and add a route for ::/0 to the NAT gateway."
        },
        {
          "label": "C",
          "content": "Update the existing VPC, and associate an Amazon-provided IPv6 CIDR block with the VPC and all subnets. Create an egress-only internet gateway. Update the VPC route tables for all private subnets, and add a route for ::/0 to the egress-only internet gateway."
        },
        {
          "label": "D",
          "content": "Update the existing VPC, and associate a custom IPv6 CIDR block with the VPC and all subnets. Create a new NAT gateway, and enable IPv6 support. Update the VPC route tables for all private subnets, and add a route for ::/0 to the IPv6-enabled NAT gateway."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Option C is the correct answer. By associating an Amazon-provided IPv6 CIDR block with the VPC and subnets, and creating an egress-only internet gateway, the architect can ensure that the instances in private subnets use IPv6 for outbound traffic without being accessible from the public internet. An egress-only internet gateway allows outbound traffic from a VPC to the internet, but does not allow inbound traffic.</p><p>The requirements are: &nbsp;</p><p>1. Migrate EC2 instances to IPv6 while maintaining current networking behavior. &nbsp;</p><p>2. Private subnets must not be publicly accessible (IPv6 equivalent of NAT for IPv4). &nbsp;</p><p>Option C meets these requirements by: &nbsp;</p><p>- Associating an Amazon-provided IPv6 CIDR block (ensures globally unique addresses). &nbsp;</p><p>- Creating an egress-only internet gateway (EIGW) → Allows outbound IPv6 traffic but blocks inbound traffic (like NAT for IPv4). &nbsp;</p><p>- Adding `::/0` routes in private subnets to the EIGW (ensures private instances can access the internet but remain unreachable from it). &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A: Routes `::/0` to an internet gateway (IGW), making private instances publicly accessible (violates security requirements). &nbsp;</p><p>- B: NAT gateways don’t support IPv6 (they’re IPv4-only). &nbsp;</p><p>- D: NAT gateways cannot be IPv6-enabled (AWS doesn’t support IPv6 NAT). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- Egress-only internet gateway (EIGW) is the IPv6 equivalent of NAT for private subnets. &nbsp;</p><p>- Amazon-provided IPv6 CIDR blocks simplify address allocation (custom blocks are unnecessary). &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "098fb5246d594a90a1ba94bb49fb3335",
      "questionNumber": 366,
      "type": "single",
      "content": "<p>Question #366</p><p>A company is using Amazon API Gateway to deploy a private REST API that will provide access to sensitive data. The API must be accessible only from an application that is deployed in a VPC. The company deploys the API successfully. However, the API is not accessible from an Amazon EC2 instance that is deployed in the VPC.</p><p><br></p><p>Which solution will provide connectivity between the EC2 instance and the API?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an interface VPC endpoint for API Gateway. Attach an endpoint policy that allows apigateway:* actions. Disable private DNS naming for the VPC endpoint. Configure an API resource policy that allows access from the VPC. Use the VPC endpoint&#39;s DNS name to access the API."
        },
        {
          "label": "B",
          "content": "Create an interface VPC endpoint for API Gateway. Attach an endpoint policy that allows the execute-api:Invoke action. Enable private DNS naming for the VPC endpoint. Configure an API resource policy that allows access from the VPC endpoint. Use the API endpoint&rsquo;s DNS names to access the API."
        },
        {
          "label": "C",
          "content": "Create a Network Load Balancer (NLB) and a VPC link. Configure private integration between API Gateway and the NLB. Use the API endpoint&rsquo;s DNS names to access the API."
        },
        {
          "label": "D",
          "content": "Create an Application Load Balancer (ALB) and a VPC Link. Configure private integration between API Gateway and the ALB. Use the ALB endpoint&rsquo;s DNS name to access the API."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Option B is the correct answer. Creating an interface VPC endpoint for API Gateway with private DNS naming enabled allows the EC2 instance within the VPC to communicate with the API Gateway using the VPC endpoint's DNS name. Attaching an endpoint policy that allows the execute-api:Invoke action and configuring an API resource policy to allow access from the VPC endpoint ensures secure and private connectivity.</p><p>The requirements are: &nbsp;</p><p>1. Private REST API (accessible only from within a VPC). &nbsp;</p><p>2. API must be reachable from an EC2 instance in the VPC. &nbsp;</p><p>3. Current issue: API is not accessible from the EC2 instance. &nbsp;</p><p>Option B resolves this by: &nbsp;</p><p>- Creating an interface VPC endpoint for API Gateway (enables private connectivity from the VPC). &nbsp;</p><p>- Attaching an endpoint policy allowing `execute-api:Invoke` (grants API access permissions). &nbsp;</p><p>- Enabling private DNS naming (simplifies API calls by using standard API Gateway DNS names). &nbsp;</p><p>- Configuring an API resource policy to restrict access to the VPC endpoint (ensures security). &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A: Disabling private DNS naming forces manual DNS configuration (adds unnecessary complexity). &nbsp;</p><p>- C: NLB + VPC Link is for integrating private backend services (not needed here; the issue is API Gateway access). &nbsp;</p><p>- D: ALB + VPC Link is also for backend integration (irrelevant for this use case). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- Interface VPC endpoints provide secure, private access to AWS services (like API Gateway). &nbsp;</p><p>- Private DNS naming avoids manual endpoint URL management. &nbsp;</p><p>- API resource policies enforce least-privilege access control. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "ff59c07a8dd245af8c75c3427070ab0c",
      "questionNumber": 367,
      "type": "single",
      "content": "<p>Question #367</p><p>A large payroll company recently merged with a small staffing company. The unified company now has multiple business units, each with its own existing AWS account. A solutions architect must ensure that the company can centrally manage the billing and access policies for all the AWS accounts. The solutions architect configures AWS Organizations by sending an invitation to all member accounts of the company from a centralized management account.</p><p><br></p><p>What should the solutions architect do next to meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create the OrganizationAccountAccess IAM group in each member account. Include the necessary IAM roles for each administrator."
        },
        {
          "label": "B",
          "content": "Create the OrganizationAccountAccessPolicy IAM policy in each member account. Connect the member accounts to the management account by using cross-account access."
        },
        {
          "label": "C",
          "content": "Create the OrganizationAccountAccessRole IAM role in each member account. Grant permission to the management account to assume the IAM role."
        },
        {
          "label": "D",
          "content": "Create the OrganizationAccountAccessRole IAM role in the management account. Attach the AdministratorAccess AWS managed policy to the IAM role. Assign the IAM role to the administrators in each member account."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the scenario where a solutions architect needs to centrally manage billing and access policies for multiple AWS accounts after configuring AWS Organizations and sending invitations to member accounts, the correct next step is:</p><p>C. Create the OrganizationAccountAccessRole IAM role in each member account. Grant permission to the management account to assume the IAM role.</p><p>Here's why this is the correct answer:</p><p>The OrganizationAccountAccessRole is the standard IAM role that AWS Organizations uses to enable cross-account access from the management account to member accounts. This is a best practice recommended by AWS for organizations.</p><p>By creating this role in each member account and granting permission to the management account to assume it, administrators in the management account can access and manage resources in the member accounts. This enables centralized management of all accounts.</p><p>This approach follows the principle of role-based access control, which is more secure than sharing credentials or creating users in each account.</p><p>The OrganizationAccountAccessRole typically has administrative permissions in the member account, allowing the management account to perform necessary management tasks, including access policy management.</p><p>For billing management, AWS Organizations automatically enables consolidated billing once member accounts join the organization, allowing the management account to centrally manage billing for all accounts.</p><p>The other options are incorrect for the following reasons:</p><p>A. Creating an OrganizationAccountAccess IAM group in each member account is not the correct approach. IAM groups are collections of IAM users within a single account and don't facilitate cross-account access. Groups cannot be assumed like roles can, making them unsuitable for cross-account management.</p><p>B. There is no standard \"OrganizationAccountAccessPolicy\" in AWS Organizations. While custom policies can be created, they need to be attached to roles or users to be effective. Simply creating a policy without attaching it to a role that can be assumed by the management account wouldn't enable cross-account access.</p><p>Creating the OrganizationAccountAccessRole in each member account is the standard and recommended approach for enabling centralized management of AWS accounts within an organization, making option C the correct answer.</p><p>Sources</p><p>Managing account invitations with AWS Organizations - AWS Organizations （https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_invites.html）</p><p>AWS Identity and Access Management and AWS Organizations - AWS Organizations （https://docs.aws.amazon.com/organizations/latest/userguide/services-that-can-integrate-iam.html）</p><p>Manage member accounts - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/transitioning-to-multiple-aws-accounts/manage-member-accounts.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "1f9602eccd094921910b7a2eebde4ce3",
      "questionNumber": 368,
      "type": "single",
      "content": "<p>Question #368</p><p>A company has application services that have been containerized and deployed on multiple Amazon EC2 instances with public IPs. An Apache Kafka cluster has been deployed to the EC2 instances. A PostgreSQL database has been migrated to Amazon RDS for PostgreSQL. The company expects a significant increase of orders on its platform when a new version of its flagship product is released.</p><p><br></p><p>What changes to the current architecture will reduce operational overhead and support the product release?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an EC2 Auto Scaling group behind an Application Load Balancer. Create additional read replicas for the DB instance. Create Amazon Kinesis data streams and configure the application services to use the data streams. Store and serve static content directly from Amazon S3."
        },
        {
          "label": "B",
          "content": "Create an EC2 Auto Scaling group behind an Application Load Balancer. Deploy the DB instance in Multi-AZ mode and enable storage auto scaling. Create Amazon Kinesis data streams and configure the application services to use the data streams. Store and serve static content directly from Amazon S3."
        },
        {
          "label": "C",
          "content": "Deploy the application on a Kubernetes cluster created on the EC2 instances behind an Application Load Balancer. Deploy the DB instance in Multi-AZ mode and enable storage auto scaling. Create an Amazon Managed Streaming for Apache Kafka cluster and configure the application services to use the cluster. Store static content in Amazon S3 behind an Amazon CloudFront distribution."
        },
        {
          "label": "D",
          "content": "Deploy the application on Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate and enable auto scaling behind an Application Load Balancer. Create additional read replicas for the DB instance. Create an Amazon Managed Streaming for Apache Kafka cluster and configure the application services to use the cluster. Store static content in Amazon S3 behind an Amazon CloudFront distribution."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Option D is the correct answer. Deploying the application on Amazon EKS with AWS Fargate allows the company to run containers without managing the underlying infrastructure, which reduces operational overhead. Using an Application Load Balancer for auto scaling and Multi-AZ deployment with storage auto scaling for the RDS instance ensures high availability and scalability. Creating an Amazon Managed Streaming for Apache Kafka cluster provides a scalable and managed streaming platform for Kafka. Storing static content in Amazon S3 and serving it through Amazon CloudFront reduces latency and improves the content delivery experience for users.</p><p>The requirements are: &nbsp;</p><p>1. Reduce operational overhead (automate scaling, minimize management). &nbsp;</p><p>2. Handle a surge in orders (scalable architecture). &nbsp;</p><p>3. Containerized services (needs orchestration). &nbsp;</p><p>4. Replace self-managed Kafka (reduce complexity). &nbsp;</p><p>Option D meets these requirements by: &nbsp;</p><p>- Using Amazon EKS with Fargate → Serverless Kubernetes (no EC2 management, auto-scaling). &nbsp;</p><p>- Multi-AZ RDS + read replicas → Scalable, highly available database. &nbsp;</p><p>- Amazon MSK → Fully managed Kafka (replaces self-managed clusters). &nbsp;</p><p>- S3 + CloudFront → Scalable static content delivery. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A & B: &nbsp;</p><p> &nbsp;- EC2 Auto Scaling requires manual capacity management (higher overhead than EKS/Fargate). &nbsp;</p><p> &nbsp;- Self-managed Kafka remains operational burden. &nbsp;</p><p>- C: &nbsp;</p><p> &nbsp;- Self-managed Kubernetes on EC2 still requires node management (vs. Fargate’s serverless approach). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- EKS + Fargate eliminates node management. &nbsp;</p><p>- Amazon MSK removes Kafka operational overhead. &nbsp;</p><p>- CloudFront optimizes static content delivery globally. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "383bf43bbaed408199c7329bc0b6e53b",
      "questionNumber": 369,
      "type": "multiple",
      "content": "<p>Question #369</p><p>A company hosts a VPN in an on-premises data center. Employees currently connect to the VPN to access files in their Windows home directories. Recently, there has been a large growth in the number of employees who work remotely. As a result, bandwidth usage for connections into the data center has begun to reach 100% during business hours.</p><p><br></p><p>The company must design a solution on AWS that will support the growth of the company's remote workforce, reduce the bandwidth usage for connections into the data center, and reduce operational overhead.</p><p><br></p><p>Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an AWS Storage Gateway Volume Gateway. Mount a volume from the Volume Gateway to the on-premises file server."
        },
        {
          "label": "B",
          "content": "Migrate the home directories to Amazon FSx for Windows File Server."
        },
        {
          "label": "C",
          "content": "Migrate the home directories to Amazon FSx for Lustre."
        },
        {
          "label": "D",
          "content": "Migrate remote users to AWS Client VPN."
        },
        {
          "label": "E",
          "content": "Create an AWS Direct Connect connection from the on-premises data center to AWS "
        }
      ],
      "correctAnswer": "BD",
      "explanation": "<p>Based on the scenario where a company needs to support a growing remote workforce, reduce bandwidth usage for connections to their on-premises data center, and reduce operational overhead, the two steps that would meet these requirements with the least operational overhead are:</p><p>B. Migrate the home directories to Amazon FSx for Windows File Server. D. Migrate remote users to AWS Client VPN.</p><p>Here's why these two options are the best combination:</p><p>Amazon FSx for Windows File Server (Option B):</p><p>It's a fully managed native Windows file system that's specifically designed for Windows workloads</p><p>It supports all the Windows file system features needed for home directories, including SMB protocol, Active Directory integration, and Windows ACLs</p><p>As a managed service, it has minimal operational overhead compared to self-managed alternatives</p><p>Moving the home directories to FSx would eliminate the need for remote users to access the on-premises data center for file access, directly addressing the bandwidth issue</p><p>It provides high availability and automatic backups, further reducing operational overhead</p><p>AWS Client VPN (Option D):</p><p>This is a managed client-based VPN service that would replace the on-premises VPN</p><p>It can scale to support the growing number of remote employees</p><p>It would direct remote user traffic to AWS instead of the on-premises data center, reducing bandwidth usage</p><p>As a managed service, it eliminates the operational overhead of maintaining an on-premises VPN solution</p><p>It can be configured to provide secure access to both the FSx file shares and any remaining on-premises resources</p><p>The other options are less suitable:</p><p>A. Creating an AWS Storage Gateway Volume Gateway would still require maintaining an on-premises file server and would not fully address the bandwidth issue, as data would still need to be synchronized between AWS and the on-premises environment. This would result in higher operational overhead compared to a fully managed solution.</p><p>C. Amazon FSx for Lustre is designed for high-performance computing workloads, not for Windows home directories. It lacks native support for Windows-specific features like Active Directory integration and would require additional components to work with Windows environments, increasing operational overhead.</p><p>The combination of migrating home directories to FSx for Windows File Server and moving remote users to AWS Client VPN provides a comprehensive solution that addresses all the requirements with minimal operational overhead. Users would connect to AWS Client VPN and access their home directories on FSx, bypassing the on-premises data center entirely for file access.</p><p>Sources</p><p>AWS Storage Gateway - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/optimize-costs-microsoft-workloads/storage-main-storage-gateway.html）</p><p>Choose the right SMB file storage - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/optimize-costs-microsoft-workloads/storage-fsx-smb.html）</p><p>PERF03-BP01 Understand storage characteristics and requirements - AWS Well-Architected Framework (2022-03-31) （https://docs.aws.amazon.com/wellarchitected/2022-03-31/framework/perf_right_storage_solution_understand_char.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "6bb4880bcd114de8823d871d4806f176",
      "questionNumber": 370,
      "type": "multiple",
      "content": "<p>Question #370</p><p>A company has multiple AWS accounts. The company recently had a security audit that revealed many unencrypted Amazon Elastic Block Store (Amazon EBS) volumes attached to Amazon EC2 instances.</p><p><br></p><p>A solutions architect must encrypt the unencrypted volumes and ensure that unencrypted volumes will be detected automatically in the future. Additionally, the company wants a solution that can centrally manage multiple AWS accounts with a focus on compliance and security.</p><p><br></p><p>Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an organization in AWS Organizations. Set up AWS Control Tower, and turn on the strongly recommended controls (guardrails). Join all accounts to the organization. Categorize the AWS accounts into OUs."
        },
        {
          "label": "B",
          "content": "Use the AWS CLI to list all the unencrypted volumes in all the AWS accounts. Run a script to encrypt all the unencrypted volumes in place."
        },
        {
          "label": "C",
          "content": "Create a snapshot of each unencrypted volume. Create a new encrypted volume from the unencrypted snapshot. Detach the existing volume, and replace it with the encrypted volume."
        },
        {
          "label": "D",
          "content": "Create an organization in AWS Organizations. Set up AWS Control Tower, and turn on the mandatory controls (guardrails). Join all accounts to the organization. Categorize the AWS accounts into OUs."
        },
        {
          "label": "E",
          "content": "Turn on AWS CloudTrail. Configure an Amazon EventBridge rule to detect and automatically encrypt unencrypted volumes."
        }
      ],
      "correctAnswer": "AC",
      "explanation": "<p>Based on the requirement to encrypt unencrypted EBS volumes, detect unencrypted volumes automatically in the future, and centrally manage multiple AWS accounts with a focus on compliance and security, the correct combination of steps is:</p><p>A. Create an organization in AWS Organizations. Set up AWS Control Tower, and turn on the strongly recommended controls (guardrails). Join all accounts to the organization. Categorize the AWS accounts into OUs.</p><p>C. Create a snapshot of each unencrypted volume. Create a new encrypted volume from the unencrypted snapshot. Detach the existing volume, and replace it with the encrypted volume.</p><p>Here's why these two options work best together:</p><p>Option A addresses the requirements for centralized management and future detection:</p><p>AWS Organizations provides a way to centrally manage multiple AWS accounts</p><p>AWS Control Tower sets up a well-architected multi-account environment with built-in governance</p><p>The strongly recommended guardrails include controls that enforce EBS volume encryption</p><p>Organizing accounts into Organizational Units (OUs) allows for applying different policies based on account types</p><p>This setup will automatically detect and prevent the creation of unencrypted volumes in the future</p><p>This solution focuses on compliance and security through centralized management</p><p>Option C addresses the requirement to encrypt the currently unencrypted volumes:</p><p>This is the standard AWS-recommended process for encrypting existing unencrypted EBS volumes</p><p>Creating a snapshot, then creating an encrypted volume from that snapshot, and finally replacing the original volume is the only way to encrypt an existing EBS volume</p><p>This process preserves all the data while applying encryption</p><p>While this process requires some operational effort, it's necessary to address the existing unencrypted volumes</p><p>Together, these options provide a complete solution that:</p><p>Encrypts all existing unencrypted volumes (Option C)</p><p>Prevents the creation of new unencrypted volumes (Option A)</p><p>Provides centralized management of multiple AWS accounts (Option A)</p><p>Focuses on compliance and security through guardrails and organizational structure (Option A)</p><p>The combination of these two approaches addresses both the immediate need to encrypt existing volumes and the long-term requirement to maintain compliance and security across multiple AWS accounts.</p><p>Sources</p><p>Automatically encrypt existing and new Amazon EBS volumes - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/automatically-encrypt-existing-and-new-amazon-ebs-volumes.html）</p><p>SEC08-BP02 Enforce encryption at rest - Security Pillar （https://docs.aws.amazon.com/wellarchitected/latest/security-pillar/sec_protect_data_rest_encrypt.html）</p><p>Use encryption with EBS-backed AMIs - Amazon Elastic Compute Cloud （https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIEncryption.html）</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "94807acad6634a2b856c79a4c31c2d66",
      "questionNumber": 371,
      "type": "single",
      "content": "<p>Question #371</p><p>A company hosts an intranet web application on Amazon EC2 instances behind an Application Load Balancer (ALB). Currently, users authenticate to the application against an internal user database.</p><p><br></p><p>The company needs to authenticate users to the application by using an existing AWS Directory Service for Microsoft Active Directory directory. All users with accounts in the directory must have access to the application.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a new app client in the directory. Create a listener rule for the ALB. Specify the authenticate-oidc action for the listener rule. Configure the listener rule with the appropriate issuer, client ID and secret, and endpoint details for the Active Directory service. Configure the new app client with the callback URL that the ALB provides."
        },
        {
          "label": "B",
          "content": "Configure an Amazon Cognito user pool. Configure the user pool with a federated identity provider (IdP) that has metadata from the directory. Create an app client. Associate the app client with the user pool. Create a listener rule for the ALB. Specify the authenticate-cognito action for the listener rule. Configure the listener rule to use the user pool and app client."
        },
        {
          "label": "C",
          "content": "Add the directory as a new IAM identity provider (IdP). Create a new IAM role that has an entity type of SAML 2.0 federation. Configure a role policy that allows access to the ALB. Configure the new role as the default authenticated user role for the IdP. Create a listener rule for the ALB. Specify the authenticate-oidc action for the listener rule."
        },
        {
          "label": "D",
          "content": "Enable AWS IAM Identity Center (AWS Single Sign-On). Configure the directory as an external identity provider (IdP) that uses SAML. Use the automatic provisioning method. Create a new IAM role that has an entity type of SAML 2.0 federation. Configure a role policy that allows access to the ALB. Attach the new role to all groups. Create a listener rule for the ALB. Specify the authenticate-cognito action for the listener rule."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>The requirements are: &nbsp;</p><p>1. Authenticate users via AWS Directory Service (Microsoft AD). &nbsp;</p><p>2. Allow all directory users access to the web app. &nbsp;</p><p>3. Integrate with ALB for seamless authentication. &nbsp;</p><p>Option B meets these requirements by: &nbsp;</p><p>- Using Amazon Cognito as an intermediary: &nbsp;</p><p> &nbsp;- Configure a user pool with a federated identity provider (IdP) linked to the existing Active Directory. &nbsp;</p><p> &nbsp;- Set up an app client for the web application. &nbsp;</p><p>- ALB Listener Rule: &nbsp;</p><p> &nbsp;- Use the `authenticate-cognito` action to redirect users to Cognito for authentication via Active Directory. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A: ALB’s `authenticate-oidc` requires manual OIDC configuration (complex and error-prone vs. Cognito’s managed integration). &nbsp;</p><p>- C: IAM SAML federation is not natively supported by ALB for authentication (ALB requires OIDC/Cognito). &nbsp;</p><p>- D: AWS IAM Identity Center (SSO) is for centralized access to AWS accounts/apps, not for ALB-integrated web app authentication. &nbsp;</p><p> Best Practices: &nbsp;</p><p>- Cognito + ALB `authenticate-cognito` is the standard for AD-integrated web app authentication. &nbsp;</p><p>- Federated identities simplify user management (sync with existing AD users). &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "be74e9fdef94403db7016751e8a51e46",
      "questionNumber": 372,
      "type": "single",
      "content": "<p>Question #372</p><p>A company has a website that serves many visitors. The company deploys a backend service for the website in a primary AWS Region and a disaster recovery (DR) Region.</p><p><br></p><p>A single Amazon CloudFront distribution is deployed for the website. The company creates an Amazon Route 53 record set with health checks and a failover routing policy for the primary Region’s backend service. The company configures the Route 53 record set as an origin for the CloudFront distribution. The company configures another record set that points to the backend service's endpoint in the DR Region as a secondary failover record type. The TTL for both record sets is 60 seconds.</p><p><br></p><p>Currently, failover takes more than 1 minute. A solutions architect must design a solution that will provide the fastest failover time.</p><p><br></p><p>Which solution will achieve this goal?</p>",
      "options": [
        {
          "label": "A",
          "content": "Deploy an additional CloudFront distribution. Create a new Route 53 failover record set with health checks for both CloudFront distributions."
        },
        {
          "label": "B",
          "content": "Set the TTL to 4 seconds for the existing Route 53 record sets that are used for the backend service in each Region."
        },
        {
          "label": "C",
          "content": "Create new record sets for the backend services by using a latency routing policy. Use the record sets as an origin in the CloudFront distribution."
        },
        {
          "label": "D",
          "content": "Create a CloudFront origin group that includes two origins, one for each backend service Region. Configure origin failover as a cache behavior for the CloudFront distribution."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>The requirement is to minimize failover time for the website’s backend service (currently &gt;1 minute due to Route 53 TTL and health check delays). &nbsp;</p><p>Option D solves this by: &nbsp;</p><p>- Using CloudFront Origin Groups: &nbsp;</p><p> &nbsp;- Configure two origins (primary and DR Region backends). &nbsp;</p><p> &nbsp;- Enable origin failover in CloudFront (automatically switches to the secondary origin if the primary fails). &nbsp;</p><p> &nbsp;- Failover happens in seconds (bypasses Route 53 TTL/health check delays). &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A: Dual CloudFront distributions + Route 53 failover still relies on Route 53 health checks (TTL delays persist). &nbsp;</p><p>- B: 4-second TTL is impractical (violates AWS’s minimum TTL of 10 seconds for alias records). &nbsp;</p><p>- C: Latency routing doesn’t address failover (it routes based on latency, not health). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- CloudFront Origin Failover provides near-instant failover (vs. Route 53’s dependency on TTL/health checks). &nbsp;</p><p>- Eliminates dependency on DNS propagation (which causes delays in Route 53-based failover). &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "2d246acc2dbd4fdf99dee0d25e7454eb",
      "questionNumber": 373,
      "type": "multiple",
      "content": "<p>Question #373</p><p>A company is using multiple AWS accounts and has multiple DevOps teams running production and non-production workloads in these accounts. The company would like to centrally-restrict access to some of the AWS services that the DevOps teams do not use. The company decided to use AWS Organizations and successfully invited all AWS accounts into the Organization. They would like to allow access to services that are currently in-use and deny a few specific services. Also, they would like to administer multiple accounts together as a single unit.</p><p><br></p><p>What combination of steps should the solutions architect take to satisfy these requirements? (Choose three.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Use a Deny list strategy."
        },
        {
          "label": "B",
          "content": "Review the Access Advisor in AWS IAM to determine services recently used."
        },
        {
          "label": "C",
          "content": "Review the AWS Trusted Advisor report to determine services recently used."
        },
        {
          "label": "D",
          "content": "Remove the default FullAWSAccess SCP."
        },
        {
          "label": "E",
          "content": "Define organizational units (OUs) and place the member accounts in the OUs."
        },
        {
          "label": "F",
          "content": "Remove the default DenyAWSAccess SCP."
        }
      ],
      "correctAnswer": "ABE",
      "explanation": "<p>Based on the requirement to centrally restrict access to some AWS services across multiple accounts while allowing access to services currently in use, the correct combination of steps is:</p><p>A. Use a Deny list strategy. B. Review the Access Advisor in AWS IAM to determine services recently used. E. Define organizational units (OUs) and place the member accounts in the OUs.</p><p>Here's why these three options are the correct choices:</p><p>Use a Deny list strategy (Option A):</p><p>A Deny list strategy is the most appropriate approach when you want to allow most services but restrict access to specific ones.</p><p>This aligns with the company's requirement to \"allow access to services that are currently in-use and deny a few specific services.\"</p><p>With a Deny list strategy, you keep the default FullAWSAccess SCP in place and create additional SCPs that explicitly deny access to specific services.</p><p>This approach requires less maintenance as new AWS services are automatically allowed unless explicitly denied.</p><p>Review the Access Advisor in AWS IAM to determine services recently used (Option B):</p><p>IAM Access Advisor provides insights into which AWS services have been accessed by IAM entities (users, groups, and roles) and when they were last used.</p><p>This tool is perfect for identifying which services are currently in use across the organization, which is necessary to determine which services should remain accessible.</p><p>Access Advisor provides historical data (typically for the last 365 days), allowing the company to make informed decisions about which services to restrict.</p><p>Define organizational units (OUs) and place the member accounts in the OUs (Option E):</p><p>Organizing accounts into OUs allows the company to \"administer multiple accounts together as a single unit\" as required.</p><p>OUs provide a hierarchical structure for applying SCPs to groups of accounts with similar requirements.</p><p>This approach simplifies policy management and allows for more granular control over which restrictions apply to which groups of accounts.</p><p>OUs can be structured to align with the company's organizational structure (e.g., separating production and non-production workloads).</p><p>The other options are not appropriate for the given requirements:</p><p>C. Review the AWS Trusted Advisor report to determine services recently used - While Trusted Advisor provides useful insights, it's not specifically designed to show which services are being used across accounts. IAM Access Advisor is the more appropriate tool for this purpose.</p><p>D. Remove the default FullAWSAccess SCP - This would be appropriate for an Allow list strategy, not a Deny list strategy. Removing this SCP would deny access to all services by default, which contradicts the requirement to \"allow access to services that are currently in-use.\"</p><p>The combination of using a Deny list strategy, reviewing IAM Access Advisor to determine which services are in use, and organizing accounts into OUs provides the most effective approach to meet the company's requirements for centrally restricting access to unused AWS services while maintaining access to necessary services.</p><p>Sources</p><p>Restrict access using AWS Organizations service control policies - AWS Account Management （https://docs.aws.amazon.com/accounts/latest/reference/using-orgs-example-scps.html）</p><p>Using AWS Organizations for security - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/security-reference-architecture/organizations-security.html）</p><p>Community | Manage Security and Costs Across the Enterprise With AWS Organizations （https://community.aws/content/2iCmFQ2kf2EWfMXQUXZh6BJIyM7/practical-cloud-guide-manage-security-and-costs-across-the-enterprise-with-aws-organizations）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "5affae832d5b430dbd018bb845b944f4",
      "questionNumber": 374,
      "type": "single",
      "content": "<p>Question #374</p><p>A live-events company is designing a scaling solution for its ticket application on AWS. The application has high peaks of utilization during sale events. Each sale event is a one-time event that is scheduled. The application runs on Amazon EC2 instances that are in an Auto Scaling group. The application uses PostgreSQL for the database layer.</p><p><br></p><p>The company needs a scaling solution to maximize availability during the sale events.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use a predictive scaling policy for the EC2 instances. Host the database on an Amazon Aurora PostgreSQL Serverless v2 Multi-AZ DB instance with automatically scaling read replicas. Create an AWS Step Functions state machine to run parallel AWS Lambda functions to pre warm the database before a sale event. Create an Amazon EventBridge rule to invoke the state machine."
        },
        {
          "label": "B",
          "content": "Use a scheduled scaling policy for the EC2 instances. Host the database on an Amazon RDS for PostgreSQL Multi-AZ DB instance with automatically scaling read replicas. Create an Amazon EventBridge rule that invokes an AWS Lambda function to create a larger read replica before a sale event. Fail over to the larger read replica. Create another EventBridge rule that invokes another Lambda function to scale down the read replica after the sale event."
        },
        {
          "label": "C",
          "content": "Use a predictive scaling policy for the EC2 instances. Host the database on an Amazon RDS for PostgreSQL MultiAZ DB instance with automatically scaling read replicas. Create an AWS Step Functions state machine to run parallel AWS Lambda functions to pre-warm the database before a sale event. Create an Amazon EventBridge rule to invoke the state machine."
        },
        {
          "label": "D",
          "content": "Use a scheduled scaling policy for the EC2 instances. Host the database on an Amazon Aurora PostgreSQL Multi-AZ DB cluster. Create an Amazon EventBridge rule that invokes an AWS Lambda function to create a larger Aurora Replica before a sale event. Fail over to the larger Aurora Replica. Create another EventBridge rule that invokes another Lambda function to scale down the Aurora Replica after the sale event."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Option D is the correct answer. Using a scheduled scaling policy for EC2 instances allows for the pre-planning of resources for known high-traffic events. Amazon Aurora's ability to handle high transaction rates and its support for scaling individual replicas makes it a good choice for the database layer. The use of AWS Lambda and Amazon EventBridge for automating the scaling of Aurora Replicas before and after the sale events ensures that the database can handle the load while keeping costs in check during off-peak times.</p><p>The requirements are: &nbsp;</p><p>1. Handle high, predictable traffic peaks (one-time scheduled sale events). &nbsp;</p><p>2. Maximize availability for both compute (EC2) and database (PostgreSQL) layers. &nbsp;</p><p>Option D meets these requirements by: &nbsp;</p><p>- Scheduled scaling for EC2: Proactively scales out before the event (predictive scaling is less precise for one-time events). &nbsp;</p><p>- Aurora PostgreSQL Multi-AZ: &nbsp;</p><p> &nbsp;- Scaling read replicas: Lambda automates replica scaling before/after the event. &nbsp;</p><p> &nbsp;- Failover to larger replicas: Ensures database performance during peak loads. &nbsp;</p><p>- EventBridge rules: Trigger Lambda functions to manage scaling dynamically. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A & C: Predictive scaling is better for recurring patterns (not one-time events). Pre-warming databases is unnecessary with Aurora’s auto-scaling. &nbsp;</p><p>- B: RDS PostgreSQL read replicas require manual promotion (Aurora’s failover is faster and more reliable). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- Scheduled scaling is ideal for known traffic spikes (e.g., ticket sales). &nbsp;</p><p>- Aurora Multi-AZ + read replicas provides high availability and scalable performance. &nbsp;</p><p>- EventBridge + Lambda automates scaling workflows. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "fd78be1fc6f1436e8d8a8dd2320bfe17",
      "questionNumber": 375,
      "type": "multiple",
      "content": "<p>Question #375</p><p>A company runs an intranet application on premises. The company wants to configure a cloud backup of the application. The company has selected AWS Elastic Disaster Recovery for this solution. The company requires that replication traffic does not travel through the public internet. The application also must not be accessible from the internet. The company does not want this solution to consume all available network bandwidth because other applications require bandwidth.</p><p><br></p><p>Which combination of steps will meet these requirements? (Choose three.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a VPC that has at least two private subnets, two NAT gateways, and a virtual private gateway."
        },
        {
          "label": "B",
          "content": "Create a VPC that has at least two public subnets, a virtual private gateway, and an internet gateway."
        },
        {
          "label": "C",
          "content": "Create an AWS Site-to-Site VPN connection between the on-premises network and the target AWS network."
        },
        {
          "label": "D",
          "content": "Create an AWS Direct Connect connection and a Direct Connect gateway between the on-premises network and the target AWS network."
        },
        {
          "label": "E",
          "content": "During configuration of the replication servers, select the option to use private IP addresses for data replication."
        },
        {
          "label": "F",
          "content": "During configuration of the launch settings for the target servers, select the option to ensure that the Recovery instance&rsquo;s private IP address matches the source server&#39;s private IP address."
        }
      ],
      "correctAnswer": "ADE",
      "explanation": "<p>Based on the requirement to configure AWS Elastic Disaster Recovery with private connectivity that doesn't use the public internet, keeps the application inaccessible from the internet, and manages bandwidth consumption, the correct combination of steps is:</p><p>A. Create a VPC that has at least two private subnets, two NAT gateways, and a virtual private gateway. </p><p><br></p><p>D. Create an AWS Direct Connect connection and a Direct Connect gateway between the on-premises network and the target AWS network. </p><p><br></p><p>E. During configuration of the replication servers, select the option to use private IP addresses for data replication.</p><p>Here's why these three options are the correct choices:</p><p>Option A: Create a VPC that has at least two private subnets, two NAT gateways, and a virtual private gateway.</p><p>The VPC provides the network infrastructure in AWS to host the disaster recovery environment</p><p>Private subnets ensure the application is not accessible from the internet</p><p>Multiple subnets across availability zones provide high availability</p><p>The virtual private gateway is necessary to terminate the Direct Connect connection</p><p>NAT gateways allow resources in private subnets to access AWS services while maintaining security</p><p>Option D: Create an AWS Direct Connect connection and a Direct Connect gateway between the on-premises network and the target AWS network.</p><p>Direct Connect provides a dedicated private connection between on-premises and AWS</p><p>This ensures that \"replication traffic does not travel through the public internet\" as required</p><p>Direct Connect offers consistent network performance and bandwidth</p><p>The Direct Connect gateway allows connectivity between on-premises networks and multiple VPCs</p><p>Option E: During configuration of the replication servers, select the option to use private IP addresses for data replication.</p><p>This setting ensures that all replication traffic uses private IP addressing</p><p>It prevents any replication traffic from traversing the public internet</p><p>This configuration works in conjunction with Direct Connect to maintain private connectivity</p><p>These three steps together create a comprehensive solution that:</p><p>Establishes private connectivity between on-premises and AWS (Direct Connect)</p><p>Creates the necessary AWS network infrastructure (VPC with private subnets)</p><p>Configures the replication to use private addressing (private IP option)</p><p>Maintains security by keeping resources in private subnets</p><p>Allows for bandwidth management through Direct Connect's dedicated connection</p><p>The combination of these three options satisfies all the requirements: private connectivity, application security, and bandwidth management for the AWS Elastic Disaster Recovery solution.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "29d02a54fed547caafb8ca79115fdfb7",
      "questionNumber": 376,
      "type": "single",
      "content": "<p>Question #376</p><p>A company that provides image storage services wants to deploy a customer-facing solution to AWS. Millions of individual customers will use the solution. The solution will receive batches of large image files, resize the files, and store the files in an Amazon S3 bucket for up to 6 months.<br><br>The solution must handle significant variance in demand. The solution must also be reliable at enterprise scale and have the ability to rerun processing jobs in the event of failure.</p><p><br></p><p>Which solution will meet these requirements MOST cost-effectively?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use AWS Step Functions to process the S3 event that occurs when a user stores an image. Run an AWS Lambda function that resizes the image in place and replaces the original file in the S3 bucket. Create an S3 Lifecycle expiration policy to expire all stored images after 6 months."
        },
        {
          "label": "B",
          "content": "Use Amazon EventBridge to process the S3 event that occurs when a user uploads an image. Run an AWS Lambda function that resizes the image in place and replaces the original file in the S3 bucket. Create an S3 Lifecycle expiration policy to expire all stored images after 6 months."
        },
        {
          "label": "C",
          "content": "Use S3 Event Notifications to invoke an AWS Lambda function when a user stores an image. Use the Lambda function to resize the image in place and to store the original file in the S3 bucket. Create an S3 Lifecycle policy to move all stored images to S3 Standard-Infrequent Access (S3 Standard-IA) after 6 months."
        },
        {
          "label": "D",
          "content": "Use Amazon Simple Queue Service (Amazon SQS) to process the S3 event that occurs when a user stores an image. Run an AWS Lambda function that resizes the image and stores the resized file in an S3 bucket that uses S3 Standard-Infrequent Access (S3 Standard-IA). Create an S3 Lifecycle policy to move all stored images to S3 Glacier Deep Archive after 6 months."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>The requirements are: &nbsp;</p><p>1. Handle variable demand (scale automatically). &nbsp;</p><p>2. Enterprise-scale reliability (retry failed jobs). &nbsp;</p><p>3. Cost-effective storage (images stored for 6 months). &nbsp;</p><p>Option D meets these requirements by: &nbsp;</p><p>- Using SQS + Lambda: &nbsp;</p><p> &nbsp;- SQS queues decouple uploads from processing, enabling retries and buffering during demand spikes. &nbsp;</p><p> &nbsp;- Lambda auto-scales to process resizing jobs. &nbsp;</p><p>- Cost-optimized storage: &nbsp;</p><p> &nbsp;- S3 Standard-IA for frequently accessed resized images. &nbsp;</p><p> &nbsp;- S3 Glacier Deep Archive after 6 months (lowest cost for archival). &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A & B: Step Functions/EventBridge + Lambda lack built-in retry mechanisms (SQS is more reliable for job reprocessing). &nbsp;</p><p>- C: In-place resizing risks data loss if Lambda fails; no archival tier for cost savings. &nbsp;</p><p> Best Practices: &nbsp;</p><p>- SQS + Lambda is ideal for scalable, fault-tolerant batch processing. &nbsp;</p><p>- S3 Lifecycle policies optimize storage costs automatically. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "7e349c0775744186bce5e2f35ad46eae",
      "questionNumber": 377,
      "type": "single",
      "content": "<p>Question #377</p><p>A company has an organization in AWS Organizations that includes a separate AWS account for each of the company’s departments. Application teams from different departments develop and deploy solutions independently.</p><p><br></p><p>The company wants to reduce compute costs and manage costs appropriately across departments. The company also wants to improve visibility into billing for individual departments. The company does not want to lose operational flexibility when the company selects compute resources.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use AWS Budgets for each department. Use Tag Editor to apply tags to appropriate resources. Purchase EC2 Instance Savings Plans."
        },
        {
          "label": "B",
          "content": "Configure AWS Organizations to use consolidated billing. Implement a tagging strategy that identifies departments. Use SCPs to apply tags to appropriate resources. Purchase EC2 Instance Savings Plans."
        },
        {
          "label": "C",
          "content": "Configure AWS Organizations to use consolidated billing. Implement a tagging strategy that identifies departments. Use Tag Editor to apply tags to appropriate resources. Purchase Compute Savings Plans."
        },
        {
          "label": "D",
          "content": "Use AWS Budgets for each department. Use SCPs to apply tags to appropriate resources. Purchase Compute Savings Plans."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the requirement to reduce compute costs, manage costs across departments, improve billing visibility, and maintain operational flexibility, the correct answer is:</p><p>C. Configure AWS Organizations to use consolidated billing. Implement a tagging strategy that identifies departments. Use Tag Editor to apply tags to appropriate resources. Purchase Compute Savings Plans.</p><p>Here's why this is the best solution:</p><p>Consolidated Billing with AWS Organizations:</p><p>This allows the company to receive a single bill for all department accounts</p><p>Enables volume discounts across all accounts, reducing overall costs</p><p>Provides a centralized view of costs across all departments</p><p>Allows Savings Plans benefits to be shared across all accounts in the organization</p><p>Tagging Strategy that Identifies Departments:</p><p>Tags allow resources to be associated with specific departments</p><p>Enables detailed cost allocation and reporting by department</p><p>Provides visibility into which departments are using which resources</p><p>Supports chargeback or showback models for internal cost management</p><p>Using Tag Editor to Apply Tags:</p><p>Tag Editor is a flexible tool that allows for bulk tagging of resources</p><p>It doesn't restrict operational flexibility, unlike SCPs which are restrictive policies</p><p>Allows application teams to maintain their independence while still implementing tags</p><p>Supports the company's requirement not to \"lose operational flexibility\"</p><p>Compute Savings Plans:</p><p>Provide significant discounts (up to 66%) compared to On-Demand pricing</p><p>Offer maximum flexibility across EC2, Lambda, and Fargate services</p><p>Allow departments to change instance types, sizes, and even compute services while still receiving discounts</p><p>Support the requirement for operational flexibility while reducing costs</p><p>Option B is not optimal because:</p><p>It uses SCPs to apply tags, which are restrictive policies that could limit operational flexibility</p><p>EC2 Instance Savings Plans are less flexible than Compute Savings Plans as they're limited to specific EC2 instance families in specific regions</p><p>This approach would restrict the application teams' ability to choose the most appropriate compute resources for their needs</p><p>The solution in option C provides the best balance of cost savings, billing visibility, and operational flexibility, making it the most suitable choice for the company's requirements.</p><p>Sources</p><p>Decide on Savings Plans or Reserved Instances for EC2 instances | AWS re:Post （https://repost.aws/knowledge-center/ec2-savings-plan-reserved-instances）</p><p>Understanding how Savings Plans apply to your usage - Savings Plans （https://docs.aws.amazon.com/savingsplans/latest/userguide/sp-applying.html）</p><p>Optimize spending for Windows on Amazon EC2 - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/optimize-costs-microsoft-workloads/savings-plans.html）</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "5e318f74f6494bbbac2b15a217aa194b",
      "questionNumber": 378,
      "type": "single",
      "content": "<p>Question #378</p><p>A company has a web application that securely uploads pictures and videos to an Amazon S3 bucket. The company requires that only authenticated users are allowed to post content. The application generates a presigned URL that is used to upload objects through a browser interface. Most users are reporting slow upload times for objects larger than 100 MB.</p><p><br></p><p>What can a solutions architect do to improve the performance of these uploads while ensuring only authenticated users are allowed to post content?</p>",
      "options": [
        {
          "label": "A",
          "content": "Set up an Amazon API Gateway with an edge-optimized API endpoint that has a resource as an S3 service proxy. Configure the PUT method for this resource to expose the S3 PutObject operation. Secure the API Gateway using a COGNITO_USER_POOLS authorizer. Have the browser interface use API Gateway instead of the presigned URL to upload objects."
        },
        {
          "label": "B",
          "content": "Set up an Amazon API Gateway with a regional API endpoint that has a resource as an S3 service proxy. Configure the PUT method for this resource to expose the S3 PutObject operation. Secure the API Gateway using an AWS Lambda authorizer. Have the browser interface use API Gateway instead of the presigned URL to upload objects."
        },
        {
          "label": "C",
          "content": "Enable an S3 Transfer Acceleration endpoint on the S3 bucket. Use the endpoint when generating the presigned URL. Have the browser interface upload the objects to this URL using the S3 multipart upload API."
        },
        {
          "label": "D",
          "content": "Configure an Amazon CloudFront distribution for the destination S3 bucket. Enable PUT and POST methods for the CloudFront cache behavior. Update the CloudFront origin to use an origin access identity (OAI). Give the OAI user 3: PutObject permissions in the bucket policy. Have the browser interface upload objects using the CloudFront distribution."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>The requirements are: &nbsp;</p><p>1. Improve upload performance for large files (&gt;100 MB). &nbsp;</p><p>2. Ensure only authenticated users can upload (presigned URLs already enforce this). &nbsp;</p><p>Option C meets these requirements by: &nbsp;</p><p>- Enabling S3 Transfer Acceleration → Uses CloudFront’s edge network for faster uploads globally. &nbsp;</p><p>- Using multipart upload API → Parallelizes large file uploads (improves speed and reliability). &nbsp;</p><p>- Retaining presigned URLs → Maintains authentication without added complexity. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A & B: API Gateway as an S3 proxy adds latency and complexity (no performance gain over direct S3 uploads). &nbsp;</p><p>- D: CloudFront with PUT/POST is not optimized for uploads (Transfer Acceleration is purpose-built for S3). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- S3 Transfer Acceleration leverages CloudFront’s backbone for faster uploads. &nbsp;</p><p>- Multipart uploads split large files into parts (improves throughput). &nbsp;</p><p>- Presigned URLs remain the simplest way to authenticate uploads. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "c2900b0f4ccf4b2495e245f606a8405e",
      "questionNumber": 379,
      "type": "multiple",
      "content": "<p>Question #379</p><p>A large company is migrating its entire IT portfolio to AWS. Each business unit in the company has a standalone AWS account that supports both development and test environments. New accounts to support production workloads will be needed soon.</p><p><br></p><p>The finance department requires a centralized method for payment but must maintain visibility into each group's spending to allocate costs. The security team requires a centralized mechanism to control IAM usage in all the company’s accounts.</p><p><br></p><p>What combination of the following options meets the company’s needs with the LEAST effort? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Use a collection of parameterized AWS CloudFormation templates defining common IAM permissions that are launched into each account. Require all new and existing accounts to launch the appropriate stacks to enforce the least privilege model."
        },
        {
          "label": "B",
          "content": "Use AWS Organizations to create a new organization from a chosen payer account and define an organizational unit hierarchy. Invite the existing accounts to join the organization and create new accounts using Organizations."
        },
        {
          "label": "C",
          "content": "Require each business unit to use its own AWS accounts. Tag each AWS account appropriately and enable Cost Explorer to administer chargebacks."
        },
        {
          "label": "D",
          "content": "Enable all features of AWS Organizations and establish appropriate service control policies that filter IAM permissions for sub-accounts."
        },
        {
          "label": "E",
          "content": "Consolidate all of the company&#39;s AWS accounts into a single AWS account. Use tags for billing purposes and the IAM&rsquo;s Access Advisor feature to enforce the least privilege model.&nbsp;"
        }
      ],
      "correctAnswer": "BD",
      "explanation": "<p>Based on the requirements for centralizing payment methods while maintaining visibility into each group's spending, and implementing centralized control over IAM usage across all accounts with the least effort, the correct combination is:</p><p>B. Use AWS Organizations to create a new organization from a chosen payer account and define an organizational unit hierarchy. Invite the existing accounts to join the organization and create new accounts using Organizations.</p><p>D. Enable all features of AWS Organizations and establish appropriate service control policies that filter IAM permissions for sub-accounts.</p><p>Here's why these two options are the best solution:</p><p>Option B addresses the finance department's requirements:</p><p>Creates a centralized billing structure through AWS Organizations</p><p>The payer account becomes the management account that handles all billing</p><p>Existing standalone accounts can be invited to join the organization</p><p>New production accounts can be created directly within the organization</p><p>Organizational unit hierarchy allows for logical grouping of accounts by business unit</p><p>Consolidated billing provides visibility into each group's spending for cost allocation</p><p>This approach requires minimal effort as it leverages existing accounts</p><p>Option D addresses the security team's requirements:</p><p>Enabling all features of AWS Organizations (not just consolidated billing) is necessary to use service control policies</p><p>Service control policies (SCPs) provide a centralized mechanism to control IAM usage across all accounts</p><p>SCPs act as permission guardrails that limit maximum permissions available in member accounts</p><p>This approach allows the security team to enforce consistent IAM policies across all accounts</p><p>SCPs can be applied at the organizational unit level, making it easy to manage permissions for groups of accounts</p><p>Together, these options provide:</p><p>Centralized billing with cost visibility (finance requirement)</p><p>Centralized IAM control (security requirement)</p><p>Support for both existing and new accounts (migration requirement)</p><p>Minimal effort implementation by leveraging AWS's purpose-built solution</p><p>This combination represents the most straightforward and least-effort approach to meeting both the finance department's need for centralized billing with visibility and the security team's requirement for centralized IAM control across all accounts.</p><p>Sources</p><p>University of British Columbia Cloud Innovation Centre: Governing an innovation hub using AWS management services | AWS Public Sector Blog （https://aws.amazon.com/cn/blogs/publicsector/university-of-british-columbia-cloud-innovation-centre-governing-an-innovation-hub-using-aws-management-services/）</p><p>When to use AWS Organizations - AWS Account Management （https://docs.aws.amazon.com/accounts/latest/reference/using-orgs.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "05d6913f4c164b409c04aeb05d53cb82",
      "questionNumber": 380,
      "type": "single",
      "content": "<p>Question #380</p><p>A company has a solution that analyzes weather data from thousands of weather stations. The weather stations send the data over an Amazon API Gateway REST API that has an AWS Lambda function integration. The Lambda function calls a third-party service for data pre-processing. The third-party service gets overloaded and fails the pre-processing, causing a loss of data.</p><p><br></p><p>A solutions architect must improve the resiliency of the solution. The solutions architect must ensure that no data is lost and that data can be processed later if failures occur.</p><p><br></p><p>What should the solutions architect do to meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure the queue as the dead-letter queue for the API."
        },
        {
          "label": "B",
          "content": "Set up two Amazon Simple Queue Service (Amazon SQS) queues: a primary queue and a secondary queue. Configure the secondary queue as the dead-letter queue for the primary queue. Update the API to use a new integration to the primary queue. Configure the Lambda function as the invocation target for the primary queue."
        },
        {
          "label": "C",
          "content": "Create two Amazon EventBridge event buses: a primary event bus and a secondary event bus. Update the API to use a new integration to the primary event bus. Configure an EventBridge rule to react to all events on the primary event bus. Specify the Lambda function as the target of the rule. Configure the secondary event bus as the failure destination for the Lambda function."
        },
        {
          "label": "D",
          "content": "Create a custom Amazon EventBridge event bus. Configure the event bus as the failure destination for the Lambda function."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Option B is the correct answer. By setting up two SQS queues, with the primary queue handling incoming data and the secondary queue acting as a dead-letter queue, the architect can ensure that data is not lost when the Lambda function is unable to process it due to the third-party service failure. This setup allows for the data to be reprocessed at a later time, thus improving the resiliency of the system.</p><p>The requirements are: &nbsp;</p><p>1. Ensure no data is lost (handle third-party service failures). &nbsp;</p><p>2. Enable delayed processing (retry failed jobs). &nbsp;</p><p>Option B meets these requirements by: &nbsp;</p><p>- Using two SQS queues: &nbsp;</p><p> &nbsp;- Primary queue: Buffers incoming weather data (decouples API Gateway from Lambda). &nbsp;</p><p> &nbsp;- Dead-letter queue (DLQ): Captures failed processing attempts (ensures no data loss). &nbsp;</p><p>- Lambda retries: Processes messages from the primary queue; failed messages move to the DLQ for later reprocessing. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A: Single SQS queue as DLQ for API Gateway is invalid (API Gateway cannot directly use SQS as DLQ). &nbsp;</p><p>- C & D: EventBridge lacks built-in retry logic (SQS is better for queuing and reprocessing). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- SQS + DLQ is the standard for resilient message processing. &nbsp;</p><p>- Decoupling (API Gateway → SQS → Lambda) prevents data loss during third-party failures. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "af87c51fa2fc4efc9e74550b865a27a0",
      "questionNumber": 381,
      "type": "multiple",
      "content": "<p>Question #381</p><p>A company built an ecommerce website on AWS using a three-tier web architecture. The application is Java-based and composed of an Amazon CloudFront distribution, an Apache web server layer of Amazon EC2 instances in an Auto Scaling group, and a backend Amazon Aurora MySQL database.</p><p><br></p><p>Last month, during a promotional sales event, users reported errors and timeouts while adding items to their shopping carts. The operations team recovered the logs created by the web servers and reviewed Aurora DB cluster performance metrics. Some of the web servers were terminated before logs could be collected and the Aurora metrics were not sufficient for query performance analysis.</p><p><br></p><p>Which combination of steps must the solutions architect take to improve application performance visibility during peak traffic events? (Choose three.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Configure the Aurora MySQL DB cluster to publish slow query and error logs to Amazon CloudWatch Logs."
        },
        {
          "label": "B",
          "content": "Implement the AWS X-Ray SDK to trace incoming HTTP requests on the EC2 instances and implement tracing of SQL queries with the X-Ray SDK for Java."
        },
        {
          "label": "C",
          "content": "Conigure the Aurora MySQL DB cluster to stream slow query and error logs to Amazon Kinesis."
        },
        {
          "label": "D",
          "content": "Install and configure an Amazon CloudWatch Logs agent on the EC2 instances to send the Apache logs to CloudWatch Logs."
        },
        {
          "label": "E",
          "content": "Enable and configure AWS CloudTrail to collect and analyze application activity from Amazon EC2 and Aurora."
        },
        {
          "label": "F",
          "content": "Enable Aurora MySQL DB cluster performance benchmarking and publish the stream to AWS X-Ray."
        }
      ],
      "correctAnswer": "ABD",
      "explanation": "<p>Option A is correct as publishing slow query and error logs to CloudWatch Logs can help in identifying and diagnosing database performance issues. Option B is correct because implementing AWS X-Ray can provide insights into the performance of the application and its underlying services, including the ability to trace SQL queries. Option D is also correct as configuring the CloudWatch Logs agent on EC2 instances can centralize log data for analysis, which is crucial for troubleshooting and performance monitoring.</p><p>The requirements are to improve performance visibility during peak traffic by: &nbsp;</p><p>1. Capturing web server logs (prevent loss during Auto Scaling terminations). &nbsp;</p><p>2. Analyzing database query performance (slow queries/errors). &nbsp;</p><p>3. Tracing request flows (identify bottlenecks). &nbsp;</p><p>Options A, B, and D meet these needs by: &nbsp;</p><p>- A: Aurora slow query/error logs in CloudWatch → Diagnose DB performance issues. &nbsp;</p><p>- B: AWS X-Ray → Trace HTTP requests and SQL queries end-to-end. &nbsp;</p><p>- D: CloudWatch Logs agent → Persist Apache logs (survives instance termination). &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- C: Kinesis for Aurora logs is overkill (CloudWatch Logs suffices). &nbsp;</p><p>- E: CloudTrail tracks API calls, not application performance. &nbsp;</p><p>- F: X-Ray doesn’t benchmark Aurora (it traces requests, unlike Performance Insights). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- CloudWatch Logs centralizes logs for Auto Scaling workloads. &nbsp;</p><p>- X-Ray maps distributed transaction flows. &nbsp;</p><p>- Aurora logs in CloudWatch enable historical query analysis. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "5b5643440c1a4d4ea019d352019f1802",
      "questionNumber": 382,
      "type": "single",
      "content": "<p>Question #382</p><p>A company that provisions job boards for a seasonal workforce is seeing an increase in traffic and usage. The backend services run on a pair of Amazon EC2 instances behind an Application Load Balancer with Amazon DynamoDB as the datastore. Application read and write traffic is slow during peak seasons.</p><p><br></p><p>Which option provides a scalable application architecture to handle peak seasons with the LEAST development effort?</p>",
      "options": [
        {
          "label": "A",
          "content": "Migrate the backend services to AWS Lambda. Increase the read and write capacity of DynamoDB."
        },
        {
          "label": "B",
          "content": "Migrate the backend services to AWS Lambda. Configure DynamoDB to use global tables."
        },
        {
          "label": "C",
          "content": "Use Auto Scaling groups for the backend services. Use DynamoDB auto scaling."
        },
        {
          "label": "D",
          "content": "Use Auto Scaling groups for the backend services. Use Amazon Simple Queue Service (Amazon SQS) and an AWS Lambda function to write to DynamoDB."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the scenario where a job board application with EC2 instances and DynamoDB is experiencing slow read and write traffic during peak seasons, the option that provides a scalable architecture with the least development effort is:</p><p>C. Use Auto Scaling groups for the backend services. Use DynamoDB auto scaling.</p><p>Here's why this is the best option:</p><p>Least Development Effort:</p><p>This approach builds on the existing architecture (EC2 instances and DynamoDB) without requiring a significant rewrite of the application code.</p><p>The current backend services can continue to run on EC2 instances with minimal modifications.</p><p>Setting up Auto Scaling groups and DynamoDB auto scaling is primarily a configuration task rather than a development task.</p><p>Scalability Benefits:</p><p>Auto Scaling groups automatically adjust the number of EC2 instances based on demand, ensuring the backend services can handle increased traffic during peak seasons.</p><p>DynamoDB auto scaling automatically adjusts read and write capacity units based on actual usage patterns, addressing the slow read and write traffic issues.</p><p>Both components scale independently and automatically in response to actual load.</p><p>Why the other options require more effort or are less suitable:</p><p>Option A (Migrate to Lambda with increased DynamoDB capacity):</p><p>Requires rewriting the backend services to run as Lambda functions</p><p>Manually increasing DynamoDB capacity doesn't provide the automatic scaling needed for seasonal fluctuations</p><p>Option B (Migrate to Lambda with DynamoDB global tables):</p><p>Requires rewriting the backend services for Lambda</p><p>Global tables are designed for multi-region replication, which adds complexity and cost but doesn't directly address the scaling issue for a single-region application</p><p>Option D (Auto Scaling groups with SQS and Lambda):</p><p>Introduces additional components (SQS and Lambda) that would require new development work</p><p>Adds complexity to the architecture that may not be necessary to solve the specific problem</p><p>Option C leverages AWS's managed scaling capabilities for both compute (EC2 Auto Scaling) and database (DynamoDB auto scaling) layers, providing a comprehensive scaling solution with minimal development effort. It allows the application to automatically adjust resources based on demand, which is ideal for handling seasonal traffic patterns.</p><p>Sources</p><p>Choose the right throughput strategy for Amazon DynamoDB applications | AWS Database Blog （https://aws.amazon.com/blogs/database/choose-the-right-throughput-strategy-for-amazon-dynamodb-applications/）</p><p>Managing throughput capacity automatically with DynamoDB auto scaling - Amazon DynamoDB （https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html）</p><p>Scaling and event processing with SNS and SQS | AWS re:Post （https://repost.aws/questions/QUcnYW9ZAkSA-Px9tyHm_sYw/scaling-and-event-processing-with-sns-and-sqs）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "ed8298ec63704b4aa5a8531a5c2e9f33",
      "questionNumber": 383,
      "type": "single",
      "content": "<p>Question #383</p><p>A company is migrating to the cloud. It wants to evaluate the configurations of virtual machines in its existing data center environment to ensure that it can size new Amazon EC2 instances accurately. The company wants to collect metrics, such as CPU, memory, and disk utilization, and it needs an inventory of what processes are running on each instance. The company would also like to monitor network connections to map communications between servers.</p><p><br></p><p>Which would enable the collection of this data MOST cost effectively?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use AWS Application Discovery Service and deploy the data collection agent to each virtual machine in the data center."
        },
        {
          "label": "B",
          "content": "Configure the Amazon CloudWatch agent on all servers within the local environment and publish metrics to Amazon CloudWatch Logs."
        },
        {
          "label": "C",
          "content": "Use AWS Application Discovery Service and enable agentless discovery in the existing virtualization environment."
        },
        {
          "label": "D",
          "content": "Enable AWS Application Discovery Service in the AWS Management Console and configure the corporate firewall to allow scans over a VPN."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Based on the company's requirements to collect metrics such as CPU, memory, disk utilization, running processes, and network connections between servers for EC2 instance sizing, the most cost-effective option is:</p><p>A. Use AWS Application Discovery Service and deploy the data collection agent to each virtual machine in the data center.</p><p>Here's why this is the most cost-effective solution:</p><p>Comprehensive Data Collection: The AWS Application Discovery Service with agent-based collection provides all the required data:</p><p>System metrics (CPU, memory, disk utilization)</p><p>Inventory of running processes</p><p>Network connections to map communications between servers</p><p>Detailed time-series performance data</p><p>Cost Effectiveness:</p><p>AWS Application Discovery Service is provided at no additional charge</p><p>The data collected is stored in AWS for 90 days at no cost</p><p>It's specifically designed for migration planning, so it collects exactly what's needed without excess</p><p>Purpose-Built for Migration: This service is specifically designed to help with cloud migration planning and EC2 instance sizing, making it the most appropriate tool for the stated requirements.</p><p>Why the other options are less suitable:</p><p>B. Configure the Amazon CloudWatch agent on all servers:</p><p>Incurs costs for metrics collection and storage in CloudWatch</p><p>Primarily designed for ongoing monitoring rather than migration planning</p><p>Would require additional configuration to collect all the required data</p><p>Not as seamlessly integrated with AWS migration tools</p><p>C. Use AWS Application Discovery Service with agentless discovery:</p><p>While also free to use, the agentless approach cannot collect information about running processes</p><p>Cannot map detailed network connections between servers</p><p>Limited to VMware environments and provides less detailed information</p><p>Doesn't meet all the requirements specified by the company</p><p>The agent-based AWS Application Discovery Service is the most cost-effective solution because it's free to use, purpose-built for migration planning, and collects all the required data types (system metrics, process inventory, and network connections). This makes it the ideal choice for accurately sizing EC2 instances based on the company's existing virtual machine configurations.</p><p>Sources</p><p>AWS Application Discovery Agent - AWS Application Discovery Service （https://docs.aws.amazon.com/application-discovery/latest/userguide/discovery-agent.html）</p><p>Welcome - AWS Application Discovery Service （https://docs.aws.amazon.com/application-discovery/latest/APIReference/Welcome.html）</p><p>Discover on-premises resources using AWS Migration Hub discovery tools - AWS Migration Hub （https://docs.aws.amazon.com/migrationhub/latest/ug/gs-new-user-discovery.html）</p><p>AWS Application Discovery Service Update – Agentless Discovery for VMware | AWS News Blog （https://aws.amazon.com/cn/blogs/aws/aws-application-discovery-service-update-agentless-discovery-for-vmware/）</p><p>Application Discovery Service Agentless Collector - AWS Application Discovery Service （https://docs.aws.amazon.com/application-discovery/latest/userguide/agentless-collector.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "a0ba3cd209584b1ba6efc6d80097f34b",
      "questionNumber": 384,
      "type": "single",
      "content": "<p>Question #384</p><p>A company provides a software as a service (SaaS) application that runs in the AWS Cloud. The application runs on Amazon EC2 instances behind a Network Load Balancer (NLB). The instances are in an Auto Scaling group and are distributed across three Availability Zones in a single AWS Region.</p><p><br></p><p>The company is deploying the application into additional Regions. The company must provide static IP addresses for the application to customers so that the customers can add the IP addresses to allow lists. The solution must automatically route customers to the Region that is geographically closest to them.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an Amazon CloudFront distribution. Create a CloudFront origin group. Add the NLB for each additional Region to the origin group. Provide customers with the IP address ranges of the distribution&rsquo;s edge locations."
        },
        {
          "label": "B",
          "content": "Create an AWS Global Accelerator standard accelerator. Create a standard accelerator endpoint for the NLB in each additional Region. Provide customers with the Global Accelerator IP address."
        },
        {
          "label": "C",
          "content": "Create an Amazon CloudFront distribution. Create a custom origin for the NLB in each additional Region. Provide customers with the IP address ranges of the distribution&rsquo;s edge locations."
        },
        {
          "label": "D",
          "content": "Create an AWS Global Accelerator custom routing accelerator. Create a listener for the custom routing accelerator. Add the IP address and ports for the NLB in each additional Region. Provide customers with the Global Accelerator IP address."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Option B is the correct answer. AWS Global Accelerator can direct traffic to the optimal regional endpoint based on the location of the user, providing the fastest response times. By creating a standard accelerator and providing the Global Accelerator IP address to customers, the company can ensure that users are routed to the closest Region without exposing the underlying NLB IP addresses.</p><p>The requirements are: &nbsp;</p><p>1. Provide static IP addresses for customers to whitelist. &nbsp;</p><p>2. Route customers to the nearest Region (geographic proximity). &nbsp;</p><p>3. Support multi-Region deployment with NLBs. &nbsp;</p><p>Option B meets these requirements by: &nbsp;</p><p>- Using AWS Global Accelerator: &nbsp;</p><p> &nbsp;- Provides two static anycast IPs (simplifies allow lists). &nbsp;</p><p> &nbsp;- Automatically routes traffic to the nearest Region based on latency. &nbsp;</p><p> &nbsp;- Supports NLB endpoints in each Region. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A & C: CloudFront uses dynamic edge location IPs (not static), and origin groups don’t support NLB. &nbsp;</p><p>- D: Custom routing accelerator is for non-standard ports/protocols (overkill for this use case). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- Global Accelerator is ideal for static IPs + low-latency multi-Region routing. &nbsp;</p><p>- NLB integration ensures high availability. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "c725d7c5454642b68fbd0c1a18b2985c",
      "questionNumber": 385,
      "type": "single",
      "content": "<p>Question #385</p><p>A company is running multiple workloads in the AWS Cloud. The company has separate units for software development. The company uses AWS Organizations and federation with SAML to give permissions to developers to manage resources in their AWS accounts. The development units each deploy their production workloads into a common production account.</p><p><br></p><p>Recently, an incident occurred in the production account in which members of a development unit terminated an EC2 instance that belonged to a different development unit. A solutions architect must create a solution that prevents a similar incident from happening in the future. The solution also must allow developers the possibility to manage the instances used for their workloads.</p><p><br></p><p>Which strategy will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create separate OUs in AWS Organizations for each development unit. Assign the created OUs to the company AWS accounts. Create separate SCP with a deny action and a StringNotEquals condition for the DevelopmentUnit resource tag that matches the development unit name. Assign the SCP to the corresponding OU."
        },
        {
          "label": "B",
          "content": "Pass an attribute for DevelopmentUnit as an AWS Security Token Service (AWS STS) session tag during SAML federation. Update the IAM policy for the developers&rsquo; assumed IAM role with a deny action and a StringNotEquals condition for the DevelopmentUnit resource tag and aws:PrincipalTag/DevelopmentUnit."
        },
        {
          "label": "C",
          "content": "Pass an attribute for DevelopmentUnit as an AWS Security Token Service (AWS STS) session tag during SAML federation. Create an SCP with an allow action and a StringEquals condition for the DevelopmentUnit resource tag and aws:PrincipalTag/DevelopmentUnit. Assign the SCP to the root OU."
        },
        {
          "label": "D",
          "content": "Create separate IAM policies for each development unit. For every IAM policy, add an allow action and a StringEquals condition for the DevelopmentUnit resource tag and the development unit name. During SAML federation, use AWS Security Token Service (AWS STS) to assign the IAM policy and match the development unit name to the assumed IAM role."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>The requirements are: &nbsp;</p><p>1. Prevent developers from managing other teams’ EC2 instances (isolate workloads by development unit). &nbsp;</p><p>2. Allow developers to manage their own instances (least privilege). &nbsp;</p><p>Option B meets these requirements by: &nbsp;</p><p>- Using SAML + STS session tags: &nbsp;</p><p> &nbsp;- Pass a `DevelopmentUnit` tag during federation (e.g., `TeamA`, `TeamB`). &nbsp;</p><p>- IAM policy with `StringNotEquals` condition: &nbsp;</p><p> &nbsp;- Denies actions if the `aws:PrincipalTag/DevelopmentUnit` (user’s team) doesn’t match the `DevelopmentUnit` tag on the EC2 instance. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A: SCPs with `StringNotEquals` are overly broad (SCPs apply to entire OUs, not fine-grained resources). &nbsp;</p><p>- C: SCPs with `allow` + `StringEquals` would block all untagged resources (risky for production). &nbsp;</p><p>- D: Static IAM policies per team are hard to scale (STS session tags dynamically enforce isolation). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- STS session tags enable dynamic, attribute-based access control (ABAC). &nbsp;</p><p>- IAM conditions (`aws:PrincipalTag`) enforce team-based isolation without manual policy updates. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "581bb0949fbb4cf297d1bbc0cf7a33f8",
      "questionNumber": 386,
      "type": "multiple",
      "content": "<p>Question #386</p><p>An enterprise company is building an infrastructure services platform for its users. The company has the following requirements:</p><p><br></p><p>- Provide least privilege access to users when launching AWS infrastructure so users cannot provision unapproved services.</p><p>- Use a central account to manage the creation of infrastructure services.</p><p>- Provide the ability to distribute infrastructure services to multiple accounts in AWS Organizations.</p><p>- Provide the ability to enforce tags on any infrastructure that is started by users.</p><p><br></p><p>Which combination of actions using AWS services will meet these requirements? (Choose three.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Develop infrastructure services using AWS CloudFormation templates. Add the templates to a central Amazon S3 bucket and add the IAM roles or users that require access to the S3 bucket policy."
        },
        {
          "label": "B",
          "content": "Develop infrastructure services using AWS CloudFormation templates. Upload each template as an AWS Service Catalog product to portfolios created in a central AWS account. Share these portfolios with the Organizations structure created for the company."
        },
        {
          "label": "C",
          "content": "Allow user IAM roles to have AWSCloudFormationFullAccess and AmazonS3ReadOnlyAccess permissions. Add an Organizations SCP at the AWS account root user level to deny all services except AWS CloudFormation and Amazon S3."
        },
        {
          "label": "D",
          "content": "Allow user IAM roles to have ServiceCatalogEndUserAccess permissions only. Use an automation script to import the central portfolios to local AWS accounts, copy the TagOption, assign users access, and apply launch constraints."
        },
        {
          "label": "E",
          "content": "Use the AWS Service Catalog TagOption Library to maintain a list of tags required by the company. Apply the TagOption to AWS Service Catalog products or portfolios."
        },
        {
          "label": "F",
          "content": "Use the AWS CloudFormation Resource Tags property to enforce the application of tags to any CloudFormation templates that will be created for users."
        }
      ],
      "correctAnswer": "BDE",
      "explanation": "<p>Option B is correct as it allows the creation of standardized infrastructure services using AWS CloudFormation templates within AWS Service Catalog, which can be shared across the organization. Option D is correct as it enables the enforcement of least privilege access by restricting user IAM roles to ServiceCatalogEndUserAccess permissions and using automation to manage access and apply launch constraints. Option E is correct as it allows for the enforcement of tagging by using the AWS Service Catalog TagOption Library, ensuring that all infrastructure provisioned through Service Catalog will have the required tags.</p><p>The requirements are: &nbsp;</p><p>1. Least privilege access (users can only launch approved services). &nbsp;</p><p>2. Central management (templates in a central account). &nbsp;</p><p>3. Multi-account distribution (via AWS Organizations). &nbsp;</p><p>4. Tag enforcement (mandatory tags on resources). &nbsp;</p><p>Options B, E, and D meet these requirements by: &nbsp;</p><p>- B: Service Catalog in a central account → CloudFormation templates are shared as products/portfolios across accounts. &nbsp;</p><p>- E: TagOption Library → Enforces consistent tags across all provisioned resources. &nbsp;</p><p>- D: Restrict users to `ServiceCatalogEndUserAccess` → Limits users to launching only approved Service Catalog products. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A: S3 + CloudFormation lacks governance (no approval workflow or tag enforcement). &nbsp;</p><p>- C: SCP to deny all except CloudFormation/S3 is too permissive (users can still deploy unapproved templates). &nbsp;</p><p>- F: CloudFormation Resource Tags are manual (Service Catalog automates enforcement). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- Service Catalog centralizes approved infrastructure as code (IaC). &nbsp;</p><p>- TagOptions ensure compliance with tagging policies. &nbsp;</p><p>- Least privilege is achieved via `ServiceCatalogEndUserAccess`. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "f3afab55a7164556a6c5615c473f5563",
      "questionNumber": 387,
      "type": "single",
      "content": "<p>Question #387</p><p>A company deploys a new web application. As part of the setup, the company configures AWS WAF to log to Amazon S3 through Amazon Kinesis Data Firehose. The company develops an Amazon Athena query that runs once daily to return AWS WAF log data from the previous 24 hours. The volume of daily logs is constant. However, over time, the same query is taking more time to run.</p><p><br></p><p>A solutions architect needs to design a solution to prevent the query time from continuing to increase. The solution must minimize operational overhead.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an AWS Lambda function that consolidates each day&#39;s AWS WAF logs into one log file."
        },
        {
          "label": "B",
          "content": "Reduce the amount of data scanned by configuring AWS WAF to send logs to a different S3 bucket each day."
        },
        {
          "label": "C",
          "content": "Update the Kinesis Data Firehose configuration to partition the data in Amazon S3 by date and time. Create external tables for Amazon Redshift. Configure Amazon Redshift Spectrum to query the data source."
        },
        {
          "label": "D",
          "content": "Modify the Kinesis Data Firehose configuration and Athena table definition to partition the data by date and time. Change the Athena query to view the relevant partitions."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Based on the requirement to prevent increasing query times for AWS WAF logs in Amazon Athena while minimizing operational overhead, the best solution is:</p><p>D. Modify the Kinesis Data Firehose configuration and Athena table definition to partition the data by date and time. Change the Athena query to view the relevant partitions.</p><p>Here's why this is the optimal solution:</p><p>Root Cause of Increasing Query Times: The query time is increasing because Athena is scanning an ever-growing amount of data in S3. Without partitioning, Athena must scan all WAF logs, even though the query only needs the last 24 hours of data.</p><p>Benefits of Partitioning:</p><p>Query Performance: Partitioning by date and time allows Athena to scan only the relevant partitions (last 24 hours) instead of the entire dataset, dramatically improving query performance.</p><p>Consistent Performance Over Time: As the total data volume grows, query performance remains stable because Athena only scans the partitions specified in the query.</p><p>Cost Efficiency: Athena pricing is based on the amount of data scanned, so partitioning reduces costs by limiting the scan to relevant data.</p><p>Implementation with Minimal Operational Overhead:</p><p>Kinesis Data Firehose can be configured to automatically partition data by date and time using dynamic prefixes like year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/hour=!{timestamp:HH}/</p><p>Athena's partition projection feature automatically manages partitions without manual intervention</p><p>Once configured, this solution runs automatically without daily maintenance</p><p>Why Option A is Less Optimal: Creating a Lambda function to consolidate daily logs would:</p><p>Introduce additional complexity and a potential point of failure</p><p>Require ongoing maintenance of the Lambda function</p><p>Not address the fundamental issue of scanning unnecessary data</p><p>Increase operational overhead rather than minimize it</p><p>The partitioning solution is a one-time configuration change that provides an automated, scalable approach to maintaining query performance as data volume grows. By modifying the Kinesis Data Firehose configuration to partition data and updating the Athena table definition to recognize these partitions, the company can ensure that queries only scan relevant data, keeping query times consistent regardless of how much historical data accumulates in S3.</p><p>Sources</p><p>Example queries for AWS WAF logs - Amazon Athena （https://docs.aws.amazon.com/athena/latest/ug/query-examples-waf-logs.html）</p><p>How to use Amazon Athena queries to analyze AWS WAF logs and provide the visibility needed for threat detection | Networking & Content Delivery （https://aws.amazon.com/cn/blogs/networking-and-content-delivery/how-to-use-amazon-athena-queries-to-analyze-aws-waf-logs-and-provide-the-visibility-needed-for-threat-detection/）</p><p>Amazon Data Firehose example - Amazon Athena （https://docs.aws.amazon.com/athena/latest/ug/partition-projection-kinesis-firehose-example.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "0cf1327e286849a78d9b92c9fac9967e",
      "questionNumber": 388,
      "type": "single",
      "content": "<p>Question #388</p><p>A company is developing a web application that runs on Amazon EC2 instances in an Auto Scaling group behind a public-facing Application Load Balancer (ALB). Only users from a specific country are allowed to access the application. The company needs the ability to log the access requests that have been blocked. The solution should require the least possible maintenance.</p><p><br></p><p>Which solution meets these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an IPSet containing a list of IP ranges that belong to the specified country. Create an AWS WAF web ACL. Configure a rule to block any requests that do not originate from an IP range in the IPSet. Associate the rule with the web ACL. Associate the web ACL with the ALB."
        },
        {
          "label": "B",
          "content": "Create an AWS WAF web ACL. Configure a rule to block any requests that do not originate from the specified country. Associate the rule with the web ACL. Associate the web ACL with the ALB."
        },
        {
          "label": "C",
          "content": "Configure AWS Shield to block any requests that do not originate from the specified country. Associate AWS Shield with the ALB."
        },
        {
          "label": "D",
          "content": "Create a security group rule that allows ports 80 and 443 from IP ranges that belong to the specified country. Associate the security group with the ALB."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the requirement to restrict access to a web application to users from a specific country while logging blocked requests with minimal maintenance, the best solution is:</p><p>B. Create an AWS WAF web ACL. Configure a rule to block any requests that do not originate from the specified country. Associate the rule with the web ACL. Associate the web ACL with the ALB.</p><p>Here's why this is the optimal solution:</p><p>Minimal Maintenance Requirement:</p><p>AWS WAF's built-in geo-matching feature automatically maintains up-to-date country-to-IP mappings</p><p>AWS handles all updates to the geolocation database, eliminating the need for manual updates</p><p>This approach requires significantly less maintenance compared to manually creating and maintaining an IPSet of country-specific IP ranges</p><p>Accuracy and Reliability:</p><p>The built-in geo-matching feature provides better accuracy as AWS continuously updates its geolocation database</p><p>This reduces false positives (blocking legitimate users) and false negatives (allowing unauthorized access)</p><p>IP ranges for countries change frequently, making manual maintenance prone to errors</p><p>Logging Capabilities:</p><p>AWS WAF can log all blocked requests to Amazon CloudWatch Logs, Amazon S3, or Amazon Kinesis Data Firehose</p><p>Logs include detailed information about the requests, including geographic origin</p><p>This satisfies the requirement to log access requests that have been blocked</p><p>Implementation Simplicity:</p><p>Creating a geo-match rule is straightforward in the AWS WAF console</p><p>You simply select the country you want to allow and set the rule to block requests not matching that country</p><p>Why Option A is Less Optimal:</p><p>Creating and maintaining an IPSet with country IP ranges requires significant ongoing maintenance</p><p>IP ranges change frequently, requiring regular updates to the IPSet</p><p>This approach is more prone to errors and gaps in coverage</p><p>It doesn't align with the requirement for \"least possible maintenance\"</p><p>The solution in Option B leverages AWS's built-in capabilities to handle the complexity of geo-matching, providing a more accurate, reliable, and low-maintenance approach to restricting access by country while ensuring all blocked requests are properly logged.</p><p>Sources</p><p>AWS WAF - AWS Best Practices for DDoS Resiliency （https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/aws-waf.html）</p><p>Restrict access based on IP address or geolocation by using AWS WAF - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/aws-waf-restrict-access-geolocation.html）</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "d692eeedb7b94534b1bf195bc20a6e88",
      "questionNumber": 389,
      "type": "single",
      "content": "<p>Question #389</p><p>A company is migrating an application from on-premises infrastructure to the AWS Cloud. During migration design meetings, the company expressed concerns about the availability and recovery options for its legacy Windows file server. The file server contains sensitive business critical data that cannot be recreated in the event of data corruption or data loss. According to compliance requirements, the data must not travel across the public internet. The company wants to move to AWS managed services where possible.</p><p><br></p><p>The company decides to store the data in an Amazon FSx for Windows File Server file system. A solutions architect must design a solution that copies the data to another AWS Region for disaster recovery (DR) purposes.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a destination Amazon S3 bucket in the DR Region. Establish connectivity between the FSx for Windows File Server file system in the primary Region and the S3 bucket in the DR Region by using Amazon FSx File Gateway. Configure the S3 bucket as a continuous backup source in FSx File Gateway."
        },
        {
          "label": "B",
          "content": "Create an FSx for Windows File Server file system in the DR Region. Establish connectivity between the VPC the primary Region and the VPC in the DR Region by using AWS Site-to-Site VPN. Configure AWS DataSync to communicate by using VPN endpoints."
        },
        {
          "label": "C",
          "content": "Create an FSx for Windows File Server file system in the DR Region. Establish connectivity between the VPC in the primary Region and the VPC in the DR Region by using VPC peering. Configure AWS DataSync to communicate by using interface VPC endpoints with AWS PrivateLink."
        },
        {
          "label": "D",
          "content": "Create an FSx for Windows File Server file system in the DR Region. Establish connectivity between the VPC in the primary Region and the VPC in the DR Region by using AWS Transit Gateway in each Region. Use AWS Transfer Family to copy files between the FSx for Windows File Server file system in the primary Region and the FSx for Windows File Server file system in the DR Region over the private AWS backbone network."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Option C is the correct solution as it meets the requirement of not allowing data to travel across the public internet by using VPC peering and AWS PrivateLink for a secure and private connection between the primary and DR Regions. Additionally, by using AWS DataSync, the company can efficiently copy data to the FSx for Windows File Server file system in the DR Region, ensuring business continuity and disaster recovery preparedness. This solution leverages AWS managed services and maintains compliance with the company's data travel restrictions.</p><p>The requirements are: &nbsp;</p><p>1. Migrate a legacy Windows file server to FSx for Windows (AWS managed service). &nbsp;</p><p>2. Replicate data to another Region for DR (no public internet traffic). &nbsp;</p><p>3. Compliance: Data must not traverse the public internet. &nbsp;</p><p>Option C meets these requirements by: &nbsp;</p><p>- Using FSx for Windows in both Regions → Fully managed, compatible with Windows file servers. &nbsp;</p><p>- VPC peering + PrivateLink → Ensures private replication (no internet exposure). &nbsp;</p><p>- AWS DataSync → Efficient, secure cross-Region file synchronization (supports PrivateLink). &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A: FSx File Gateway + S3 is for hybrid storage (not cross-Region DR for FSx). S3 is not a direct replacement for FSx. &nbsp;</p><p>- B: Site-to-Site VPN works but is less scalable than VPC peering/PrivateLink. &nbsp;</p><p>- D: Transfer Family is for SFTP/FTP (not designed for FSx replication). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- FSx for Windows supports native Windows features (SMB, Active Directory integration). &nbsp;</p><p>- DataSync + PrivateLink ensures encrypted, low-latency replication over AWS backbone. &nbsp;</p><p>- VPC peering simplifies private connectivity between Regions. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "8e0a4a5539524a4c89d8e5bcc7695d38",
      "questionNumber": 390,
      "type": "single",
      "content": "<p>Question #390</p><p>A company is currently in the design phase of an application that will need an RPO of less than 5 minutes and an RTO of less than 10 minutes. The solutions architecture team is forecasting that the database will store approximately 10 TB of data. As part of the design, they are looking for a database solution that will provide the company with the ability to fail over to a secondary Region.</p><p><br></p><p>Which solution will meet these business requirements at the LOWEST cost?</p>",
      "options": [
        {
          "label": "A",
          "content": "Deploy an Amazon Aurora DB cluster and take snapshots of the cluster every 5 minutes. Once a snapshot is complete, copy the snapshot to a secondary Region to serve as a backup in the event of a failure."
        },
        {
          "label": "B",
          "content": "Deploy an Amazon RDS instance with a cross-Region read replica in a secondary Region. In the event of a failure, promote the read replica to become the primary."
        },
        {
          "label": "C",
          "content": "Deploy an Amazon Aurora DB cluster in the primary Region and another in a secondary Region. Use AWS DMS to keep the secondary Region in sync."
        },
        {
          "label": "D",
          "content": "Deploy an Amazon RDS instance with a read replica in the same Region. In the event of a failure, promote the read replica to become the primary."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the requirement for a database solution with an RPO of less than 5 minutes, an RTO of less than 10 minutes, and the ability to fail over to a secondary Region for a 10TB database, the solution that will meet these business requirements at the LOWEST cost is:</p><p>B. Deploy an Amazon RDS instance with a cross-Region read replica in a secondary Region. In the event of a failure, promote the read replica to become the primary.</p><p>Here's why this is the most cost-effective solution that meets all requirements:</p><p>RPO (Recovery Point Objective) &lt; 5 minutes:</p><p>Cross-Region read replicas in Amazon RDS typically have replication lag measured in seconds, not minutes</p><p>This provides an RPO of seconds rather than minutes, well within the requirement of less than 5 minutes</p><p>The snapshot approach (Option A) would meet the 5-minute RPO requirement but with less margin for error</p><p>RTO (Recovery Time Objective) &lt; 10 minutes:</p><p>Promoting a read replica to primary is a relatively quick operation that can be completed within minutes</p><p>This allows the solution to meet the RTO requirement of less than 10 minutes</p><p>In contrast, restoring a 10TB database from a snapshot would take several hours, far exceeding the 10-minute RTO requirement</p><p>Failover to Secondary Region:</p><p>The cross-Region read replica is already running and contains a near real-time copy of the data</p><p>Failover simply requires promoting the read replica to primary status</p><p>This provides a much faster and more reliable failover mechanism than restoring from snapshots</p><p>Cost Considerations:</p><p>While maintaining a cross-Region read replica does incur ongoing costs for the additional instance and storage, it's the only option that can meet the strict RTO requirement of less than 10 minutes</p><p>The snapshot approach would be less expensive for ongoing operations but would fail to meet the RTO requirement by a significant margin</p><p>For a 10TB database, restoring from a snapshot would take hours, not minutes</p><p>Additional Benefits:</p><p>The read replica can serve read traffic in the secondary region, potentially improving application performance</p><p>The replica provides a warm standby that's ready to take over immediately</p><p>The solution offers better testing capabilities as you can verify the replica's data integrity without disrupting production</p><p>Option A (Aurora DB cluster with snapshots every 5 minutes) would be less expensive from an ongoing operational standpoint, but it cannot meet the 10-minute RTO requirement for a 10TB database. The time required to restore a 10TB snapshot in a secondary region would be several hours, making this approach unsuitable despite its lower cost.</p><p>Therefore, Option B is the LOWEST cost solution that meets ALL the business requirements, including the critical RTO of less than 10 minutes.</p><p>Sources</p><p>Disaster recovery options in the cloud - Disaster Recovery of Workloads on AWS: Recovery in the Cloud （https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-options-in-the-cloud.html）</p><p>Modernize SQL Server databases - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/optimize-costs-microsoft-workloads/modernize-sql-server.html）</p><p>DR solution decision matrix - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/dr-standard-edition-amazon-rds/dr-matrix.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "56b98cb7ec274e9b84fcb7e2ce4357c4",
      "questionNumber": 391,
      "type": "single",
      "content": "<p>Question #391</p><p>A financial company needs to create a separate AWS account for a new digital wallet application. The company uses AWS Organizations to manage its accounts. A solutions architect uses the IAM user Support1 from the management account to create a new member account with finance1@example.com as the email address.</p><p><br></p><p>What should the solutions architect do to create IAM users in the new member account?</p>",
      "options": [
        {
          "label": "A",
          "content": "Sign in to the AWS Management Console with AWS account root user credentials by using the 64-character password from the initial AWS Organizations email sent to finance1@example.com. Set up the IAM users as required."
        },
        {
          "label": "B",
          "content": "From the management account, switch roles to assume the OrganizationAccountAccessRole role with the account ID of the new member account. Set up the IAM users as required."
        },
        {
          "label": "C",
          "content": "Go to the AWS Management Console sign-in page. Choose &ldquo;Sign in using root account credentials.&rdquo; Sign in by using the email address finance1@example.com and the management account&#39;s root password. Set up the IAM users as required."
        },
        {
          "label": "D",
          "content": "Go to the AWS Management Console sign-in page. Sign in by using the account ID of the new member account and the Support1 IAM credentials. Set up the IAM users as required."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the information provided about creating IAM users in a new member account within AWS Organizations, the correct approach is:</p><p>B. From the management account, switch roles to assume the OrganizationAccountAccessRole role with the account ID of the new member account. Set up the IAM users as required.</p><p>Here's why this is the correct approach:</p><p>Automatic Role Creation: When a new member account is created in AWS Organizations, an IAM role called OrganizationAccountAccessRole is automatically created in that account. This role is specifically designed to provide administrative access from the management account.</p><p>Administrative Permissions: The OrganizationAccountAccessRole has full administrative permissions (via the AdministratorAccess policy) in the member account, which allows for creating IAM users and performing any other necessary account setup tasks.</p><p>Security Best Practice: Using the OrganizationAccountAccessRole follows AWS security best practices by avoiding the use of root user credentials. AWS recommends minimizing the use of root credentials and instead using IAM roles for administrative tasks.</p><p>Centralized Management: This approach leverages the centralized management capabilities of AWS Organizations, allowing administrators to manage multiple accounts from the management account without needing separate credentials for each account.</p><p>Process: The solutions architect would:</p><p>Sign in to the AWS Management Console using their IAM user (Support1) credentials in the management account</p><p>Click on their account name in the top right corner and select \"Switch Role\"</p><p>Enter the new member account ID and the role name \"OrganizationAccountAccessRole\"</p><p>After switching roles, they would have administrative access to create IAM users in the new member account</p><p>Option A (using root user credentials) is not recommended because:</p><p>It requires accessing and using the root user of the member account, which is against AWS security best practices</p><p>Root user access should be reserved for only a few specific tasks that cannot be performed by IAM users or roles</p><p>Using root credentials increases security risks and makes it harder to track and audit administrative actions</p><p>The OrganizationAccountAccessRole provides a secure, auditable, and centralized way to manage member accounts within AWS Organizations, making Option B the correct approach for creating IAM users in the new member account.</p><p>Sources</p><p>Add initial users - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/transitioning-to-multiple-aws-accounts/add-initial-users.html）</p><p>Accessing a member account that has OrganizationAccountAccessRole with AWS Organizations - AWS Organizations （https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_access-cross-account-role.html）</p><p>Secure root user access for member accounts in AWS Organizations | AWS Security Blog （https://aws.amazon.com/cn/blogs/security/secure-root-user-access-for-member-accounts-in-aws-organizations/）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "8035705bdef240a78122f36d7f95d843",
      "questionNumber": 392,
      "type": "single",
      "content": "<p>Question #392</p><p>A car rental company has built a serverless REST API to provide data to its mobile app. The app consists of an Amazon API Gateway API with a Regional endpoint, AWS Lambda functions, and an Amazon Aurora MySQL Serverless DB cluster. The company recently opened the API to mobile apps of partners. A significant increase in the number of requests resulted, causing sporadic database memory errors.</p><p><br></p><p>Analysis of the API traffic indicates that clients are making multiple HTTP GET requests for the same queries in a short period of time. Traffic is concentrated during business hours, with spikes around holidays and other events.</p><p><br></p><p>The company needs to improve its ability to support the additional usage while minimizing the increase in costs associated with the solution.</p><p><br></p><p>Which strategy meets these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Convert the API Gateway Regional endpoint to an edge-optimized endpoint. Enable caching in the production stage."
        },
        {
          "label": "B",
          "content": "Implement an Amazon ElastiCache for Redis cache to store the results of the database calls. Modify the Lambda functions to use the cache."
        },
        {
          "label": "C",
          "content": "Modify the Aurora Serverless DB cluster configuration to increase the maximum amount of available memory."
        },
        {
          "label": "D",
          "content": "Enable throttling in the API Gateway production stage. Set the rate and burst values to limit the incoming calls."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Correct Answer: B &nbsp;</p><p>Option B: Implement an Amazon ElastiCache for Redis cache to store database query results and modify Lambda functions to use the cache. &nbsp;</p><p> Explanation: &nbsp;</p><p> Key Requirements Analysis &nbsp;</p><p>- Problem: Repeated database queries lead to memory errors due to traffic spikes. &nbsp;</p><p>- Goal: Reduce database load without restricting functionality or incurring high costs. &nbsp;</p><p><br></p><p> Option Analysis &nbsp;</p><p>1. Option B (Correct) &nbsp;</p><p> &nbsp; - Strategy: Use ElastiCache for Redis as an in-memory cache to store query results. Lambda functions first check the cache before accessing the database. &nbsp;</p><p> &nbsp; - Advantages: &nbsp;</p><p> &nbsp; &nbsp; - Addresses root cause: Caches frequent queries to minimize redundant database calls, reducing memory pressure. &nbsp;</p><p> &nbsp; &nbsp; - Cost-efficient: ElastiCache scales dynamically and reduces database compute usage during spikes. &nbsp;</p><p> &nbsp; &nbsp; - Performance improvement: Cache responses are faster than database queries, enhancing user experience. &nbsp;</p><p>2. Option A (Incorrect) &nbsp;</p><p> &nbsp; - Issue: Converting to an edge-optimized API Gateway endpoint reduces latency for global users but does not address repeated queries. &nbsp;</p><p> &nbsp; - Limitation: API Gateway caching (up to 1 MB) is less effective for complex database results and requires per-route configuration, making it less suitable for deep query caching. &nbsp;</p><p>3. Option C (Incorrect) &nbsp;</p><p> &nbsp; - Flaw: Increasing Aurora Serverless memory directly raises costs without resolving redundant queries. &nbsp;</p><p> &nbsp; - Trade-off: This is a reactive solution that does not optimize resource utilization for repetitive requests. &nbsp;</p><p>4. Option D (Incorrect) &nbsp;</p><p> &nbsp; - Problem: Enabling API Gateway throttling limits incoming requests, which may disrupt partner apps and restrict legitimate traffic growth. &nbsp;</p><p> &nbsp; - Mismatch: Throttling addresses volume but not the underlying issue of redundant queries. &nbsp;</p><p> Conclusion &nbsp;</p><p>Option B optimizes performance by caching query results, reducing database load and memory pressure while maintaining cost efficiency. This approach directly targets the root cause of repeated queries without compromising functionality or incurring unnecessary expenses.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "7bcce5fffafd4b839db11369e4dfe3f1",
      "questionNumber": 393,
      "type": "single",
      "content": "<p>Question #393</p><p>A company is migrating an on-premises application and a MySQL database to AWS. The application processes highly sensitive data, and new data is constantly updated in the database. The data must not be transferred over the internet. The company also must encrypt the data in transit and at rest.</p><p><br></p><p>The database is 5 TB in size. The company already has created the database schema in an Amazon RDS for MySQL DB instance. The company has set up a 1 Gbps AWS Direct Connect connection to AWS. The company also has set up a public VIF and a private VIF. A solutions architect needs to design a solution that will migrate the data to AWS with the least possible downtime.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Perform a database backup. Copy the backup files to an AWS Snowball Edge Storage Optimized device. Import the backup to Amazon S3. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3) for encryption at rest. Use TLS for encryption in transit. Import the data from Amazon S3 to the DB instance."
        },
        {
          "label": "B",
          "content": "Use AWS Database Migration Service (AWS DMS) to migrate the data to AWS. Create a DMS replication instance in a private subnet. Create VPC endpoints for AWS DMS. Configure a DMS task to copy data from the on-premises database to the DB instance by using full load plus change data capture (CDC). Use the AWS Key Management Service (AWS KMS) default key for encryption at rest. Use TLS for encryption in transit."
        },
        {
          "label": "C",
          "content": "Perform a database backup. Use AWS DataSync to transfer the backup files to Amazon S3. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3) for encryption at rest. Use TLS for encryption in transit. Import the data from Amazon S3 to the DB instance."
        },
        {
          "label": "D",
          "content": "Use Amazon S3 File Gateway. Set up a private connection to Amazon S3 by using AWS PrivateLink. Perform a database backup. Copy the backup files to Amazon S3. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3) for encryption at rest. Use TLS for encryption in transit. Import the data from Amazon S3 to the DB instance."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Option B is the correct solution for migrating the data with the least possible downtime while meeting the encryption requirements. AWS DMS supports the migration of large databases like the one described (5 TB) and can perform the migration over the Direct Connect connection, which ensures that the data does not travel over the public internet. By using CDC, the migration can be done with minimal downtime as only the initial full load is required to be complete, followed by the ongoing replication of changes. Additionally, AWS DMS can integrate with VPC endpoints for a secure and private connection, and the use of AWS KMS for encryption at rest aligns with the requirement for data encryption.</p><p>The requirements are: &nbsp;</p><p>1. Migrate a 5 TB MySQL database to AWS without internet exposure (use Direct Connect). &nbsp;</p><p>2. Encrypt data in transit and at rest (TLS + KMS). &nbsp;</p><p>3. Minimize downtime (near-zero RPO/RTO). &nbsp;</p><p>Option B meets these requirements by: &nbsp;</p><p>- Using AWS DMS with Direct Connect private VIF: &nbsp;</p><p> &nbsp;- Full load + CDC replicates existing data and ongoing changes with minimal downtime. &nbsp;</p><p> &nbsp;- Private subnet + VPC endpoints ensure no internet exposure. &nbsp;</p><p> &nbsp;- TLS encryption for in-transit data + AWS KMS for at-rest encryption. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A & C: Snowball/DataSync + S3 requires manual import steps (downtime during restore). &nbsp;</p><p>- D: S3 File Gateway is for file storage (not optimal for database migration). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- DMS with CDC is the gold standard for minimal-downtime database migrations. &nbsp;</p><p>- Direct Connect private VIF provides secure, high-throughput connectivity. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "2e0211249b4f48c9ab84d45f44bc9031",
      "questionNumber": 394,
      "type": "single",
      "content": "<p>Question #394</p><p>A company is deploying a new cluster for big data analytics on AWS. The cluster will run across many Linux Amazon EC2 instances that are spread across multiple Availability Zones.</p><p><br></p><p>All of the nodes in the cluster must have read and write access to common underlying file storage. The file storage must be highly available, must be resilient, must be compatible with the Portable Operating System Interface (POSIX), and must accommodate high levels of throughput.</p><p><br></p><p>Which storage solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Provision an AWS Storage Gateway file gateway NFS file share that is attached to an Amazon S3 bucket. Mount the NFS file share on each EC2 instance in the cluster."
        },
        {
          "label": "B",
          "content": "Provision a new Amazon Elastic File System (Amazon EFS) file system that uses General Purpose performance mode. Mount the EFS file system on each EC2 instance in the cluster."
        },
        {
          "label": "C",
          "content": "Provision a new Amazon Elastic Block Store (Amazon EBS) volume that uses the io2 volume type. Attach the EBS volume to all of the EC2 instances in the cluster."
        },
        {
          "label": "D",
          "content": "Provision a new Amazon Elastic File System (Amazon EFS) file system that uses Max I/O performance mode. Mount the EFS file system on each EC2 instance in the cluster."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Based on the requirements for a big data analytics cluster that needs shared file storage across multiple EC2 instances in different Availability Zones, the correct solution is:</p><p>D. Provision a new Amazon Elastic File System (Amazon EFS) file system that uses Max I/O performance mode. Mount the EFS file system on each EC2 instance in the cluster.</p><p>Here's why Amazon EFS with Max I/O performance mode is the appropriate choice for this scenario:</p><p>Shared Access Requirement:</p><p>The requirement states that \"All of the nodes in the cluster must have read and write access to common underlying file storage\"</p><p>Amazon EFS is designed to be mounted concurrently by thousands of EC2 instances, allowing all cluster nodes to have simultaneous read and write access</p><p>EBS volumes (even io2) can only be attached to a single EC2 instance at a time in standard mode, or up to 16 instances with Multi-Attach (which still wouldn't support a large cluster)</p><p>Multi-Availability Zone Support:</p><p>The cluster will run across EC2 instances \"spread across multiple Availability Zones\"</p><p>Amazon EFS is natively designed to work across multiple Availability Zones within a region</p><p>EBS volumes are restricted to a single Availability Zone, making them unsuitable for a multi-AZ deployment</p><p>POSIX Compatibility:</p><p>The requirement explicitly states that the storage \"must be compatible with the Portable Operating System Interface (POSIX)\"</p><p>Amazon EFS is fully POSIX-compliant, supporting standard file operations, permissions, and locking</p><p>EBS provides block-level storage that requires additional configuration to achieve POSIX compliance</p><p>High Availability and Resilience:</p><p>The storage must be \"highly available\" and \"resilient\"</p><p>Amazon EFS is designed for 99.999999999% (11 nines) durability and high availability</p><p>EFS automatically stores data redundantly across multiple AZs, providing built-in resilience</p><p>High Throughput:</p><p>The requirement states the storage must \"accommodate high levels of throughput\"</p><p>Amazon EFS with Max I/O performance mode is specifically designed for high-throughput workloads</p><p>Max I/O mode is optimized for highly parallelized applications like big data analytics</p><p>Option C (using an EBS io2 volume) would not meet the requirements because:</p><p>EBS volumes cannot be simultaneously attached to multiple EC2 instances across different Availability Zones</p><p>EBS volumes are limited to a single Availability Zone, lacking the multi-AZ resilience required</p><p>Even with Multi-Attach, an io2 volume can only be attached to up to 16 instances, which may not be sufficient for a big data cluster</p><p>Amazon EFS with Max I/O performance mode provides the shared access, high availability across multiple AZs, POSIX compatibility, and high throughput capabilities required for this big data analytics cluster.</p><p>Sources</p><p>Amazon EFS performance specifications - Amazon Elastic File System （https://docs.aws.amazon.com/efs/latest/ug/performance.html）</p><p>Features and benefits of Amazon EBS volumes - Amazon EBS （https://docs.aws.amazon.com/ebs/latest/userguide/EBSFeatures.html）</p><p>Amazon EFS volumes - Amazon Elastic Container Service （https://docs.aws.amazon.com/AmazonECS/latest/bestpracticesguide/storage-efs.html）</p><p>Amazon EBS Provisioned IOPS Volume - Amazon Web Services （https://aws.amazon.com/cn/ebs/provisioned-iops/）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "dea222c43f4b4c0ba29160e7436df769",
      "questionNumber": 395,
      "type": "single",
      "content": "<p>Question #395</p><p>A company hosts a software as a service (SaaS) solution on AWS. The solution has an Amazon API Gateway API that serves an HTTPS endpoint. The API uses AWS Lambda functions for compute. The Lambda functions store data in an Amazon Aurora Serverless v1 database.</p><p><br></p><p>The company used the AWS Serverless Application Model (AWS SAM) to deploy the solution. The solution extends across multiple Availability Zones and has no disaster recovery (DR) plan.</p><p><br></p><p>A solutions architect must design a DR strategy that can recover the solution in another AWS Region. The solution has an RTO of 5 minutes and an RPO of 1 minute.</p><p><br></p><p>What should the solutions architect do to meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a read replica of the Aurora Serverless v1 database in the target Region. Use AWS SAM to create a runbook to deploy the solution to the target Region. Promote the read replica to primary in case of disaster."
        },
        {
          "label": "B",
          "content": "Change the Aurora Serverless v1 database to a standard Aurora MySQL global database that extends across the source Region and the target Region. Use AWS SAM to create a runbook to deploy the solution to the target Region."
        },
        {
          "label": "C",
          "content": "Create an Aurora Serverless v1 DB cluster that has multiple writer instances in the target Region. Launch the solution in the target Region. Configure the two Regional solutions to work in an active-passive configuration."
        },
        {
          "label": "D",
          "content": "Change the Aurora Serverless v1 database to a standard Aurora MySQL global database that extends across the source Region and the target Region. Launch the solution in the target Region. Configure the two Regional solutions to work in an active-passive configuration."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Option D is the correct approach to meet the specified RTO and RPO requirements for disaster recovery. By changing the Aurora Serverless v1 database to a standard Aurora MySQL global database, the company can leverage Aurora's global database feature to replicate data across Regions. This setup allows for a fast failover to the target Region while ensuring that the data is replicated in near real-time, which aligns with the 1-minute RPO. Additionally, launching the solution in the target Region and configuring it to work in an active-passive configuration helps meet the 5-minute RTO. This configuration ensures that in the event of a disaster, the application can quickly switch over to the secondary Region with minimal downtime.</p><p>The requirements are: &nbsp;</p><p>1. Disaster Recovery (DR) with RTO ≤5 min and RPO ≤1 min (near-zero data loss). &nbsp;</p><p>2. Multi-Region resilience for a serverless SaaS application. &nbsp;</p><p>Option D meets these requirements by: &nbsp;</p><p>- Using Aurora Global Database: &nbsp;</p><p> &nbsp;- Cross-Region replication with &lt;1s latency (meets RPO). &nbsp;</p><p> &nbsp;- 1-click promotion of secondary Region (meets RTO). &nbsp;</p><p>- Active-Passive deployment: &nbsp;</p><p> &nbsp;- Pre-deployed infrastructure in DR Region (API Gateway + Lambda). &nbsp;</p><p> &nbsp;- Fast failover by promoting the secondary Aurora cluster. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A: Aurora Serverless v1 read replicas cannot be promoted (no DR capability). &nbsp;</p><p>- B: Global Database + runbook lacks pre-deployed compute (violates RTO). &nbsp;</p><p>- C: Aurora Serverless v1 doesn’t support multi-writer (invalid configuration). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- Aurora Global Database is the only serverless-compatible solution for sub-minute RPO. &nbsp;</p><p>- Pre-deployed serverless components (Lambda/API Gateway) ensure 5-minute RTO. </p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "ace88f4702c74f85a53daf0bcfcfda93",
      "questionNumber": 396,
      "type": "single",
      "content": "<p>Question #396</p><p>A company owns a chain of travel agencies and is running an application in the AWS Cloud. Company employees use the application to search for information about travel destinations. Destination content is updated four times each year.</p><p><br></p><p>Two fixed Amazon EC2 instances serve the application. The company uses an Amazon Route 53 public hosted zone with a multivalue record of travel.example.com that returns the Elastic IP addresses for the EC2 instances. The application uses Amazon DynamoDB as its primary data store. The company uses a self-hosted Redis instance as a caching solution.</p><p><br></p><p>During content updates, the load on the EC2 instances and the caching solution increases drastically. This increased load has led to downtime on several occasions. A solutions architect must update the application so that the application is highly available and can handle the load that is generated by the content updates.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Set up DynamoDB Accelerator (DAX) as an in-memory cache. Update the application to use DAX. Create an Auto Scaling group for the EC2 instances. Create an Application Load Balancer (ALB). Set the Auto Scaling group as a target for the ALB. Update the Route 53 record to use a simple routing policy that targets the ALB&#39;s DNS alias. Configure scheduled scaling for the EC2 instances before the content updates."
        },
        {
          "label": "B",
          "content": "Set up Amazon ElastiCache for Redis. Update the application to use ElastiCache. Create an Auto Scaling group for the EC2 instances. Create an Amazon CloudFront distribution, and set the Auto Scaling group as an origin for the distribution. Update the Route 53 record to use a simple routing policy that targets the CloudFront distribution&rsquo;s DNS alias. Manually scale up EC2 instances before the content updates."
        },
        {
          "label": "C",
          "content": "Set up Amazon ElastiCache for Memcached. Update the application to use ElastiCache. Create an Auto Scaling group for the EC2 instances. Create an Application Load Balancer (ALB). Set the Auto Scaling group as a target for the ALB. Update the Route 53 record to use a simple routing policy that targets the ALB&#39;s DNS alias. Configure scheduled scaling for the application before the content updates."
        },
        {
          "label": "D",
          "content": "Set up DynamoDB Accelerator (DAX) as an in-memory cache. Update the application to use DAX. Create an Auto Scaling group for the EC2 instances. Create an Amazon CloudFront distribution, and set the Auto Scaling group as an origin for the distribution. Update the Route 53 record to use a simple routing policy that targets the CloudFront distribution&#39;s DNS alias. Manually scale up EC2 instances before the content updates."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Option A is the correct solution to ensure high availability and the ability to handle increased load during content updates. By setting up DAX as an in-memory cache, the application can significantly reduce the latency of read operations on DynamoDB, which is crucial during peak load times. The use of an Auto Scaling group for EC2 instances ensures that the compute resources can scale out to meet demand, and an ALB can distribute the incoming traffic across these instances. Additionally, updating the Route 53 record to target the ALB's DNS alias allows for smooth traffic routing, and configuring scheduled scaling can proactively increase the EC2 capacity before anticipated content updates, thus preventing downtime.</p><p>The requirements are: &nbsp;</p><p>1. Handle high load during quarterly content updates (predictable spikes). &nbsp;</p><p>2. Maintain high availability (eliminate downtime). &nbsp;</p><p>3. Replace self-hosted Redis with a managed solution. &nbsp;</p><p>Option A meets these requirements by: &nbsp;</p><p>- Replacing Redis with DynamoDB Accelerator (DAX): &nbsp;</p><p> &nbsp;- Fully managed, in-memory cache for DynamoDB (reduces database load). &nbsp;</p><p>- Auto Scaling + ALB: &nbsp;</p><p> &nbsp;- Scales EC2 instances horizontally during traffic spikes. &nbsp;</p><p> &nbsp;- Distributes traffic evenly across instances. &nbsp;</p><p>- Scheduled scaling: &nbsp;</p><p> &nbsp;- Proactively scales before content updates (avoids reactive scaling delays). &nbsp;</p><p>- Route 53 simple routing: &nbsp;</p><p> &nbsp;- Directs traffic to the ALB (simpler than multivalue routing). &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- B: CloudFront is unnecessary (content updates are backend-driven, not static). Manual scaling is less reliable than scheduled scaling. &nbsp;</p><p>- C: Memcached lacks persistence (not ideal for travel data caching). &nbsp;</p><p>- D: CloudFront + DAX is mismatched (CloudFront caches static content, not DynamoDB queries). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- DAX reduces DynamoDB read latency by 10x (ideal for high-read workloads). &nbsp;</p><p>- Scheduled scaling ensures capacity is ready before traffic spikes. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "d8c885d561c0492eaeac1a2336ddccd8",
      "questionNumber": 397,
      "type": "multiple",
      "content": "<p>Question #397</p><p>A company needs to store and process image data that will be uploaded from mobile devices using a custom mobile app. Usage peaks between 8 AM and 5 PM on weekdays, with thousands of uploads per minute. The app is rarely used at any other time. A user is notified when image processing is complete.</p><p><br></p><p>Which combination of actions should a solutions architect take to ensure image processing can scale to handle the load? (Choose three.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Upload files from the mobile software directly to Amazon S3. Use S3 event notifications to create a message in an Amazon MQ queue."
        },
        {
          "label": "B",
          "content": "Upload files from the mobile software directly to Amazon S3. Use S3 event notifications to create a message in an Amazon Simple Queue Service (Amazon SQS) standard queue."
        },
        {
          "label": "C",
          "content": "Invoke an AWS Lambda function to perform image processing when a message is available in the queue."
        },
        {
          "label": "D",
          "content": "Invoke an S3 Batch Operations job to perform image processing when a message is available in the queue."
        },
        {
          "label": "E",
          "content": "Send a push notification to the mobile app by using Amazon Simple Notification Service (Amazon SNS) when processing is complete."
        },
        {
          "label": "F",
          "content": "Send a push notification to the mobile app by using Amazon Simple Email Service (Amazon SES) when processing is complete."
        }
      ],
      "correctAnswer": "BCE",
      "explanation": "<p>Based on the requirement to store and process image data uploaded from mobile devices with high peak usage during business hours, the best combination of actions to ensure scalable image processing is:</p><p>B. Upload files from the mobile software directly to Amazon S3. Use S3 event notifications to create a message in an Amazon Simple Queue Service (Amazon SQS) standard queue.</p><p>C. Invoke an AWS Lambda function to perform image processing when a message is available in the queue.</p><p>E. Send a push notification to the mobile app by using Amazon Simple Notification Service (Amazon SNS) when processing is complete.</p><p>Here's why this combination provides the optimal solution:</p><p>Direct S3 Uploads (Option B):</p><p>Allows mobile devices to upload images directly to S3 without requiring an intermediary server</p><p>S3 can handle thousands of concurrent uploads per minute, easily meeting the peak demand</p><p>S3 event notifications automatically trigger the next step in the workflow when new files are uploaded</p><p>SQS standard queue provides a buffer between uploads and processing, absorbing traffic spikes</p><p>This decoupling ensures the system remains responsive even during peak upload times</p><p>Lambda for Processing (Option C):</p><p>Lambda functions automatically scale to process messages from the SQS queue</p><p>Lambda's serverless nature means you only pay for actual processing time, ideal for the variable workload described (busy during business hours, minimal usage otherwise)</p><p>Lambda can be configured to process multiple images concurrently, handling the high volume during peak times</p><p>The queue-based approach ensures no uploads are lost even if processing temporarily falls behind</p><p>SNS Push Notifications (Option E):</p><p>After processing is complete, SNS can send push notifications directly to the mobile app</p><p>This satisfies the requirement that \"A user is notified when image processing is complete\"</p><p>SNS is highly scalable and can handle thousands of notifications per minute</p><p>This architecture creates a fully serverless, event-driven pipeline that:</p><p>Scales automatically to handle the variable load (thousands of uploads during business hours, minimal usage otherwise)</p><p>Decouples each step of the process for better resilience</p><p>Provides end-to-end notification when processing is complete</p><p>Minimizes costs by only using resources when needed (pay-per-use model)</p><p>Requires minimal operational overhead</p><p>The combination of S3 for storage, SQS for buffering, Lambda for processing, and SNS for notifications creates a robust, scalable solution that perfectly addresses the requirements for this image processing workflow.</p><p>Sources</p><p>What is the recommended convention for storing images in a serverless event-driven architecture? Base64 image in SNS event exceeds message size limit. | AWS re:Post （https://repost.aws/questions/QUde2ybRheSWiw6Mr0uA0_2A/what-is-the-recommended-convention-for-storing-images-in-a-serverless-event-driven-architecture-base64-image-in-sns-event-exceeds-message-size-limit）</p><p>Performance design patterns for Amazon S3 - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance-design-patterns.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "0c3790537cc14abd8e0449aa1e5dac8d",
      "questionNumber": 398,
      "type": "single",
      "content": "<p>Question #398</p><p>A company is building an application on AWS. The application sends logs to an Amazon OpenSearch Service cluster for analysis. All data must be stored within a VPC.</p><p><br></p><p>Some of the company’s developers work from home. Other developers work from three different company office locations. The developers need to access OpenSearch Service to analyze and visualize logs directly from their local development machines.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Configure and set up an AWS Client VPN endpoint. Associate the Client VPN endpoint with a subnet in the VPC. Configure a Client VPN self-service portal. Instruct the developers to connect by using the client for Client VPN."
        },
        {
          "label": "B",
          "content": "Create a transit gateway, and connect it to the VPC. Create an AWS Site-to-Site VPN. Create an attachment to the transit gateway. Instruct the developers to connect by using an OpenVPN client."
        },
        {
          "label": "C",
          "content": "Create a transit gateway, and connect it to the VPC. Order an AWS Direct Connect connection. Set up a public VIF on the Direct Connect connection. Associate the public VIF with the transit gateway. Instruct the developers to connect to the Direct Connect connection."
        },
        {
          "label": "D",
          "content": "Create and configure a bastion host in a public subnet of the VPC. Configure the bastion host security group to allow SSH access from the company CIDR ranges. Instruct the developers to connect by using SSH."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Based on the requirement to provide developers with access to an Amazon OpenSearch Service cluster within a VPC from both home and office locations, the best solution is:</p><p>A. Configure and set up an AWS Client VPN endpoint. Associate the Client VPN endpoint with a subnet in the VPC. Configure a Client VPN self-service portal. Instruct the developers to connect by using the client for Client VPN.</p><p>Here's why this is the correct solution:</p><p>Access Requirements:</p><p>The scenario involves developers working from multiple locations (home and three different office locations)</p><p>Developers need direct access from their local development machines to the OpenSearch Service cluster</p><p>AWS Client VPN is specifically designed for individual user remote access to AWS resources in a VPC</p><p>AWS Client VPN Benefits for This Scenario:</p><p>Provides secure, encrypted connections for individual developers regardless of their location</p><p>Works consistently whether developers are at home or in any of the three office locations</p><p>Allows direct access from developers' local machines to the OpenSearch Service cluster</p><p>Supports user-based authentication and authorization for granular access control</p><p>Self-Service Portal Advantage:</p><p>Makes it easy for developers to download the necessary client software and configuration files</p><p>Simplifies the onboarding process for new developers</p><p>Provides a consistent connection method across all developer locations</p><p>Implementation Process:</p><p>The Client VPN endpoint is associated with a subnet in the VPC where the OpenSearch Service cluster resides</p><p>Developers connect using an OpenVPN-compatible client (such as the AWS provided client)</p><p>Once connected, developers can directly access the OpenSearch Service cluster to analyze and visualize logs</p><p>Why Option B is Not Optimal:</p><p>AWS Site-to-Site VPN with Transit Gateway is designed for connecting entire networks, not individual users</p><p>It would be overly complex for individual developer access from home locations</p><p>It would require additional VPN infrastructure at each developer's home</p><p>It's not designed for the individual access pattern described in the scenario</p><p>AWS Client VPN provides the most straightforward and secure solution for this specific use case, allowing developers to access the OpenSearch Service cluster from any location while ensuring all data remains within the VPC as required. The self-service portal further simplifies the process by providing an easy way for developers to get the necessary connection tools and configurations.</p><p>Sources</p><p>Client VPN with VPC peering vs Client VPN withTransit gateway | AWS re:Post （https://repost.aws/questions/QUUotXjy2mTV6noMA-J86AXQ/client-vpn-with-vpc-peering-vs-client-vpn-withtransit-gateway）</p><p>Building secure multicloud access with AWS Client VPN and AWS Site-to-Site VPN | Networking & Content Delivery （https://aws.amazon.com/blogs/networking-and-content-delivery/building-secure-multicloud-access-with-aws-client-vpn-and-aws-site-to-site-vpn/）</p><p>AWS network connectivity management tools - Management and Governance Cloud Environment Guide （https://docs.aws.amazon.com/wellarchitected/latest/management-and-governance-guide/aws-network-connectivity-management-tools.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "250bf13457d14ebbb6cac833f7d174b6",
      "questionNumber": 399,
      "type": "multiple",
      "content": "<p>Question #399</p><p>A company wants to migrate its website from an on-premises data center onto AWS. At the same time, it wants to migrate the website to a containerized microservice-based architecture to improve the availability and cost efficiency. The company’s security policy states that privileges and network permissions must be configured according to best practice, using least privilege.</p><p><br></p><p>A solutions architect must create a containerized architecture that meets the security requirements and has deployed the application to an Amazon ECS cluster.</p><p><br></p><p>What steps are required after the deployment to meet the requirements? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Create tasks using the bridge network mode."
        },
        {
          "label": "B",
          "content": "Create tasks using the awsvpc network mode."
        },
        {
          "label": "C",
          "content": "Apply security groups to Amazon EC2 instances, and use IAM roles for EC2 instances to access other resources."
        },
        {
          "label": "D",
          "content": "Apply security groups to the tasks, and pass IAM credentials into the container at launch time to access other resources."
        },
        {
          "label": "E",
          "content": "Apply security groups to the tasks, and use IAM roles for tasks to access other resources."
        }
      ],
      "correctAnswer": "BE",
      "explanation": "<p>Based on the requirement to migrate a website to a containerized microservice-based architecture on AWS while ensuring privileges and network permissions follow least privilege best practices, the two steps required after deploying the application to an Amazon ECS cluster are:</p><p>B. Create tasks using the awsvpc network mode.</p><p>E. Apply security groups to the tasks, and use IAM roles for tasks to access other resources.</p><p>Here's why these are the correct choices:</p><p>Create tasks using the awsvpc network mode (Option B):</p><p>The awsvpc network mode provides each task with its own elastic network interface (ENI) and a private IP address</p><p>This creates network isolation at the task level, improving security by separating network traffic between different microservices</p><p>It enables you to apply security groups directly to individual tasks/microservices, allowing for fine-grained network access control</p><p>This network mode is essential for implementing least privilege at the network level, as it allows you to restrict network traffic precisely to what each microservice needs</p><p>It's considered a best practice for containerized microservice architectures on ECS</p><p>Apply security groups to the tasks, and use IAM roles for tasks to access other resources (Option E):</p><p>Security groups act as virtual firewalls that control inbound and outbound traffic at the task level when using awsvpc mode</p><p>This allows you to implement least privilege by restricting network access to only what each microservice requires</p><p>IAM roles for tasks enable you to grant specific AWS service permissions to individual containers/tasks</p><p>This implements least privilege for AWS API access by ensuring each microservice only has the permissions it needs</p><p>Using task-specific IAM roles instead of broader instance-level permissions is a security best practice</p><p>Why the other options are not correct:</p><p>Create tasks using the bridge network mode (Option A):</p><p>Bridge mode doesn't allow for security groups at the task level</p><p>It makes it difficult to implement least privilege for network access</p><p>It requires opening broader port ranges on the host, which contradicts the least privilege principle</p><p>It doesn't provide the same level of network isolation between microservices</p><p>The combination of awsvpc network mode with security groups and IAM roles for tasks creates a secure containerized architecture that follows the principle of least privilege by:</p><p>Isolating network traffic between microservices</p><p>Restricting network access through task-specific security groups</p><p>Limiting AWS service permissions through task-specific IAM roles</p><p>This approach ensures both network permissions and service privileges are configured according to security best practices, as required by the company's security policy.</p><p>Sources</p><p>Security considerations for running containers on Amazon ECS | AWS Security Blog （https://aws.amazon.com/blogs/security/security-considerations-for-running-containers-on-amazon-ecs/）</p><p>Using IAM roles with Amazon ECS tasks - Amazon Elastic Container Service （https://docs.aws.amazon.com/AmazonECS/latest/bestpracticesguide/security-iam-roles.html）</p><p>Amazon EC2 container instance security considerations for Amazon ECS - Amazon Elastic Container Service （https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ec2-security-considerations.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "32bed8c81b8b444f91bb0b42d0c49cc4",
      "questionNumber": 400,
      "type": "multiple",
      "content": "<p>Question #400</p><p>A company is running a serverless application that consists of several AWS Lambda functions and Amazon DynamoDB tables. The company has created new functionality that requires the Lambda functions to access an Amazon Neptune DB cluster. The Neptune DB cluster is located in three subnets in a VPC.</p><p><br></p><p>Which of the possible solutions will allow the Lambda functions to access the Neptune DB cluster and DynamoDB tables? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Create three public subnets in the Neptune VPC, and route traffic through an internet gateway. Host the Lambda functions in the three new public subnets."
        },
        {
          "label": "B",
          "content": "Create three private subnets in the Neptune VPC, and route internet traffic through a NAT gateway. Host the Lambda functions in the three new private subnets."
        },
        {
          "label": "C",
          "content": "Host the Lambda functions outside the VPC. Update the Neptune security group to allow access from the IP ranges of the Lambda functions."
        },
        {
          "label": "D",
          "content": "Host the Lambda functions outside the VPC. Create a VPC endpoint for the Neptune database, and have the Lambda functions access Neptune over the VPC endpoint."
        },
        {
          "label": "E",
          "content": "Create three private subnets in the Neptune VPC. Host the Lambda functions in the three new isolated subnets. Create a VPC endpoint for DynamoDB, and route DynamoDB traffic to the VPC endpoint."
        }
      ],
      "correctAnswer": "BE",
      "explanation": "<p>Based on the requirement to allow Lambda functions to access both an Amazon Neptune DB cluster in a VPC and DynamoDB tables, the two correct solutions are:</p><p>B. Create three private subnets in the Neptune VPC, and route internet traffic through a NAT gateway. Host the Lambda functions in the three new private subnets.</p><p>E. Create three private subnets in the Neptune VPC. Host the Lambda functions in the three new isolated subnets. Create a VPC endpoint for DynamoDB, and route DynamoDB traffic to the VPC endpoint.</p><p>Here's why these are the correct solutions:</p><p>Option B: Private Subnets with NAT Gateway</p><p>This solution places Lambda functions in private subnets within the same VPC as the Neptune DB cluster</p><p>Lambda functions can directly access the Neptune DB cluster through the VPC's internal network</p><p>The NAT gateway provides outbound internet access for the Lambda functions to reach DynamoDB (which is an AWS service accessible via public endpoints)</p><p>This approach maintains security by keeping Lambda functions in private subnets while still allowing them to access both Neptune (internal) and DynamoDB (external)</p><p>Option E: Private Subnets with VPC Endpoint for DynamoDB</p><p>This solution also places Lambda functions in private subnets within the same VPC as the Neptune DB cluster</p><p>Lambda functions can directly access the Neptune DB cluster through the VPC's internal network</p><p>The VPC endpoint for DynamoDB allows Lambda functions to access DynamoDB without going through the public internet</p><p>This approach provides enhanced security and performance by keeping all traffic within the AWS network</p><p>Why the other option is incorrect:</p><p>Option A: Public Subnets with Internet Gateway</p><p>While this would technically allow Lambda functions to access both Neptune and DynamoDB, it's not a recommended approach from a security perspective</p><p>Placing Lambda functions in public subnets unnecessarily exposes them to potential security risks</p><p>This violates the principle of least privilege and AWS best practices for serverless applications</p><p>Both options B and E provide more secure alternatives that achieve the same connectivity requirements</p><p>Both correct solutions (B and E) allow Lambda functions to access:</p><p>The Neptune DB cluster directly within the VPC</p><p>DynamoDB tables either through a NAT gateway (Option B) or through a VPC endpoint (Option E)</p><p>Option E with the VPC endpoint is generally considered the better practice as it provides better security, lower latency, and reduced data transfer costs compared to using a NAT gateway. However, both solutions will successfully meet the technical requirements specified in the question.</p><p>Sources</p><p>How to get Dynamo DB access from VPC lambda function? | AWS re:Post （https://repost.aws/questions/QUuzLLsN-5Tlyavc8otBGgMw/how-to-get-dynamo-db-access-from-vpc-lambda-function）</p><p>Accessing s3 from a Lambda within a vpc | AWS re:Post （https://repost.aws/questions/QUDdulOqQTQV6zIuR887sEdQ/accessing-s3-from-a-lambda-within-a-vpc）</p><p>AWS Lambda function not connecting to external services | AWS re:Post （https://repost.aws/questions/QUhE14aOWtQFmqgh5eBlF82g/aws-lambda-function-not-connecting-to-external-services）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "e8ab5d3dc9f24f3cb06f45d9facff681",
      "questionNumber": 401,
      "type": "single",
      "content": "Question #401<p>A company wants to design a disaster recovery (DR) solution for an application that runs in the company’s data center. The application writes to an SMB file share and creates a copy on a second file share. Both file shares are in the data center. The application uses two types of files: metadata files and image files.</p><p><br></p><p>The company wants to store the copy on AWS. The company needs the ability to use SMB to access the data from either the data center or AWS if a disaster occurs. The copy of the data is rarely accessed but must be available within 5 minutes.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Deploy AWS Outposts with Amazon S3 storage. Configure a Windows Amazon EC2 instance on Outposts as a file server."
        },
        {
          "label": "B",
          "content": "Deploy an Amazon FSx File Gateway. Configure an Amazon FSx for Windows File Server Multi-AZ file system that uses SSD storage."
        },
        {
          "label": "C",
          "content": "Deploy an Amazon S3 File Gateway. Configure the S3 File Gateway to use Amazon S3 Standard-Infrequent Access (S3 Standard-IA) for the metadata files and to use S3 Glacier Deep Archive for the image files."
        },
        {
          "label": "D",
          "content": "Deploy an Amazon S3 File Gateway. Configure the S3 File Gateway to use Amazon S3 Standard-Infrequent Access (S3 Standard-IA) for the metadata files and image files."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. Deploy an Amazon S3 File Gateway. Configure the S3 File Gateway to use Amazon S3 Standard-Infrequent Access (S3 Standard-IA) for the metadata files and image files.</p><p>This solution meets all the requirements for the following reasons:</p><p>SMB Protocol Access: S3 File Gateway supports the SMB protocol, allowing the company to access the data using the same protocol they currently use in their data center. This ensures compatibility with their existing application.</p><p>Availability within 5 minutes: S3 Standard-IA provides millisecond retrieval times, easily meeting the requirement that data must be available within 5 minutes. This storage class is designed for data that is accessed infrequently but requires rapid access when needed.</p><p>Cost Optimization: Since the copy of the data is \"rarely accessed,\" using S3 Standard-IA is cost-effective as it offers a lower storage cost compared to S3 Standard while still maintaining rapid retrieval capabilities.</p><p>Disaster Recovery: This solution enables the company to access their data from either the data center or AWS in the event of a disaster, providing the flexibility needed for their DR strategy.</p><p>Option C (using S3 Glacier Deep Archive for image files) would not meet the requirements because:</p><p>S3 Glacier Deep Archive has a retrieval time of 12+ hours, which far exceeds the 5-minute availability requirement specified by the company.</p><p>S3 Glacier Deep Archive is designed for long-term archival of data that may be accessed once or twice a year, not for disaster recovery scenarios where data needs to be quickly accessible.</p><p>S3 File Gateway does not support direct access to S3 Glacier Deep Archive storage class. Files stored in Glacier Deep Archive would need to be restored to S3 before they could be accessed through the gateway, adding complexity and delays to the recovery process.</p><p>By using S3 Standard-IA for both metadata files and image files, the company ensures that all their data is readily available within the required timeframe while still optimizing costs for infrequently accessed data. This approach provides a simple, consistent solution that meets all the specified requirements for their disaster recovery strategy.</p><p>Sources</p><p>Secure archive storage – Amazon S3 Glacier storage classes – AWS （https://aws.amazon.com/s3/storage-classes/glacier/?ams%23interactive-card-vertical%23pattern-data.filter=%257B%2522filters%2522%253A%255B%255D%257D）</p><p>AWS Storage Gateway - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/optimize-costs-microsoft-workloads/storage-main-storage-gateway.html）</p><p>Backup and recovery from on-premises infrastructure to AWS - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/backup-recovery/on-premises-to-aws.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "d20e89f71570432ab369524d1283b497",
      "questionNumber": 402,
      "type": "single",
      "content": "<p>Question #402</p><p>A company is creating a solution that can move 400 employees into a remote working environment in the event of an unexpected disaster. The user desktops have a mix of Windows and Linux operating systems. Multiple types of software, such as web browsers and mail clients, are installed on each desktop.</p><p><br></p><p>A solutions architect needs to implement a solution that can be integrated with the company’s on-premises Active Directory to allow employees to use their existing identity credentials. The solution must provide multifactor authentication (MFA) and must replicate the user experience from the existing desktops.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use Amazon WorkSpaces for the cloud desktop service. Set up a VPN connection to the on-premises network. Create an AD Connector, and connect to the on-premises Active Directory. Activate MFA for Amazon WorkSpaces by using the AWS Management Console."
        },
        {
          "label": "B",
          "content": "Use Amazon AppStream 2.0 as an application streaming service. Configure Desktop View for the employees. Set up a VPN connection to the on-premises network. Set up Active Directory Federation Services (AD FS) on premises. Connect the VPC network to AD FS through the VPN connection."
        },
        {
          "label": "C",
          "content": "Use Amazon WorkSpaces for the cloud desktop service. Set up a VPN connection to the on-premises network. Create an AD Connector, and connect to the on-premises Active Directory. Configure a RADIUS server for MFA."
        },
        {
          "label": "D",
          "content": "Use Amazon AppStream 2.0 as an application streaming service. Set up Active Directory Federation Services on premises. Configure MFA to grant users access on AppStream 2.0."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Option C is the correct solution. Amazon WorkSpaces provides a cloud-based desktop service that can be integrated with an on-premises Active Directory through an AD Connector. This allows employees to use their existing identity credentials. By setting up a VPN connection, the company can ensure a secure connection to the on-premises network. Configuring a RADIUS server for MFA adds an additional layer of security and meets the requirement for multifactor authentication, thus replicating a secure user experience from the existing desktops.</p><p>The requirements are: &nbsp;</p><p>1. Support for both Windows and Linux desktops → Amazon WorkSpaces supports both, while AppStream 2.0 is primarily for application streaming (not full desktops). &nbsp;</p><p>2. Integration with on-premises Active Directory → AD Connector allows WorkSpaces to authenticate against on-premises AD. &nbsp;</p><p>3. Multifactor Authentication (MFA) → WorkSpaces can integrate with a RADIUS server (e.g., AWS MFA or third-party MFA solutions). &nbsp;</p><p>4. Replicate the existing user experience → WorkSpaces provides full virtual desktops, matching the employees' current setup. &nbsp;</p><p> Why not the other options? &nbsp;</p><p>- A: Incorrect because MFA for WorkSpaces via the AWS Management Console is not the best practice; a RADIUS server is preferred for on-prem AD integration. &nbsp;</p><p>- B: Incorrect because AppStream 2.0 does not provide full desktop replication (only application streaming). &nbsp;</p><p>- D: Incorrect because AppStream 2.0 does not fully replicate the desktop experience and is not ideal for mixed OS environments. &nbsp;</p><p> Conclusion: &nbsp;</p><p>C is the best solution as it meets all requirements by leveraging Amazon WorkSpaces with AD Connector and RADIUS-based MFA. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "06f941e8bad14565a5f8ef56ea5cbda9",
      "questionNumber": 403,
      "type": "single",
      "content": "Question #403<p>A company has deployed an Amazon Connect contact center. Contact center agents are reporting large numbers of computer-generated calls. The company is concerned about the cost and productivity effects of these calls. The company wants a solution that will allow agents to flag the call as spam and automatically block the numbers from going to an agent in the future.</p><p><br></p><p>What is the MOST operationally efficient solution to meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Customize the Contact Control Panel (CCP) by adding a flag call button that will invoke an AWS Lambda function that calls the UpdateContactAttributes API. Use an Amazon DynamoDB table to store the spam numbers. Modify the contact flows to look for the updated attribute and to use a Lambda function to read and write to the DynamoDB table."
        },
        {
          "label": "B",
          "content": "Use a Contact Lens for Amazon Connect rule that will look for spam calls. Use an Amazon DynamoDB table to store the spam numbers. Modify the contact flows to look for the rule and to invoke an AWS Lambda function to read and write to the DynamoDB table."
        },
        {
          "label": "C",
          "content": "Use an Amazon DynamoDB table to store the spam numbers. Create a quick connect that the agents can transfer the spam call to from the Contact Control Panel (CCP). Modify the quick connect contact flow to invoke an AWS Lambda function to write to the DynamoDB table."
        },
        {
          "label": "D",
          "content": "Modify the initial contact flow to ask for caller input. If the agent does not receive input, the agent should mark the caller as spam. Use an Amazon DynamoDB table to store the spam numbers. Use an AWS Lambda function to read and write to the DynamoDB table."
        }
      ],
      "correctAnswer": "A",
      "explanation": "To address the issue of computer-generated calls in the Amazon Connect contact center while allowing agents to flag spam calls and automatically block these numbers in the future, the most operationally efficient solution would be:<p>Use Amazon Connect Contact Lens:</p><p>Enable Contact Lens for Amazon Connect, which provides real-time analytics and post-call analytics.</p><p>Implement a custom Contact Flow:</p><p>Create a custom contact flow that includes a \"Flag as Spam\" option for agents.</p><p>Utilize AWS Lambda integration:</p><p>Integrate an AWS Lambda function within the contact flow.</p><p>When an agent flags a call as spam, the Lambda function will: a. Record the caller's number in an Amazon DynamoDB table. b. Add the number to an Amazon Connect block list.</p><p>Modify the initial Contact Flow:</p><p>At the beginning of each call, check the caller's number against the DynamoDB table of spam numbers.</p><p>If the number is found in the table, automatically end the call or route it to a pre-recorded message.</p><p>Use Amazon Connect Streams API:</p><p>Implement a custom agent interface using the Amazon Connect Streams API.</p><p>Add a \"Flag as Spam\" button in the agent's interface for easy reporting.</p><p>Set up Amazon CloudWatch Alarms:</p><p>Create alarms to monitor the rate of spam calls and alert administrators if it exceeds a certain threshold.</p><p>Implement Amazon QuickSight dashboard:</p><p><br></p><p>Create a dashboard to visualize spam call trends and the effectiveness of the blocking mechanism.</p><p>This solution is operationally efficient because:</p><p><br></p><p>It leverages existing Amazon Connect features and integrations.</p><p>It automates the process of identifying and blocking spam calls.</p><p>It requires minimal ongoing maintenance once set up.</p><p>It provides agents with an easy way to flag spam calls.</p><p>It allows for continuous monitoring and improvement of the spam detection process.</p><p>By implementing this solution, the company can effectively reduce the number of computer-generated calls reaching agents, thereby improving productivity and reducing costs associated with handling these calls. The solution also provides flexibility for future enhancements and analytics to further optimize the contact center operations.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "8348e5f5c6bb444584ae482c936e2cbc",
      "questionNumber": 404,
      "type": "single",
      "content": "<p>Question #404</p><p>A company has mounted sensors to collect information about environmental parameters such as humidity and light throughout all the company's factories. The company needs to stream and analyze the data in the AWS Cloud in real time. If any of the parameters fall out of acceptable ranges, the factory operations team must receive a notification immediately.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Stream the data to an Amazon Kinesis Data Firehose delivery stream. Use AWS Step Functions to consume and analyze the data in the Kinesis Data Firehose delivery stream. Use Amazon Simple Notification Service (Amazon SNS) to notify the operations team."
        },
        {
          "label": "B",
          "content": "Stream the data to an Amazon Managed Streaming for Apache Kafka (Amazon MSK) cluster. Set up a trigger in Amazon MSK to invoke an AWS Fargate task to analyze the data. Use Amazon Simple Email Service (Amazon SES) to notify the operations team."
        },
        {
          "label": "C",
          "content": "Stream the data to an Amazon Kinesis data stream. Create an AWS Lambda function to consume the Kinesis data stream and to analyze the data. Use Amazon Simple Notification Service (Amazon SNS) to notify the operations team."
        },
        {
          "label": "D",
          "content": "Stream the data to an Amazon Kinesis Data Analytics application. Use an automatically scaled and containerized service in Amazon Elastic Container Service (Amazon ECS) to consume and analyze the data. Use Amazon Simple Email Service (Amazon SES) to notify the operations team."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Stream the data to an Amazon Kinesis data stream. Create an AWS Lambda function to consume the Kinesis data stream and to analyze the data. Use Amazon Simple Notification Service (Amazon SNS) to notify the operations team.</p><p>This solution meets the requirements for the following reasons:</p><p>Real-time processing: The company needs to analyze the data in real time and send immediate notifications when parameters fall out of acceptable ranges. Kinesis Data Streams provides true real-time processing with sub-second latency, which is essential for monitoring critical factory environmental parameters.</p><p>Immediate analysis: Lambda functions can be triggered by new records in a Kinesis data stream, allowing for immediate analysis of the incoming sensor data. Lambda can execute custom code to check if humidity, light, or other parameters are outside acceptable ranges.</p><p>Immediate notifications: When Lambda detects out-of-range parameters, it can immediately trigger Amazon SNS to send notifications to the factory operations team. This ensures the team is alerted without delay, as required.</p><p>Scalability: This architecture can easily scale to handle data from multiple factories and thousands of sensors, automatically adjusting to varying data volumes.</p><p>Option A (using Kinesis Data Firehose with Step Functions) is less suitable because:</p><p>Kinesis Data Firehose is designed primarily for data delivery to destinations like S3, Redshift, or Elasticsearch, not for real-time analysis. It typically has a minimum delivery time of 60 seconds, which doesn't meet the requirement for immediate notifications.</p><p>AWS Step Functions is an orchestration service designed for complex workflows, not for continuous real-time data processing. It would introduce unnecessary complexity and potential delays in the notification process.</p><p>This architecture would likely result in higher latency between when a parameter falls out of range and when the operations team receives a notification.</p><p>The combination of Kinesis Data Streams, Lambda, and SNS creates a streamlined, event-driven architecture that can process sensor data in real time, analyze it immediately, and send notifications without delay when issues are detected. This makes it the optimal solution for the company's requirements of real-time analysis and immediate notifications.</p><p>Sources</p><p>Unlocking Scalable IoT Analytics on AWS | The Internet of Things on AWS – Official Blog （https://aws.amazon.com/cn/blogs/iot/unlocking-scalable-iot-analytics-on-aws/）</p><p>Data processing - Data Warehousing on AWS （https://docs.aws.amazon.com/whitepapers/latest/data-warehousing-on-aws/data-processing.html）</p><p>Step 1: Redirect ongoing data ingestion - AWS IoT Analytics （https://docs.aws.amazon.com/iotanalytics/latest/userguide/redirect-ongoing-data.html）</p><p>Messaging and streaming layer - Serverless Applications Lens （https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/messaging-and-streaming-layer.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "3ad5039286494149b493b96e78381eef",
      "questionNumber": 405,
      "type": "single",
      "content": "<p>Question #405</p><p>A company is preparing to deploy an Amazon Elastic Kubernetes Service (Amazon EKS) cluster for a workload. The company expects the cluster to support an unpredictable number of stateless pods. Many of the pods will be created during a short time period as the workload automatically scales the number of replicas that the workload uses.</p><p><br></p><p>Which solution will MAXIMIZE node resilience?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use a separate launch template to deploy the EKS control plane into a second cluster that is separate from the workload node groups."
        },
        {
          "label": "B",
          "content": "Update the workload node groups. Use a smaller number of node groups and larger instances in the node groups."
        },
        {
          "label": "C",
          "content": "Configure the Kubernetes Cluster Autoscaler to ensure that the compute capacity of the workload node groups stays underprovisioned."
        },
        {
          "label": "D",
          "content": "Configure the workload to use topology spread constraints that are based on Availability Zone."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. Configure the workload to use topology spread constraints that are based on Availability Zone.</p><p>This solution will maximize node resilience for the Amazon EKS cluster for the following reasons:</p><p>High Availability Across Multiple Zones: Topology spread constraints allow you to distribute pods evenly across different Availability Zones (AZs). This ensures that if an entire AZ experiences an outage or degradation, your application will continue to run in other AZs with minimal impact.</p><p>Fault Isolation: By spreading pods across multiple AZs, you create natural fault boundaries. This means that infrastructure failures in one zone won't affect the entire workload, significantly improving overall resilience.</p><p>Balanced Resource Utilization: Topology spread constraints help the Kubernetes scheduler make intelligent decisions about pod placement, ensuring that resources are utilized evenly across the cluster. This prevents overloading specific nodes or zones.</p><p>Compatibility with Autoscaling: This approach works well with the unpredictable scaling requirements mentioned in the scenario. As new pods are created during rapid scaling events, they will be distributed according to the topology constraints, maintaining resilience even during dynamic scaling.</p><p>Stateless Workload Advantage: Since the pods are stateless, they can be easily redistributed across zones without concerns about data persistence, making topology spread constraints particularly effective.</p><p>Let's examine why the other options are less effective:</p><p>Option A (separate launch template for EKS control plane in a second cluster) doesn't directly address node resilience for the workload. The EKS control plane is already managed by AWS across multiple AZs, so this approach adds unnecessary complexity without improving workload resilience.</p><p>Option B (smaller number of node groups with larger instances) could actually reduce resilience by creating fewer points of failure. If a large instance fails, it would affect more pods than if the same workload were spread across more numerous, smaller instances.</p><p>Option C (configuring Cluster Autoscaler to keep compute capacity underprovisioned) would likely lead to resource shortages during rapid scaling events, potentially causing pod scheduling delays or failures. Underprovisioning works against resilience by limiting available resources when they're needed most.</p><p>Topology spread constraints based on Availability Zone provide the most effective way to maximize node resilience for this unpredictable, rapidly scaling stateless workload by ensuring pods are distributed across multiple failure domains.</p><p>Sources</p><p>Scaling - Containers on AWS （https://docs.aws.amazon.com/whitepapers/latest/containers-on-aws/scaling.html）</p><p>Running highly-available applications - Amazon EKS （https://docs.aws.amazon.com/eks/latest/best-practices/application.html）</p><p>EKS Data Plane - Amazon EKS （https://docs.aws.amazon.com/eks/latest/best-practices/data-plane.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "cf0a3100b81a47399ae13bf618697d5c",
      "questionNumber": 406,
      "type": "single",
      "content": "<p>Question #406</p><p>A company needs to implement a disaster recovery (DR) plan for a web application. The application runs in a single AWS Region.</p><p><br></p><p>The application uses microservices that run in containers. The containers are hosted on AWS Fargate in Amazon Elastic Container Service (Amazon ECS). The application has an Amazon RDS for MySQL DB instance as its data layer and uses Amazon Route 53 for DNS resolution. An Amazon CloudWatch alarm invokes an Amazon EventBridge rule if the application experiences a failure.</p><p><br></p><p>A solutions architect must design a DR solution to provide application recovery to a separate Region. The solution must minimize the time necessary to recover from a failure.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Setup a second ECS cluster and ECS service on Fargate in the separate Region. Create an AWS Lambda function to perform the following actions: take a snapshot of the RDS DB instance, copy the snapshot to the separate Region, create a new RDS DB instance from the snapshot, and update Route 53 to route traffic to the second ECS cluster. Update the EventBridge rule to add a target that will invoke the Lambda function."
        },
        {
          "label": "B",
          "content": "Create an AWS Lambda function that creates a second ECS cluster and ECS service in the separate Region. Configure the Lambda function to perform the following actions: take a snapshot of the RDS DB instance, copy the snapshot to the separate Region, create a new RDS DB instance from the snapshot, and update Route 53 to route traffic to the second ECS cluster. Update the EventBridge rule to add a target that will invoke the Lambda function."
        },
        {
          "label": "C",
          "content": "Setup a second ECS cluster and ECS service on Fargate in the separate Region. Create a cross-Region read replica of the RDS DB instance in the separate Region. Create an AWS Lambda function to promote the read replica to the primary database. Configure the Lambda function to update Route 53 to route traffic to the second ECS cluster. Update the EventBridge rule to add a target that will invoke the Lambda function."
        },
        {
          "label": "D",
          "content": "Setup a second ECS cluster and ECS service on Fargate in the separate Region. Take a snapshot of the RDS DB instance. Convert the snapshot to an Amazon DynamoDB global table. Create an AWS Lambda function to update Route 53 to route traffic to the second ECS cluster. Update the EventBridge rule to add a target that will invoke the Lambda function."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Option C is the solution that will meet the requirements for a disaster recovery plan with minimal recovery time. By setting up a second ECS cluster and ECS service on Fargate in a separate Region, the company can ensure that the application can be quickly switched over to the backup Region in the event of a failure. Creating a cross-Region read replica of the RDS DB instance allows for data redundancy and the ability to promote the read replica to primary quickly, minimizing data loss. Additionally, configuring an AWS Lambda function to update Route 53 ensures that traffic is automatically routed to the healthy ECS cluster, reducing downtime.</p><p>The requirements are: &nbsp;</p><p>1. Disaster Recovery (DR) in a separate Region → Need a standby setup in another AWS Region. &nbsp;</p><p>2. Minimize recovery time (RTO) → Using a cross-Region RDS read replica allows faster promotion to primary compared to snapshot-based recovery. &nbsp;</p><p>3. Microservices on ECS Fargate → A pre-configured ECS cluster in the DR Region ensures quick failover. &nbsp;</p><p>4. Automated DNS failover with Route 53 → Lambda can update DNS to redirect traffic to the DR Region. &nbsp;</p><p> Why Option C is Best? &nbsp;</p><p>- Cross-Region RDS Read Replica: Enables near-instant promotion to primary (much faster than snapshot restores). &nbsp;</p><p>- Pre-Provisioned ECS Cluster: Eliminates deployment delays during failover. &nbsp;</p><p>- Automated Lambda Failover: Triggered by EventBridge, updates Route 53 for seamless traffic redirection. &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- A & B: Rely on snapshot-based recovery, which is slower than promoting a read replica. &nbsp;</p><p>- D: Incorrectly suggests converting RDS MySQL to DynamoDB (not a valid DR strategy for relational data). &nbsp;</p><p> Conclusion: &nbsp;</p><p>C is the most efficient solution, leveraging cross-Region RDS read replicas and pre-deployed ECS for minimal downtime. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "29d7c36131c049a6a2a3135dba0d854a",
      "questionNumber": 407,
      "type": "single",
      "content": "<p>Question #407</p><p>A company has AWS accounts that are in an organization in AWS Organizations. The company wants to track Amazon EC2 usage as a metric. The company’s architecture team must receive a daily alert if the EC2 usage is more than 10% higher than the average EC2 usage from the last 30 days.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Configure AWS Budgets in the organization&#39;s management account. Specify a usage type of EC2 running hours. Specify a daily period. Set the budget amount to be 10% more than the reported average usage for the last 30 days from AWS Cost Explorer. Configure an alert to notify the architecture team if the usage threshold is met."
        },
        {
          "label": "B",
          "content": "Configure AWS Cost Anomaly Detection in the organization&#39;s management account. Configure a monitor type of AWS Service. Apply a filter of Amazon EC2. Configure an alert subscription to notify the architecture team if the usage is 10% more than the average usage for the last 30 days."
        },
        {
          "label": "C",
          "content": "Enable AWS Trusted Advisor in the organization&#39;s management account. Configure a cost optimization advisory alert to notify the architecture team if the EC2 usage is 10% more than the reported average usage for the last 30 days."
        },
        {
          "label": "D",
          "content": "Configure Amazon Detective in the organization&#39;s management account. Configure an EC2 usage anomaly alert to notify the architecture team if Detective identifies a usage anomaly of more than 10%."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Configure AWS Budgets in the organization's management account. Specify a usage type of EC2 running hours. Specify a daily period. Set the budget amount to be 10% more than the reported average usage for the last 30 days from AWS Cost Explorer. Configure an alert to notify the architecture team if the usage threshold is met.</p><p>This solution meets the requirements for the following reasons:</p><p>Precise metric tracking: AWS Budgets allows you to specifically track EC2 running hours as a usage metric, which directly aligns with the requirement to track EC2 usage.</p><p>Daily monitoring period: The solution specifies a daily period for the budget, ensuring that the architecture team receives alerts on a daily basis as required.</p><p>Exact threshold calculation: By setting the budget amount to be 10% more than the reported average usage for the last 30 days from AWS Cost Explorer, the solution precisely implements the requirement to alert when usage is more than 10% higher than the average from the last 30 days.</p><p>Appropriate notification mechanism: The solution includes configuring an alert to notify the architecture team when the usage threshold is met, fulfilling the notification requirement.</p><p>Organization-wide visibility: By configuring AWS Budgets in the organization's management account, the solution can track EC2 usage across all AWS accounts in the organization.</p><p>Option B (using AWS Cost Anomaly Detection) is less suitable for this specific requirement because:</p><p>AWS Cost Anomaly Detection is designed to detect unusual spending patterns using machine learning algorithms, rather than comparing against a specific, predefined threshold like \"10% above the 30-day average.\"</p><p>Cost Anomaly Detection doesn't guarantee daily alerts. It may detect anomalies less frequently or combine multiple days into a single alert, which doesn't satisfy the requirement for daily alerts.</p><p>While Cost Anomaly Detection can monitor EC2 usage, it's primarily designed to detect unexpected cost patterns rather than tracking usage against a specific historical average.</p><p>The configuration described in option B doesn't specify how the 10% threshold would be implemented within Cost Anomaly Detection, as this service typically uses machine learning to determine what constitutes an anomaly rather than fixed percentage thresholds.</p><p>AWS Budgets provides the precise control needed to implement the specific requirement of daily alerts when EC2 usage exceeds the 30-day average by 10%, making option A the correct solution.</p><p>Sources</p><p>COST02-BP05 Implement cost controls - AWS Well-Architected Framework （https://docs.aws.amazon.com/wellarchitected/2024-06-27/framework/cost_govern_usage_controls.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "56a2784b5a404a1ca82023dd85b4f420",
      "questionNumber": 408,
      "type": "single",
      "content": "<p>Question #408</p><p>An e-commerce company is revamping its IT infrastructure and is planning to use AWS services. The company’s CIO has asked a solutions architect to design a simple, highly available, and loosely coupled order processing application. The application is responsible for receiving and processing orders before storing them in an Amazon DynamoDB table. The application has a sporadic traffic pattern and should be able to scale during marketing campaigns to process the orders with minimal delays.</p><p><br></p><p>Which of the following is the MOST reliable approach to meet the requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Receive the orders in an Amazon EC2-hosted database and use EC2 instances to process them."
        },
        {
          "label": "B",
          "content": "Receive the orders in an Amazon SQS queue and invoke an AWS Lambda function to process them."
        },
        {
          "label": "C",
          "content": "Receive the orders using the AWS Step Functions program and launch an Amazon ECS container to process them."
        },
        {
          "label": "D",
          "content": "Receive the orders in Amazon Kinesis Data Streams and use Amazon EC2 instances to process them."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Receive the orders in an Amazon SQS queue and invoke an AWS Lambda function to process them.</p><p>This approach is the MOST reliable solution for meeting the e-commerce company's requirements for the following reasons:</p><p>High Availability: Amazon SQS is a fully managed message queuing service that provides high availability by default. It stores messages across multiple servers and data centers, ensuring that orders are not lost even if some AWS infrastructure components fail.</p><p>Loose Coupling: Using SQS creates a buffer between the components that receive orders and those that process them. This decoupling is essential for building resilient systems where failures in one component don't affect others.</p><p>Scalability for Sporadic Traffic: The SQS queue can handle varying loads of incoming orders without any configuration changes. During marketing campaigns when order volume spikes, the queue will simply accumulate messages without any performance degradation.</p><p>Automatic Scaling: AWS Lambda functions automatically scale based on the number of messages in the SQS queue. As more orders come in, more Lambda function instances are automatically created to process them, without any manual intervention or pre-provisioning.</p><p>Cost Efficiency: With this serverless approach, the company only pays for actual usage. During periods of low activity, costs are minimal, and during high-traffic marketing campaigns, resources scale up as needed.</p><p>Minimal Delays: The combination of SQS and Lambda ensures that orders are processed with minimal delays, even during traffic spikes. Lambda functions can be configured to process multiple messages in parallel.</p><p>Simplified Operations: This architecture eliminates the need to manage servers, databases, or scaling policies, reducing operational complexity.</p><p>Option A (using EC2 instances and an EC2-hosted database) is less reliable because:</p><p>It requires manual scaling or complex auto-scaling configurations to handle sporadic traffic patterns.</p><p>The EC2-hosted database could become a bottleneck during high-traffic periods.</p><p>It introduces more potential points of failure with multiple EC2 instances and a self-managed database.</p><p>It requires more operational overhead to ensure high availability.</p><p>It's not as cost-effective for sporadic workloads since EC2 instances would need to be running even during low-traffic periods.</p><p>The SQS and Lambda approach provides a simple, highly available, loosely coupled, and automatically scaling solution that meets all the requirements specified by the company's CIO, making it the most reliable choice for this e-commerce order processing application.</p><p>Sources</p><p>Scaling and event processing with SNS and SQS | AWS re:Post （https://repost.aws/questions/QUcnYW9ZAkSA-Px9tyHm_sYw/scaling-and-event-processing-with-sns-and-sqs）</p><p>Amazon SQS FIFO queue and Lambda concurrency behavior - Amazon Simple Queue Service （https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/fifo-queue-lambda-behavior.html）</p><p>Building resilient applications: design patterns for handling database outages | AWS Database Blog （https://aws.amazon.com/cn/blogs/database/building-resilient-applications-design-patterns-for-handling-database-outages/）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "d6003f9ce14d4e01b5edbabd2908b9ff",
      "questionNumber": 409,
      "type": "single",
      "content": "<p>Question #409</p><p>A company is deploying AWS Lambda functions that access an Amazon RDS for PostgreSQL database. The company needs to launch the Lambda functions in a QA environment and in a production environment. <br><br>The company must not expose credentials within application code and must rotate passwords automatically.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Store the database credentials for both environments in AWS Systems Manager Parameter Store. Encrypt the credentials by using an AWS Key Management Service (AWS KMS) key. Within the application code of the Lambda functions, pull the credentials from the Parameter Store parameter by using the AWS SDK for Python (Boto3). Add a role to the Lambda functions to provide access to the Parameter Store parameter."
        },
        {
          "label": "B",
          "content": "Store the database credentials for both environments in AWS Secrets Manager with distinct key entries for the QA environment and the production environment. Turn on rotation. Provide a reference to the Secrets Manager key as an environment variable for the Lambda functions."
        },
        {
          "label": "C",
          "content": "Store the database credentials for both environments in AWS Key Management Service (AWS KMS). Turn on rotation. Provide a reference to the credentials that are stored in AWS KMS as an environment variable for the Lambda functions."
        },
        {
          "label": "D",
          "content": "Create separate S3 buckets for the QA environment and the production environment. Turn on server-side encryption with AWS KMS keys (SSE-KMS) for the S3 buckets. Use an object naming pattern that gives each Lambda function&rsquo;s application code the ability to pull the correct credentials for the function&#39;s corresponding environment. Grant each Lambda function&#39;s execution role access to Amazon S3."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Option B is the correct solution to meet the requirements of not exposing credentials within application code and automating password rotation. AWS Secrets Manager is designed to securely store, manage, and rotate database credentials, along with other types of secrets. By storing the credentials in Secrets Manager with distinct key entries for each environment (QA and production), the company ensures that sensitive information is not hard-coded in the application code. The rotation feature of Secrets Manager allows for automatic password rotation, which enhances security by regularly updating credentials. Providing a reference to the Secrets Manager key as an environment variable for the Lambda functions enables the Lambda functions to access the credentials without including them in the code.</p><p>The requirements are: &nbsp;</p><p>1. No credentials in application code → Secrets must be stored externally. &nbsp;</p><p>2. Automatic password rotation → Requires a service that supports rotation. &nbsp;</p><p>3. Environment separation (QA & Production) → Must manage credentials per environment securely. &nbsp;</p><p> Why Option B is Best? &nbsp;</p><p>- AWS Secrets Manager: &nbsp;</p><p> &nbsp;- Securely stores credentials (encrypted at rest). &nbsp;</p><p> &nbsp;- Supports automatic rotation (key requirement). &nbsp;</p><p> &nbsp;- Allows environment separation (distinct keys for QA/prod). &nbsp;</p><p>- Lambda Integration: &nbsp;</p><p> &nbsp;- Secrets can be referenced via environment variables (no hardcoding). &nbsp;</p><p> &nbsp;- IAM roles grant Lambda access to Secrets Manager. &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- A (Parameter Store): Does not support automatic rotation (only Secrets Manager does). &nbsp;</p><p>- C (AWS KMS): KMS encrypts but does not store or rotate secrets. &nbsp;</p><p>- D (S3): Not secure for credential storage (S3 is not designed for secrets management). &nbsp;</p><p> Conclusion: &nbsp;</p><p>B is the only solution that meets all requirements (secure storage, rotation, and environment separation). &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "1e4589ccf66c47ce99e96fe02424399a",
      "questionNumber": 410,
      "type": "single",
      "content": "<p>Question #410</p><p>A company is using AWS Control Tower to manage AWS accounts in an organization in AWS Organizations. The company has an OU that contains accounts. The company must prevent any new or existing Amazon EC2 instances in the OU's accounts from gaining a public IP address.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Configure all instances in each account in the OU to use AWS Systems Manager. Use a Systems Manager Automation runbook to prevent public IP addresses from being attached to the instances."
        },
        {
          "label": "B",
          "content": "Implement the AWS Control Tower proactive control to check whether instances in the OU&#39;s accounts have a public IP address. Set the AssociatePublicIpAddress property to False. Attach the proactive control to the OU."
        },
        {
          "label": "C",
          "content": "Create an SCP that prevents the launch of instances that have a public IP address. Additionally, configure the SCP to prevent the attachment of a public IP address to existing instances. Attach the SCP to the OU."
        },
        {
          "label": "D",
          "content": "Create an AWS Config custom rule that detects instances that have a public IP address. Configure a remediation action that uses an AWS Lambda function to detach the public IP addresses from the instances."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Create an SCP that prevents the launch of instances that have a public IP address. Additionally, configure the SCP to prevent the attachment of a public IP address to existing instances. Attach the SCP to the OU.</p><p>This solution will meet the requirements for the following reasons:</p><p>Comprehensive Prevention: Service Control Policies (SCPs) act as permission guardrails that apply to all accounts within an OU. By creating an SCP that specifically prevents both the launch of instances with public IP addresses and the attachment of public IPs to existing instances, the company ensures complete coverage of the requirement.</p><p>Preventative Control: SCPs work as preventative controls that deny actions before they can occur, rather than detecting and remediating after the fact. This ensures that no EC2 instance in the affected accounts can ever gain a public IP address.</p><p>Organization-Wide Enforcement: By attaching the SCP to the OU, the policy automatically applies to all accounts within that OU, including any new accounts that might be added in the future, providing consistent enforcement.</p><p>No Operational Overhead: Once implemented, the SCP requires no ongoing maintenance or monitoring to continue enforcing the policy.</p><p>Option A (using Systems Manager Automation runbooks) is less effective because:</p><p>It's a reactive approach that would need to continuously monitor and remediate instances</p><p>It requires all instances to have Systems Manager agents installed and properly configured</p><p>It introduces operational complexity and potential points of failure</p><p>It might not prevent the initial assignment of public IPs during instance launch</p><p>Option B (implementing AWS Control Tower proactive controls) is incorrect because:</p><p>AWS Control Tower proactive controls (also known as preventive guardrails) don't have a specific built-in control for the \"AssociatePublicIpAddress\" property</p><p>Proactive controls in Control Tower are typically implemented using SCPs or AWS CloudFormation hooks, not as a direct property setting</p><p>This approach as described wouldn't address existing instances that might already have public IPs or prevent other methods of attaching public IPs</p><p>The SCP approach (Option C) provides the most comprehensive, preventative, and low-maintenance solution to ensure that no EC2 instances in the OU's accounts can gain public IP addresses, making it the correct answer.</p><p>Sources</p><p>ec2-instance-no-public-ip - AWS Config （https://docs.aws.amazon.com/config/latest/developerguide/ec2-instance-no-public-ip.html）</p><p>Data residency controls with detective behavior - AWS Control Tower （https://docs.aws.amazon.com/controltower/latest/controlreference/data-residency-detective-controls.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "246734ca3142497290f3a2b9d8d7a415",
      "questionNumber": 411,
      "type": "single",
      "content": "<p>Question #411</p><p>A company is deploying a third-party web application on AWS. The application is packaged as a Docker image. The company has deployed the Docker image as an AWS Fargate service in Amazon Elastic Container Service (Amazon ECS). An Application Load Balancer (ALB) directs traffic to the application.<br><br></p><p>The company needs to give only a specific list of users the ability to access the application from the internet. The company cannot change the application and cannot integrate the application with an identity provider. All users must be authenticated through multi-factor authentication (MFA).<br><br>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a user pool in Amazon Cognito. Configure the pool for the application. Populate the pool with the required users. Configure the pool to require MFA. Configure a listener rule on the ALB to require authentication through the Amazon Cognito hosted UI."
        },
        {
          "label": "B",
          "content": "Configure the users in AWS Identity and Access Management (IAM). Attach a resource policy to the Fargate service to require users to use MFA. Configure a listener rule on the ALB to require authentication through IAM."
        },
        {
          "label": "C",
          "content": "Configure the users in AWS Identity and Access Management (IAM). Enable AWS IAM Identity Center (AWS Single Sign-On). Configure resource protection for the ALB. Create a resource protection rule to require users to use MFA."
        },
        {
          "label": "D",
          "content": "Create a user pool in AWS Amplify. Configure the pool for the application. Populate the pool with the required users. Configure the pool to require MFA. Configure a listener rule on the ALB to require authentication through the Amplify hosted UI."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Create a user pool in Amazon Cognito. Configure the pool for the application. Populate the pool with the required users. Configure the pool to require MFA. Configure a listener rule on the ALB to require authentication through the Amazon Cognito hosted UI.</p><p>This solution meets all the requirements for the following reasons:</p><p>Restricted Access: By creating a user pool in Amazon Cognito and populating it with only the specific list of users who should have access, the company can control exactly who can access the application.</p><p>No Application Changes: This approach doesn't require any modifications to the third-party web application itself. The authentication happens at the ALB level, before traffic even reaches the application running in the Fargate containers.</p><p>No Identity Provider Integration: The solution doesn't require integrating the application with an identity provider. Instead, it leverages Amazon Cognito's hosted UI for authentication, which sits in front of the application.</p><p>Multi-Factor Authentication (MFA): Amazon Cognito supports MFA, and the solution specifically mentions configuring the user pool to require MFA, satisfying this security requirement.</p><p>Internet Access: The solution maintains the ability for authorized users to access the application from the internet while preventing unauthorized access.</p><p>Works with Containerized Applications: This approach is compatible with the Docker image running as a Fargate service in Amazon ECS.</p><p>The implementation works as follows:</p><p>When a user attempts to access the application, the ALB listener rule intercepts the request.</p><p>The ALB redirects the user to the Amazon Cognito hosted UI for authentication.</p><p>The user provides their credentials and completes the MFA challenge.</p><p>Upon successful authentication, Amazon Cognito redirects the user back to the ALB with authentication information.</p><p>The ALB validates the authentication and forwards the request to the Fargate service running the application.</p><p>Users not in the Cognito user pool cannot authenticate and therefore cannot access the application.</p><p>This solution provides a secure authentication layer in front of the application without requiring any changes to the application itself, making it the correct answer for the given requirements.</p><p>Sources</p><p>ECS Task with public ip connect permanent | AWS re:Post （https://repost.aws/questions/QUYfik8-3vSqO8fLIq2Z4GeQ/ecs-task-with-public-ip-connect-permanent）</p><p>Customer identity management - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/security-reference-architecture/customer-identity-management.html）</p><p>Control Access to Web Applications and Mobile Apps - Navigating GDPR Compliance on AWS （https://docs.aws.amazon.com/whitepapers/latest/navigating-gdpr-compliance/control-access-to-web-applications-and-mobile-apps.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "1e9c965184124385abec15b5cc0b5933",
      "questionNumber": 412,
      "type": "single",
      "content": "<p>Question #412</p><p>A solutions architect is preparing to deploy a new security tool into several previously unused AWS Regions. The solutions architect will deploy the tool by using an AWS CloudFormation stack set. The stack set's template contains an IAM role that has a custom name. Upon creation of the stack set, no stack instances are created successfully.<br><br>What should the solutions architect do to deploy the stacks successfully? </p>",
      "options": [
        {
          "label": "A",
          "content": "Enable the new Regions in all relevant accounts. Specify the CAPABILITY_NAMED_IAM capability during the creation of the stack set."
        },
        {
          "label": "B",
          "content": "Use the Service Quotas console to request a quota increase for the number of CloudFormation stacks in each new Region in all relevant accounts. Specify the CAPABILITY_IAM capability during the creation of the stack set."
        },
        {
          "label": "C",
          "content": "Specify the CAPABILITY_NAMED_IAM capability and the SELF_MANAGED permissions model during the creation of the stack set."
        },
        {
          "label": "D",
          "content": "Specify an administration role ARN and the CAPABILITY_IAM capability during the creation of the stack set."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Enable the new Regions in all relevant accounts. Specify the CAPABILITY_NAMED_IAM capability during the creation of the stack set.</p><p>This solution addresses both key issues preventing the successful deployment of the stack instances:</p><p>Region Enablement:</p><p>The problem mentions \"previously unused AWS Regions,\" which indicates that these Regions may not be enabled in the AWS accounts.</p><p>By default, some AWS Regions are not enabled in AWS accounts and must be explicitly activated before resources can be deployed there.</p><p>Enabling the new Regions in all relevant accounts is a necessary first step to allow CloudFormation to deploy resources in those Regions.</p><p>IAM Permissions:</p><p>The CloudFormation template contains \"an IAM role that has a custom name.\"</p><p>When creating CloudFormation stacks or stack sets that include IAM resources with custom names, you must explicitly acknowledge this capability.</p><p>The CAPABILITY_NAMED_IAM capability is specifically required when creating or updating stacks that contain custom-named IAM resources.</p><p>Without this capability specified, CloudFormation will refuse to create the stack instances as a safety measure to prevent unintended IAM changes.</p><p>Option B is incorrect for the following reasons:</p><p>Requesting a quota increase for the number of CloudFormation stacks would not resolve the issue, as the problem is not related to reaching a stack limit.</p><p>The CAPABILITY_IAM capability is insufficient for templates that include IAM resources with custom names. This capability only covers IAM resources with auto-generated names, not custom-named resources.</p><p>The issue is related to permissions and Region enablement, not service quotas.</p><p>By enabling the previously unused Regions and specifying the CAPABILITY_NAMED_IAM capability during stack set creation, the solutions architect will address both barriers preventing the successful deployment of the security tool across the new Regions.</p><p>Sources</p><p>Control CloudFormation access with AWS Identity and Access Management - AWS CloudFormation （https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/control-access-with-iam.html）</p><p>CreateStackSet - AWS CloudFormation （https://docs.aws.amazon.com/AWSCloudFormation/latest/APIReference/API_CreateStackSet.html）</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "681151266ed94b3580b06e66cf555ffb",
      "questionNumber": 413,
      "type": "single",
      "content": "<p>Question #413</p><p>A company has an application that uses an Amazon Aurora PostgreSQL DB cluster for the application's database. The DB cluster contains one small primary instance and three larger replica instances. The application runs on an AWS Lambda function. The application makes many short-lived connections to the database's replica instances to perform read-only operations.<br><br></p><p>During periods of high traffic, the application becomes unreliable and the database reports that too many connections are being established. The frequency of high-traffic periods is unpredictable.</p><p><br>Which solution will improve the reliability of the application?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use Amazon RDS Proxy to create a proxy for the DB cluster. Configure a read-only endpoint for the proxy. Update the Lambda function to connect to the proxy endpoint."
        },
        {
          "label": "B",
          "content": "Increase the max_connections setting on the DB cluster&#39;s parameter group. Reboot all the instances in the DB cluster. Update the Lambda function to connect to the DB cluster endpoint."
        },
        {
          "label": "C",
          "content": "Configure instance scaling for the DB cluster to occur when the DatabaseConnections metric is close to the max connections setting. Update the Lambda function to connect to the Aurora reader endpoint."
        },
        {
          "label": "D",
          "content": "Use Amazon RDS Proxy to create a proxy for the DB cluster. Configure a read-only endpoint for the Aurora Data API on the proxy. Update the Lambda function to connect to the proxy endpoint."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Option A is the correct answer. Amazon RDS Proxy can be used to manage database connections efficiently, especially when dealing with numerous short-lived connections from AWS Lambda functions. By configuring a read-only endpoint for the proxy, the Lambda function can connect to the Aurora replica instances through the proxy, which will help manage the connection pool and reduce the number of concurrent connections to the database.</p><p>The problem stems from too many short-lived connections from Lambda to Aurora PostgreSQL replicas during high traffic, overwhelming the database. &nbsp;</p><p> Why Option A is Best? &nbsp;</p><p>- Amazon RDS Proxy is designed to manage connection pooling, reducing the overhead of frequent connections. &nbsp;</p><p> &nbsp;- It sits between the Lambda function and the Aurora cluster, reusing connections efficiently. &nbsp;</p><p> &nbsp;- The read-only endpoint ensures read traffic is distributed across replicas. &nbsp;</p><p>- Lambda Integration: &nbsp;</p><p> &nbsp;- Updating the Lambda function to use the RDS Proxy endpoint (instead of direct DB connections) solves the connection exhaustion issue. &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- B (Increasing `max_connections`): &nbsp;</p><p> &nbsp;- Only a temporary fix; does not address connection churn (short-lived connections still cause overhead). &nbsp;</p><p> &nbsp;- Replicas may still get overwhelmed during unpredictable spikes. &nbsp;</p><p>- C (Instance Scaling): &nbsp;</p><p> &nbsp;- Scaling Aurora instances does not solve connection pooling issues. &nbsp;</p><p> &nbsp;- The Aurora reader endpoint alone doesn’t manage connections efficiently. &nbsp;</p><p>- D (Aurora Data API): &nbsp;</p><p> &nbsp;- The Aurora Data API is for serverless apps (not connection pooling). &nbsp;</p><p> &nbsp;- Misleading—RDS Proxy itself is the solution, not the Data API. &nbsp;</p><p> Conclusion: &nbsp;</p><p>A is the most reliable solution because RDS Proxy directly addresses the connection management problem without requiring manual scaling or configuration changes. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "7da7494ad63e4f96a708d5c9d1d0d104",
      "questionNumber": 414,
      "type": "single",
      "content": "<p>Question #414</p><p>A retail company is mounting IoT sensors in all of its stores worldwide. During the manufacturing of each sensor, the company’s private certificate authority (CA) issues an X.509 certificate that contains a unique serial number. The company then deploys each certificate to its respective sensor.</p><p><br></p><p>A solutions architect needs to give the sensors the ability to send data to AWS after they are installed. Sensors must not be able to send data to AWS until they are installed.</p><p><br>Which solution will meet these requirements? </p>",
      "options": [
        {
          "label": "A",
          "content": "Create an AWS Lambda function that can validate the serial number. Create an AWS IoT Core provisioning template. Include the SerialNumber parameter in the Parameters section. Add the Lambda function as a pre-provisioning hook. During manufacturing, call the RegisterThing API operation and specify the template and parameters."
        },
        {
          "label": "B",
          "content": "Create an AWS Step Functions state machine that can validate the serial number. Create an AWS IoT Core provisioning template. Include the SerialNumber parameter in the Parameters section. Specify the Step Functions state machine to validate parameters. Call the StartThingRegistrationTask API operation during installation."
        },
        {
          "label": "C",
          "content": "Create an AWS Lambda function that can validate the serial number. Create an AWS IoT Core provisioning template. Include the SerialNumber parameter in the Parameters section. Add the Lambda function as a pre-provisioning hook. Register the CA with AWS IoT Core, specify the provisioning template, and set the allow-auto-registration parameter."
        },
        {
          "label": "D",
          "content": "Create an AWS IoT Core provisioning template. Include the SerialNumber parameter in the Parameters section. Include parameter validation in the template. Provision a claim certificate and a private key for each device that uses the CA. Grant AWS IoT Core service permissions to update AWS IoT things during provisioning."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Create an AWS Lambda function that can validate the serial number. Create an AWS IoT Core provisioning template. Include the SerialNumber parameter in the Parameters section. Add the Lambda function as a pre-provisioning hook. Register the CA with AWS IoT Core, specify the provisioning template, and set the allow-auto-registration parameter.</p><p>This solution meets the requirements for the following reasons:</p><p>Security Control: The solution ensures that sensors can only send data to AWS after they are installed by implementing a validation step through a Lambda function that acts as a pre-provisioning hook.</p><p>Certificate Utilization: The solution leverages the X.509 certificates that are already being issued by the company's private CA during manufacturing, which contain unique serial numbers.</p><p>Automated Provisioning: By registering the CA with AWS IoT Core and enabling auto-registration, the solution allows for a streamlined process where devices can be automatically recognized when they connect.</p><p>Installation Validation: The Lambda function can validate the serial number against a database of installed sensors, ensuring that only sensors that have been properly installed are allowed to send data.</p><p>Scalability: This approach works well for a global deployment across all stores worldwide, as it doesn't require manual intervention for each sensor installation.</p><p>The workflow of this solution is as follows:</p><p>During manufacturing, each sensor receives an X.509 certificate with a unique serial number from the company's private CA.</p><p>The company registers their private CA with AWS IoT Core and configures it to use the provisioning template with auto-registration enabled.</p><p>When a sensor is installed in a store, it attempts to connect to AWS IoT Core using its certificate.</p><p>AWS IoT Core recognizes the certificate as being issued by the registered CA and initiates the provisioning process using the specified template.</p><p>The provisioning template extracts the SerialNumber parameter and passes it to the Lambda function pre-provisioning hook.</p><p>The Lambda function validates the serial number against a database or list of sensors that have been officially installed.</p><p>If the validation is successful, the Lambda function allows the provisioning to continue, and the sensor is fully registered with AWS IoT Core, enabling it to send data.</p><p>If the validation fails (e.g., for a sensor that hasn't been installed yet), the provisioning is rejected, and the sensor cannot send data to AWS.</p><p>This approach provides a secure, automated way to ensure that only properly installed sensors can communicate with AWS, meeting the company's requirements.</p><p>Sources</p><p>Provisioning identity in AWS IoT Core for device connections - Device Manufacturing and Provisioning with X.509 Certificates in AWS IoT Core （https://docs.aws.amazon.com/whitepapers/latest/device-manufacturing-provisioning/provisioning-identity-in-aws-iot-core-for-device-connections.html）</p><p>Device provisioning - IoT Lens （https://docs.aws.amazon.com/wellarchitected/latest/iot-lens/device-provisioning.html）</p><p>Create your own client certificates - AWS IoT Core （https://docs.aws.amazon.com/iot/latest/developerguide/device-certs-your-own.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "bd65643adbe948c9a3366c3a0d1a0d33",
      "questionNumber": 415,
      "type": "single",
      "content": "<p>Question #415</p><p>A startup company recently migrated a large ecommerce website to AWS. The website has experienced a 70% increase in sales. Software engineers are using a private GitHub repository to manage code. The DevOps team is using Jenkins for builds and unit testing. The engineers need to receive notifications for bad builds and zero downtime during deployments. The engineers also need to ensure any changes to production are seamless for users and can be rolled back in the event of a major issue.</p><p><br></p><p>The software engineers have decided to use AWS CodePipeline to manage their build and deployment process.<br><br>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use GitHub websockets to trigger the CodePipeline pipeline. Use the Jenkins plugin for AWS CodeBuild to conduct unit testing. Send alerts to an Amazon SNS topic for any bad builds. Deploy in an in-place, all-at-once deployment configuration using AWS CodeDeploy."
        },
        {
          "label": "B",
          "content": "Use GitHub webhooks to trigger the CodePipeline pipeline. Use the Jenkins plugin for AWS CodeBuild to conduct unit testing. Send alerts to an Amazon SNS topic for any bad builds. Deploy in a blue/green deployment using AWS CodeDeploy."
        },
        {
          "label": "C",
          "content": "Use GitHub websockets to trigger the CodePipeline pipeline. Use AWS X-Ray for unit testing and static code analysis. Send alerts to an Amazon SNS topic for any bad builds. Deploy in a blue/green deployment using AWS CodeDeploy."
        },
        {
          "label": "D",
          "content": "Use GitHub webhooks to trigger the CodePipeline pipeline. Use AWS X-Ray for unit testing and static code analysis. Send alerts to an Amazon SNS topic for any bad builds. Deploy in an in-place, all-at-once deployment configuration using AWS CodeDeploy."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Option B is the correct answer. Using GitHub webhooks to trigger the AWS CodePipeline ensures a seamless integration between code commits and the start of the build process. Utilizing the Jenkins plugin for AWS CodeBuild allows the team to continue using their existing Jenkins configurations for unit testing within the AWS ecosystem. Sending alerts to an Amazon SNS topic for bad builds provides a notification mechanism for the team. Finally, deploying using a blue/green deployment strategy with AWS CodeDeploy helps achieve zero downtime and seamless user experience during deployments, with the ability to roll back to the previous version in case of any issues.</p><p>The requirements are: &nbsp;</p><p>1. GitHub integration → Webhooks (not websockets) are the correct way to trigger CodePipeline from GitHub. &nbsp;</p><p>2. Jenkins for builds/unit testing → The Jenkins plugin for AWS CodeBuild allows integration with existing Jenkins workflows. &nbsp;</p><p>3. Notifications for bad builds → Amazon SNS can send alerts for failed pipeline stages. &nbsp;</p><p>4. Zero downtime deployments & rollback capability → Blue/Green deployment (via AWS CodeDeploy) ensures seamless updates and easy rollback. &nbsp;</p><p> Why Option B is Best? &nbsp;</p><p>- GitHub Webhooks: Properly triggers CodePipeline on code changes. &nbsp;</p><p>- Jenkins + CodeBuild: Maintains existing CI workflows while integrating with AWS. &nbsp;</p><p>- SNS Alerts: Ensures engineers are notified of build failures. &nbsp;</p><p>- Blue/Green Deployment: Guarantees zero downtime and safe rollback. &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- A & D: Use in-place deployments, which cause downtime and lack rollback safety. &nbsp;</p><p>- C & D: Incorrectly suggest X-Ray for unit testing (X-Ray is for distributed tracing, not testing). &nbsp;</p><p>- A & C: Incorrectly mention websockets (GitHub uses webhooks for pipeline triggers). &nbsp;</p><p> Conclusion: &nbsp;</p><p>B is the only solution that meets all requirements (GitHub webhooks, Jenkins integration, SNS alerts, and blue/green deployments). &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "cd9ab881103f48e58f2728165ec9a5ce",
      "questionNumber": 416,
      "type": "single",
      "content": "<p>Question #416</p><p>A software as a service (SaaS) company has developed a multi-tenant environment. The company uses Amazon DynamoDB tables that the tenants share for the storage layer. The company uses AWS Lambda functions for the application services.<br><br></p><p>The company wants to offer a tiered subscription model that is based on resource consumption by each tenant. Each tenant is identified by a unique tenant ID that is sent as part of each request to the Lambda functions. The company has created an AWS Cost and Usage Report (AWS CUR) in an AWS account. The company wants to allocate the DynamoDB costs to each tenant to match that tenant's resource consumption.<br><br>Which solution will provide a granular view of the DynamoDB cost for each tenant with the LEAST operational effort? </p>",
      "options": [
        {
          "label": "A",
          "content": "Associate a new tag that is named tenant ID with each table in DynamoDB. Activate the tag as a cost allocation tag in the AWS Billing and Cost Management console. Deploy new Lambda function code to log the tenant ID in Amazon CloudWatch Logs. Use the AWS CUR to separate DynamoDB consumption cost for each tenant ID."
        },
        {
          "label": "B",
          "content": "Configure the Lambda functions to log the tenant ID and the number of RCUs and WCUs consumed from DynamoDB for each transaction to Amazon CloudWatch Logs. Deploy another Lambda function to calculate the tenant costs by using the logged capacity units and the overall DynamoDB cost from the AWS Cost Explorer API. Create an Amazon EventBridge rule to invoke the calculation Lambda function on a schedule."
        },
        {
          "label": "C",
          "content": "Create a new partition key that associates DynamoDB items with individual tenants. Deploy a Lambda function to populate the new column as part of each transaction. Deploy another Lambda function to calculate the tenant costs by using Amazon Athena to calculate the number of tenant items from DynamoDB and the overall DynamoDB cost from the AWS CUR. Create an Amazon EventBridge rule to invoke the calculation Lambda function on a schedule."
        },
        {
          "label": "D",
          "content": "Deploy a Lambda function to log the tenant ID, the size of each response, and the duration of the transaction call as custom metrics to Amazon CloudWatch Logs. Use CloudWatch Logs Insights to query the custom metrics for each tenant. Use AWS Pricing Calculator to obtain the overall DynamoDB costs and to calculate the tenant costs."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Option B is the correct answer. By configuring Lambda functions to log the tenant ID along with the consumed read and write capacity units (RCUs and WCUs) for each transaction, the company can track individual tenant resource consumption. Utilizing the AWS Cost Explorer API to calculate tenant costs based on these logged metrics allows for a dynamic and automated approach to cost allocation. Scheduling the invocation of the calculation Lambda function using Amazon EventBridge ensures that the cost allocation is performed regularly and stays up-to-date.</p><p>The requirements are: &nbsp;</p><p>1. Granular DynamoDB cost allocation per tenant → Need to track Read Capacity Units (RCUs) and Write Capacity Units (WCUs) per tenant. &nbsp;</p><p>2. Least operational effort → Avoid schema changes or complex ETL processes. &nbsp;</p><p>3. Tiered subscription model → Must accurately measure each tenant's resource usage. &nbsp;</p><p> Why Option B is Best? &nbsp;</p><p>- Log RCUs/WCUs per Tenant: &nbsp;</p><p> &nbsp;- Lambda functions can log tenant_id + consumed capacity units to CloudWatch. &nbsp;</p><p> &nbsp;- This provides direct visibility into DynamoDB usage per tenant. &nbsp;</p><p>- Automated Cost Calculation: &nbsp;</p><p> &nbsp;- A scheduled Lambda function can aggregate usage data and calculate costs using the AWS Cost Explorer API. &nbsp;</p><p> &nbsp;- This avoids manual analysis and provides near-real-time cost allocation. &nbsp;</p><p>- No Schema Changes: &nbsp;</p><p> &nbsp;- Unlike Option C, this does not require modifying DynamoDB tables. &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- A (Cost Allocation Tags): &nbsp;</p><p> &nbsp;- DynamoDB does not support per-request tagging, so tags cannot accurately track tenant-level usage in shared tables. &nbsp;</p><p> &nbsp;- Only works for table-level costs, not tenant-level consumption. &nbsp;</p><p>- C (Partition Key + Athena): &nbsp;</p><p> &nbsp;- Requires schema changes (operational overhead). &nbsp;</p><p> &nbsp;- Athena queries add complexity and latency. &nbsp;</p><p>- D (Custom Metrics + Pricing Calculator): &nbsp;</p><p> &nbsp;- Response size/duration ≠ RCU/WCU consumption (inaccurate for billing). &nbsp;</p><p> &nbsp;- Pricing Calculator is manual and not automated. &nbsp;</p><p> Conclusion: &nbsp;</p><p>B is the most accurate and least operationally intensive solution, as it: &nbsp;</p><p>1. Directly measures RCU/WCU consumption per tenant. &nbsp;</p><p>2. Automates cost allocation without schema changes. &nbsp;</p><p>3. Aligns with AWS-native tools (CloudWatch, Cost Explorer). &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "cabf10b5b2b14e5a935e20965e839f79",
      "questionNumber": 417,
      "type": "single",
      "content": "<p>Question #417</p><p>A company has an application that stores data in a single Amazon S3 bucket. The company must keep all data for 1 year. The company’s security team is concerned that an attacker could gain access to the AWS account through leaked long-term credentials.<br><br>Which solution will ensure that existing and future objects in the S3 bucket are protected? </p>",
      "options": [
        {
          "label": "A",
          "content": "Create a new AWS account that is accessible only to the security team through an assumed role. Create an S3 bucket in the new account. Enable S3 Versioning and S3 Object Lock. Configure a default retention period of 1 year. Set up replication from the existing S3 bucket to the new S3 bucket. Create an S3 Batch Replication job to copy all existing data."
        },
        {
          "label": "B",
          "content": "Use the s3-bucket-versioning-enabled AWS Config managed rule. Configure an automatic remediation action that uses an AWS Lambda function to enable S3 Versioning and MFA Delete on noncompliant resources. Add an S3 Lifecycle rule to delete objects after 1 year."
        },
        {
          "label": "C",
          "content": "Explicitly deny bucket creation from all users and roles except for an AWS Service Catalog launch constraint role. Define a Service Catalog product for the creation of the S3 bucket to force S3 Versioning and MFA Delete to be enabled. Authorize users to launch the product when they need to create an S3 bucket."
        },
        {
          "label": "D",
          "content": "Enable Amazon GuardDuty with the S3 protection feature for the account and the AWS Region. Add an S3 Lifecycle rule to delete objects after 1 year."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Create a new AWS account that is accessible only to the security team through an assumed role. Create an S3 bucket in the new account. Enable S3 Versioning and S3 Object Lock. Configure a default retention period of 1 year. Set up replication from the existing S3 bucket to the new S3 bucket. Create an S3 Batch Replication job to copy all existing data.</p><p>This solution will ensure that existing and future objects in the S3 bucket are protected for the following reasons:</p><p>Account Isolation: Creating a separate AWS account that's only accessible to the security team provides strong isolation from the primary account. If an attacker gains access to the primary account through leaked credentials, they won't have access to the security account where the protected data is stored.</p><p>Immutable Storage: S3 Object Lock with a 1-year retention period makes the data immutable - it cannot be deleted or modified by anyone, including administrators and even AWS, until the retention period expires. This is critical for protecting against attackers who might try to delete or corrupt data.</p><p>Data Versioning: S3 Versioning ensures that multiple versions of objects are maintained, protecting against accidental or malicious overwrites.</p><p>Complete Data Protection: The combination of replication for new objects and batch replication for existing data ensures that all data (both current and future) is protected in the secure secondary account.</p><p>Least Privilege Access: Using assumed roles for the security team follows the principle of least privilege, further reducing the attack surface.</p><p>Option B (using AWS Config with automatic remediation) is less effective because:</p><p>Same Account Vulnerability: It keeps all data in the same AWS account. If an attacker gains access to the account through leaked credentials, they could potentially disable the AWS Config rules or modify the Lambda functions used for remediation.</p><p>No True Immutability: While MFA Delete adds a layer of protection, it doesn't provide the same level of immutability as Object Lock. An attacker with sufficient permissions could still disable MFA Delete.</p><p>No Data Isolation: Without cross-account replication, there's no isolation between the production data and its protected copies.</p><p>Lifecycle Rules Only: The lifecycle rule would only delete objects after 1 year but doesn't prevent deletion or modification before that time.</p><p>The solution in option A creates a comprehensive defense-in-depth strategy by combining account isolation, immutable storage, versioning, and replication to ensure that data is protected even if the primary account is compromised. This approach aligns with AWS security best practices for protecting critical data against threats from compromised credentials.</p><p>Sources</p><p>Data protection in Amazon S3 - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/DataDurability.html）</p><p>Remediating exposures for Amazon S3 buckets - AWS Security Hub （https://docs.aws.amazon.com/securityhub/latest/userguide/exposure-s3-bucket.html）</p><p>Data protection in Amazon S3 - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/data-protection.html）</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "b917ff9b07824c1aa7c9d3f632a3a967",
      "questionNumber": 418,
      "type": "single",
      "content": "<p>Question #418</p><p>A company needs to improve the security of its web-based application on AWS. The application uses Amazon CloudFront with two custom origins. The first custom origin routes requests to an Amazon API Gateway HTTP API. The second custom origin routes traffic to an Application Load Balancer (ALB). The application integrates with an OpenID Connect (OIDC) identity provider (IdP) for user management.<br><br></p><p>A security audit shows that a JSON Web Token (JWT) authorizer provides access to the API. The security audit also shows that the ALB accepts requests from unauthenticated users.<br><br></p><p>A solutions architect must design a solution to ensure that all backend services respond to only authenticated users. <br><br></p><p>Which solution will meet this requirement? </p>",
      "options": [
        {
          "label": "A",
          "content": "Configure the ALB to enforce authentication and authorization by integrating the ALB with the IdP. Allow only authenticated users to access the backend services."
        },
        {
          "label": "B",
          "content": "Modify the CloudFront configuration to use signed URLs. Implement a permissive signing policy that allows any request to access the backend services."
        },
        {
          "label": "C",
          "content": "Create an AWS WAF web ACL that filters out unauthenticated requests at the ALB level. Allow only authenticated traffic to reach the backend services."
        },
        {
          "label": "D",
          "content": "Enable AWS CloudTrail to log all requests that come to the ALB. Create an AWS Lambda function to analyze the logs and block any requests that come from unauthenticated users."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Option A is the correct answer. By configuring the ALB to work with the OIDC identity provider, the company can ensure that only authenticated users are able to access the backend services. This integration will allow the ALB to validate the JWT tokens provided by the IdP, thereby preventing unauthenticated requests from reaching the backend.</p><p>The key requirements are:</p><p>1. All backend services must only respond to authenticated users - Currently the ALB accepts unauthenticated requests</p><p>2. Application already uses OIDC for authentication - The solution should leverage the existing identity provider</p><p>3. Two backend origins need protection - API Gateway (already secured with JWT) and ALB (needs securing)</p><p> Why Option A is Correct:</p><p>- ALB Native OIDC Integration: </p><p> &nbsp;- ALB can directly integrate with OIDC identity providers for authentication</p><p> &nbsp;- This provides the same authentication flow as the API Gateway's JWT authorizer</p><p> &nbsp;- Only authenticated users will reach backend services</p><p>- Consistent Security Model:</p><p> &nbsp;- Both API Gateway and ALB will use the same OIDC provider</p><p> &nbsp;- Provides uniform authentication across all backend services</p><p>- Minimal Configuration:</p><p> &nbsp;- Uses existing infrastructure (ALB and OIDC provider)</p><p> &nbsp;- No additional services needed</p><p> Why Other Options Are Incorrect:</p><p>B (CloudFront Signed URLs):</p><p>- Signed URLs don't integrate with OIDC</p><p>- Would require completely separate authentication system</p><p>- \"Permissive signing policy\" contradicts security requirements</p><p>C (AWS WAF):</p><p>- WAF can't natively authenticate users</p><p>- Would require complex rules to inspect authentication tokens</p><p>- Not the proper tool for authentication</p><p>D (CloudTrail + Lambda):</p><p>- Reactive rather than preventive approach</p><p>- Significant latency in blocking unauthenticated requests</p><p>- Overly complex solution for the requirement</p><p> Conclusion:</p><p>Option A provides the most secure, efficient, and maintainable solution by:</p><p>1. Using native ALB-OIDC integration</p><p>2. Providing consistent authentication across all services</p><p>3. Requiring minimal additional configuration</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "6035bff5721849a7b09f7be7fd637907",
      "questionNumber": 419,
      "type": "single",
      "content": "<p>Question #419</p><p>A company creates an AWS Control Tower landing zone to manage and govern a multi-account AWS environment. The company's security team will deploy preventive controls and detective controls to monitor AWS services across all the accounts. The security team needs a centralized view of the security state of all the accounts.</p><p><br>Which solution will meet these requirements? </p>",
      "options": [
        {
          "label": "A",
          "content": "From the AWS Control Tower management account, use AWS CloudFormation StackSets to deploy an AWS Config conformance pack to all accounts in the organization."
        },
        {
          "label": "B",
          "content": "Enable Amazon Detective for the organization in AWS Organizations. Designate one AWS account as the delegated administrator for Detective."
        },
        {
          "label": "C",
          "content": "From the AWS Control Tower management account, deploy an AWS CloudFormation stack set that uses the automatic deployment option to enable Amazon Detective for the organization."
        },
        {
          "label": "D",
          "content": "Enable AWS Security Hub for the organization in AWS Organizations. Designate one AWS account as the delegated administrator for Security Hub."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. Enable AWS Security Hub for the organization in AWS Organizations. Designate one AWS account as the delegated administrator for Security Hub.</p><p>This solution will meet the requirements for providing a centralized view of the security state across all accounts in the AWS Control Tower landing zone for the following reasons:</p><p>Comprehensive Security Visibility: AWS Security Hub provides a comprehensive view of security alerts and compliance status across all AWS accounts in an organization. It aggregates, organizes, and prioritizes security findings from multiple AWS services (such as Amazon GuardDuty, Amazon Inspector, and Amazon Macie) and AWS Partner solutions.</p><p>Centralized Management: By enabling Security Hub at the organization level and designating a delegated administrator account, the security team can manage security findings, conduct investigations, and take remediation actions from a single account rather than having to switch between multiple accounts.</p><p>Integration with Control Tower: Security Hub integrates seamlessly with AWS Control Tower and complements its governance capabilities. While Control Tower provides the landing zone and guardrails, Security Hub provides the ongoing security monitoring and compliance checking.</p><p>Support for Both Preventive and Detective Controls: Security Hub supports both types of controls mentioned in the requirements:</p><p>It can monitor preventive controls implemented through AWS Organizations SCPs and Control Tower guardrails</p><p>It provides detective controls through its security standards and checks</p><p>Automated Compliance Checking: Security Hub automatically checks resources against industry standards and best practices (such as CIS AWS Foundations Benchmark, AWS Foundational Security Best Practices, and PCI DSS).</p><p>Option C (deploying Amazon Detective via CloudFormation stack set) is less suitable because:</p><p>Narrower Focus: While Detective is powerful for investigating security findings, it's primarily designed for deep analysis of specific security issues rather than providing a comprehensive overview of security posture across accounts.</p><p>Limited Integration: Detective doesn't provide the same breadth of integration with security standards and compliance frameworks as Security Hub does.</p><p>Different Purpose: Detective is more focused on investigation after a security finding has been identified, rather than providing an overall security posture assessment and continuous monitoring.</p><p>Complementary Service: Detective works best as a complementary service to Security Hub rather than as the primary tool for centralized security monitoring.</p><p>By enabling AWS Security Hub for the organization and designating a delegated administrator account, the security team will have the centralized view they need to effectively monitor and manage security across all accounts in their AWS Control Tower landing zone.</p><p>Sources</p><p>Control services - Management and Governance Cloud Environment Guide （https://docs.aws.amazon.com/wellarchitected/latest/management-and-governance-guide/aws-controls-and-guardrails-services.html）</p><p>Security incident response for a multi-account architecture - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/transitioning-to-multiple-aws-accounts/security-incident-response.html）</p><p>Managing the multi-account environment using AWS Organizations and AWS Control Tower | AWS Cloud Operations & Migrations Blog （https://aws.amazon.com/cn/blogs/mt/managing-the-multi-account-environment-using-aws-organizations-and-aws-control-tower/）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "2b22056ddeaf4e78b89dc518de848880",
      "questionNumber": 420,
      "type": "single",
      "content": "<p>Question #420</p><p>A company that develops consumer electronics with offices in Europe and Asia has 60 TB of software images stored on premises in Europe. The company wants to transfer the images to an Amazon S3 bucket in the ap-northeast-1 Region. New software images are created daily and must be encrypted in transit. The company needs a solution that does not require custom development to automatically transfer all existing and new software images to Amazon S3.<br><br>Which solution will meet these requirements? </p>",
      "options": [
        {
          "label": "A",
          "content": "Deploy an AWS DataSync agent and configure a task to transfer the images to the S3 bucket."
        },
        {
          "label": "B",
          "content": "Configure Amazon Kinesis Data Firehose to transfer the images using S3 Transfer Acceleration."
        },
        {
          "label": "C",
          "content": "Use an AWS Snowball device to transfer the images with the S3 bucket as the target."
        },
        {
          "label": "D",
          "content": "Transfer the images over a Site-to-Site VPN connection using the S3 API with multipart upload."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Deploy an AWS DataSync agent and configure a task to transfer the images to the S3 bucket.</p><p>This solution is the most appropriate next step in the transfer process for the following reasons:</p><p>Volume of Data: With 60 TB of software images to transfer, AWS DataSync is designed to handle large-scale data transfers efficiently between on-premises storage and AWS services.</p><p>Encryption in Transit: DataSync automatically encrypts data in transit using TLS 1.2, meeting the company's requirement for encryption during transfer.</p><p>No Custom Development: DataSync is a fully managed service with a straightforward setup process that doesn't require any custom code development. It provides a graphical interface and CLI for configuration.</p><p>Automatic Transfer of New Files: DataSync can be scheduled to run on a recurring basis, which will automatically transfer new software images as they are created daily. It can detect and transfer only the new or changed files.</p><p>Cross-Region Capability: DataSync can efficiently transfer data from Europe (on-premises) to the ap-northeast-1 Region (Asia Pacific - Tokyo) as specified in the requirements.</p><p>Optimized for WAN: DataSync uses a protocol that's optimized for WAN transfers, which is important for transferring large amounts of data across continents (Europe to Asia).</p><p>Let's examine why the other options are less suitable:</p><p>Option B (Kinesis Data Firehose with S3 Transfer Acceleration) is not appropriate because:</p><p>Kinesis Data Firehose is designed for streaming data, not for bulk file transfers</p><p>It would require custom development to integrate with the company's systems</p><p>Not ideal for transferring large files like software images</p><p>Option C (AWS Snowball device) is not the best choice because:</p><p>While good for the initial transfer, it doesn't provide an automated solution for transferring new images created daily</p><p>It's a manual process requiring physical device handling</p><p>Not suitable for ongoing, regular transfers of new data</p><p>Option D (Site-to-Site VPN with S3 API) is not optimal because:</p><p>It would require setting up and managing VPN infrastructure</p><p>Would likely need custom scripts to automate the transfer process</p><p>Less efficient for large-scale transfers compared to DataSync</p><p>AWS DataSync provides the most comprehensive solution that meets all the stated requirements: handling the large volume of data, encrypting data in transit, requiring no custom development, and automatically transferring both existing and new software images to the S3 bucket in ap-northeast-1.</p><p>Sources</p><p>Using AWS DataSync to transfer differentials after Snowball device is transferred to S3 | AWS re:Post （https://repost.aws/questions/QUXziUu0T0QFKmQWu9N_lb4g/using-aws-datasync-to-transfer-differentials-after-snowball-device-is-transferred-to-s3）</p><p>How can we transfer TBs of postgresql from on prem to s3 | AWS re:Post （https://repost.aws/questions/QUHzSzZG16QjOAho5BZvDUfA/how-can-we-transfer-tbs-of-postgresql-from-on-prem-to-s3）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "7fbf1c849b9b45cbae42d675bea0dc30",
      "questionNumber": 421,
      "type": "single",
      "content": "<p>Question #421</p><p>A company has a web application that uses Amazon API Gateway, AWS Lambda, and Amazon DynamoDB. A recent marketing campaign has increased demand. Monitoring software reports that many requests have significantly longer response times than before the marketing campaign.<br><br>A solutions architect enabled Amazon CloudWatch Logs for API Gateway and noticed that errors are occurring on 20% of the requests. In CloudWatch, the Lambda function Throttles metric represents 1% of the requests and the Errors metric represents 10% of the requests. Application logs indicate that, when errors occur, there is a call to DynamoDB.<br><br>What change should the solutions architect make to improve the current response times as the web application becomes more popular? </p>",
      "options": [
        {
          "label": "A",
          "content": "Increase the concurrency limit of the Lambda function."
        },
        {
          "label": "B",
          "content": "Implement DynamoDB auto scaling on the table."
        },
        {
          "label": "C",
          "content": "Increase the API Gateway throttle limit."
        },
        {
          "label": "D",
          "content": "Re-create the DynamoDB table with a better-partitioned primary index."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Implement DynamoDB auto scaling on the table.</p><p>This solution will improve the current response times as the web application becomes more popular for the following reasons:</p><p>Root Cause Analysis: The monitoring data provides important clues about where the bottleneck is occurring:</p><p>20% of API Gateway requests have errors</p><p>Lambda function has 1% throttles and 10% errors</p><p>Application logs show that errors occur when there's a call to DynamoDB</p><p>This pattern strongly suggests that DynamoDB is the primary bottleneck in the system, not Lambda concurrency.</p><p>Error Distribution: While Lambda is experiencing some throttling (1%), the majority of Lambda errors (10%) are occurring during DynamoDB operations. This indicates that the Lambda function is failing when it tries to interact with DynamoDB, likely due to DynamoDB throttling or capacity limitations.</p><p>Scaling Solution: DynamoDB auto scaling automatically adjusts the provisioned throughput capacity of your table based on actual traffic patterns. When implemented:</p><p>It will increase read and write capacity units during the high-demand periods caused by the marketing campaign</p><p>It will decrease capacity during periods of lower usage to optimize costs</p><p>It will help prevent throttling of DynamoDB operations that are currently causing errors</p><p>End-to-End Improvement: By addressing the DynamoDB bottleneck, you'll see cascading improvements:</p><p>Fewer DynamoDB throttling errors</p><p>Reduced Lambda function errors (since most errors occur during DynamoDB calls)</p><p>Improved API Gateway response times and fewer errors returned to clients</p><p>Option A (increasing the Lambda function concurrency limit) would only address the 1% of requests that are being throttled at the Lambda level. This would not solve the larger issue where 10% of Lambda executions are failing during DynamoDB operations. Increasing Lambda concurrency might actually make the problem worse by allowing more concurrent requests to hit DynamoDB, potentially increasing the throttling at the database level.</p><p>By implementing DynamoDB auto scaling, the solutions architect will address the root cause of the performance issues, allowing the web application to handle the increased demand from the marketing campaign and scale appropriately as it becomes more popular.</p><p>Sources</p><p>AWS REST apigateway throws 502 even though integration timeout is set to 59000 ms | AWS re:Post （https://repost.aws/questions/QUNv9DIMb4Tv-ySS25pdvIaw/aws-rest-apigateway-throws-502-even-though-integration-timeout-is-set-to-59000-ms）</p><p>Lambda scaling behavior - AWS Lambda （https://docs.aws.amazon.com/lambda/latest/dg/scaling-behavior.html）</p><p>Scale lambdas to tens of thousands concurrent runnings | AWS re:Post （https://repost.aws/questions/QUjv5PAE_dQ56EXvEpkJ1WTQ/scale-lambdas-to-tens-of-thousands-concurrent-runnings）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "466dea7454eb4f08bdd33bdead35e527",
      "questionNumber": 422,
      "type": "single",
      "content": "<p>Question #422</p><p>A company has an application that has a web frontend. The application runs in the company's on-premises data center and requires access to file storage for critical data. The application runs on three Linux VMs for redundancy. The architecture includes a load balancer with HTTP request-based routing.<br><br></p><p>The company needs to migrate the application to AWS as quickly as possible. The architecture on AWS must be highly available.<br><br>Which solution will meet these requirements with the FEWEST changes to the architecture? </p>",
      "options": [
        {
          "label": "A",
          "content": "Migrate the application to Amazon Elastic Container Service (Amazon ECS) containers that use the Fargate launch type in three Availability Zones. Use Amazon S3 to provide file storage for all three containers. Use a Network Load Balancer to direct traffic to the containers."
        },
        {
          "label": "B",
          "content": "Migrate the application to Amazon EC2 instances in three Availability Zones. Use Amazon Elastic File System (Amazon EFS) for file storage. Mount the file storage on all three EC2 instances. Use an Application Load Balancer to direct traffic to the EC2 instances."
        },
        {
          "label": "C",
          "content": "Migrate the application to Amazon Elastic Kubernetes Service (Amazon EKS) containers that use the Fargate launch type in three Availability Zones. Use Amazon FSx for Lustre to provide file storage for all three containers. Use a Network Load Balancer to direct traffic to the containers."
        },
        {
          "label": "D",
          "content": "Migrate the application to Amazon EC2 instances in three AWS Regions. Use Amazon Elastic Block Store (Amazon EBS) for file storage. Enable Cross-Region Replication (CRR) for all three EC2 instances. Use an Application Load Balancer to direct traffic to the EC2 instances."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Migrate the application to Amazon EC2 instances in three Availability Zones. Use Amazon Elastic File System (Amazon EFS) for file storage. Mount the file storage on all three EC2 instances. Use an Application Load Balancer to direct traffic to the EC2 instances.</p><p>This solution will meet the requirements with the fewest changes to the architecture for the following reasons:</p><p>Direct Mapping of Current Architecture:</p><p>The current architecture has three Linux VMs for redundancy, which maps directly to three EC2 instances in AWS.</p><p>The on-premises load balancer with HTTP request-based routing maps directly to an Application Load Balancer (ALB) in AWS, which specializes in HTTP/HTTPS traffic routing.</p><p>The shared file storage requirement is fulfilled by Amazon EFS, which can be mounted on all EC2 instances simultaneously.</p><p>High Availability:</p><p>Placing the EC2 instances across three Availability Zones provides high availability and fault tolerance.</p><p>Amazon EFS is designed to be highly available by default, with data stored redundantly across multiple Availability Zones.</p><p>The Application Load Balancer automatically distributes incoming traffic across multiple Availability Zones and only routes traffic to healthy instances.</p><p>Minimal Changes:</p><p>This approach maintains the same basic architecture: three servers behind a load balancer with shared file storage.</p><p>The application code likely requires no changes since it will still run on Linux instances with the same file system mount points.</p><p>The load balancing logic remains similar, with HTTP request-based routing capabilities preserved in the ALB.</p><p>Quick Migration Path:</p><p>EC2 instances can be created using AMIs that closely match the on-premises Linux environment.</p><p>Tools like AWS Application Migration Service (formerly CloudEndure Migration) can help migrate the existing VMs to EC2 with minimal downtime.</p><p>EFS provides a standard NFS interface that the application can use without code changes.</p><p>Operational Simplicity:</p><p>No need to refactor the application for containers or serverless architecture.</p><p>Familiar Linux administration and troubleshooting procedures can be maintained.</p><p>Standard mounting procedures for NFS file systems apply to EFS.</p><p>This solution provides a \"lift and shift\" approach that meets the requirement to migrate as quickly as possible while ensuring high availability in the AWS environment. It minimizes the architectural changes required, allowing the company to maintain a similar operational model while gaining the benefits of AWS's infrastructure reliability and scalability.</p><p>Sources</p><p>Moving to an AWS architecture - Hosting Static Websites on AWS （https://docs.aws.amazon.com/whitepapers/latest/build-static-websites-aws/moving-to-an-aws-architecture.html）</p><p>Set up a highly available PeopleSoft architecture on AWS - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/set-up-a-highly-available-peoplesoft-architecture-on-aws.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "187b807aa1af4cb0a7d0e99358780c5d",
      "questionNumber": 423,
      "type": "single",
      "content": "<p>Question #423</p><p>A company is planning to migrate an on-premises data center to AWS. The company currently hosts the data center on Linux-based VMware VMs.A solutions architect must collect information about network dependencies between the VMs. The information must be in the form of a diagram that details host IP addresses, hostnames, and network connection information.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use AWS Application Discovery Service. Select an AWS Migration Hub home AWS Region. Install the AWS Application Discovery Agent on the on-premises servers for data collection. Grant permissions to Application Discovery Service to use the Migration Hub network diagrams."
        },
        {
          "label": "B",
          "content": "Use the AWS Application Discovery Service Agentless Collector for server data collection. Export the network diagrams from the AWS Migration Hub in .png format."
        },
        {
          "label": "C",
          "content": "Install the AWS Application Migration Service agent on the on-premises servers for data collection. Use AWS Migration Hub data in Workload Discovery on AWS to generate network diagrams."
        },
        {
          "label": "D",
          "content": "Install the AWS Application Migration Service agent on the on-premises servers for data collection. Export data from AWS Migration Hub in .csv format into an Amazon CloudWatch dashboard to generate network diagrams."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Use AWS Application Discovery Service. Select an AWS Migration Hub home AWS Region. Install the AWS Application Discovery Agent on the on-premises servers for data collection. Grant permissions to Application Discovery Service to use the Migration Hub network diagrams.</p><p>This solution will meet the requirements for collecting network dependency information between the Linux-based VMware VMs for the following reasons:</p><p>Comprehensive Data Collection: The AWS Application Discovery Agent provides detailed information about each server, including IP addresses, hostnames, and network connections. When installed directly on the Linux VMs, the agent can capture real-time network traffic and connection details that are essential for creating accurate dependency diagrams.</p><p>Detailed Network Dependency Mapping: The agent-based approach collects more granular data about network communications between servers, including connection frequency, ports used, and data transfer volumes. This level of detail is crucial for understanding the complete picture of how the VMs interact with each other.</p><p>Migration Hub Network Diagrams: By granting permissions to Application Discovery Service to use Migration Hub network diagrams, the company can visualize the collected data in diagram form, which directly meets the requirement for information \"in the form of a diagram that details host IP addresses, hostnames, and network connection information.\"</p><p>Accuracy for Linux-Based Systems: The Application Discovery Agent is specifically designed to work with Linux systems and can collect OS-specific information that might be missed by agentless approaches.</p><p>Option B (using the AWS Application Discovery Service Agentless Collector) is less suitable because:</p><p>Limited Network Visibility: The Agentless Collector primarily gathers information through VMware vCenter and doesn't have the same level of visibility into the actual network traffic between VMs as the agent-based approach does.</p><p>Less Detailed Connection Information: While the Agentless Collector can identify some connections between servers, it may not capture all the network dependency details required, especially for complex applications with dynamic communications.</p><p>Export Limitations: Simply exporting network diagrams in .png format doesn't ensure that all the required information (IP addresses, hostnames, and network connection details) will be included in a comprehensive and usable way.</p><p>The agent-based approach provides the most thorough and accurate network dependency information, which is critical for planning a successful migration from on-premises to AWS. By understanding these dependencies in detail, the solutions architect can ensure that all necessary connections are maintained when the workloads are moved to AWS, minimizing the risk of application failures after migration.</p><p>Sources</p><p>Welcome - AWS Application Discovery Service （https://docs.aws.amazon.com/application-discovery/latest/APIReference/Welcome.html）</p><p>What is AWS Application Discovery Service? - AWS Application Discovery Service （https://docs.aws.amazon.com/application-discovery/latest/userguide/what-is-appdiscovery.html）</p><p>Viewing network connections in AWS Migration Hub - AWS Migration Hub （https://docs.aws.amazon.com/migrationhub/latest/ug/network-diagram.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "03decf19560b45fbacf781c56d6d95d0",
      "questionNumber": 424,
      "type": "single",
      "content": "<p>Question #424</p><p>A company runs a software-as-a-service (SaaS) application on AWS. The application consists of AWS Lambda functions and an Amazon RDS for MySQL Multi-AZ database. During market events, the application has a much higher workload than normal. Users notice slow response times during the peak periods because of many database connections. The company needs to improve the scalable performance and availability of the database.<br><br>Which solution meets these requirements? </p>",
      "options": [
        {
          "label": "A",
          "content": "Create an Amazon CloudWatch alarm action that triggers a Lambda function to add an Amazon RDS for MySQL read replica when resource utilization hits a threshold."
        },
        {
          "label": "B",
          "content": "Migrate the database to Amazon Aurora, and add a read replica. Add a database connection pool outside of the Lambda handler function."
        },
        {
          "label": "C",
          "content": "Migrate the database to Amazon Aurora, and add a read replica. Use Amazon Route 53 weighted records."
        },
        {
          "label": "D",
          "content": "Migrate the database to Amazon Aurora, and add an Aurora Replica. Configure Amazon RDS Proxy to manage database connection pools."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. Migrate the database to Amazon Aurora, and add an Aurora Replica. Configure Amazon RDS Proxy to manage database connection pools.</p><p>This solution best meets the requirements for improving the scalable performance and availability of the database for the following reasons:</p><p>Connection Management: The primary issue identified is \"slow response times during the peak periods because of many database connections.\" RDS Proxy directly addresses this by efficiently managing database connection pools. It maintains a pool of established database connections that can be reused by Lambda functions, which significantly reduces the connection overhead that's causing the performance issues.</p><p>Serverless Integration: Since the application uses AWS Lambda functions, which can scale rapidly during market events, RDS Proxy provides an ideal solution for handling the connection surge. Lambda functions create and destroy connections frequently, which can overwhelm a database. RDS Proxy acts as a buffer between the Lambda functions and the database.</p><p>Improved Scalability: Amazon Aurora itself offers better performance and scalability compared to standard RDS for MySQL. Adding an Aurora Replica further enhances read scalability by distributing read operations across multiple instances.</p><p>Enhanced Availability: Aurora's architecture already provides better availability than standard RDS MySQL. The combination of Aurora's built-in replication, Aurora Replicas, and RDS Proxy's faster failover capabilities (reducing failover time by up to 66%) creates a highly available solution.</p><p>No Application Code Changes: This solution can be implemented without significant changes to the application code, making it easier and faster to deploy.</p><p>Option C (Migrating to Aurora with a read replica and using Route 53 weighted records) is less effective because:</p><p>It doesn't address the core issue of connection management. Even with weighted routing, the database would still face the same connection overhead problems during peak periods.</p><p>Using Route 53 weighted records for database connections would require application-level changes to handle different connection endpoints for reads and writes, adding complexity.</p><p>Route 53 weighted records are typically used for distributing traffic across different application instances, not for managing database connections within a single application.</p><p>By migrating to Amazon Aurora with an Aurora Replica and implementing RDS Proxy, the company will directly address the connection bottleneck that's causing slow response times during peak periods, while also improving the overall scalability and availability of their database infrastructure.</p><p>Sources</p><p>Amazon RDS Proxy - Oracle to Aurora MySQL Migration Playbook （https://docs.aws.amazon.com/dms/latest/oracle-to-aurora-mysql-migration-playbook/chap-oracle-aurora-mysql.tools.rdsproxy.html）</p><p>Amazon RDS Proxy - Oracle to Aurora PostgreSQL Migration Playbook （https://docs.aws.amazon.com/dms/latest/oracle-to-aurora-postgresql-migration-playbook/chap-oracle-aurora-pg.tools.rdsproxy.html）</p><p>Highly Available Database Proxy – Amazon RDS Proxy – Amazon Web Services （https://aws.amazon.com/cn/rds/proxy/）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "353f91ddda3f4efaa771037e62d8fed4",
      "questionNumber": 425,
      "type": "single",
      "content": "<p>Question #425</p><p>A company is planning to migrate an application from on premises to the AWS Cloud. The company will begin the migration by moving the application’s underlying data storage to AWS. The application data is stored on a shared file system on premises, and the application servers connect to the shared file system through SMB.</p><p><br></p><p>A solutions architect must implement a solution that uses an Amazon S3 bucket for shared storage. Until the application is fully migrated and code is rewritten to use native Amazon S3 APIs, the application must continue to have access to the data through SMB. The solutions architect must migrate the application data to AWS to its new location while still allowing the on-premises application to access the data.<br><br>Which solution will meet these requirements? </p>",
      "options": [
        {
          "label": "A",
          "content": "Create a new Amazon FSx for Windows File Server file system. Configure AWS DataSync with one location for the on-premises file share and one location for the new Amazon FSx file system. Create a new DataSync task to copy the data from the on-premises file share location to the Amazon FSx file system."
        },
        {
          "label": "B",
          "content": "Create an S3 bucket for the application. Copy the data from the on-premises storage to the S3 bucket."
        },
        {
          "label": "C",
          "content": "Deploy an AWS Server Migration Service (AWS SMS) VM to the on-premises environment. Use AWS SMS to migrate the file storage server from on premises to an Amazon EC2 instance."
        },
        {
          "label": "D",
          "content": "Create an S3 bucket for the application. Deploy a new AWS Storage Gateway file gateway on an on-premises VM. Create a new file share that stores data in the S3 bucket and is associated with the file gateway. Copy the data from the on-premises storage to the new file gateway endpoint."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>The requirements are:</p><p>1. Migrate application data to Amazon S3 - Need to move from on-premises file share to S3</p><p>2. Maintain SMB access during migration - Application still needs SMB protocol access until rewritten</p><p>3. Allow on-premises access during transition - Existing on-premises servers must continue accessing the data</p><p> Why Option D is Correct:</p><p>- Storage Gateway File Gateway:</p><p> &nbsp;- Provides SMB/NFS interface to on-premises applications</p><p> &nbsp;- Stores data natively in Amazon S3</p><p> &nbsp;- Allows seamless transition to cloud storage</p><p>- Data Migration Path:</p><p> &nbsp;- Files can be copied from existing on-premises share to the new gateway endpoint</p><p> &nbsp;- Applications continue using same SMB paths during migration</p><p>- Future Ready:</p><p> &nbsp;- Once application is rewritten, it can access S3 directly</p><p> &nbsp;- No need for further storage migration</p><p> Why Other Options Are Incorrect:</p><p>A (FSx + DataSync):</p><p>- Migrates to FSx instead of S3</p><p>- Doesn't meet requirement to use S3 as primary storage</p><p>- More expensive than needed solution</p><p>B (Direct to S3):</p><p>- Doesn't provide SMB access during transition</p><p>- Would break existing application functionality</p><p>C (AWS SMS):</p><p>- Migrates entire VM rather than just storage</p><p>- Doesn't achieve S3 storage goal</p><p>- More complex than necessary</p><p> Key Benefits of Solution D:</p><p>1. Meets immediate SMB access requirement</p><p>2. Achieves S3 storage target</p><p>3. Provides smooth migration path</p><p>4. Minimizes operational disruption</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "04de3187b49e4255bdffd5abba45f5b8",
      "questionNumber": 426,
      "type": "single",
      "content": "<p>Question #426</p><p>A global company has a mobile app that displays ticket barcodes. Customers use the tickets on the mobile app to attend live events. Event scanners read the ticket barcodes and call a backend API to validate the barcode data against data in a database. After the barcode is scanned, the backend logic writes to the database's single table to mark the barcode as used.</p><p><br></p><p>The company needs to deploy the app on AWS with a DNS name of api.example.com. The company will host the database in three AWS Regions around the world.<br><br>Which solution will meet these requirements with the LOWEST latency? </p>",
      "options": [
        {
          "label": "A",
          "content": "Host the database on Amazon Aurora global database clusters. Host the backend on three Amazon Elastic Container Service (Amazon ECS) clusters that are in the same Regions as the database. Create an accelerator in AWS Global Accelerator to route requests to the nearest ECS cluster. Create an Amazon Route 53 record that maps api.example.com to the accelerator endpoint."
        },
        {
          "label": "B",
          "content": "Host the database on Amazon Aurora global database clusters. Host the backend on three Amazon Elastic Kubernetes Service (Amazon EKS) clusters that are in the same Regions as the database. Create an Amazon CloudFront distribution with the three clusters as origins. Route requests to the nearest EKS cluster. Create an Amazon Route 53 record that maps api.example.com to the CloudFront distribution."
        },
        {
          "label": "C",
          "content": "Host the database on Amazon DynamoDB global tables. Create an Amazon CloudFront distribution. Associate the CloudFront distribution with a CloudFront function that contains the backend logic to validate the barcodes. Create an Amazon Route 53 record that maps api.example.com to the CloudFront distribution."
        },
        {
          "label": "D",
          "content": "Host the database on Amazon DynamoDB global tables. Create an Amazon CloudFront distribution. Associate the CloudFront distribution with a Lambda@Edge function that contains the backend logic to validate the barcodes. Create an Amazon Route 53 record that maps api.example.com to the CloudFront distribution."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Option D is the correct answer. By hosting the database on Amazon DynamoDB global tables, the company can ensure that the database is replicated across multiple AWS Regions, providing low latency access to the data. Using Lambda@Edge, the backend logic can be executed close to the end-users in AWS Lambda's edge locations, which are integrated with CloudFront. This setup can offer the lowest latency for the mobile app users while also allowing the company to route requests to the nearest region using Amazon Route 53.</p><p>The requirements are:</p><p>1. Global low-latency access - Need to serve customers worldwide with minimal delay</p><p>2. Database in 3 Regions - Data must be globally distributed</p><p>3. Barcode validation API - Requires backend logic to validate and update ticket status</p><p>4. DNS name api.example.com - Needs proper routing</p><p> Why Option D is Correct:</p><p>- DynamoDB Global Tables:</p><p> &nbsp;- Provides multi-region, multi-active database</p><p> &nbsp;- Automatically syncs data across regions</p><p> &nbsp;- Single-digit millisecond latency for reads/writes</p><p>- Lambda@Edge:</p><p> &nbsp;- Runs validation logic at edge locations</p><p> &nbsp;- Processes requests closest to users</p><p> &nbsp;- Can read/write to DynamoDB global tables</p><p>- CloudFront + Route 53:</p><p> &nbsp;- Provides global distribution</p><p> &nbsp;- Routes to nearest edge location</p><p> &nbsp;- Simple DNS configuration</p><p> Why Other Options Are Incorrect:</p><p>A (Aurora Global + ECS + Global Accelerator):</p><p>- Aurora Global has higher replication lag than DynamoDB</p><p>- ECS clusters add management overhead</p><p>- Global Accelerator doesn't provide edge computing</p><p>B (Aurora Global + EKS + CloudFront):</p><p>- Same Aurora limitations as Option A</p><p>- EKS adds unnecessary complexity</p><p>- CloudFront can't route to EKS directly</p><p>C (CloudFront Functions):</p><p>- Functions have severe limitations (no DB access)</p><p>- Can't perform database writes</p><p>- Execution time limited to 1ms</p><p> Key Advantages of Solution D:</p><p>1. True global low-latency with edge processing</p><p>2. Fully serverless architecture</p><p>3. Simple data synchronization with DynamoDB</p><p>4. Automatic scaling for event spikes</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "0b668975484844199e699ab1e867b88a",
      "questionNumber": 427,
      "type": "single",
      "content": "<p>Question #427</p><p>A medical company is running a REST API on a set of Amazon EC2 instances. The EC2 instances run in an Auto Scaling group behind an Application Load Balancer (ALB). The ALB runs in three public subnets, and the EC2 instances run in three private subnets. The company has deployed an Amazon CloudFront distribution that has the ALB as the only origin.<br><br>Which solution should a solutions architect recommend to enhance the origin security? </p>",
      "options": [
        {
          "label": "A",
          "content": "Store a random string in AWS Secrets Manager. Create an AWS Lambda function for automatic secret rotation. Configure CloudFront to inject the random string as a custom HTTP header for the origin request. Create an AWS WAF web ACL rule with a string match rule for the custom header. Associate the web ACL with the ALB."
        },
        {
          "label": "B",
          "content": "Create an AWS WAF web ACL rule with an IP match condition of the CloudFront service IP address ranges. Associate the web ACL with the ALB. Move the ALB into the three private subnets."
        },
        {
          "label": "C",
          "content": "Store a random string in AWS Systems Manager Parameter Store. Configure Parameter Store automatic rotation for the string. Configure CloudFront to inject the random string as a custom HTTP header for the origin request. Inspect the value of the custom HTTP header, and block access in the ALB."
        },
        {
          "label": "D",
          "content": "Configure AWS Shield Advanced. Create a security group policy to allow connections from CloudFront service IP address ranges. Add the policy to AWS Shield Advanced, and attach the policy to the ALB."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Store a random string in AWS Secrets Manager. Create an AWS Lambda function for automatic secret rotation. Configure CloudFront to inject the random string as a custom HTTP header for the origin request. Create an AWS WAF web ACL rule with a string match rule for the custom header. Associate the web ACL with the ALB.</p><p>This solution enhances origin security through a custom header verification approach, which is considered a best practice for securing CloudFront to ALB communication. Here's why this is the better solution:</p><p>Custom Origin Header Authentication: By configuring CloudFront to add a secret custom header to requests sent to the ALB, you create a verification mechanism that ensures only requests coming through CloudFront are processed by the ALB. This prevents direct access to the ALB, even if someone discovers its DNS name.</p><p>Dynamic Secret Management: Storing the random string in AWS Secrets Manager with automatic rotation via Lambda ensures that the authentication mechanism remains secure over time. Even if the header value is somehow compromised, it will be automatically changed on a regular basis.</p><p>Defense in Depth: This approach implements multiple layers of security - the secret itself, the custom header mechanism, and the WAF rule that validates the header.</p><p>Maintains Existing Architecture: This solution doesn't require moving the ALB from public to private subnets, which could potentially require significant architectural changes.</p><p>Option B (creating a WAF rule based on CloudFront IP ranges and moving the ALB to private subnets) has several limitations:</p><p>CloudFront IP Ranges Change: CloudFront operates from many edge locations with IP addresses that can change over time. Maintaining an accurate list of all CloudFront IP addresses in WAF rules would require frequent updates.</p><p>Less Secure Authentication: IP-based filtering alone is generally considered less secure than cryptographic verification methods like secret headers.</p><p>Architectural Changes: Moving the ALB from public to private subnets would require additional components like NAT gateways for the EC2 instances to access the internet, potentially increasing complexity and cost.</p><p>The custom header approach (Option A) is a widely recommended best practice by AWS for securing communication between CloudFront and origin servers. It provides strong security through cryptographic verification rather than relying solely on network-level controls, making it the more robust solution for enhancing origin security in this scenario.</p><p>Sources</p><p>Custom origin with CloudFront - Secure Content Delivery with Amazon CloudFront （https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/SecurityAndPrivateContent.html）</p><p>WAF on ALB or CloudFront ? | AWS re:Post （https://repost.aws/questions/QUTY5hPVxgS6Caa3eZHX7-nQ/waf-on-alb-or-cloudfront）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "3a400293bc29404db313525c643f9ce6",
      "questionNumber": 428,
      "type": "single",
      "content": "<p>Question #428</p><p>To abide by industry regulations, a solutions architect must design a solution that will store a company's critical data in multiple public AWS Regions, including in the United States, where the company's headquarters is located. The solutions architect is required to provide access to the data stored in AWS to the company’s global WAN network. The security team mandates that no traffic accessing this data should traverse the public internet.<br><br>How should the solutions architect design a highly available solution that meets the requirements and is cost-effective? </p>",
      "options": [
        {
          "label": "A",
          "content": "Establish AWS Direct Connect connections from the company headquarters to all AWS Regions in use. Use the company WAN to send traffic over to the headquarters and then to the respective DX connection to access the data."
        },
        {
          "label": "B",
          "content": "Establish two AWS Direct Connect connections from the company headquarters to an AWS Region. Use the company WAN to send traffic over a DX connection. Use inter-region VPC peering to access the data in other AWS Regions."
        },
        {
          "label": "C",
          "content": "Establish two AWS Direct Connect connections from the company headquarters to an AWS Region. Use the company WAN to send traffic over a DX connection. Use an AWS transit VPC solution to access data in other AWS Regions."
        },
        {
          "label": "D",
          "content": "Establish two AWS Direct Connect connections from the company headquarters to an AWS Region. Use the company WAN to send traffic over a DX connection. Use Direct Connect Gateway to access data in other AWS Regions."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. Establish two AWS Direct Connect connections from the company headquarters to an AWS Region. Use the company WAN to send traffic over a DX connection. Use Direct Connect Gateway to access data in other AWS Regions.</p><p>This solution meets all the requirements for the following reasons:</p><p>Multi-Region Data Storage: The solution supports storing critical data in multiple public AWS Regions, including in the United States where the company's headquarters is located, which satisfies the industry regulation requirement.</p><p>Private Network Access: By using AWS Direct Connect, the company establishes a dedicated private connection between their headquarters and AWS, ensuring that no traffic traverses the public internet as mandated by the security team.</p><p>High Availability: The solution includes two Direct Connect connections from the company headquarters to an AWS Region, providing redundancy and high availability. If one connection fails, traffic can still flow through the second connection.</p><p>Global Access: The company can leverage their existing global WAN network to route traffic to the Direct Connect connections at headquarters. This allows offices worldwide to access the AWS resources through the company's private network.</p><p>Multi-Region Connectivity: Direct Connect Gateway is a global service that enables the company to access multiple AWS Regions through their Direct Connect connections. This means they can access data stored in any connected Region without having to establish separate Direct Connect connections to each Region.</p><p>Cost-Effectiveness: This solution is cost-effective because:</p><p>It leverages the existing company WAN infrastructure</p><p>It requires Direct Connect connections only at the headquarters location, not at every global office</p><p>Direct Connect Gateway eliminates the need for separate connections to each AWS Region</p><p>Traffic between Regions stays on AWS's private global network, avoiding data transfer charges associated with public internet traffic</p><p>The solution creates a secure, private network path from the company's global offices to multiple AWS Regions:</p><p>Global offices connect to the company headquarters via the existing global WAN</p><p>Headquarters connects to AWS via redundant Direct Connect connections</p><p>Direct Connect Gateway enables access to multiple AWS Regions</p><p>All traffic remains on private networks (company WAN and AWS global network), never traversing the public internet</p><p>This architecture satisfies the industry regulations for multi-region data storage while meeting the security team's requirement for private network access, all while maintaining high availability and cost-effectiveness.</p><p>Sources</p><p>AWS Direct Connect &nbsp;- Building a Scalable and Secure Multi-VPC AWS Network Infrastructure （https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/direct-connect.html）</p><p>Direct Connect gateway attachments in AWS Cloud WAN - AWS Network Manager （https://docs.aws.amazon.com/network-manager/latest/cloudwan/cloudwan-dxattach-about.html）</p><p>How Salesforce Business Technology uses AWS Direct Connect SiteLink for reliable global connectivity | Networking & Content Delivery （https://aws.amazon.com/cn/blogs/networking-and-content-delivery/how-salesforce-business-technology-uses-aws-direct-connect-sitelink-for-reliable-global-connectivity/）</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "4a5a2d6c42af4ed282e87e4d212d8e5e",
      "questionNumber": 429,
      "type": "single",
      "content": "<p>Question #429</p><p>A company has developed an application that is running Windows Server on VMware vSphere VMs that the company hosts on premises. The application data is stored in a proprietary format that must be read through the application. The company manually provisioned the servers and the application.</p><p><br></p><p>As part of its disaster recovery plan, the company wants the ability to host its application on AWS temporarily if the company's on-premises environment becomes unavailable. The company wants the application to return to on-premises hosting after a disaster recovery event is complete. The RPO is 5 minutes.<br><br>Which solution meets these requirements with the LEAST amount of operational overhead? </p>",
      "options": [
        {
          "label": "A",
          "content": "Configure AWS DataSync. Replicate the data to Amazon Elastic Block Store (Amazon EBS) volumes. When the on-premises environment is unavailable, use AWS CloudFormation templates to provision Amazon EC2 instances and attach the EBS volumes."
        },
        {
          "label": "B",
          "content": "Configure AWS Elastic Disaster Recovery. Replicate the data to replication Amazon EC2 instances that are attached to Amazon Elastic Block Store (Amazon EBS) volumes. When the on-premises environment is unavailable, use Elastic Disaster Recovery to launch EC2 instances that use the replicated volumes."
        },
        {
          "label": "C",
          "content": "Provision an AWS Storage Gateway file gateway. Replicate the data to an Amazon S3 bucket. When the on-premises environment is unavailable, use AWS Backup to restore the data to Amazon Elastic Block Store (Amazon EBS) volumes and launch Amazon EC2 instances from these EBS volumes."
        },
        {
          "label": "D",
          "content": "Provision an Amazon FSx for Windows File Server file system on AWS. Replicate the data to the file system. When the on-premises environment is unavailable, use AWS CloudFormation templates to provision Amazon EC2 instances and use AWS::CloudFormation::Init commands to mount the Amazon FSx file shares."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>The correct answer is B. Configure AWS Elastic Disaster Recovery. Replicate the data to replication Amazon EC2 instances that are attached to Amazon Elastic Block Store (Amazon EBS) volumes. When the on-premises environment is unavailable, use Elastic Disaster Recovery to launch EC2 instances that use the replicated volumes.</p><p> Explanation:</p><p>1. AWS Elastic Disaster Recovery (DRS) is designed specifically for this use case—disaster recovery with minimal operational overhead. It continuously replicates on-premises VMware vSphere VMs (or physical servers) to AWS, maintaining a ready-to-launch state in a low-cost staging area.</p><p>2. RPO of 5 minutes: Elastic Disaster Recovery supports frequent replication (as low as seconds), meeting the 5-minute RPO requirement.</p><p>3. Automated failover and recovery: When a disaster occurs, you can quickly launch fully provisioned EC2 instances from the replicated data with minimal manual intervention.</p><p>4. Return to on-premises: After the disaster is resolved, you can fail back to your on-premises environment.</p><p>5. Least operational overhead: Elastic Disaster Recovery handles the replication, orchestration, and recovery process automatically, reducing manual steps compared to other options.</p><p> Why not the other options?</p><p>- A (AWS DataSync + CloudFormation): DataSync is for data transfer but doesn’t automate VM replication or disaster recovery orchestration. Manual steps for provisioning EC2 instances and attaching EBS volumes increase overhead.</p><p>- C (Storage Gateway + AWS Backup): Storage Gateway is not designed for low-RPO disaster recovery. Restoring from S3 to EBS and launching EC2 instances manually is time-consuming and complex.</p><p>- D (Amazon FSx + CloudFormation): FSx is for file storage, not full VM replication. Mounting file shares doesn’t address the need for application server recovery, and manual steps add overhead.</p><p>B is the best solution because it provides automated, continuous replication and recovery with minimal operational effort.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "b1930b7cdd9f42a69f1e4c3668ccd668",
      "questionNumber": 430,
      "type": "single",
      "content": "<p>Question #430</p><p>A company runs a highly available data collection application on Amazon EC2 in the eu-north-1 Region. The application collects data from end-user devices and writes records to an Amazon Kinesis data stream and a set of AWS Lambda functions that process the records. The company persists the output of the record processing to an Amazon S3 bucket in eu-north-1. The company uses the data in the S3 bucket as a data source for Amazon Athena.</p><p><br></p><p>The company wants to increase its global presence. A solutions architect must launch the data collection capabilities in the sa-east-1 and ap-northeast-1 Regions. The solutions architect deploys the application, the Kinesis data stream, and the Lambda functions in the two new Regions.The solutions architect keeps the S3 bucket in eu-north-1 to meet a requirement to centralize the data analysis. </p><p><br></p><p>During testing of the new setup, the solutions architect notices a significant lag on the arrival of data from the new Regions to the S3 bucket.<br><br>Which solution will improve this lag time the MOST?</p>",
      "options": [
        {
          "label": "A",
          "content": "In each of the two new Regions, set up the Lambda functions to run in a VPC. Set up an S3 gateway endpoint in that VPC."
        },
        {
          "label": "B",
          "content": "Turn on S3 Transfer Acceleration on the S3 bucket in eu-north-1. Change the application to use the new S3 accelerated endpoint when the application uploads data to the S3 bucket."
        },
        {
          "label": "C",
          "content": "Create an S3 bucket in each of the two new Regions. Set the application in each new Region to upload to its respective S3 bucket. Set up S3 Cross-Region Replication to replicate data to the S3 bucket in eu-north-1."
        },
        {
          "label": "D",
          "content": "Increase the memory requirements of the Lambda functions to ensure that they have multiple cores available. Use the multipart upload feature when the application uploads data to Amazon S3 from Lambda."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Answer: C</p><p>The company is experiencing significant lag when data is uploaded from applications running in new AWS Regions (sa-east-1 and ap-northeast-1) to an S3 bucket in the eu-north-1 Region. The requirement is to minimize this lag while maintaining centralized storage for data analysis in the eu-north-1 S3 bucket.</p><p>Options Analysis:</p><ul><li>A: Setting up Lambda functions in a VPC with an S3 Gateway Endpoint does not address the lag issue. While this can optimize network traffic within a region, it does not improve cross-region data transfer latency.</li><li>B: Enabling S3 Transfer Acceleration in the eu-north-1 bucket can improve upload speed by utilizing Amazon’s globally distributed edge locations to accelerate data transfer. However, this may not fully eliminate the lag compared to replicating data across regions.</li><li>C: Creating an S3 bucket in each new Region (sa-east-1 and ap-northeast-1) allows the application to upload data locally, minimizing latency. Setting up S3 Cross-Region Replication ensures that data is automatically synchronized with the centralized S3 bucket in eu-north-1, maintaining the centralized data storage for analysis. This approach effectively reduces lag while meeting the requirement for centralization.</li><li>D: Increasing memory for Lambda functions or enabling the multipart upload feature may slightly improve upload performance but does not directly address the lag caused by cross-region data transfer.</li></ul><p>Key Benefits of Option C:</p><ul><li>Reduced Latency: Local S3 buckets in the same region as the application minimize the time taken to upload data.</li><li>Centralized Data Storage: S3 Cross-Region Replication ensures that all data is eventually centralized in the eu-north-1 bucket for analysis without manual intervention.</li><li>Scalability: This solution efficiently supports the company’s global expansion without requiring significant changes to existing workflows.</li></ul><p>Thus, Option C is the most effective solution for improving lag time while meeting the requirements.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "f15703bfca404512b85487c15e76fcb2",
      "questionNumber": 431,
      "type": "single",
      "content": "<p>Question #431</p><p>A company provides a centralized Amazon EC2 application hosted in a single shared VPC. The centralized application must be accessible from client applications running in the VPCs of other business units. The centralized application front end is configured with a Network Load Balancer (NLB) for scalability.</p><p><br></p><p>Up to 10 business unit VPCs will need to be connected to the shared VPC. Some of the business unit VPC CIDR blocks overlap with the shared VPC, and some overlap with each other. Network connectivity to the centralized application in the shared VPC should be allowed from authorized business unit VPCs only.<br><br></p><p>Which network configuration should a solutions architect use to provide connectivity from the client applications in the business unit VPCs to the centralized application in the shared VPC?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an AWS Transit Gateway. Attach the shared VPC and the authorized business unit VPCs to the transit gateway. Create a single transit gateway route table and associate it with all of the attached VPCs. Allow automatic propagation of routes from the attachments into the route table. Configure VPC routing tables to send traffic to the transit gateway."
        },
        {
          "label": "B",
          "content": "Create a VPC endpoint service using the centralized application NLB and enable the option to require endpoint acceptance. Create a VPC endpoint in each of the business unit VPCs using the service name of the endpoint service. Accept authorized endpoint requests from the endpoint service console."
        },
        {
          "label": "C",
          "content": "Create a VPC peering connection from each business unit VPC to the shared VPC. Accept the VPC peering connections from the shared VPC console. Configure VPC routing tables to send traffic to the VPC peering connection."
        },
        {
          "label": "D",
          "content": "Configure a virtual private gateway for the shared VPC and create customer gateways for each of the authorized business unit VPCs. Establish a Site-to-Site VPN connection from the business unit VPCs to the shared VPC. Configure VPC routing tables to send traffic to the VPN connection."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Answer: B</p><p>The company hosts a centralized EC2 application in a shared VPC, fronted by a Network Load Balancer (NLB). Multiple business unit VPCs (up to 10) need connectivity to this centralized application. However, the following challenges must be addressed:</p><ol><li>Overlapping CIDR blocks: Some business unit VPC CIDR blocks overlap with the shared VPC or with each other.</li><li>Restrict access: Only authorized business unit VPCs should be able to connect to the centralized application.</li></ol><p>Options Analysis:</p><ul><li>A: Use AWS Transit Gateway (TGW):A Transit Gateway could connect multiple VPCs, but it does not solve the problem of overlapping CIDR blocks. Transit Gateway operates at the IP layer and cannot manage overlapping IP ranges efficiently.</li><li>B: Create a VPC Endpoint Service using the NLB:Creating a VPC Endpoint Service backed by the NLB allows authorized VPCs to connect to the centralized application without IP layer routing. This solves the overlapping CIDR issue because VPC endpoints use private DNS and do not rely on IP addressing.Requiring endpoint acceptance ensures that only authorized VPCs can connect to the centralized application. This satisfies the requirement to restrict access.This option is scalable and avoids the complexity of managing additional route tables or gateways.</li><li>C: VPC Peering Connections:VPC peering does not support overlapping CIDR blocks, making it unsuitable for this scenario.It would also require manual configuration and maintenance of individual peering connections, making it less scalable.</li><li>D: Site-to-Site VPN:Configuring a Site-to-Site VPN for each business unit VPC is unnecessarily complex and costly for this use case.VPN connections are better suited for on-premises connectivity rather than inter-VPC communication.</li></ul><p>Why Option B is Correct:</p><ol><li>Scalability: VPC Endpoint Services can scale easily to handle multiple client VPCs.</li><li>CIDR Overlap Resolution: VPC Endpoint Services operate at the DNS layer and do not depend on IP routing, which eliminates issues caused by overlapping CIDR blocks.</li><li>Access Control: Requiring endpoint acceptance ensures only authorized business units can connect to the application.</li></ol><p>Conclusion:</p><p>Option B is the most effective solution because it addresses overlapping CIDR blocks, supports access control, and provides a scalable way to connect business unit VPCs to the centralized application.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "e41e7a562c3c48618fa0bd27f421b7a4",
      "questionNumber": 432,
      "type": "single",
      "content": "<p>Question #432</p><p>A company wants to migrate its website to AWS. The website uses microservices and runs on containers that are deployed in an on-premises, self-managed Kubernetes cluster. All the manifests that define the deployments for the containers in the Kubernetes deployment are in source control.</p><p><br></p><p>All data for the website is stored in a PostgreSQL database. An open source container image repository runs alongside the on-premises environment.</p><p><br></p><p>A solutions architect needs to determine the architecture that the company will use for the website on AWS.</p><p><br></p><p>Which solution will meet these requirements with the LEAST effort to migrate?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an AWS App Runner service. Connect the App Runner service to the open source container image repository. Deploy the manifests from on premises to the App Runner service. Create an Amazon RDS for PostgreSQL database."
        },
        {
          "label": "B",
          "content": "Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster that has managed node groups. Copy the application containers to a new Amazon Elastic Container Registry (Amazon ECR) repository. Deploy the manifests from on premises to the EKS cluster. Create an Amazon Aurora PostgreSQL DB cluster."
        },
        {
          "label": "C",
          "content": "Create an Amazon Elastic Container Service (Amazon ECS) cluster that has an Amazon EC2 capacity pool. Copy the application containers to a new Amazon Elastic Container Registry (Amazon ECR) repository. Register each container image as a new task definition. Configure ECS services for each task definition to match the original Kubernetes deployments. Create an Amazon Aurora PostgreSQL DB cluster."
        },
        {
          "label": "D",
          "content": "Rebuild the on-premises Kubernetes cluster by hosting the cluster on Amazon EC2 instances. Migrate the open source container image repository to the EC2 instances. Deploy the manifests from on premises to the new cluster on AWS. Deploy an open source PostgreSQL database on the new cluster."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>The correct answer is B. Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster that has managed node groups. Copy the application containers to a new Amazon Elastic Container Registry (Amazon ECR) repository. Deploy the manifests from on premises to the EKS cluster. Create an Amazon Aurora PostgreSQL DB cluster.</p><p> Explanation:</p><p>1. Least Effort Migration: &nbsp;</p><p> &nbsp; - The company already uses Kubernetes (self-managed on-premises), so migrating to Amazon EKS (AWS-managed Kubernetes) requires minimal changes. &nbsp;</p><p> &nbsp; - Kubernetes manifests can be reused as-is (no need to rewrite deployments for ECS or App Runner). &nbsp;</p><p>2. Container Management: &nbsp;</p><p> &nbsp; - Moving containers to Amazon ECR (AWS-managed container registry) is straightforward and more secure than maintaining an open-source repository. &nbsp;</p><p>3. Database Migration: &nbsp;</p><p> &nbsp; - Amazon Aurora PostgreSQL is fully compatible with PostgreSQL and offers better performance, scalability, and managed operations compared to self-managed PostgreSQL. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A (AWS App Runner): &nbsp;</p><p> &nbsp;- App Runner is a simpler PaaS service but does not support Kubernetes manifests, requiring a rewrite of deployment logic. &nbsp;</p><p> &nbsp;- Tying App Runner to an on-premises container registry is not ideal for AWS migration. &nbsp;</p><p>- C (Amazon ECS): &nbsp;</p><p> &nbsp;- ECS is a good alternative but requires converting Kubernetes manifests to ECS task definitions, increasing migration effort. &nbsp;</p><p>- D (Self-managed Kubernetes on EC2): &nbsp;</p><p> &nbsp;- This does not reduce operational overhead—it just moves the same complexity to AWS (still requires managing EC2, Kubernetes, and PostgreSQL manually). &nbsp;</p><p> Conclusion: &nbsp;</p><p>Option B (EKS + ECR + Aurora PostgreSQL) is the best choice because: &nbsp;</p><p>✔ Minimal changes (reuse existing Kubernetes manifests). &nbsp;</p><p>✔ Fully managed services (EKS, ECR, Aurora reduce operational burden). &nbsp;</p><p>✔ Seamless PostgreSQL migration (Aurora is PostgreSQL-compatible). &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "5437654e68f04a9e91b9f0dff05fb54e",
      "questionNumber": 433,
      "type": "single",
      "content": "<p>Question #433</p><p>A company uses a mobile app on AWS to run online contests. The company selects a winner at random at the end of each contest. The contests run for variable lengths of time. The company does not need to retain any data from a contest after the contest is finished.</p><p><br></p><p>The company uses custom code that is hosted on Amazon EC2 instances to process the contest data and select a winner. The EC2 instances run behind an Application Load Balancer and store contest entries on Amazon RDS DB instances. The company must design a new architecture to reduce the cost of running the contests.<br><br>Which solution will meet these requirements MOST cost-effectively? </p>",
      "options": [
        {
          "label": "A",
          "content": "Migrate storage of the contest entries to Amazon DynamoDB. Create a DynamoDB Accelerator (DAX) cluster. Rewrite the code to run as Amazon Elastic Container Service (Amazon ECS) containers that use the Fargate launch type. At the end of the contest, delete the DynamoDB table."
        },
        {
          "label": "B",
          "content": "Migrate the storage of the contest entries to Amazon Redshift. Rewrite the code as AWS Lambda functions. At the end of the contest, delete the Redshift cluster."
        },
        {
          "label": "C",
          "content": "Add an Amazon ElastiCache for Redis cluster in front of the RDS DB instances to cache the contest entries. Rewrite the code to run as Amazon Elastic Container Service (Amazon ECS) containers that use the Fargate launch type. Set the ElastiCache TTL attribute on each entry to expire each entry at the end of the contest."
        },
        {
          "label": "D",
          "content": "Migrate the storage of the contest entries to Amazon DynamoDB. Rewrite the code as AWS Lambda functions. Set the DynamoDB TTL attribute on each entry to expire each entry at the end of the contest."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. Migrate the storage of the contest entries to Amazon DynamoDB. Rewrite the code as AWS Lambda functions. Set the DynamoDB TTL attribute on each entry to expire each entry at the end of the contest.</p><p>This solution will meet the requirements most cost-effectively for the following reasons:</p><p>Pay-per-use Model: By migrating from EC2 instances and RDS databases to Lambda functions and DynamoDB, the company will shift from paying for continuously running resources to paying only for what they use. Since contests run for variable lengths of time, this approach eliminates costs during periods of inactivity.</p><p>Automatic Scaling: Lambda functions automatically scale based on the number of incoming requests, which is ideal for handling variable contest loads without overprovisioning resources. Similarly, DynamoDB can scale to accommodate any number of contest entries.</p><p>Automated Data Cleanup: The DynamoDB Time-to-Live (TTL) feature allows for automatic deletion of data after a specified time. This perfectly aligns with the requirement that \"the company does not need to retain any data from a contest after the contest is finished.\" The TTL attribute can be set to expire entries at the end of each contest, eliminating the need for manual cleanup processes.</p><p>Reduced Operational Overhead: Both Lambda and DynamoDB are fully managed services, which means AWS handles the underlying infrastructure, patching, and maintenance. This reduces the operational burden and associated costs.</p><p>No Idle Resources: With the serverless approach, there are no idle resources to pay for between contests or during periods of low activity. The company only pays for actual compute time used by Lambda and the actual storage and throughput consumed in DynamoDB.</p><p>Simplified Architecture: The serverless architecture eliminates the need to manage load balancers, EC2 instances, and RDS databases, resulting in a simpler, more cost-effective solution that still meets the functional requirements of running contests and selecting winners.</p><p>Efficient Processing: Lambda functions are well-suited for the task of randomly selecting a winner at the end of each contest, as this is a relatively short-lived, event-driven process.</p><p>This solution provides the most cost-effective approach by aligning the cost structure with the actual usage patterns of the contest application, automatically managing resources based on demand, and eliminating unnecessary costs for data retention after contests end.</p><p>Sources</p><p>AWS Well-Architected design considerations - Account Assessment for AWS Organizations （https://docs.aws.amazon.com/solutions/latest/account-assessment-for-aws-organizations/aws-well-architected.html）</p><p>How to Trigger RDS and EC2 to automatically start up | AWS re:Post （https://repost.aws/questions/QUv5qKD3q3QxOWoISBJiy3UA/how-to-trigger-rds-and-ec2-to-automatically-start-up）</p><p>Community | EC2 cost optimization:15 ways to save on Amazon EC2 （https://community.aws/content/2hQNGTh6vpwOHTgKELzL8mkqmKb/ec2-cost-optimization-15-ways-to-save-on-amazon-ec2）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "eb7d4a51c95f46bfbe524ac25b0e57d6",
      "questionNumber": 434,
      "type": "single",
      "content": "<p>Question #434</p><p>A company has implemented a new security requirement. According to the new requirement, the company must scan all traffic from corporate AWS instances in the company's VPC for violations of the company's security policies. As a result of these scans, the company can block access to and from specific IP addresses.</p><p><br></p><p>To meet the new requirement, the company deploys a set of Amazon EC2 instances in private subnets to serve as transparent proxies. The company installs approved proxy server software on these EC2 instances. The company modifies the route tables on all subnets to use the corresponding EC2 instances with proxy software as the default route. The company also creates security groups that are compliant with the security policies and assigns these security groups to the EC2 instances.</p><p><br></p><p>Despite these configurations, the traffic of the EC2 instances in their private subnets is not being properly forwarded to the internet.<br><br>What should a solutions architect do to resolve this issue? </p>",
      "options": [
        {
          "label": "A",
          "content": "Disable source/destination checks on the EC2 instances that run the proxy software."
        },
        {
          "label": "B",
          "content": "Add a rule to the security group that is assigned to the proxy EC2 instances to allow all traffic between instances that have this security group. Assign this security group to all EC2 instances in the VPC."
        },
        {
          "label": "C",
          "content": "Change the VPCs DHCP options set. Set the DNS server options to point to the addresses of the proxy EC2 instances."
        },
        {
          "label": "D",
          "content": "Assign one additional elastic network interface to each proxy EC2 instance. Ensure that one of these network interfaces has a route to the private subnets. Ensure that the other network interface has a route to the internet."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>The correct answer is A. Disable source/destination checks on the EC2 instances that run the proxy software.</p><p> Explanation:</p><p>1. Problem Identification:</p><p> &nbsp; - The proxy EC2 instances are deployed as transparent proxies, meaning they intercept and forward traffic for other instances in the VPC.</p><p> &nbsp; - By default, EC2 instances perform source/destination checks, meaning they drop traffic that is not explicitly destined for or originating from their own IP addresses.</p><p> &nbsp; - Since the proxies are forwarding traffic on behalf of other instances, the source/destination checks must be disabled to allow this behavior.</p><p>2. Why Option A Works:</p><p> &nbsp; - Disabling source/destination checks allows the proxy instances to handle traffic that is not destined for their own IPs, which is necessary for transparent proxying.</p><p> &nbsp; - This is a well-documented requirement for EC2 instances acting as NAT instances, firewalls, or proxies.</p><p>3. Why Not the Other Options?</p><p> &nbsp; - B (Security group rules for all traffic): &nbsp;</p><p> &nbsp; &nbsp; - While security groups must allow traffic between instances, this alone does not solve the source/destination check issue.</p><p> &nbsp; - C (Modify DHCP options set for DNS): &nbsp;</p><p> &nbsp; &nbsp; - This is unrelated to the forwarding of internet-bound traffic through proxies.</p><p> &nbsp; - D (Add additional ENIs with specific routes): &nbsp;</p><p> &nbsp; &nbsp; - While multi-homed proxies can be used in some architectures, this is not necessary for basic transparent proxying and does not address the core issue (source/destination checks).</p><p> Conclusion:</p><p>Option A is the correct solution because it directly addresses the source/destination check issue, which is the primary reason traffic is not being forwarded properly. Disabling this feature allows the proxy instances to handle traffic for other instances in the VPC.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "7dc9e972f6124922aef28e4d80b6d95f",
      "questionNumber": 435,
      "type": "single",
      "content": "<p>Question #435</p><p>A company is running its solution on AWS in a manually created VPC. The company is using AWS CloudFormation to provision other parts of the infrastructure. According to a new requirement, the company must manage all infrastructure in an automatic way.<br><br>What should the company do to meet this new requirement with the LEAST effort?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a new AWS Cloud Development Kit (AWS CDK) stack that strictly provisions the existing VPC resources and configuration. Use AWS CDK to import the VPC into the stack and to manage the VPC."
        },
        {
          "label": "B",
          "content": "Create a CloudFormation stack set that creates the VPC. Use the stack set to import the VPC into the stack."
        },
        {
          "label": "C",
          "content": "Create a new CloudFormation template that strictly provisions the existing VPC resources and configuration. From the CloudFormation console, create a new stack by importing the Existing resources."
        },
        {
          "label": "D",
          "content": "Create a new CloudFormation template that creates the VPC. Use the AWS Serverless Application Model (AWS SAM) CLI to import the VPC."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Create a new CloudFormation template that strictly provisions the existing VPC resources and configuration. From the CloudFormation console, create a new stack by importing the Existing resources.</p><p>This solution will meet the requirement to manage all infrastructure in an automatic way with the least effort for the following reasons:</p><p>Direct Import Capability: CloudFormation provides a built-in feature specifically designed for importing existing resources into a stack. This \"Import resources into stack\" feature allows you to bring manually created resources under CloudFormation management without having to recreate them.</p><p>No Additional Tools Required: Unlike option A (using AWS CDK), this approach doesn't require setting up and learning an additional tool or programming model. The company is already using CloudFormation for other parts of their infrastructure, so continuing with CloudFormation maintains consistency.</p><p>Simplified Process: The CloudFormation console provides a guided workflow for importing existing resources:</p><p>Create a template that matches the existing VPC configuration</p><p>Use the \"Create stack\" &gt; \"With existing resources (import resources)\" option</p><p>Select the resources to import</p><p>Complete the import process</p><p>Preservation of Existing Resources: This approach doesn't require creating new resources or making changes to the existing VPC. It simply brings the existing VPC under CloudFormation management.</p><p>Integration with Existing Infrastructure: Since the company is already using CloudFormation for other parts of their infrastructure, this approach allows for seamless integration between the newly imported VPC and the existing CloudFormation-managed resources.</p><p>Option A (using AWS CDK) would require additional effort to set up the CDK environment, learn the CDK programming model, and write code to define the infrastructure, which is more complex than using CloudFormation directly.</p><p>Option B (using CloudFormation stack sets) is not appropriate for this scenario. Stack sets are designed for deploying stacks across multiple accounts and regions, not for importing existing resources. This approach would likely create new resources rather than importing existing ones.</p><p>By creating a CloudFormation template that matches the existing VPC configuration and using the import feature, the company can bring their manually created VPC under automated management with minimal effort while preserving their existing infrastructure.</p><p>Sources</p><p>Import entire applications into AWS CloudFormation | AWS DevOps Blog （https://aws.amazon.com/cn/blogs/devops/import-entire-applications-into-aws-cloudformation/）</p><p>How can I move a serverless VPC endpoint to another CloudFormation stack? | AWS re:Post （https://repost.aws/questions/QUKDx-TuTYRXa1wzy_xOfMyA/how-can-i-move-a-serverless-vpc-endpoint-to-another-cloudformation-stack）</p><p>Import AWS resources into a CloudFormation stack - AWS CloudFormation （https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/import-resources.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "2c2c8a7906f844b28d0a233dbb1da685",
      "questionNumber": 436,
      "type": "single",
      "content": "<p>Question #436</p><p>A company has developed a new release of a popular video game and wants to make it available for public download. The new release package is approximately 5 GB in size. The company provides downloads for existing releases from a Linux-based, publicly facing FTP site hosted in an on-premises data center. The company expects the new release will be downloaded by users worldwide. The company wants a solution that provides improved download performance and low transfer costs, regardless of a user's location.</p>",
      "options": [
        {
          "label": "A",
          "content": "Store the game files on Amazon EBS volumes mounted on Amazon EC2 instances within an Auto Scaling group. Configure an FTP service on the EC2 instances. Use an Application Load Balancer in front of the Auto Scaling group. Publish the game download URL for users to download the package."
        },
        {
          "label": "B",
          "content": "Store the game files on Amazon EFS volumes that are attached to Amazon EC2 instances within an Auto Scaling group. Configure an FTP service on each of the EC2 instances. Use an Application Load Balancer in front of the Auto Scaling group. Publish the game download URL for users to download the package."
        },
        {
          "label": "C",
          "content": "Configure Amazon Route 53 and an Amazon S3 bucket for website hosting. Upload the game files to the S3 bucket. Use Amazon CloudFront for the website. Publish the game download URL for users to download the package."
        },
        {
          "label": "D",
          "content": "Configure Amazon Route 53 and an Amazon S3 bucket for website hosting. Upload the game files to the S3 bucket. Set Requester Pays for the S3 bucket. Publish the game download URL for users to download the package."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Configure Amazon Route 53 and an Amazon S3 bucket for website hosting. Upload the game files to the S3 bucket. Use Amazon CloudFront for the website. Publish the game download URL for users to download the package.</p><p>This solution will provide improved download performance and low transfer costs for the video game release for the following reasons:</p><p>Global Content Delivery Network: Amazon CloudFront is a content delivery network (CDN) with a global network of edge locations. When users request the game download, CloudFront delivers it from the edge location that provides the lowest latency, significantly improving download performance regardless of the user's location.</p><p>Cost-Effective Data Transfer: CloudFront offers cost-effective data transfer rates, especially for large files like the 5GB game package. CloudFront's pricing model includes:</p><p>Reduced data transfer costs compared to serving directly from S3</p><p>Regional data transfer pricing that can be more economical for global distribution</p><p>Caching at edge locations that reduces origin fetches and associated costs</p><p>Scalability: This architecture can easily handle the high demand expected for a popular video game release. S3 provides virtually unlimited storage capacity, while CloudFront can scale to handle millions of concurrent users downloading the game.</p><p>Simple Implementation: The solution is straightforward to implement:</p><p>Amazon S3 provides reliable storage for the game files</p><p>Route 53 handles DNS resolution to direct users to the CloudFront distribution</p><p>CloudFront distributes the content globally with minimal configuration</p><p>Improved Reliability: Moving from an on-premises FTP server to AWS cloud services increases reliability and availability. The solution eliminates single points of failure that might exist in the current on-premises setup.</p><p>Support for Large Files: The solution is well-suited for distributing large files like the 5GB game package. CloudFront supports efficient delivery of large objects and can handle partial downloads if users need to resume interrupted downloads.</p><p>Reduced Origin Load: By caching content at edge locations, CloudFront reduces the load on the origin S3 bucket, which helps maintain consistent performance even during peak download periods.</p><p>This architecture represents a significant improvement over the current on-premises FTP solution, providing better global performance, lower costs, and improved reliability for distributing the new game release to users worldwide.</p><p>Sources</p><p>Speeding up your Amazon S3-based website using Amazon CloudFront - Hosting Static Websites on AWS （https://docs.aws.amazon.com/whitepapers/latest/build-static-websites-aws/speeding-up-your-amazon-s3-based-website-using-amazon-cloudfront.html）</p><p>Cost-effective resources - Games Industry Lens （https://docs.aws.amazon.com/wellarchitected/latest/games-industry-lens/games-cost-cost-eff-resc.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "7a4db8ec664741bc89f0b3c028d32e9a",
      "questionNumber": 437,
      "type": "multiple",
      "content": "<p>Question #437</p><p>A company runs an application in the cloud that consists of a database and a website. Users can post data to the website, have the data processed, and have the data sent back to them in an email. Data is stored in a MySQL database running on an Amazon EC2 instance. The database is running in a VPC with two private subnets. The website is running on Apache Tomcat in a single EC2 instance in a different VPC with one public subnet. There is a single VPC peering connection between the database and website VPC.</p><p><br></p><p>The website has suffered several outages during the last month due to high traffic.<br><br>Which actions should a solutions architect take to increase the reliability of the application? (Choose three.) </p>",
      "options": [
        {
          "label": "A",
          "content": "Place the Tomcat server in an Auto Scaling group with multiple EC2 instances behind an Application<p>&nbsp;Load Balancer.</p>"
        },
        {
          "label": "B",
          "content": "Provision an additional VPC peering connection."
        },
        {
          "label": "C",
          "content": "Migrate the MySQL database to Amazon Aurora with one Aurora Replica."
        },
        {
          "label": "D",
          "content": "Provision two NAT gateways in the database VPC."
        },
        {
          "label": "E",
          "content": "Move the Tomcat server to the database VPC."
        },
        {
          "label": "F",
          "content": "Create an additional public subnet in a different Availability Zone in the website VPC."
        }
      ],
      "correctAnswer": "ACF",
      "explanation": "<p>1. A. Place the Tomcat server in an Auto Scaling group with multiple EC2 instances behind an Application Load Balancer. &nbsp;</p><p> &nbsp; - Why? The website outages are caused by high traffic. Auto Scaling ensures the application can scale horizontally to handle increased load, while an ALB distributes traffic across instances. &nbsp;</p><p> &nbsp; - Impact: Improves fault tolerance and availability by preventing single-instance failures.</p><p>2. C. Migrate the MySQL database to Amazon Aurora with one Aurora Replica. &nbsp;</p><p> &nbsp; - Why? The current single EC2-based MySQL database is a single point of failure. Aurora provides high availability with automatic failover to a replica in another AZ. &nbsp;</p><p> &nbsp; - Impact: Increases database reliability and performance while reducing management overhead.</p><p>3. F. Create an additional public subnet in a different Availability Zone in the website VPC. &nbsp;</p><p> &nbsp; - Why? The website runs in a single public subnet, making it vulnerable to AZ failures. Adding a subnet in another AZ improves redundancy when combined with Auto Scaling. &nbsp;</p><p> &nbsp; - Impact: Ensures multi-AZ resilience for the web tier.</p><p>---</p><p> Why Not the Other Options?</p><p>- B (Additional VPC peering connection): &nbsp;</p><p> &nbsp;- VPC peering is not a bottleneck in this scenario (only one connection is needed). &nbsp;</p><p>- D (NAT gateways in the database VPC): &nbsp;</p><p> &nbsp;- The database is in private subnets and likely doesn’t need NAT for outbound internet access. This doesn’t address website outages. &nbsp;</p><p>- E (Move Tomcat to the database VPC): &nbsp;</p><p> &nbsp;- This doesn’t improve reliability—it just consolidates resources but doesn’t solve scaling or high availability issues. &nbsp;</p><p>---</p><p> Summary of Correct Actions: &nbsp;</p><p>✔ A (Auto Scaling + ALB) → Handles traffic spikes. &nbsp;</p><p>✔ C (Aurora with replica) → Eliminates database single point of failure. &nbsp;</p><p>✔ F (Multi-AZ subnets) → Ensures web tier redundancy. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "efb60c8400ab410dbd6d338efad1706b",
      "questionNumber": 438,
      "type": "multiple",
      "content": "<p>Question #438</p><p>A retail company is operating its ecommerce application on AWS. The application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The company uses an Amazon RDS DB instance as the database backend. Amazon CloudFront is configured with one origin that points to the ALB. Static content is cached. Amazon Route 53 is used to host all public zones.</p><p><br></p><p>After an update of the application, the ALB occasionally returns a 502 status code (Bad Gateway) error. The root cause is malformed HTTP headers that are returned to the ALB. The webpage returns successfully when a solutions architect reloads the webpage immediately after the error occurs.</p><p><br></p><p>While the company is working on the problem, the solutions architect needs to provide a custom error page instead of the standard ALB error page to visitors.<br><br>Which combination of steps will meet this requirement with the LEAST amount of operational overhead? (Choose two.) </p>",
      "options": [
        {
          "label": "A",
          "content": "Create an Amazon S3 bucket. Configure the S3 bucket to host a static webpage. Upload the custom error pages to Amazon S3."
        },
        {
          "label": "B",
          "content": "Create an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health check response Target.FailedHealthChecks is greater than 0. Configure the Lambda function to modify the forwarding rule at the ALB to point to a publicly accessible web server."
        },
        {
          "label": "C",
          "content": "Modify the existing Amazon Route 53 records by adding health checks. Configure a fallback target if the health check fails. Modify DNS records to point to a publicly accessible webpage."
        },
        {
          "label": "D",
          "content": "Create an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health check response Elb.InternalError is greater than 0. Configure the Lambda function to modify the forwarding rule at the ALB to point to a public accessible web server."
        },
        {
          "label": "E",
          "content": "Add a custom error response by configuring a CloudFront custom error page. Modify DNS records to point to a publicly accessible webpage."
        }
      ],
      "correctAnswer": "AE",
      "explanation": "<p> A. Create an Amazon S3 bucket. Configure the S3 bucket to host a static webpage. Upload the custom error pages to Amazon S3.</p><p>- Why? &nbsp;</p><p> &nbsp;- S3 is a simple, cost-effective way to host static error pages (e.g., `502.html`). &nbsp;</p><p> &nbsp;- Requires minimal operational overhead—just upload the custom error page and enable static website hosting. &nbsp;</p><p> &nbsp;- Works seamlessly with CloudFront (next step). &nbsp;</p><p> E. Add a custom error response by configuring a CloudFront custom error page. Modify DNS records to point to a publicly accessible web page.</p><p>- Why? &nbsp;</p><p> &nbsp;- CloudFront allows custom error page configurations (e.g., redirecting `502` errors to a static S3 page). &nbsp;</p><p> &nbsp;- Instead of showing the default ALB error, CloudFront can serve a friendly error page from S3. &nbsp;</p><p> &nbsp;- No need to modify ALB or RDS—this solution works at the CDN layer. &nbsp;</p><p> Why Not the Other Options?</p><p>- B & D (CloudWatch + Lambda + ALB modification): &nbsp;</p><p> &nbsp;- Overly complex for this use case. &nbsp;</p><p> &nbsp;- Requires real-time ALB rule changes, which introduce latency and potential failures. &nbsp;</p><p> &nbsp;- Higher operational overhead than using S3 + CloudFront. &nbsp;</p><p>- C (Route 53 health checks + DNS failover): &nbsp;</p><p> &nbsp;- DNS changes take time to propagate, making this unsuitable for transient `502` errors. &nbsp;</p><p> &nbsp;- Health checks won’t react fast enough for intermittent issues. &nbsp;</p><p> Summary of Correct Steps: &nbsp;</p><p>1. Host the custom error page in S3 (A). &nbsp;</p><p>2. Configure CloudFront to serve the custom error page when ALB returns `502` (E). &nbsp;</p><p>This approach is simple, scalable, and low-maintenance, meeting the requirement with the least operational overhead. &nbsp;</p><p>Thus, A and E are the correct answers.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "b04a0d7d9552483f955f4643485ee36a",
      "questionNumber": 439,
      "type": "multiple",
      "content": "<p>Question #439</p><p>A company wants to migrate an Amazon Aurora MySQL DB cluster from an existing AWS account to a new AWS account in the same AWS Region. Both accounts are members of the same organization in AWS Organizations.<br><br>The company must minimize database service interruption before the company performs DNS cutover to the new database.<br><br>Which migration strategy will meet this requirement? (Choose two.) </p>",
      "options": [
        {
          "label": "A",
          "content": "Take a snapshot of the existing Aurora database. Share the snapshot with the new AWS account. Create an Aurora DB cluster in the new account from the snapshot."
        },
        {
          "label": "B",
          "content": "Create an Aurora DB cluster in the new AWS account. Use AWS Database Migration Service (AWS DMS) to migrate data between the two Aurora DB clusters."
        },
        {
          "label": "C",
          "content": "Use AWS Backup to share an Aurora database backup from the existing AWS account to the new AWS account. Create an Aurora DB cluster in the new AWS account from the snapshot."
        },
        {
          "label": "D",
          "content": "Create an Aurora DB cluster in the new AWS account. Use AWS Application Migration Service to migrate data between the two Aurora DB clusters."
        }
      ],
      "correctAnswer": "AB",
      "explanation": "<p>Based on the information provided, the correct answers are:</p><p>A. Take a snapshot of the existing Aurora database. Share the snapshot with the new AWS account. Create an Aurora DB cluster in the new account from the snapshot.</p><p>B. Create an Aurora DB cluster in the new AWS account. Use AWS Database Migration Service (AWS DMS) to migrate data between the two Aurora DB clusters.</p><p>Both of these migration strategies will help minimize database service interruption before performing DNS cutover to the new database. Let me explain why each is appropriate:</p><p>Option A: Snapshot-based migration</p><p>This approach allows you to create a point-in-time copy of your entire Aurora database</p><p>Since both accounts are in the same AWS Organization, you can easily share snapshots between accounts</p><p>The snapshot restore process creates an exact replica of your database structure and data</p><p>While there is some downtime required to take the final snapshot, this approach is relatively simple to implement</p><p>After restoring the snapshot in the new account, you can set up replication from the source database to catch any changes that occurred during the migration process</p><p>This method is particularly efficient for databases that aren't extremely large or don't have extremely high transaction rates</p><p>Option B: AWS DMS-based migration</p><p>AWS Database Migration Service allows for continuous data replication between the source and target databases</p><p>You can set up initial data load and ongoing change data capture (CDC) to keep the databases in sync</p><p>This approach can achieve near-zero downtime as it continuously replicates changes from the source to the target</p><p>DMS supports Aurora MySQL as both source and target endpoints</p><p>When it's time to cut over, the target database will already be in sync with the source, minimizing interruption</p><p>This method works well for databases of any size and is especially valuable for databases with high transaction volumes</p><p>Both strategies allow you to:</p><p>Prepare the target database in the new account while the source database continues to operate</p><p>Minimize the cutover window by having the data already migrated or continuously replicating</p><p>Perform testing on the new database before the final cutover</p><p>Execute a clean DNS cutover with minimal service interruption</p><p>The company could even use a hybrid approach, using the snapshot method for the initial bulk data transfer and then setting up DMS to replicate any changes that occur after the snapshot is taken. This would combine the simplicity of the snapshot approach with the continuous replication benefits of DMS.</p><p>Either way, both options A and B are valid strategies that would meet the requirement to minimize database service interruption before performing the DNS cutover to the new database.</p><p>Sources</p><p>Perform cross-account Amazon Aurora MySQL migration with minimal downtime | AWS Database Blog （https://aws.amazon.com/cn/blogs/database/perform-cross-account-amazon-aurora-mysql-migration-with-minimal-downtime/）</p><p>Migrating from Amazon RDS for MySQL - Amazon Aurora MySQL Migration Handbook （https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.AuroraMySQL.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "3db3c16a1ae3412eaf4ae69182ef3967",
      "questionNumber": 440,
      "type": "multiple",
      "content": "<p>Question #440</p><p>A software as a service (SaaS) company provides a media software solution to customers. The solution is hosted on 50 VPCs across various AWS Regions and AWS accounts. One of the VPCs is designated as a management VPC. The compute resources in the VPCs work independently.</p><p><br></p><p>The company has developed a new feature that requires all 50 VPCs to be able to communicate with each other. The new feature also requires one-way access from each customer's VPC to the company's management VPC. The management VPC hosts a compute resource that validates licenses for the media software solution.</p><p><br></p><p>The number of VPCs that the company will use to host the solution will continue to increase as the solution grows.<br><br>Which combination of steps will provide the required VPC connectivity with the LEAST operational overhead? (Choose two.) </p>",
      "options": [
        {
          "label": "A",
          "content": "Create a transit gateway. Attach all the company&#39;s VPCs and relevant subnets to the transit gateway."
        },
        {
          "label": "B",
          "content": "Create VPC peering connections between all the company&#39;s VPCs."
        },
        {
          "label": "C",
          "content": "Create a Network Load Balancer (NLB) that points to the compute resource for license validation. Create an AWS PrivateLink endpoint service that is available to each customer&#39;s VPC. Associate the endpoint service with the NLB."
        },
        {
          "label": "D",
          "content": "Create a VPN appliance in each customer&#39;s VPC. Connect the company&#39;s management VPC to each customer&#39;s VPC by using AWS Site-to-Site VPN."
        },
        {
          "label": "E",
          "content": "Create a VPC peering connection between the company&#39;s management VPC and each customer&#39;s VPC."
        }
      ],
      "correctAnswer": "AC",
      "explanation": "<p> A. Create a transit gateway. Attach all the company's VPCs and relevant subnets to the transit gateway.</p><p>- Why? &nbsp;</p><p> &nbsp;- AWS Transit Gateway is designed for scalable, hub-and-spoke VPC connectivity across multiple accounts and regions. &nbsp;</p><p> &nbsp;- It eliminates the need for complex VPC peering meshes (which become unmanageable as the number of VPCs grows). &nbsp;</p><p> &nbsp;- Supports dynamic routing and simplifies network management as new VPCs are added. &nbsp;</p><p> C. Create a Network Load Balancer (NLB) that points to the compute resource for license validation. Create an AWS PrivateLink endpoint service that is available to each customer's VPC. Associate the endpoint service with the NLB.</p><p>- Why? &nbsp;</p><p> &nbsp;- PrivateLink provides secure, one-way access from customer VPCs to the management VPC (license validation). &nbsp;</p><p> &nbsp;- Avoids exposing the management VPC to the internet or requiring complex VPN setups. &nbsp;</p><p> &nbsp;- Scales automatically as new customers are onboarded. &nbsp;</p><p> Why Not the Other Options?</p><p>- B (VPC peering between all VPCs): &nbsp;</p><p> &nbsp;- Does not scale—50+ VPCs would require a full mesh of peering connections, which is operationally prohibitive. &nbsp;</p><p> &nbsp;- Manual management is required for each new VPC. &nbsp;</p><p>- D (Site-to-Site VPN per customer VPC): &nbsp;</p><p> &nbsp;- High operational overhead—each VPN requires manual setup and maintenance. &nbsp;</p><p> &nbsp;- Not scalable for a growing SaaS solution. &nbsp;</p><p>- E (VPC peering between management VPC and each customer VPC): &nbsp;</p><p> &nbsp;- Limited to 125 peering connections per VPC, which is insufficient for large-scale growth. &nbsp;</p><p> &nbsp;- Manual effort for each new customer VPC. &nbsp;</p><p> Summary of Correct Steps: &nbsp;</p><p>1. Use Transit Gateway (A) for scalable, centralized communication between all SaaS VPCs. &nbsp;</p><p>2. Use PrivateLink (C) for secure, one-way access to the management VPC (license validation). &nbsp;</p><p>This approach provides scalability, security, and minimal operational overhead as the SaaS solution grows. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "e066aaa8b163461683b24010be0e9ed9",
      "questionNumber": 441,
      "type": "multiple",
      "content": "<p>Question #441</p><p>A company has multiple lines of business (LOBs) that roll up to the parent company. The company has asked its solutions architect to develop a solution with the following requirements:</p><p><br></p><p>• Produce a single AWS invoice for all of the AWS accounts used by its LOBs.</p><p>• The costs for each LOB account should be broken out on the invoice.</p><p>• Provide the ability to restrict services and features in the LOB accounts, as defined by the company's governance policy.</p><p>• Each LOB account should be delegated full administrator permissions, regardless of the governance policy.<br><br>Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Use AWS Organizations to create an organization in the parent account for each LOB. Then invite each LOB account to the appropriate organization."
        },
        {
          "label": "B",
          "content": "Use AWS Organizations to create a single organization in the parent account. Then, invite each LOB&#39;s AWS account to join the organization."
        },
        {
          "label": "C",
          "content": "Implement service quotas to define the services and features that are permitted and apply the quotas to each LOB, as appropriate."
        },
        {
          "label": "D",
          "content": "Create an SCP that allows only approved services and features, then apply the policy to the LOB accounts."
        },
        {
          "label": "E",
          "content": "Enable consolidated billing in the parent account&#39;s billing console and link the LOB accounts."
        }
      ],
      "correctAnswer": "BD",
      "explanation": "<p> B. Use AWS Organizations to create a single organization in the parent account. Then, invite each LOB's AWS account to join the organization.</p><p>- Why? &nbsp;</p><p> &nbsp;- AWS Organizations allows the parent company to consolidate billing under a single invoice while breaking out costs per LOB. &nbsp;</p><p> &nbsp;- Provides a centralized way to manage multiple accounts under a single organization. &nbsp;</p><p> &nbsp;- Supports Service Control Policies (SCPs) for governance (next step). &nbsp;</p><p> D. Create an SCP that allows only approved services and features, then apply the policy to the LOB accounts.</p><p>- Why? &nbsp;</p><p> &nbsp;- Service Control Policies (SCPs) enforce governance by restricting services/features in LOB accounts while still allowing full admin permissions within allowed boundaries. &nbsp;</p><p> &nbsp;- Ensures compliance with company policies without limiting LOB admins' control over permitted services. &nbsp;</p><p> Why Not the Other Options?</p><p>- A (Multiple Organizations per LOB): &nbsp;</p><p> &nbsp;- Incorrect: AWS allows only one organization per account hierarchy. Creating multiple organizations is not feasible. &nbsp;</p><p>- C (Service Quotas): &nbsp;</p><p> &nbsp;- Service quotas limit usage (e.g., EC2 instance count) but do not restrict services/features like SCPs do. &nbsp;</p><p>- E (Consolidated Billing Only): &nbsp;</p><p> &nbsp;- Consolidated billing is outdated (replaced by AWS Organizations). &nbsp;</p><p> &nbsp;- Does not provide governance controls (SCPs are needed). &nbsp;</p><p> Summary of Correct Steps: &nbsp;</p><p>1. Set up AWS Organizations (B) to consolidate billing and manage accounts. &nbsp;</p><p>2. Apply SCPs (D) to enforce governance while allowing LOB admins full access within defined limits. &nbsp;</p><p>This approach meets all requirements with minimal operational overhead. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "aa7c27f1d53a45cdbbe16bebed9cd7b7",
      "questionNumber": 442,
      "type": "multiple",
      "content": "<p>Question #442</p><p>A solutions architect has deployed a web application that serves users across two AWS Regions under a custom domain. The application uses Amazon Route 53 latency-based routing. The solutions architect has associated weighted record sets with a pair of web servers in separate Availability Zones for each Region.</p><p><br></p><p>The solutions architect runs a disaster recovery scenario. When all the web servers in one Region are stopped, Route 53 does not automatically redirect users to the other Region.<br><br>Which of the following are possible root causes of this issue? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "The weight for the Region where the web servers were stopped is higher than the weight for the other Region."
        },
        {
          "label": "B",
          "content": "One of the web servers in the secondary Region did not pass its HTTP health check."
        },
        {
          "label": "C",
          "content": "Latency resource record sets cannot be used in combination with weighted resource record sets."
        },
        {
          "label": "D",
          "content": "The setting to evaluate target health is not turned on for the latency alias resource record set that is associated with the domain in the Region where the web servers were stopped."
        },
        {
          "label": "E",
          "content": "An HTTP health check has not been set up for one or more of the weighted resource record sets associated with the stopped web servers."
        }
      ],
      "correctAnswer": "DE",
      "explanation": "<p>Based on the information provided, the correct answers are:</p><p>D. The setting to evaluate target health is not turned on for the latency alias resource record set that is associated with the domain in the Region where the web servers were stopped.</p><p>E. An HTTP health check has not been set up for one or more of the weighted resource record sets associated with the stopped web servers.</p><p>These are the two possible root causes of the issue where Route 53 is not automatically redirecting users to the other Region during the disaster recovery scenario. Let me explain why:</p><p>Evaluate Target Health Setting (Option D):</p><p>In a Route 53 configuration with latency-based routing, the \"Evaluate Target Health\" setting is crucial for failover functionality.</p><p>When this setting is enabled on a latency alias record, Route 53 checks the health of the underlying resources (in this case, the weighted record sets for the web servers).</p><p>If this setting is turned off, Route 53 will continue to route traffic based solely on latency calculations, regardless of whether the resources in that Region are actually available.</p><p>Without evaluating target health, Route 53 has no way to know that the web servers in one Region have been stopped, so it continues to direct some users to that Region based on latency.</p><p>Missing HTTP Health Checks (Option E):</p><p>For Route 53 to determine if resources are healthy, specific health checks must be configured and associated with the resource record sets.</p><p>In this architecture, the weighted record sets for individual web servers need HTTP health checks to monitor their availability.</p><p>Without these health checks, Route 53 cannot detect that the web servers have been stopped.</p><p>Even if \"Evaluate Target Health\" is enabled, without proper health checks, Route 53 has no data to evaluate and will assume the resources are healthy.</p><p>The combination of these two issues prevents the automatic failover mechanism from working properly. When the web servers in one Region are stopped:</p><p>Without health checks (Option E), Route 53 doesn't know the servers are down.</p><p>Without evaluating target health (Option D), Route 53 doesn't consider health status in its routing decisions.</p><p>To fix this issue, the solutions architect would need to:</p><p>Enable the \"Evaluate Target Health\" setting for the latency alias resource record sets</p><p>Set up HTTP health checks for all weighted resource record sets associated with the web servers</p><p>Associate these health checks with the appropriate record sets</p><p>With these configurations in place, Route 53 would detect when servers in one Region are unavailable and automatically redirect traffic to the healthy servers in the other Region, ensuring the disaster recovery mechanism works as expected.</p><p>Sources</p><p>How health checks work in complex Amazon Route 53 configurations - Amazon Route 53 （https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-complex-configs.html）</p><p>Fast Failover using Active-Passive failover policy | AWS re:Post （https://repost.aws/questions/QUyGuTcle8Q1-GsnNKThE6zQ/fast-failover-using-active-passive-failover-policy）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "7620d85946244bc982c2f2e472a4f282",
      "questionNumber": 443,
      "type": "single",
      "content": "<p>Question #443</p><p>A flood monitoring agency has deployed more than 10,000 water-level monitoring sensors. Sensors send continuous data updates, and each update is less than 1 MB in size. The agency has a fleet of on-premises application servers. These servers receive updates from the sensors, convert the raw data into a human-readable format, and write the results to an on-premises relational database server. Data analysts then use simple SQL queries to monitor the data.</p><p><br></p><p>The agency wants to increase overall application availability and reduce the effort that is required to perform maintenance tasks. These maintenance tasks, which include updates and patches to the application servers, cause downtime. While an application server is down, data is lost from sensors because the remaining servers cannot handle the entire workload.</p><p><br></p><p>The agency wants a solution that optimizes operational overhead and costs. A solutions architect recommends the use of AWS IoT Core to collect the sensor data.</p><p><br></p><p>What else should the solutions architect recommend to meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Send the sensor data to Amazon Kinesis Data Firehose. Use an AWS Lambda function to read the Kinesis Data Firehose data, convert it to .csv format, and insert it into an Amazon Aurora MySQL DB instance. Instruct the data analysts to query the data directly from the DB instance."
        },
        {
          "label": "B",
          "content": "Send the sensor data to Amazon Kinesis Data Firehose. Use an AWS Lambda function to read the Kinesis Data Firehose data, convert it to Apache Parquet format, and save it to an Amazon S3 bucket. Instruct the data analysts to query the data by using Amazon Athena."
        },
        {
          "label": "C",
          "content": "Send the sensor data to an Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) application to convert the data to .csv format and store it in an Amazon S3 bucket. Import the data into an Amazon Aurora MySQL DB instance. Instruct the data analysts to query the data directly from the DB instance."
        },
        {
          "label": "D",
          "content": "Send the sensor data to an Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) application to convert the data to Apache Parquet format and store it in an Amazon S3 bucket. Instruct the data analysts to query the data by using Amazon Athena."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>The correct answer is B. Send the sensor data to Amazon Kinesis Data Firehose. Use an AWS Lambda function to read the Kinesis Data Firehose data, convert it to Apache Parquet format, and save it to an Amazon S3 bucket. Instruct the data analysts to query the data by using Amazon Athena.</p><p> Explanation:</p><p>1. Problem Requirements:</p><p> &nbsp; - High availability & reduced maintenance: Eliminate downtime caused by on-premises server maintenance.</p><p> &nbsp; - Handle 10,000+ sensors: Scalable ingestion of continuous, small (&lt;1 MB) data updates.</p><p> &nbsp; - Cost & operational efficiency: Minimize overhead while ensuring analysts can query data easily.</p><p>2. Why Option B Works Best:</p><p> &nbsp; - AWS IoT Core + Kinesis Data Firehose: &nbsp;</p><p> &nbsp; &nbsp; - IoT Core scales seamlessly for 10,000+ sensors. &nbsp;</p><p> &nbsp; &nbsp; - Firehose buffers and batches data efficiently, reducing costs. &nbsp;</p><p> &nbsp; - Lambda for transformation: &nbsp;</p><p> &nbsp; &nbsp; - Converts raw data to Parquet (columnar format), optimizing storage and query performance. &nbsp;</p><p> &nbsp; - Amazon S3 + Athena: &nbsp;</p><p> &nbsp; &nbsp; - Serverless architecture (no maintenance, high availability). &nbsp;</p><p> &nbsp; &nbsp; - Analysts use simple SQL queries (familiar, no changes needed). &nbsp;</p><p> &nbsp; &nbsp; - Cost-effective (pay per query, no database provisioning). &nbsp;</p><p>3. Why Not the Other Options?</p><p> &nbsp; - A (Aurora MySQL): &nbsp;</p><p> &nbsp; &nbsp; - Operational overhead (database management, scaling, patching). &nbsp;</p><p> &nbsp; &nbsp; - Higher cost for continuous writes from 10,000 sensors. &nbsp;</p><p> &nbsp; - C (Managed Flink → CSV → Aurora): &nbsp;</p><p> &nbsp; &nbsp; - Overkill for simple transformations (Flink is for complex streaming analytics). &nbsp;</p><p> &nbsp; &nbsp; - CSV is less efficient than Parquet for analytics. &nbsp;</p><p> &nbsp; &nbsp; - Aurora adds unnecessary cost/complexity. &nbsp;</p><p> &nbsp; - D (Managed Flink → Parquet + Athena): &nbsp;</p><p> &nbsp; &nbsp; - Flink is excessive for basic data conversion (Lambda is simpler/cheaper). &nbsp;</p><p> Key Benefits of Option B:</p><p>✔ Fully serverless (zero maintenance, scales automatically). &nbsp;</p><p>✔ Cost-optimized (Firehose batching, S3 storage, Athena pay-per-query). &nbsp;</p><p>✔ High performance (Parquet + Athena enables fast SQL queries). &nbsp;</p><p>✔ Meets all requirements with minimal operational effort. &nbsp;</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "ace000a65ad1462d95f5b9e5261e9e58",
      "questionNumber": 444,
      "type": "multiple",
      "content": "<p>Question #444</p><p>A public retail web application uses an Application Load Balancer (ALB) in front of Amazon EC2 instances running across multiple Availability Zones (AZs) in a Region backed by an Amazon RDS MySQL Multi-AZ deployment. Target group health checks are configured to use HTTP and pointed at the product catalog page. Auto Scaling is configured to maintain the web fleet size based on the ALB health check.</p><p><br></p><p>Recently, the application experienced an outage. Auto Scaling continuously replaced the instances during the outage. A subsequent investigation determined that the web server metrics were within the normal range, but the database tier was experiencing high load, resulting in severely elevated query response times.</p><p><br></p><p>Which of the following changes together would remediate these issues while improving monitoring capabilities for the availability and functionality of the entire application stack for future growth? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Configure read replicas for Amazon RDS MySQL and use the single reader endpoint in the web application to reduce the load on the backend database tier."
        },
        {
          "label": "B",
          "content": "Configure the target group health check to point at a simple HTML page instead of a product catalog page and the Amazon Route 53 health check against the product page to evaluate full application functionality. Configure Amazon CloudWatch alarms to notify administrators when the site fails."
        },
        {
          "label": "C",
          "content": "Configure the target group health check to use a TCP check of the Amazon EC2 web server and the Amazon Route 53 health check against the product page to evaluate full application functionality. Configure Amazon CloudWatch alarms to notify administrators when the site fails."
        },
        {
          "label": "D",
          "content": "Configure an Amazon CloudWatch alarm for Amazon RDS with an action to recover a high-load, impaired RDS instance in the database tier."
        },
        {
          "label": "E",
          "content": "Configure an Amazon ElastiCache cluster and place it between the web application and RDS MySQL instances to reduce the load on the backend database tier."
        }
      ],
      "correctAnswer": "BE",
      "explanation": "<p> B. Configure the target group health check to point at a simple HTML page instead of a product catalog page and the Amazon Route 53 health check against the product page to evaluate full application functionality. Configure Amazon CloudWatch alarms to notify administrators when the site fails.</p><p>- Why? &nbsp;</p><p> &nbsp;- The ALB health check was failing because the product catalog page depended on the database, which was overloaded. &nbsp;</p><p> &nbsp;- A simple HTML page ensures the web server itself is healthy, while Route 53 health checks monitor full application functionality (including DB-dependent pages). &nbsp;</p><p> &nbsp;- CloudWatch alarms provide proactive monitoring and alerting. &nbsp;</p><p> E. Configure an Amazon ElastiCache cluster and place it between the web application and RDS MySQL instances to reduce the load on the backend database tier.</p><p>- Why? &nbsp;</p><p> &nbsp;- ElastiCache (Redis/Memcached) caches frequent database queries, reducing RDS load and preventing high-latency issues. &nbsp;</p><p> &nbsp;- Scales read-heavy workloads (like product catalogs) efficiently. &nbsp;</p><p> Why Not the Other Options?</p><p>- A (RDS read replicas): &nbsp;</p><p> &nbsp;- Helps with read scaling, but does not cache results like ElastiCache (still hits the database). &nbsp;</p><p> &nbsp;- Not as effective for reducing query load compared to caching. &nbsp;</p><p>- C (TCP health check + Route 53): &nbsp;</p><p> &nbsp;- TCP checks only verify connectivity, not application logic. This doesn’t solve the root cause (DB dependency). &nbsp;</p><p>- D (CloudWatch RDS recovery): &nbsp;</p><p> &nbsp;- Reactive, not preventive—doesn’t reduce database load or improve health checks. &nbsp;</p><p> Summary of Correct Actions: &nbsp;</p><p>✔ B: Fix health checks (simple page for ALB, Route 53 for full app checks) + monitoring. &nbsp;</p><p>✔ E: Add ElastiCache to offload database queries and prevent future slowdowns. &nbsp;</p><p>This combination improves availability, reduces DB load, and enhances monitoring for future growth. &nbsp;</p><p>Thus, B and E are the correct answers.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "8cbae6b9100247e6a07fc63de18b8d5b",
      "questionNumber": 445,
      "type": "single",
      "content": "<p>Question #445</p><p>A company has an on-premises data center and is using Kubernetes to develop a new solution on AWS. The company uses Amazon Elastic Kubernetes Service (Amazon EKS) clusters for its development and test environments.</p><p><br></p><p>The EKS control plane and data plane for production workloads must reside on-premises. The company needs an AWS managed solution for Kubernetes management.</p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Install an AWS Outposts server in the on-premises data center. Deploy Amazon EKS by using a local cluster configuration on the Outposts server for the production workloads."
        },
        {
          "label": "B",
          "content": "Install Amazon EKS Anywhere on the company&#39;s hardware in the on-premises data center. Deploy the production workloads on an EKS Anywhere cluster."
        },
        {
          "label": "C",
          "content": "Install an AWS Outposts server in the on-premises data center. Deploy Amazon EKS by using an extended cluster configuration on the Outposts server for the production workloads."
        },
        {
          "label": "D",
          "content": "Install an AWS Outposts server in the on-premises data center. Install Amazon EKS Anywhere on the Outposts server. Deploy the production workloads on an EKS Anywhere cluster."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Install an AWS Outposts server in the on-premises data center. Deploy Amazon EKS by using a local cluster configuration on the Outposts server for the production workloads.</p><p>This solution will meet the requirements with the least operational overhead for the following reasons:</p><p>On-premises Control Plane and Data Plane: AWS Outposts with Amazon EKS local cluster configuration allows both the Kubernetes control plane and data plane to run entirely on-premises on the Outposts hardware, satisfying the requirement that \"EKS control plane and data plane for production workloads must reside on premises.\"</p><p>AWS Managed Solution: AWS Outposts is a fully managed service by AWS. With EKS on Outposts in local cluster mode, AWS manages the underlying infrastructure, hardware maintenance, and software updates. This provides an AWS managed solution for Kubernetes management as required.</p><p>Minimal Operational Overhead: With AWS Outposts:</p><p>AWS handles the maintenance and updates of the underlying hardware</p><p>AWS manages the EKS control plane software updates and patches</p><p>The solution integrates seamlessly with AWS services and management tools</p><p>AWS provides unified support for both hardware and software components</p><p>Consistent operational experience with the company's existing EKS development and test environments</p><p>Option B (Amazon EKS Anywhere) would require more operational overhead because:</p><p>The company would be responsible for managing their own hardware infrastructure</p><p>They would need to handle hardware failures, replacements, and capacity planning</p><p>While EKS Anywhere provides tools for Kubernetes management, it places more responsibility on the company for maintaining the overall environment</p><p>It would create a different operational model compared to their existing EKS development and test environments</p><p>AWS Outposts with EKS local cluster configuration provides a true extension of AWS into the on-premises environment, with the same managed experience the company is already using for development and testing. This creates a consistent operational model across all environments while meeting the requirement for on-premises control and data planes, all with the least operational overhead.</p><p>Sources</p><p>Amazon Elastic Kubernetes Service - Overview of Deployment Options on AWS （https://docs.aws.amazon.com/whitepapers/latest/overview-deployment-options/amazon-elastic-kubernetes-service.html）</p><p>AWS re:Invent 2024 - Amazon EKS for edge and hybrid use cases | AWS re:Post （https://repost.aws/articles/ARP74Xj00HRkGHaiN1HrI52Q/aws-re-invent-2024-amazon-eks-for-edge-and-hybrid-use-cases）</p><p>Managed Kubernetes Service - Amazon EKS Features - Amazon Web Services （https://aws.amazon.com/cn/eks/features/）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "a7639483ccea4b38a2faea75ae994bad",
      "questionNumber": 446,
      "type": "single",
      "content": "Question #446<p>A company uses AWS Organizations to manage its development environment. Each development team at the company has its own AWS account. Each account has a single VPC and CIDR blocks that do not overlap.</p><p><br></p><p>The company has an Amazon Aurora DB cluster in a shared services account. All the development teams need to work with live data from the DB cluster.</p><p><br></p><p>Which solution will provide the required connectivity to the DB cluster with the LEAST operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an AWS Resource Access Manager (AWS RAM) resource share for the DB cluster. Share the DB cluster with all the development accounts."
        },
        {
          "label": "B",
          "content": "Create a transit gateway in the shared services account. Create an AWS Resource Access Manager (AWS RAM) resource share for the transit gateway. Share the transit gateway with all the development accounts. Instruct the developers to accept the resource share and configure networking."
        },
        {
          "label": "C",
          "content": "Create an Application Load Balancer (ALB) that points to the IP address of the DB cluster. Create an AWS PrivateLink endpoint service that uses the ALB. Add permissions to allow each development account to connect to the endpoint service."
        },
        {
          "label": "D",
          "content": "Create an AWS Site-to-Site VPN connection in the shared services account. Configure networking. Use AWS Marketplace VPN software in each development account to connect to the Site-to-Site VPN connection."
        }
      ],
      "correctAnswer": "B",
      "explanation": "Option B is correct as it allows for a scalable and manageable way to connect multiple development accounts to the shared DB cluster using transit gateway and AWS RAM, with minimal operational overhead. Option A is not correct because AWS RAM cannot directly share an Amazon Aurora DB cluster. Option C is incorrect because PrivateLink requires a Network Load Balancer (NLB), not an ALB. Option D is less efficient as it requires managing VPN connections for each development account.",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "7e2951142649484eb891f3fbb67ecf3c",
      "questionNumber": 447,
      "type": "single",
      "content": "<p>Question #447</p><p>A company used AWS CloudFormation to create all new infrastructure in its AWS member accounts. The resources rarely change and are properly sized for the expected load. The monthly AWS bill is consistent.</p><p><br></p><p>Occasionally, a developer creates a new resource for testing and forgets to remove the resource when the test is complete. Most of these tests last a few days before the resources are no longer needed.</p><p><br></p><p>The company wants to automate the process of finding unused resources. A solutions architect needs to design a solution that determines whether the cost in the AWS bill is increasing. The solution must help identify resources that cause an increase in cost and must automatically notify the company's operations team.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Turn on billing alerts. Use AWS Cost Explorer to determine the costs for the past month. Create an Amazon CloudWatch alarm for total estimated charges. Specify a cost threshold that is higher than the costs that Cost Explorer determined. Add a notification to alert the operations team if the alarm threshold is breached."
        },
        {
          "label": "B",
          "content": "Turn on billing alerts. Use AWS Cost Explorer to determine the average monthly costs for the past 3 months. Create an Amazon CloudWatch alarm for total estimated charges. Specify a cost threshold that is higher than the costs that Cost Explorer determined. Add a notification to alert the operations team if the alarm threshold is breached."
        },
        {
          "label": "C",
          "content": "Use AWS Cost Anomaly Detection to create a cost monitor that has a monitor type of Linked account. Create a subscription to send daily AWS cost summaries to the operations team. Specify a threshold for cost variance."
        },
        {
          "label": "D",
          "content": "Use AWS Cost Anomaly Detection to create a cost monitor that has a monitor type of AWS services. Create a subscription to send daily AWS cost summaries to the operations team. Specify a threshold for cost variance."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>The correct answer is D. Use AWS Cost Anomaly Detection to create a cost monitor that has a monitor type of AWS services. Create a subscription to send daily AWS cost summaries to the operations team. Specify a threshold for cost variance.</p><p> Explanation:</p><p>1. Problem Requirements:</p><p> &nbsp; - Detect unused resources (e.g., forgotten test instances).</p><p> &nbsp; - Monitor cost increases in the AWS bill.</p><p> &nbsp; - Automatically notify the operations team when anomalies occur.</p><p>2. Why Option D Works Best:</p><p> &nbsp; - AWS Cost Anomaly Detection:</p><p> &nbsp; &nbsp; - Automatically identifies unusual spending patterns (e.g., sudden spikes from forgotten test resources).</p><p> &nbsp; &nbsp; - Monitor type: AWS services → Tracks cost changes per service (e.g., EC2, S3), making it easy to pinpoint which resource caused the increase.</p><p> &nbsp; - Daily cost summaries + threshold alerts:</p><p> &nbsp; &nbsp; - Proactively notifies the operations team when costs deviate from expected baselines.</p><p> &nbsp; - Fully automated (no manual analysis needed).</p><p>3. Why Not the Other Options?</p><p> &nbsp; - A & B (Billing alerts + CloudWatch alarms):</p><p> &nbsp; &nbsp; - Reactive—only triggers after costs exceed a fixed threshold.</p><p> &nbsp; &nbsp; - Does not identify the root cause (e.g., which service/resource caused the spike).</p><p> &nbsp; - C (Cost Anomaly Detection with \"Linked account\" monitor type):</p><p> &nbsp; &nbsp; - Tracks anomalies per account, not per service → Less granular for identifying unused resources.</p><p> Key Benefits of Option D:</p><p>✔ Automatic anomaly detection (machine learning-based, no manual setup). &nbsp;</p><p>✔ Service-level granularity (identifies exactly which AWS service/resource is causing cost increases). &nbsp;</p><p>✔ Proactive alerts (notifies operations team before costs spiral). &nbsp;</p><p>https://aws.amazon.com/aws-cost-management/aws-cost-anomaly</p><p>detection/faqs/#:~:text=What%20is%20the%20difference%20between%20a%20linked%20account%20monitor%20in%20a%20 </p><p>payer%20account,%20and%20a%20services%20monitor%20in%20a%20linked%20account</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "548862b3765c45a5bab427f5407b9d29",
      "questionNumber": 448,
      "type": "single",
      "content": "<p>Question #448</p><p>A company is deploying a new web-based application and needs a storage solution for the Linux application servers. The company wants to create a single location for updates to application data for all instances. The active dataset will be up to 100 GB in size. A solutions architect has determined that peak operations will occur for 3 hours daily and will require a total of 225 MiBps of read throughput.</p><p><br></p><p>The solutions architect must design a Multi-AZ solution that makes a copy of the data available in another AWS Region for disaster recovery (DR).The DR copy has an RPO of less than 1 hour.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Deploy a new Amazon Elastic File System (Amazon EFS) Multi-AZ file system. Configure the file system for 75 MiBps of provisioned throughput. Implement replication to a file system in the DR Region."
        },
        {
          "label": "B",
          "content": "Deploy a new Amazon FSx for Lustre file system. Configure Bursting Throughput mode for the file system. Use AWS Backup to back up the file system to the DR Region."
        },
        {
          "label": "C",
          "content": "Deploy a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume with 225 MiBps of throughput. Enable Multi-Attach for the EBS volume. Use AWS Elastic Disaster Recovery to replicate the EBS volume to the DR Region."
        },
        {
          "label": "D",
          "content": "Deploy an Amazon FSx for OpenZFS file system in both the production Region and the DR Region. Create an AWS DataSync scheduled task to replicate the data from the production file system to the DR file system every 10 minutes."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Deploy a new Amazon ElasticFile System (Amazon EFS) Multi-AZ file system. Configure the file system for 75 MiBps of provisioned throughput. Implement replication to a file system in the DR Region.</p><p>This solution will meet all the requirements for the following reasons:</p><p>Single Shared Storage Location: Amazon EFS provides a fully managed file system that can be mounted on multiple Linux application servers simultaneously, creating a single location for updates to application data that all instances can access.</p><p>Sufficient Capacity: EFS can easily handle the 100 GB active dataset size, as it's designed to scale to petabytes of data without needing to provision storage in advance.</p><p>Required Throughput: The requirement states a need for 225 MiBps of read throughput during peak operations. EFS with 75 MiBps of provisioned throughput will meet this need because:</p><p>EFS provisioned throughput is measured per unit of storage (in MiB/s per TiB)</p><p>For a 100 GB dataset, 75 MiBps of provisioned throughput provides sufficient performance</p><p>EFS automatically scales throughput based on the size of your file system and can burst to higher throughput levels when needed</p><p>The 75 MiBps provisioned throughput ensures a baseline performance level, while EFS's bursting capability can handle the peak 225 MiBps read operations during the 3-hour daily peak</p><p>Multi-AZ Solution: EFS is designed to be Multi-AZ by default, storing data redundantly across multiple Availability Zones within a region, providing high availability and durability.</p><p>Disaster Recovery Requirements: EFS supports cross-region replication, which continuously replicates data to a file system in another AWS Region. This replication is asynchronous but typically achieves an RPO of minutes, well within the required RPO of less than 1 hour.</p><p>Operational Simplicity: As a fully managed service, EFS eliminates the operational overhead of setting up, patching, and maintaining file servers and storage volumes.</p><p>The EFS solution provides the perfect balance of:</p><p>Shared access for all application servers</p><p>Sufficient performance for the workload</p><p>Built-in Multi-AZ redundancy</p><p>Cross-region replication for disaster recovery</p><p>Managed service with minimal operational overhead</p><p>This makes it the ideal choice for meeting the stated requirements with the least complexity and management burden.</p><p>Sources</p><p>Plan your deployment - Simple File Manager for Amazon EFS （https://docs.aws.amazon.com/solutions/latest/simple-file-manager-for-amazon-efs/plan-your-deployment.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "5988e2f1286d49d68fd21f9dd1b81382",
      "questionNumber": 449,
      "type": "single",
      "content": "<p>Question #449</p><p>A company needs to gather data from an experiment in a remote location that does not have internet connectivity. During the experiment, sensors that are connected to a local network will generate 6 TB of data in a proprietary format over the course of 1 week. The sensors can be configured to upload their data files to an FTP server periodically, but the sensors do not have their own FTP server. The sensors also do not support other protocols. The company needs to collect the data centrally and move the data to object storage in the AWS Cloud as soon as possible after the experiment.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Order an AWS Snowball Edge Compute Optimized device. Connect the device to the local network. Configure AWS DataSync with a target bucket name, and unload the data over NFS to the device. After the experiment, return the device to AWS so that the data can be loaded into Amazon S3."
        },
        {
          "label": "B",
          "content": "Order an AWS Snowcone device, including an Amazon Linux 2 AMI. Connect the device to the local network. Launch an Amazon EC2 instance on the device. Create a shell script that periodically downloads data from each sensor. After the experiment, return the device to AWS so that the data can be loaded as an Amazon Elastic Block Store (Amazon EBS) volume."
        },
        {
          "label": "C",
          "content": "Order an AWS Snowcone device, including an Amazon Linux 2 AMI. Connect the device to the local network. Launch an Amazon EC2 instance on the device. Install and configure an FTP server on the EC2 instance. Configure the sensors to upload data to the EC2 instance. After the experiment, return the device to AWS so that the data can be loaded into Amazon S3."
        },
        {
          "label": "D",
          "content": "Order an AWS Snowcone device. Connect the device to the local network. Configure the device to use Amazon FSx. Configure the sensors to upload data to the device. Configure AWS DataSync on the device to synchronize the uploaded data with an Amazon S3 bucket. Return the device to AWS so that the data can be loaded as an Amazon Elastic Block Store (Amazon EBS) volume.<p><br></p>"
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>The correct answer is C. Order an AWS Snowcone device, including an Amazon Linux 2 AMI. Connect the device to the local network. Launch an Amazon EC2 instance on the device. Install and configure an FTP server on the EC2 instance. Configure the sensors to upload data to the EC2 instance. After the experiment, return the device to AWS so that the data can be loaded into Amazon S3.</p><p> Explanation:</p><p>1. Problem Requirements:</p><p> &nbsp; - Remote location with no internet connectivity.</p><p> &nbsp; - 6 TB of data generated over 1 week (proprietary format).</p><p> &nbsp; - Sensors only support FTP uploads (no other protocols).</p><p> &nbsp; - Need to move data to AWS (Amazon S3) after the experiment.</p><p>2. Why Option C Works Best:</p><p> &nbsp; - AWS Snowcone (small, portable edge device) is ideal for offline data collection.</p><p> &nbsp; - Amazon Linux 2 AMI + EC2 instance allows running an FTP server (matching sensor requirements).</p><p> &nbsp; - Data is stored locally on Snowcone during the experiment.</p><p> &nbsp; - After the experiment, return Snowcone to AWS to automatically load data into Amazon S3.</p><p>3. Why Not the Other Options?</p><p> &nbsp; - A (Snowball Edge Compute Optimized + DataSync/NFS):</p><p> &nbsp; &nbsp; - Overkill for 6 TB (Snowcone is sufficient).</p><p> &nbsp; &nbsp; - Sensors don’t support NFS (only FTP).</p><p> &nbsp; - B (Snowcone + shell script to download data):</p><p> &nbsp; &nbsp; - Sensors can’t be pulled from (they only push via FTP).</p><p> &nbsp; &nbsp; - Data ends up on EBS volume, not S3 (directly).</p><p> &nbsp; - D (Snowcone + FSx + DataSync):</p><p> &nbsp; &nbsp; - FSx is unnecessary complexity for FTP.</p><p> &nbsp; &nbsp; - DataSync requires internet (not available in this scenario).</p><p> Key Benefits of Option C:</p><p>✔ Matches sensor capabilities (FTP-only support). &nbsp;</p><p>✔ Snowcone is cost-effective for 6 TB. &nbsp;</p><p>✔ Automatic S3 upload after device return. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "4891d0fbf0044f4a932112b3a9b9689b",
      "questionNumber": 450,
      "type": "single",
      "content": "<p>Question #450</p><p>A company that has multiple business units is using AWS Organizations with all features enabled. The company has implemented an account structure in which each business unit has its own AWS account. Administrators in each AWS account need to view detailed cost and utilization data for their account by using Amazon Athena.</p><p>Each business unit can have access to only its own cost and utilization data. The IAM policies that govern the ability to set up AWS Cost and Usage Reports are in place. A central Cost and Usage Report that contains all data for the organization is already available in an Amazon S3 bucket.</p><p>Which solution will meet these requirements with the LEAST operational complexity?</p>",
      "options": [
        {
          "label": "A",
          "content": "In the organization&#39;s management account, use AWS Resource Access Manager (AWS RAM) to share the Cost and Usage Report data with each member account."
        },
        {
          "label": "B",
          "content": "In the organization&#39;s management account, configure an S3 event to invoke an AWS Lambda function each time a new file arrives in the S3 bucket that contains the central Cost and Usage Report. Configure the Lambda function to extract each member account&rsquo;s data and to place the data in Amazon S3 under a separate prefix. Modify the S3 bucket policy to allow each member account to access its own prefix."
        },
        {
          "label": "C",
          "content": "In each member account, access AWS Cost Explorer. Create a new report that contains relevant cost information for the account. Save the report in Cost Explorer. Provide instructions that the account administrators can use to access the saved report."
        },
        {
          "label": "D",
          "content": "In each member account, create a new S3 bucket to store Cost and Usage Report data. Set up a Cost and Usage Report to deliver the data to the new S3 bucket."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Option B is the correct choice as it provides an automated solution that minimizes operational complexity. By using a Lambda function triggered by S3 events, the process of separating and storing each member account's data is automated, requiring no manual intervention. Each account is then given access to only its own data prefix in the S3 bucket, ensuring that business units cannot access each other's data.</p><p>The requirements are: &nbsp;</p><p>1. Each business unit (AWS account) should have access to only its own cost and usage data. &nbsp;</p><p>2. The solution should use the existing centralized Cost and Usage Report (CUR) in an S3 bucket. &nbsp;</p><p>3. The solution should have the least operational complexity. &nbsp;</p><p>Let's analyze the options: &nbsp;</p><p> Option A: Using AWS RAM to share the CUR data &nbsp;</p><p>- AWS RAM is typically used for sharing resources like VPC subnets or License Manager configurations, not S3 data. &nbsp;</p><p>- Even if shared, the entire CUR dataset would be accessible to all accounts, violating the requirement of restricting access to only each account's own data. &nbsp;</p><p>- Not a valid solution. &nbsp;</p><p> Option B: Using S3 events + Lambda to filter and partition data &nbsp;</p><p>1. A Lambda function is triggered whenever a new CUR file arrives in the S3 bucket. &nbsp;</p><p>2. The Lambda function extracts data per account and stores it under separate prefixes (e.g., `BusinessUnitA/`, `BusinessUnitB/`). &nbsp;</p><p>3. An S3 bucket policy is updated to restrict each account to its own prefix using IAM conditions (e.g., `s3:prefix` or `aws:PrincipalAccount`). &nbsp;</p><p>- This ensures least operational complexity because: &nbsp;</p><p> &nbsp; - No manual setup is needed for each new account. &nbsp;</p><p> &nbsp; - The CUR remains centralized, but access is restricted per account. &nbsp;</p><p>- Meets all requirements. &nbsp;</p><p> Option C: Using AWS Cost Explorer &nbsp;</p><p>- Cost Explorer provides high-level cost data, not the detailed CUR data required for Athena queries. &nbsp;</p><p>- Saved reports in Cost Explorer cannot be queried with Athena. &nbsp;</p><p>- Does not meet the requirement. &nbsp;</p><p> Option D: Setting up separate CURs per account &nbsp;</p><p>- This would require managing multiple CURs, increasing operational overhead. &nbsp;</p><p>- The question states that a central CUR is already available, so this approach is redundant. &nbsp;</p><p>- Higher operational complexity. &nbsp;</p><p> Conclusion: &nbsp;</p><p>Option B is the best solution because it: &nbsp;</p><p>✔ Uses the existing centralized CUR. &nbsp;</p><p>✔ Automatically filters and partitions data per account. &nbsp;</p><p>✔ Restricts access using S3 bucket policies. &nbsp;</p><p>✔ Minimizes operational complexity. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "933cab30cc0c47668655d5752b91ca8b",
      "questionNumber": 451,
      "type": "single",
      "content": "<p>Question #451</p><p>A company is designing an AWS environment for a manufacturing application. The application has been successful with customers, and the application's user base has increased. The company has connected the AWS environment to the company's on-premises data center through a 1 Gbps AWS Direct Connect connection. The company has configured BGP for the connection.</p><p>The company must update the existing network connectivity solution to ensure that the solution is highly available, fault tolerant, and secure.</p><p>Which solution will meet these requirements MOST cost-effectively?</p>",
      "options": [
        {
          "label": "A",
          "content": "Add a dynamic private IP AWS Site-to-Site VPN as a secondary path to secure data in transit and provide resilience for the Direct Connect connection. Configure MACsec to encrypt traffic inside the Direct Connect connection."
        },
        {
          "label": "B",
          "content": "Provision another Direct Connect connection between the company&#39;s on-premises data center and AWS to increase the transfer speed and provide resilience. Configure MACsec to encrypt traffic inside the Direct Connect connection."
        },
        {
          "label": "C",
          "content": "Configure multiple private VIFs. Load balance data across the VIFs between the on-premises data center and AWS to provide resilience."
        },
        {
          "label": "D",
          "content": "Add a static AWS Site-to-Site VPN as a secondary path to secure data in transit and to provide resilience for the Direct Connect connection."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. Add a static AWS Site-to-Site VPN as a secondary path to secure data in transit and to provide resilience for the Direct Connect connection.</p><p>This solution will meet the requirements most cost-effectively for the following reasons:</p><p>High Availability and Fault Tolerance:</p><p>The combination of Direct Connect and Site-to-Site VPN provides two diverse network paths</p><p>If the Direct Connect connection fails, traffic can automatically failover to the VPN connection</p><p>This creates a highly available and fault-tolerant network architecture without requiring a second Direct Connect connection</p><p>Enhanced Security:</p><p>The Site-to-Site VPN adds encryption for data in transit, which Direct Connect alone doesn't provide</p><p>This addresses the security requirement by ensuring that all traffic is protected, even if it has to traverse the public internet via the VPN connection</p><p>The VPN connection uses IPsec encryption protocols to secure the data</p><p>Cost-Effectiveness:</p><p>Adding a Site-to-Site VPN is significantly less expensive than provisioning a second Direct Connect connection</p><p>VPN connections have a low monthly cost plus data transfer charges</p><p>This approach maximizes the existing investment in Direct Connect while adding resilience at minimal additional cost</p><p>Simplicity of Implementation:</p><p>Setting up a Site-to-Site VPN as a backup path is relatively straightforward</p><p>It can be configured to work with the existing BGP setup for automatic failover</p><p>No additional physical infrastructure is required at the Direct Connect location</p><p>Option C (configuring multiple private VIFs and load balancing) would not be as effective because:</p><p>It doesn't inherently provide encryption for data in transit</p><p>Multiple private VIFs on the same physical connection don't provide true path diversity or resilience against physical connection failures</p><p>If the single 1 Gbps Direct Connect connection fails, all VIFs would be affected</p><p>This approach doesn't address the security requirement as effectively as adding encryption</p><p>The combination of Direct Connect with a Site-to-Site VPN backup provides the optimal balance of high availability, fault tolerance, and security while being the most cost-effective solution. This hybrid connectivity model is a recommended AWS best practice for creating resilient network connections between on-premises environments and AWS.</p><p>Sources</p><p>AWS Direct Connect + AWS Site-to-Site VPN - Amazon Virtual Private Cloud Connectivity Options （https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-site-to-site-vpn.html）</p><p>Client VPN with VPC peering vs Client VPN withTransit gateway | AWS re:Post (https://repost.aws/questions/QUUotXjy2mTV6noMA-J86AXQ/client-vpn-with-vpc-peering-vs-client-vpn-withtransit-gateway)</p><p>REL02-BP02 Provision redundant connectivity between private networks in the cloud and on-premises environments - Reliability Pillar (https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/rel_planning_network_topology_ha_conn_private_networks.html)</p><p>Security - Hybrid Connectivity (https://docs.aws.amazon.com/whitepapers/latest/hybrid-connectivity/security.html)</p><p>REL02-BP02 Provision redundant connectivity between private networks in the cloud and on-premises environments - AWS Well-Architected Framework (https://docs.aws.amazon.com/wellarchitected/2024-06-27/framework/rel_planning_network_topology_ha_conn_private_networks.html)</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "61c438d797bd47038336993d965039ee",
      "questionNumber": 452,
      "type": "single",
      "content": "<p>Question #452</p><p>A company needs to modernize an application and migrate the application to AWS. The application stores user profile data as text in a single table in an on-premises MySQL database.</p><p>After the modernization, users will use the application to upload video files that are up to 4 GB in size. Other users must be able to download the video files from the application. The company needs a video storage solution that provides rapid scaling. The solution must not affect application performance.</p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Migrate the database to Amazon Aurora PostgreSQL by using AWS Database Migration Service (AWS DMS). Store the videos as base64 encoded strings in a TEXT column in the database."
        },
        {
          "label": "B",
          "content": "Migrate the database to Amazon DynamoDB by using AWS Database Migration Service (AWS DMS) with the AWS Schema Conversion Tool (AWS SCT). Store the videos as objects in Amazon S3. Store the S3 key in the corresponding DynamoDB item."
        },
        {
          "label": "C",
          "content": "Migrate the database to Amazon Keyspaces (for Apache Cassandra) by using AWS Database Migration Service (AWS DMS) with the AWS Schema Conversion Tool (AWS SCT). Store the videos as objects in Amazon S3. Store the S3 object identifier in the corresponding Amazon Keyspaces entry."
        },
        {
          "label": "D",
          "content": "Migrate the database to Amazon DynamoDB by using AWS Database Migration Service (AWS DMS) with the AWS Schema Conversion Tool (AWS SCT). Store the videos as base64-encoded strings in the corresponding DynamoDB item."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Migrate the database to Amazon DynamoDB by using AWS Database Migration Service (AWS DMS) with the AWS Schema Conversion Tool (AWS SCT). Store the videos as objects in Amazon S3. Store the S3 key in the corresponding DynamoDB item.</p><p>This solution will meet the requirements for the following reasons:</p><p>Rapid Scaling for Video Storage: Amazon S3 is specifically designed for object storage and can scale virtually without limits to store large files like the 4GB videos mentioned in the requirements. S3 can handle any number of uploads and downloads concurrently without performance degradation.</p><p>Performance Optimization: By storing only the S3 object keys (references) in DynamoDB and keeping the actual video files in S3, the database remains lightweight and optimized for fast queries. This separation ensures that the application's performance is not affected by the large video files.</p><p>Cost Efficiency: S3's tiered storage options provide cost-effective storage for large binary files like videos, which would be much more expensive to store in a database.</p><p>Appropriate Use of Services: This solution follows AWS best practices by using each service for its intended purpose:</p><p>DynamoDB for fast, scalable access to structured user profile data</p><p>S3 for scalable, durable storage of large binary objects (videos)</p><p>Migration Path: AWS DMS with AWS SCT provides a straightforward path to migrate from the on-premises MySQL database to DynamoDB, handling the schema differences between relational and NoSQL databases.</p><p>Option A (storing videos as base64-encoded strings in Aurora PostgreSQL) would not meet the requirements because:</p><p>Poor Scalability: Storing large binary files as base64 strings in a database doesn't scale well as the number of videos increases.</p><p>Performance Impact: Base64 encoding increases the size of the data by approximately 33%, making a 4GB video closer to 5.3GB when stored. This would significantly impact database performance, query times, and backup/restore operations.</p><p>Database Bloat: As more videos are uploaded, the database would grow rapidly, leading to increased costs and management complexity.</p><p>Inefficient Use of Resources: Relational databases are not optimized for storing and retrieving large binary objects, especially at scale.</p><p>The solution that uses DynamoDB for user profile data and S3 for video storage represents a modern, cloud-native approach that aligns with AWS best practices for handling different types of data. It provides the rapid scaling required for video storage while ensuring that application performance remains optimal.</p><p>Sources</p><p>Migrating an Amazon RDS for MySQL Database to an Amazon DynamoDB target - Database Migration Guide （https://docs.aws.amazon.com/dms/latest/sbs/chap-manageddatabases.mysql2dynamodb.html）</p><p>Best practices for storing large items and attributes in DynamoDB - Amazon DynamoDB （https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-use-s3-too.html）</p><p>Modernizing your application by migrating from an RDBMS to Amazon DynamoDB - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/modernization-rdbms-dynamodb/welcome.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "fd795dbef8b34d8b8722eafe9117a355",
      "questionNumber": 453,
      "type": "single",
      "content": "Question #453<p>A company stores and manages documents in an Amazon Elastic File System (Amazon EFS) file system. The file system is encrypted with an AWS Key Management Service (AWS KMS) key. The file system is mounted to an Amazon EC2 instance that runs proprietary software.</p><p>The company has enabled automatic backups for the file system. The automatic backups use the AWS Backup default backup plan.</p><p>A solutions architect must ensure that deleted documents can be recovered within an RPO of 100 minutes.</p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a new IAM role. Create a new backup plan. Use the new IAM role to create backups. Update the KMS key policy to allow the new IAM role to use the key. Implement an hourly backup schedule for the file system."
        },
        {
          "label": "B",
          "content": "Create a new backup plan. Update the KMS key policy to allow the AWSServiceRoleForBackup IAM role to use the key. Implement a custom cron expression to run a backup of the file system every 30 minutes."
        },
        {
          "label": "C",
          "content": "Create a new IAM role. Use the existing backup plan. Update the KMS key policy to allow the new IAM role to use the key. Enable continuous backups for point-in-time recovery."
        },
        {
          "label": "D",
          "content": "Use the existing backup plan. Update the KMS key policy to allow the AWSServiceRoleForBackup IAM role to use the key. Enable Cross Region Replication for the file system."
        }
      ],
      "correctAnswer": "A",
      "explanation": "Option A is correct because it allows for a more frequent backup schedule that meets the RPO requirement of 100 minutes. By creating a new backup plan with an hourly schedule, the company can ensure that deleted documents can be recovered within the specified time frame. The other options do not provide a means to achieve the RPO as effectively: Option B would require additional setup and may not integrate as seamlessly with AWS Backup, Option C's continuous backups are not supported for EFS, and Option D's Cross Region Replication does not provide point-in-time recovery for deleted documents.",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "6245b207de654a9a8f645e556e1ed8af",
      "questionNumber": 454,
      "type": "single",
      "content": "<p>Question #454</p><p>A solutions architect must provide a secure way for a team of cloud engineers to use the AWS CLI to upload objects into an Amazon S3 bucket.</p><p>Each cloud engineer has an IAM user, IAM access keys, and a virtual multi-factor authentication (MFA) device. The IAM users for the cloud engineers are in a group that is named S3-access. The cloud engineers must use MFA to perform any actions in Amazon S3.</p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Attach a policy to the S3 bucket to prompt the IAM user for an MFA code when the IAM user performs actions on the S3 bucket. Use IAM access keys with the AWS CLI to call Amazon S3."
        },
        {
          "label": "B",
          "content": "Update the trust policy for the S3-access group to require principals to use MFA when principals assume the group. Use IAM access keys with the AWS CLI to call Amazon S3."
        },
        {
          "label": "C",
          "content": "Attach a policy to the S3-access group to deny all S3 actions unless MFA is present. Use IAM access keys with the AWS CLI to call Amazon S3."
        },
        {
          "label": "D",
          "content": "Attach a policy to the S3-access group to deny all S3 actions unless MFA is present. Request temporary credentials from AWS Security Token Service (AWS STS). Attach the temporary credentials in a profile that Amazon S3 will reference when the user performs actions in Amazon S3."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. Attach a policy to the S3-access group to deny all S3 actions unless MFA is present. Request temporary credentials from AWS Security Token Service (AWS STS). Attach the temporary credentials in a profile that Amazon S3 will reference when the user performs actions in Amazon S3.</p><p>This solution will meet the requirements for providing a secure way for cloud engineers to use the AWS CLI to upload objects to an S3 bucket with MFA for the following reasons:</p><p>MFA Enforcement: The policy attached to the S3-access group will deny all S3 actions unless MFA is present, ensuring that MFA is required for any S3 operations.</p><p>Temporary Credentials: Using AWS STS to request temporary credentials is more secure than using long-term IAM access keys directly. These temporary credentials:</p><p>Have a limited lifespan (typically 15 minutes to 36 hours)</p><p>Automatically expire after their configured duration</p><p>Reduce the risk if credentials are accidentally exposed</p><p>MFA Verification Process: When requesting temporary credentials from AWS STS using operations like GetSessionToken or AssumeRole, users must provide their MFA token as part of the request. This ensures that only authenticated users with physical possession of their MFA device can obtain valid credentials.</p><p>AWS CLI Profile: By attaching the temporary credentials to a profile that Amazon S3 will reference, the cloud engineers can use the AWS CLI with the proper authentication while maintaining security.</p><p>Security Best Practices: This approach follows AWS security best practices by:</p><p>Minimizing the use of long-term credentials</p><p>Enforcing MFA for sensitive operations</p><p>Using temporary, limited-privilege credentials</p><p>Implementing the principle of least privilege</p><p>Option C is less secure because it would use long-term IAM access keys directly with the AWS CLI. While the policy would still enforce MFA, using long-term credentials presents several security concerns:</p><p>They don't expire automatically</p><p>They remain valid until manually revoked</p><p>If compromised, they could potentially be used for longer periods</p><p>They don't provide the same level of auditability as temporary credentials</p><p>The solution in option D provides a more comprehensive security approach that aligns with AWS best practices for securing API access with MFA, making it the correct choice for this scenario.</p><p>Sources</p><p>Making requests - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/API/MakingRequests.html）</p><p>Secure API access with MFA - AWS Identity and Access Management （https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_configure-api-require.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "3380b2af88c44e35962853c52b755f4f",
      "questionNumber": 455,
      "type": "single",
      "content": "<p>Question #455</p><p>A company needs to migrate 60 on-premises legacy applications to AWS. The applications are based on the .NET Framework and run on Windows. The company needs a solution that minimizes migration time and requires no application code changes. The company also does not want to manage the infrastructure.</p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Refactor the applications and containerize them by using AWS Toolkit for .NET Refactoring. Use Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type to host the containerized applications."
        },
        {
          "label": "B",
          "content": "Use the Windows Web Application Migration Assistant to migrate the applications to AWS Elastic Beanstalk. Use Elastic Beanstalk to deploy and manage the applications."
        },
        {
          "label": "C",
          "content": "Use the Windows Web Application Migration Assistant to migrate the applications to Amazon EC2 instances. Use the EC2 instances to deploy and manage the applications."
        },
        {
          "label": "D",
          "content": "Refactor the applications and containerize them by using AWS Toolkit for .NET Refactoring. Use Amazon Elastic Kubernetes Service (Amazon EKS) with the Fargate launch type to host the containerized applications."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Use the Windows Web Application Migration Assistant to migrate the applications to AWS Elastic Beanstalk. Use Elastic Beanstalk to deploy and manage the applications.</p><p>This solution will meet the company's requirements for the following reasons:</p><p>Minimal Migration Time: The Windows Web Application Migration Assistant is specifically designed to streamline the migration of Windows-based applications to AWS with minimal effort. It automates much of the migration process, allowing for faster deployment compared to refactoring and containerizing applications.</p><p>No Application Code Changes: This approach allows the legacy .NET Framework applications to be migrated \"as-is\" without requiring any code modifications. The Windows Web Application Migration Assistant preserves the existing application structure and configurations, focusing on moving the applications rather than changing them.</p><p>Managed Infrastructure: AWS Elastic Beanstalk is a fully managed service that handles the underlying infrastructure, including capacity provisioning, load balancing, auto-scaling, and application health monitoring. The company won't need to manage servers, which aligns with their requirement of not wanting to manage infrastructure.</p><p>Windows Support: Elastic Beanstalk has built-in support for Windows environments and .NET applications, making it well-suited for hosting legacy .NET Framework applications.</p><p>Option A (refactoring and containerizing the applications) would not meet the requirements because:</p><p>It would require significant code changes through the refactoring process, which contradicts the requirement of \"no application code changes.\"</p><p>Containerizing 60 legacy applications would be time-consuming and complex, increasing the overall migration time.</p><p>While Fargate reduces infrastructure management compared to traditional container deployments, the containerization process itself would require additional expertise and effort that doesn't align with the goal of minimizing migration time.</p><p>The Windows Web Application Migration Assistant with Elastic Beanstalk provides the most straightforward path for migrating the legacy .NET Framework applications to AWS while meeting all the stated requirements: minimizing migration time, requiring no application code changes, and eliminating the need to manage infrastructure.</p><p>Sources</p><p>Migrating and Rehosting .NET Applications - Developing and Deploying .NET Applications on AWS （https://docs.aws.amazon.com/whitepapers/latest/develop-deploy-dotnet-apps-on-aws/migrating-and-rehosting-.net-applications.html）</p><p>Easily Migrate .NET Web Apps to AWS with Elastic Beanstalk, Part 1 | .NET on AWS Blog（https://aws.amazon.com/cn/blogs/dotnet/easily-migrate-net-web-apps-to-aws-with-elastic-beanstalk-part-1/） </p><p>Modernize your .NET application using AWS App2Container and AWS Toolkit for .NET | Containers （https://aws.amazon.com/cn/blogs/containers/replatform-and-refactor-your-workload-to-aws-using-aws-app2container-and-aws-toolkit-for-net-refactoring-application/）</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "cf51a9a0b340483b9cbb646ddb52e9ae",
      "questionNumber": 456,
      "type": "single",
      "content": "<p>Question #456</p><p>A company needs to run large batch-processing jobs on data that is stored in an Amazon S3 bucket. The jobs perform simulations. The results of the jobs are not time sensitive, and the process can withstand interruptions.</p><p>Each job must process 15-20 GB of data when the data is stored in the S3 bucket. The company will store the output from the jobs in a different Amazon S3 bucket for further analysis.</p><p>Which solution will meet these requirements MOST cost-effectively?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a serverless data pipeline. Use AWS Step Functions for orchestration. Use AWS Lambda functions with provisioned capacity to process the data."
        },
        {
          "label": "B",
          "content": "Create an AWS Batch compute environment that includes Amazon EC2 Spot Instances. Specify the SPOT_CAPACITY_OPTIMIZED allocation strategy."
        },
        {
          "label": "C",
          "content": "Create an AWS Batch compute environment that includes Amazon EC2 On-Demand Instances and Spot Instances. Specify the SPOT_CAPACITY_OPTIMIZED allocation strategy for the Spot Instances."
        },
        {
          "label": "D",
          "content": "Use Amazon Elastic Kubernetes Service (Amazon EKS) to run the processing jobs. Use managed node groups that contain a combination of Amazon EC2 On-Demand Instances and Spot Instances."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Create an AWS Batch compute environment that includes Amazon EC2 Spot Instances. Specify the SPOT_CAPACITY_OPTIMIZED allocation strategy.</p><p>This solution will meet the requirements most cost-effectively for the following reasons:</p><p>Large Data Processing Capability: AWS Batch is specifically designed to handle large-scale batch processing jobs. It can easily process the 15-20 GB of data per job without the limitations that would affect a serverless approach.</p><p>Cost Optimization: EC2 Spot Instances offer significant cost savings (up to 90% compared to On-Demand instances) for workloads that can tolerate interruptions. Since the requirements explicitly state that \"the process can withstand interruptions\" and \"results are not time sensitive,\" Spot Instances are an ideal fit.</p><p>Interruption Handling: The SPOT_CAPACITY_OPTIMIZED allocation strategy specifically selects Spot Instances from the pools that are least likely to be interrupted, further improving job reliability while maintaining cost benefits.</p><p>Flexibility for Resource-Intensive Jobs: AWS Batch allows you to select appropriate instance types and sizes to efficiently handle the computational requirements of the simulation jobs, which may be resource-intensive.</p><p>Built-in Job Management: AWS Batch includes features for job queuing, dependency tracking, and automatic retries, which are valuable for managing batch processing workflows.</p><p>Option A (using Step Functions with Lambda functions) would be less suitable because:</p><p>Lambda Limitations: Lambda functions have constraints that make them less ideal for processing large datasets:</p><p>15-minute maximum execution time</p><p>Memory limitations (up to 10 GB)</p><p>Temporary storage limitations</p><p>Data Size Challenges: Processing 15-20 GB of data would likely require complex chunking and orchestration with Lambda, increasing development complexity.</p><p>Cost Considerations: Provisioned capacity for Lambda functions can be expensive for long-running or resource-intensive workloads compared to Spot Instances.</p><p>Resource Efficiency: Lambda is optimized for short, event-driven processing rather than large-scale batch computations.</p><p>AWS Batch with EC2 Spot Instances provides a purpose-built solution for exactly this type of workload - large-scale, interruptible batch processing jobs - at the lowest possible cost, making it the most cost-effective solution that meets all the stated requirements.</p><p>Sources</p><p>Use Amazon EC2 Spot best practices for AWS Batch - AWS Batch （https://docs.aws.amazon.com/batch/latest/userguide/bestpractice6.html）</p><p>Running batch jobs at scale with EC2 Spot Instances （https://aws.amazon.com/cn/tutorials/run-batch-jobs-at-scale-with-ec2-spot/）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "1bb8f9cecedd4d7a8478866374d1818a",
      "questionNumber": 457,
      "type": "single",
      "content": "<p>Question #457</p><p>A company has an application that analyzes and stores image data on premises. The application receives millions of new image files every day. Files are an average of 1 MB in size. The files are analyzed in batches of 1 GB. When the application analyzes a batch, the application zips the images together. The application then archives the images as a single file in an on-premises NFS server for long-term storage.</p><p>The company has a Microsoft Hyper-V environment on premises and has compute capacityavailable. The company does not have storage capacity and wants to archive the images on AWS. The company needs the ability to retrieve archived data within 1 week of a request.</p><p>The company has a 10 Gbps AWS Direct Connect connection between its on-premises data center and AWS. The company needs to set bandwidth limits and schedule archived images to be copied to AWS during non-business hours.</p><p>Which solution will meet these requirements MOST cost-effectively?</p>",
      "options": [
        {
          "label": "A",
          "content": "Deploy an AWS DataSync agent on a new GPU-based Amazon EC2 instance. Configure the DataSync agent to copy the batch of files from the NFS on-premises server to Amazon S3 Glacier Instant Retrieval. After the successful copy, delete the data from the on-premises storage."
        },
        {
          "label": "B",
          "content": "Deploy an AWS DataSync agent as a Hyper-V VM on premises. Configure the DataSync agent to copy the batch of files from the NFS on-premises server to Amazon S3 Glacier Deep Archive. After the successful copy, delete the data from the on-premises storage."
        },
        {
          "label": "C",
          "content": "Deploy an AWS DataSync agent on a new general purpose Amazon EC2 instance. Configure the DataSync agent to copy the batch of files from the NFS on-premises server to Amazon S3 Standard. After the successful copy, delete the data from the on-premises storage. Create an S3 Lifecycle rule to transition objects from S3 Standard to S3 Glacier Deep Archive after 1 day."
        },
        {
          "label": "D",
          "content": "Deploy an AWS Storage Gateway Tape Gateway on premises in the Hyper-V environment. Connect the Tape Gateway to AWS. Use automatic tape creation. Specify an Amazon S3 Glacier Deep Archive pool. Eject the tape after the batch of images is copied."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Option B is the most cost-effective solution for archiving image data to AWS, considering the company's requirements. Using AWS DataSync with a Hyper-V VM on-premises allows for efficient data transfer to Amazon S3 Glacier Deep Archive, which is an affordable storage option for long-term archiving. This approach also aligns with the need to retrieve archived data within 1 week, as S3 Glacier Deep Archive provides retrieval options that can meet this requirement. Option A's Instant Retrieval is not necessary for data that is accessed within a week, and Option C's S3 Standard is more expensive for storing large amounts of data before transitioning to Deep Archive. Option D's Tape Gateway is less efficient and more complex for the described use case.</p><p>Deploy an AWS DataSync agent as a Hyper-V VM on premises. Configure the DataSync agent to copy the batch of files from the NFS on-premises server to Amazon S3 Glacier Deep Archive. After the successful copy, delete the data from the on-premises storage.</p><p> Why? &nbsp;</p><p>1. Cost-Effective Storage: &nbsp;</p><p> &nbsp; - S3 Glacier Deep Archive is the cheapest storage option for long-term archival (retrieval within 12 hours or more, but the requirement allows for retrieval within 1 week, which fits). &nbsp;</p><p> &nbsp; - S3 Standard (Option C) is more expensive for long-term storage, and transitioning to Glacier Deep Archive after 1 day adds complexity. &nbsp;</p><p>2. Efficient Transfer: &nbsp;</p><p> &nbsp; - AWS DataSync is optimized for high-speed data transfer over Direct Connect (10 Gbps). &nbsp;</p><p> &nbsp; - Running DataSync on-premises as a Hyper-V VM (instead of an EC2 instance) saves costs since the company already has available compute capacity. &nbsp;</p><p>3. Bandwidth Control & Scheduling: &nbsp;</p><p> &nbsp; - DataSync allows bandwidth throttling and scheduling transfers during non-business hours, meeting the requirement. &nbsp;</p><p>4. No Unnecessary Components: &nbsp;</p><p> &nbsp; - Tape Gateway (Option D) is overkill since the data is already in files (not tapes). &nbsp;</p><p> &nbsp; - GPU-based EC2 (Option A) is unnecessary because no GPU processing is required. &nbsp;</p><p> &nbsp; - S3 Glacier Instant Retrieval (Option A) is more expensive than needed (retrieval in milliseconds is not required). &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "4820a18a838740388f15688cd8785715",
      "questionNumber": 458,
      "type": "single",
      "content": "<p>Question #458</p><p>A company wants to record key performance indicators (KPIs) from its application as part of a strategy to convert to a user-based licensing schema. The application is a multi-tier application with a web-based UI. The company saves all log files to Amazon CloudWatch by using the CloudWatch agent. All logins to the application are saved in a log file.</p><p>As part of the new license schema, the company needs to find out how many unique users each client has on a daily basis, weekly basis, and monthly basis.</p><p>Which solution will provide this information with the LEAST change to the application?</p>",
      "options": [
        {
          "label": "A",
          "content": "Configure an Amazon CloudWatch Logs metric filter that saves each successful login as a metric. Configure the user name and client name as dimensions for the metric."
        },
        {
          "label": "B",
          "content": "Change the application logic to make each successful login generate a call to the AWS SDK to increment a custom metric that records user name and client name dimensions in CloudWatch."
        },
        {
          "label": "C",
          "content": "Configure the CloudWatch agent to extract successful login metrics from the logs. Additionally, configure the CloudWatch agent to save the successful login metrics as a custom metric that uses the user name and client name as dimensions for the metric."
        },
        {
          "label": "D",
          "content": "Configure an AWS Lambda function to consume an Amazon CloudWatch Logs stream of the application logs. Additionally, configure the Lambda function to increment a custom metric in CloudWatch that uses the user name and client name as dimensions for the metric."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Configure an Amazon CloudWatch Logs metric filter that saves each successful login as a metric. Configure the user name and client name as dimensions for the metric.</p><p>This solution will provide the required information with the least change to the application for the following reasons:</p><p>No Application Code Changes: The CloudWatch Logs metric filter approach works with the existing logs that are already being saved to CloudWatch using the CloudWatch agent. This means no modifications to the application code are required.</p><p>Leveraging Existing Infrastructure: Since the company is already saving all log files to CloudWatch and login information is already being recorded in a log file, this solution simply extracts and transforms that existing data.</p><p>Powerful Metric Creation: CloudWatch Logs metric filters can parse log entries to extract specific information (like user names and client names) and convert them into CloudWatch metrics with dimensions.</p><p>Dimensional Analysis: By configuring user name and client name as dimensions for the metric, the company can easily analyze unique user counts per client across different time periods (daily, weekly, monthly) using CloudWatch's built-in aggregation capabilities.</p><p>Flexibility: If requirements change in the future, the metric filters can be modified without touching the application code.</p><p>Option B would require changing the application logic to make API calls to CloudWatch for each successful login, which represents a more significant change to the application. This approach would:</p><p>Require code modifications to the login process</p><p>Introduce dependencies on the AWS SDK within the application</p><p>Need deployment and testing of the modified application</p><p>Potentially impact application performance with additional API calls</p><p>Create tighter coupling between the application and AWS services</p><p>The CloudWatch Logs metric filter approach (Option A) achieves the same outcome with no application changes, making it the solution that provides the required information with the least change to the application. It's a non-invasive way to extract the KPIs needed for the new licensing schema while leveraging the existing logging infrastructure.</p><p>Sources</p><p>Create a metric filter for a log group - Amazon CloudWatch Logs （https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CreateMetricFilterProcedure.html）</p><p>Metric filter for specific log-stream | AWS re:Post （https://repost.aws/questions/QUJhK3p6NQRXyTeJliG-a-bg/metric-filter-for-specific-log-stream）</p><p>Application-level CloudWatch configuration - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/implementing-logging-monitoring-cloudwatch/application-level-configuration.html）</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "168672addc554d8ebdba1241df05be97",
      "questionNumber": 459,
      "type": "single",
      "content": "<p>Question #459</p><p>A company is using GitHub Actions to run a CI/CD pipeline that accesses resources on AWS. The company has an IAM user that uses a secret key in the pipeline to authenticate to AWS. An existing IAM role with an attached policy grants the required permissions to deploy resources.</p><p>The company’s security team implements a new requirement that pipelines can no longer use long-lived secret keys. A solutions architect must replace the secret key with a short-lived solution.</p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an IAM SAML 2.0 identity provider (IdP) in AWS Identity and Access Management (IAM). Create a new IAM role with the appropriate trust policy that allows the sts:AssumeRole API call. Attach the existing IAM policy to the new IAM role. Update GitHub to use SAML authentication for the pipeline."
        },
        {
          "label": "B",
          "content": "Create an IAM OpenID Connect (OIDC) identity provider (IdP) in AWS Identity and Access Management (IAM). Create a new IAM role with the appropriate trust policy that allows the sts:AssumeRoleWithWebIdentity API call from the GitHub OIDC IdP. Update GitHub to assume the role for the pipeline."
        },
        {
          "label": "C",
          "content": "Create an Amazon Cognito identity pool. Configure the authentication provider to use GitHub. Create a new IAM role with the appropriate trust policy that allows the sts:AssumeRoleWithWebIdentity API call from the GitHub authentication provider. Configure the pipeline to use Cognito as its authentication provider."
        },
        {
          "label": "D",
          "content": "Create a trust anchor to AWS Private Certificate Authority. Generate a client certificate to use with AWS IAM Roles Anywhere. Create a new IAM role with the appropriate trust policy that allows the sts:AssumeRole API call. Attach the existing IAM policy to the new IAM role. Configure the pipeline to use the credential helper tool and to reference the client certificate public key to assume the new IAM role."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Create an IAM OpenID Connect (OIDC) identity provider (IdP) in AWS Identity and Access Management (IAM). Create a new IAM role with the appropriate trust policy that allows the sts:AssumeRoleWithWebIdentity API call from the GitHub OIDC IdP. Update GitHub to assume the role for the pipeline.</p><p>This solution will meet the requirements with the least operational overhead for the following reasons:</p><p>Short-lived Credentials: Using OIDC federation allows GitHub Actions to obtain temporary security credentials through the AssumeRoleWithWebIdentity API call. These credentials are automatically rotated and have a limited lifespan, satisfying the security team's requirement to eliminate long-lived secret keys.</p><p>Minimal Operational Overhead: Once the initial setup is complete, the solution requires very little ongoing maintenance:</p><p>No manual credential rotation is needed</p><p>No secrets need to be stored in GitHub</p><p>No additional services or infrastructure to manage</p><p>Direct Integration: GitHub Actions has built-in support for OIDC, making it straightforward to configure workflows to use this authentication method.</p><p>Security Best Practices: This approach follows the principle of least privilege by:</p><p>Eliminating long-term credentials</p><p>Using temporary, scoped permissions</p><p>Creating a trust relationship specific to your GitHub repository/organization</p><p>Simplified Workflow: The CI/CD pipeline can authenticate to AWS without managing or rotating credentials, reducing complexity and potential points of failure.</p><p>The implementation process involves:</p><p>Creating an OIDC provider in IAM that trusts GitHub's OIDC service</p><p>Creating an IAM role with a trust policy that allows GitHub Actions to assume the role</p><p>Attaching the existing permissions policy to this new role</p><p>Updating the GitHub Actions workflow to use the OIDC provider for authentication</p><p>This federated approach provides a secure, automated way to handle authentication between GitHub Actions and AWS services without storing any long-lived credentials. It represents the most modern and secure approach to CI/CD authentication with the least operational overhead, making it the best solution for the given requirements.</p><p>Sources</p><p>Security best practices - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/terraform-aws-provider-best-practices/security.html）</p><p>SEC02-BP02 Use temporary credentials - AWS Well-Architected Framework (2022-03-31) （https://docs.aws.amazon.com/wellarchitected/2022-03-31/framework/sec_identities_unique.html）</p><p>Best practices working with self-hosted GitHub Action runners at scale on AWS | AWS DevOps & Developer Productivity Blog （https://aws.amazon.com/cn/blogs/devops/best-practices-working-with-self-hosted-github-action-runners-at-scale-on-aws/）</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "4e1a7f88ee5845ea8bcd12e4ba9e4b49",
      "questionNumber": 460,
      "type": "multiple",
      "content": "<p>Question #460</p><p>A company is running a web-crawling process on a list of target URLs to obtain training documents for machine learning training algorithms. A fleet of Amazon EC2 t2.micro instances pulls the target URLs from an Amazon Simple Queue Service (Amazon SQS) queue. The instances then write the result of the crawling algorithm as a .csv file to an Amazon Elastic File System (Amazon EFS) volume. The EFS volume is mounted on all instances of the fleet.</p><p><br></p><p>A separate system adds the URLs to the SQS queue at infrequent rates. The instances crawl each URL in 10 seconds or less. </p><p><br></p><p>Metrics indicate that some instances are idle when no URLs are in the SQS queue. A solutions architect needs to redesign the architecture to optimize costs.</p><p><br></p><p>Which combination of steps will meet these requirements MOST cost-effectively? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Use m5.8xlarge instances instead of t2.micro instances for the web-crawling process. Reduce the number of instances in the fleet by 50%."
        },
        {
          "label": "B",
          "content": "Convert the web-crawling process into an AWS Lambda function. Configure the Lambda function to pull URLs from the SQS queue."
        },
        {
          "label": "C",
          "content": "Modify the web-crawling process to store results in Amazon Neptune."
        },
        {
          "label": "D",
          "content": "Modify the web-crawling process to store results in an Amazon Aurora Serverless MySQL instance."
        },
        {
          "label": "E",
          "content": "Modify the web-crawling process to store results in Amazon S3."
        }
      ],
      "correctAnswer": "BE",
      "explanation": "<p>Option B is correct as converting the web-crawling process into an AWS Lambda function would allow for on-demand execution without the need to maintain a fleet of EC2 instances. This would significantly reduce costs when the workload is variable and sporadic. Option E is also correct as storing results in Amazon S3 is a cost-effective solution for storing large amounts of data generated by the web-crawling process. S3 is a scalable and durable storage solution that can handle the output of the crawling process efficiently. Option A is not cost-effective as larger instances are more expensive and do not address the issue of idle time. Options C and D involve using more expensive database services that are not necessary for the use case and do not optimize for cost.</p><p> Answer: B & E &nbsp;</p><p>B. Convert the web-crawling process into an AWS Lambda function. Configure the Lambda function to pull URLs from the SQS queue. &nbsp;</p><p>E. Modify the web-crawling process to store results in Amazon S3. &nbsp;</p><p> Why? &nbsp;</p><p>1. Cost Optimization with AWS Lambda (Option B) &nbsp;</p><p> &nbsp; - Currently, EC2 t2.micro instances run continuously, leading to idle time when no URLs are in the queue. &nbsp;</p><p> &nbsp; - Lambda is event-driven, so it only runs when there’s a message in the SQS queue, eliminating idle costs. &nbsp;</p><p> &nbsp; - Since each crawl takes ≤10 seconds, Lambda fits well within its execution limits. &nbsp;</p><p> &nbsp; - Lambda scales automatically, removing the need to manage an EC2 fleet. &nbsp;</p><p>2. Cheaper Storage with Amazon S3 (Option E) &nbsp;</p><p> &nbsp; - EFS is expensive for storing large volumes of `.csv` files (charged per GB, plus throughput costs). &nbsp;</p><p> &nbsp; - S3 is significantly cheaper for object storage (pay only for storage used, no mounting or throughput fees). &nbsp;</p><p> &nbsp; - S3 is highly durable and integrates well with Lambda. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A (m5.8xlarge instances): Overkill for this workload (high cost, still runs idle). &nbsp;</p><p>- C (Neptune): A graph database, unsuitable for `.csv` storage (expensive for this use case). &nbsp;</p><p>- D (Aurora Serverless): More expensive than S3 for simple `.csv` storage (better for structured queries). &nbsp;</p><p> Conclusion &nbsp;</p><p>By using Lambda (B) for compute and S3 (E) for storage, the solution eliminates idle costs and reduces storage expenses, making it the most cost-effective choice. ✅</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "9ad51934c3154c6e9b2398b734ee9887",
      "questionNumber": 461,
      "type": "single",
      "content": "<p>Question #461</p><p>A company needs to migrate its website from an on-premises data center to AWS. The website consists of a load balancer, a content management system (CMS) that runs on a Linux operating system, and a MySQL database.</p><p>The CMS requires persistent NFS-compatible storage for a file system. The new solution on AWS must be able to scale from 2 Amazon EC2 instances to 30 EC2 instances in response to unpredictable traffic increases. The new solution also must require no changes to the website and must prevent data loss.</p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an Amazon Elastic File System (Amazon EFS) file system. Deploy the CMS to AWS Elastic Beanstalk with an Application Load Balancer and an Auto Scaling group. Use .ebextensions to mount the EFS file system to the EC2 instances. Create an Amazon Aurora MySQL database that is separate from the Elastic Beanstalk environment."
        },
        {
          "label": "B",
          "content": "Create an Amazon Elastic Block Store (Amazon EBS) Multi-Attach volume. Deploy the CMS to AWS Elastic Beanstalk with a Network Load Balancer and an Auto Scaling group. Use .ebextensions to mount the EBS volume to the EC2 instances. Create an Amazon RDS for MySQL database in the Elastic Beanstalk environment."
        },
        {
          "label": "C",
          "content": "Create an Amazon Elastic File System (Amazon EFS) file system. Create a launch template and an Auto Scaling group to launch EC2 instances to support the CMS. Create a Network Load Balancer to distribute traffic. Create an Amazon Aurora MySQL database. Use an EC2 Auto Scaling scale-in lifecycle hook to mount the EFS file system to the EC2 instances."
        },
        {
          "label": "D",
          "content": "Create an Amazon Elastic Block Store (Amazon EBS) Multi-Attach volume. Create a launch template and an Auto Scaling group to launch EC2 instances to support the CMS. Create an Application Load Balancer to distribute traffic. Create an Amazon ElastiCache for Redis cluster to support the MySQL database. Use EC2 user data to attach the EBS volume to the EC2 instances."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Create an Amazon ElasticFile System (Amazon EFS) file system. Deploy the CMS to AWS Elastic Beanstalk with an Application Load Balancer and an Auto Scaling group. Use .ebextensions to mount the EFS file system to the EC2 instances. Create an Amazon Aurora MySQL database that is separate from the Elastic Beanstalk environment.</p><p>This solution will meet all the requirements for the following reasons:</p><p>NFS-compatible Storage Requirement:</p><p>Amazon EFS provides a fully managed, scalable NFS file system that can be mounted on multiple EC2 instances simultaneously.</p><p>This satisfies the CMS requirement for persistent NFS-compatible storage for its file system.</p><p>Scalability Requirement:</p><p>AWS Elastic Beanstalk with an Auto Scaling group can easily handle scaling from 2 to 30 EC2 instances based on traffic demands.</p><p>The Application Load Balancer will distribute incoming traffic across all available instances, ensuring optimal performance during traffic spikes.</p><p>No Changes to Website Requirement:</p><p>By using .ebextensions to mount the EFS file system to the EC2 instances, the shared storage configuration is handled automatically for each instance.</p><p>This ensures that all instances access the same files, maintaining consistency across the environment without requiring changes to the website code.</p><p>The CMS will continue to operate as it did on-premises, with its file system accessible via the same paths.</p><p>Data Loss Prevention Requirement:</p><p>Amazon EFS provides high durability and availability for the file system data, storing data redundantly across multiple Availability Zones.</p><p>Amazon Aurora MySQL offers enhanced durability and reliability compared to standard MySQL, with automated backups and point-in-time recovery.</p><p>Separating the database from the Elastic Beanstalk environment ensures that database data persists independently of the application tier.</p><p>Additional Benefits:</p><p>Elastic Beanstalk simplifies deployment and management of the application environment.</p><p>Aurora MySQL is compatible with standard MySQL, allowing for a seamless migration from the on-premises MySQL database.</p><p>The architecture separates concerns (compute, storage, database) for better scalability and management.</p><p>This solution provides a comprehensive approach that addresses all the requirements while leveraging AWS managed services to reduce operational overhead. It creates a highly available, scalable, and resilient architecture that can handle unpredictable traffic patterns while ensuring data persistence and requiring no changes to the existing website.</p><p>Sources</p><p>Moving to an AWS architecture - Hosting Static Websites on AWS （https://docs.aws.amazon.com/whitepapers/latest/build-static-websites-aws/moving-to-an-aws-architecture.html）</p><p>Reference architecture - Best Practices for WordPress on AWS （https://docs.aws.amazon.com/whitepapers/latest/best-practices-wordpress/reference-architecture.html）</p><p>Deploying a high-availability Drupal website with an external Amazon RDS database to Elastic Beanstalk - AWS Elastic Beanstalk （https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/php-hadrupal-tutorial.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "610e4044bd7045c0bc3693656b51266c",
      "questionNumber": 462,
      "type": "single",
      "content": "<p>Question #462</p><p>A company needs to implement disaster recovery for a critical application that runs in a single AWS Region. The application's users interact with a web frontend that is hosted on Amazon EC2 instances behind an Application Load Balancer (ALB). The application writes to an Amazon RDS for MySQL DB instance. The application also outputs processed documents that are stored in an Amazon S3 bucket.</p><p>The company’s finance team directly queries the database to run reports. During busy periods, these queries consume resources and negatively affect application performance.</p><p>A solutions architect must design a solution that will provide resiliency during a disaster. The solution must minimize data loss and must resolve the performance problems that result from the finance team's queries.</p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Migrate the database to Amazon DynamoDB and use DynamoDB global tables. Instruct the finance team to query a global table in a separate Region. Create an AWS Lambda function to periodically synchronize the contents of the original S3 bucket to a new S3 bucket in the separate Region. Launch EC2 instances and create an ALB in the separate Region. Configure the application to point to the new S3 bucket."
        },
        {
          "label": "B",
          "content": "Launch additional EC2 instances that host the application in a separate Region. Add the additional instances to the existing ALB in the separate Region, create a read replica of the RDS DB instance. Instruct the finance team to run queries against the read replica. Use S3 Cross Region Replication (CRR) from the original S3 bucket to a new S3 bucket in the separate Region. During a disaster, promote the read replica to a standalone DB instance. Configure the application to point to the new S3 bucket and to the newly promoted read replica."
        },
        {
          "label": "C",
          "content": "Create a read replica of the RDS DB instance in a separate Region. Instruct the finance team to run queries against the read replica. Create AMIs of the EC2 instances that host the application frontend. Copy the AMIs to the separate Region. Use S3 Cross-Region Replication (CRR) from the original S3 bucket to a new S3 bucket in the separate Region. During a disaster, promote the read replica to a standalone DB instance. Launch EC2 instances from the AMIs and create an ALB to present the application to end users. Configure the application to point to the new S3 bucket."
        },
        {
          "label": "D",
          "content": "Create hourly snapshots of the RDS DB instance. Copy the snapshots to a separate Region. Add an Amazon ElastiCache cluster in front of the existing RDS database. Create AMIs of the EC2 instances that host the application frontend. Copy the AMIs to the separate Region. Use S3 Cross-Region Replication (CRR) from the original S3 bucket to a new S3 bucket in the separate Region. During a disaster, restore the database from the latest RDS snapshot. Launch EC2 instances from the AMIs and create an ALB to present the application to end users. Configure the application to point to the new S3 bucket."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Option C is the correct solution as it addresses both disaster recovery and performance optimization. By creating a read replica of the RDS DB instance in a separate Region, the solution ensures that the finance team can run their queries without impacting the performance of the primary application. Additionally, the use of AMIs for EC2 instances allows for rapid recovery in the event of a disaster, and S3 Cross-Region Replication ensures that the documents stored in S3 are also protected. Option A is less suitable because migrating to DynamoDB would require changes to the application, which is not desired. Option B is not as efficient because it does not address the performance issue during normal operations. Option D is less preferable because it relies on snapshots, which do not provide the same level of real-time replication and availability as a read replica.</p><p> Answer: C &nbsp;</p><p>Create a read replica of the RDS DB instance in a separate Region. Instruct the finance team to run queries against the read replica. Create AMIs of the EC2 instances that host the application frontend. Copy the AMIs to the separate Region. Use S3 Cross-Region Replication (CRR) from the original S3 bucket to a new S3 bucket in the separate Region. During a disaster, promote the read replica to a standalone DB instance. Launch EC2 instances from the AMIs and create an ALB to present the application to end users. Configure the application to point to the new S3 bucket. &nbsp;</p><p> Why? &nbsp;</p><p>1. Disaster Recovery (Minimal Data Loss & Fast Recovery) &nbsp;</p><p> &nbsp; - RDS Read Replica in a separate Region provides near real-time replication (low RPO). &nbsp;</p><p> &nbsp; - Promoting the read replica during a disaster ensures fast failover (low RTO). &nbsp;</p><p> &nbsp; - S3 Cross-Region Replication (CRR) ensures documents are available in the secondary Region. &nbsp;</p><p> &nbsp; - AMI copy + ALB setup allows quick application deployment in the secondary Region. &nbsp;</p><p>2. Performance Issue Resolution &nbsp;</p><p> &nbsp; - Finance team queries run on the read replica, preventing impact on the primary database. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A (DynamoDB + Lambda sync): &nbsp;</p><p> &nbsp;- Migrating to DynamoDB is unnecessary (RDS is suitable for MySQL workloads). &nbsp;</p><p> &nbsp;- Lambda sync for S3 is less efficient than built-in S3 CRR. &nbsp;</p><p> &nbsp;- DynamoDB global tables are for multi-active workloads, not disaster recovery. &nbsp;</p><p>- B (Adding instances to ALB in a separate Region): &nbsp;</p><p> &nbsp;- ALB does not support cross-Region routing, so this setup is invalid. &nbsp;</p><p> &nbsp;- The rest is similar to Option C, but the ALB approach is incorrect. &nbsp;</p><p>- D (RDS Snapshots + ElastiCache): &nbsp;</p><p> &nbsp;- Hourly snapshots mean potential data loss (RPO of 1 hour) vs. near real-time with read replicas. &nbsp;</p><p> &nbsp;- ElastiCache helps with performance but doesn’t solve disaster recovery as effectively as a read replica. &nbsp;</p><p> Conclusion &nbsp;</p><p>Option C provides: &nbsp;</p><p>✅ Minimal data loss (read replica sync). &nbsp;</p><p>✅ Fast recovery (promote replica, deploy AMIs). &nbsp;</p><p>✅ Performance isolation (finance team uses read replica). &nbsp;</p><p>✅ S3 CRR for document resilience. &nbsp;</p><p>Thus, C is the best solution. ✅</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "ac4710fbb9224d88925fdc1affd21995",
      "questionNumber": 463,
      "type": "single",
      "content": "<p>Question #463</p><p>A company has many services running in its on-premises data center. The data center is connected to AWS using AWS Direct Connect (DX) and an IPSec VPN. The service data is sensitive and connectivity cannot traverse the internet. The company wants to expand into a new market segment and begin offering its services to other companies that are using AWS.</p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a VPC Endpoint Service that accepts TCP traffic, host it behind a Network Load Balancer, and make the service available over DX."
        },
        {
          "label": "B",
          "content": "Create a VPC Endpoint Service that accepts HTTP or HTTPS traffic, host it behind an Application Load Balancer, and make the service available over DX."
        },
        {
          "label": "C",
          "content": "Attach an internet gateway to the VPC, and ensure that network access control and security group rules allow the relevant inbound and outbound traffic."
        },
        {
          "label": "D",
          "content": "Attach a NAT gateway to the VPC, and ensure that network access control and security group rules allow the relevant inbound and outbound traffic."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Create a VPC Endpoint Service that accepts TCP traffic, host it behind a Network Load Balancer, and make the service available over DX.</p><p>This solution will meet the requirements for the following reasons:</p><p>Private Connectivity Requirement: The company needs to ensure that sensitive service data does not traverse the internet. Using a VPC Endpoint Service with AWS PrivateLink in combination with Direct Connect creates a private connection that keeps all traffic off the public internet.</p><p>Network Load Balancer Advantage: A Network Load Balancer (NLB) operates at Layer 4 (transport layer) and supports TCP traffic, which provides:</p><p>Greater protocol flexibility for various types of services</p><p>Lower latency and higher throughput for data-intensive applications</p><p>Support for both web and non-web based protocols</p><p>AWS PrivateLink Integration: VPC Endpoint Services are designed to work with AWS PrivateLink, allowing the company to expose their services to other AWS customers without requiring internet connectivity. The service can be made available to specific AWS accounts, organizations, or publicly to all AWS customers.</p><p>Direct Connect Compatibility: The solution leverages the existing Direct Connect connection between the on-premises data center and AWS, ensuring that traffic between the company's on-premises services and AWS remains on a private network.</p><p>Security for Sensitive Data: By keeping all traffic on private networks (either Direct Connect or AWS's private network), the solution maintains the security of sensitive data by eliminating exposure to the public internet.</p><p>Option B would not be as suitable because:</p><p>An Application Load Balancer (ALB) only supports HTTP/HTTPS traffic, which may be too restrictive if the company's services require other protocols</p><p>ALBs operate at Layer 7 (application layer), which adds overhead and may not be necessary for all types of services</p><p>Some services may not be compatible with HTTP/HTTPS protocols</p><p>The solution in Option A provides the most flexible and secure approach for the company to expand its market segment by offering its on-premises services to other AWS customers while ensuring that sensitive data remains protected and never traverses the public internet.</p><p>Sources</p><p>AWS PrivateLink: Enhancing Security and Connectivity with AWS PrivateLink | AWS re:Post（https://repost.aws/articles/ARw-phoCRnSlqXN512Ibr9SA/aws-privatelink-enhancing-security-and-connectivity-with-aws-privatelink） </p><p>AWS Direct Connect &nbsp;- Building a Scalable and Secure Multi-VPC AWS Network Infrastructure （https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/direct-connect.html）</p><p>AWS PrivateLink: Enhancing Security and Connectivity with AWS PrivateLink | AWS re:Post （https://repost.aws/articles/AR3l2gl0W5TpePneYc_E1mDA/aws-privatelink-enhancing-security-and-connectivity-with-aws-privatelink）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "df34762b52374fc0b3d6b2982f45271d",
      "questionNumber": 464,
      "type": "single",
      "content": "<p>Question #464</p><p>A company uses AWS Organizations to manage its AWS accounts. A solutions architect must design a solution in which only administrator roles are allowed to use IAM actions. However, the solutions architect does not have access to all the AWS accounts throughout the company.</p><p>Which solution meets these requirements with the LEAST operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an SCP that applies to all the AWS accounts to allow IAM actions only for administrator roles. Apply the SCP to the root OU."
        },
        {
          "label": "B",
          "content": "Configure AWS CloudTrail to invoke an AWS Lambda function for each event that is related to IAM actions. Configure the function to deny the action if the user who invoked the action is not an administrator."
        },
        {
          "label": "C",
          "content": "Create an SCP that applies to all the AWS accounts to deny IAM actions for all users except for those with administrator roles. Apply the SCP to the root OU."
        },
        {
          "label": "D",
          "content": "Set an IAM permissions boundary that allows IAM actions. Attach the permissions boundary to every administrator role across all the AWS accounts."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the requirement to allow only administrator roles to use IAM actions across AWS accounts in an organization with the least operational overhead, the correct answer is:</p><p>C. Create an SCP that applies to all the AWS accounts to deny IAM actions for all users except for those with administrator roles. Apply the SCP to the root OU.</p><p>This solution is the most effective with the least operational overhead for several key reasons:</p><p>1.Centralized implementation: By applying the SCP to the root OU, it automatically affects all accounts in the organization without requiring access to each individual account.</p><p>2.Deny-based approach: Using a deny statement with an exception is more secure than an allow-based approach. SCPs use a deny-by-default model, so explicitly denying IAM actions except for administrator roles creates a stronger security boundary.</p><p>3.Low maintenance: Once implemented, this solution requires minimal ongoing management. The SCP will automatically apply to new accounts added to the organization.</p><p>4.Preventive control: This approach prevents unauthorized IAM actions before they occur, rather than detecting and responding to them after the fact.</p><p><br></p><p>The other options have significant drawbacks:</p><p>A. Creating an SCP that only allows IAM actions for administrator roles is less effective because SCPs work as permission boundaries (they don't grant permissions, they limit them). An allow-only SCP would need to be carefully crafted to avoid unintended consequences and would be more complex to maintain.</p><p><br></p><p>B. Using CloudTrail with Lambda would be a detective and reactive approach rather than preventive. This would have much higher operational overhead due to:</p><p><br></p><p>Setting up and maintaining the Lambda function</p><p>Processing CloudTrail logs (which could incur additional costs)</p><p>Potential latency between when an action occurs and when it's detected</p><p>The solution would only detect violations after they occur rather than preventing them</p><p>The SCP approach in option C provides a preventive, organization-wide control that requires minimal maintenance and doesn't require access to individual accounts, making it the solution with the least operational overhead.</p><p>Sources</p><p>AWS Identity and Access Management and AWS Organizations - AWS Organizations （https://docs.aws.amazon.com/organizations/latest/userguide/services-that-can-integrate-iam.html）</p><p>How to implement a read-only service control policy (SCP) for accounts in AWS Organizations | AWS Cloud Operations & Migrations Blog （https://aws.amazon.com/blogs/mt/implement-read-only-service-control-policy-in-aws-organizations/）</p><p>Service control policies (SCPs) - AWS Organizations （https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "a48308fee18d410387d5fbf66c9ea458",
      "questionNumber": 465,
      "type": "single",
      "content": "<p>Question #465</p><p>A company uses an organization in AWS Organizations to manage multiple AWS accounts. The company hosts some applications in a VPC in the company's shared services account.</p><p>The company has attached a transit gateway to the VPC in the shared services account.</p><p>The company is developing a new capability and has created a development environment that requires access to the applications that are in the shared services account. The company intends to delete and recreate resources frequently in the development account. The company also wants to give a development team the ability to recreate the team's connection to the shared services account as required.</p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a transit gateway in the development account. Create a transit gateway peering request to the shared services account. Configure the shared services transit gateway to automatically accept peering connections."
        },
        {
          "label": "B",
          "content": "Turn on automatic acceptance for the transit gateway in the shared services account. Use AWS Resource Access Manager (AWS RAM) to share the transit gateway resource in the shared services account with the development account. Accept the resource in the development account. Create a transit gateway attachment in the development account."
        },
        {
          "label": "C",
          "content": "Turn on automatic acceptance for the transit gateway in the shared services account. Create a VPC endpoint. Use the endpoint policy to grant permissions on the VPC endpoint for the development account. Configure the endpoint service to automatically accept connection requests."
        },
        {
          "label": "D",
          "content": "Create an Amazon EventBridge rule to invoke an AWS Lambda function that accepts the transit gateway attachment when the development account makes an attachment request. Use AWS Network Manager to share the transit gateway in the shared services account with the development account. Accept the transit gateway in the development account."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Option B is the correct solution as it allows for the sharing of the transit gateway with the development account using AWS Resource Access Manager, which supports the ability to automatically accept connections. This approach provides the flexibility for the development team to recreate their connection to the shared services account as needed, meeting the requirement for frequent deletion and recreation of resources. Option A is incorrect because peering is used between VPCs in different accounts, not for connecting to a transit gateway. Option C is incorrect because VPC endpoints do not support automatic acceptance of connections in this context. Option D is overly complex and does not directly address the requirement for transit gateway connectivity.</p><p>Turn on automatic acceptance for the transit gateway in the shared services account. Use AWS Resource Access Manager (AWS RAM) to share the transit gateway resource in the shared services account with the development account. Accept the resource in the development account. Create a transit gateway attachment in the development account. &nbsp;</p><p> Why? &nbsp;</p><p>1. Meets Key Requirements: &nbsp;</p><p> &nbsp; - Frequent Recreations in Dev Account: The development team can create and delete transit gateway attachments as needed (self-service capability). &nbsp;</p><p> &nbsp; - Access to Shared Services: AWS RAM allows secure sharing of the transit gateway from the shared services account to the development account. &nbsp;</p><p> &nbsp; - Automatic Acceptance: Enabling auto-acceptance ensures seamless connectivity without manual approvals. &nbsp;</p><p>2. Simplified & Scalable: &nbsp;</p><p> &nbsp; - AWS RAM is the recommended way to share transit gateways across accounts in an AWS Organization. &nbsp;</p><p> &nbsp; - No complex scripting (Lambda/EventBridge) is needed (unlike Option D). &nbsp;</p><p> &nbsp; - No peering (Option A) is required, which is more complex and unnecessary for this use case. &nbsp;</p><p> &nbsp; - VPC endpoints (Option C) are not suitable for transit gateway connectivity. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A (Transit Gateway Peering): &nbsp;</p><p> &nbsp;- Peering is not needed—RAM sharing is simpler and more efficient. &nbsp;</p><p> &nbsp;- Peering requires manual acceptance unless auto-accept is configured, but RAM is still the better choice. &nbsp;</p><p>- C (VPC Endpoint): &nbsp;</p><p> &nbsp;- VPC endpoints are for private AWS services (S3, DynamoDB, etc.), not transit gateway connections. &nbsp;</p><p> &nbsp;- This does not provide the required networking setup. &nbsp;</p><p>- D (EventBridge + Lambda + Network Manager): &nbsp;</p><p> &nbsp;- Overly complex—RAM already provides a native, automated way to share transit gateways. &nbsp;</p><p> &nbsp;- Network Manager is for monitoring, not resource sharing. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "50ee4dd5b5e0428dbced91a98916403a",
      "questionNumber": 466,
      "type": "single",
      "content": "<p>Question #466</p><p>A company wants to migrate virtual Microsoft workloads from an on-premises data center to AWS. The company has successfully tested a few sample workloads on AWS. The company also has created an AWS Site-to-Site VPN connection to a VPC. A solutions architect needs to generate a total cost of ownership (TCO) report for the migration of all the workloads from the data center.</p><p>Simple Network Management Protocol (SNMP) has been enabled on each VM in the data center. The company cannot add more VMs in the data center and cannot install additional software on the VMs. The discovery data must be automatically imported into AWS Migration Hub.</p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use the AWS Application Migration Service agentless service and the AWS Migration Hub Strategy Recommendations to generate the TCO report."
        },
        {
          "label": "B",
          "content": "Launch a Windows Amazon EC2 instance. Install the Migration Evaluator agentless collector on the EC2 instance. Configure Migration Evaluator to generate the TCO report."
        },
        {
          "label": "C",
          "content": "Launch a Windows Amazon EC2 instance. Install the Migration Evaluator agentless collector on the EC2 instance. Configure Migration Hub to generate the TCO report."
        },
        {
          "label": "D",
          "content": "Use the AWS Migration Readiness Assessment tool inside the VPC. Configure Migration Evaluator to generate the TCO report."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Launch a Windows Amazon EC2 instance. Install the Migration Evaluator agentless collector on the EC2 instance. Configure Migration Evaluator to generate the TCO report.</p><p>This solution will meet the requirements for the following reasons:</p><p>Non-intrusive Data Collection: The Migration Evaluator agentless collector can gather information from the on-premises VMs that have SNMP enabled without requiring any additional software installation on those VMs. This satisfies the requirement that the company cannot install additional software on the VMs.</p><p>TCO Report Generation: Migration Evaluator (formerly TSO Logic) is specifically designed to generate total cost of ownership (TCO) reports for migrations to AWS. It can analyze the current on-premises environment, including Microsoft workloads, and provide detailed cost comparisons between on-premises and AWS environments.</p><p>Utilization of Existing Infrastructure: The solution leverages the existing AWS Site-to-Site VPN connection to the VPC, allowing the EC2 instance running the Migration Evaluator agentless collector to communicate with the on-premises environment.</p><p>No Additional On-premises VMs: Since the collector is installed on an EC2 instance in AWS, there's no need to add more VMs in the on-premises data center, which the company cannot do according to the requirements.</p><p>Microsoft Workload Support: Migration Evaluator has specific capabilities for assessing Microsoft workloads, including analyzing server profiles, utilization data, and Microsoft licensing considerations.</p><p>While the Migration Evaluator itself doesn't directly integrate with AWS Migration Hub for automatic import of discovery data, it can collect the necessary information to generate the TCO report, which is the primary requirement stated in the question. The EC2 instance with the agentless collector can communicate with the on-premises VMs via SNMP over the existing Site-to-Site VPN connection.</p><p>This solution provides the most efficient way to generate a TCO report for the migration without installing additional software on the VMs or adding new VMs to the on-premises environment, making it the best match for the stated requirements.</p><p>Sources</p><p>Creating a directional business case - AWS Prescriptive Guidance（https://docs.aws.amazon.com/prescriptive-guidance/latest/application-portfolio-assessment-guide/directional-business-case.html） </p><p>Seeking Advice on Utilizing Migration Evaluator for AWS Migration Business Cases | AWS re:Post （https://repost.aws/questions/QU11RsGVWsQpOYHehhrbHpPA/seeking-advice-on-utilizing-migration-evaluator-for-aws-migration-business-cases）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "dbdaf690faa34c2c9dd50d4a30010428",
      "questionNumber": 467,
      "type": "single",
      "content": "Question #467<p>A company that is developing a mobile game is making game assets available in two AWS Regions. Game assets are served from a set of Amazon EC2 instances behind an Application Load Balancer (ALB) in each Region. The company requires game assets to be fetched from the closest Region. If game assets become unavailable in the closest Region, they should be fetched from the other Region.</p><p>What should a solutions architect do to meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an Amazon CloudFront distribution. Create an origin group with one origin for each ALB. Set one of the origins as primary."
        },
        {
          "label": "B",
          "content": "Create an Amazon Route 53 health check for each ALB. Create a Route 53 failover routing record pointing to the two ALBs. Set the Evaluate Target Health value to Yes."
        },
        {
          "label": "C",
          "content": "Create two Amazon CloudFront distributions, each with one ALB as the origin. Create an Amazon Route 53 failover routing record pointing to the two CloudFront distributions. Set the Evaluate Target Health value to Yes."
        },
        {
          "label": "D",
          "content": "Create an Amazon Route 53 health check for each ALB. Create a Route 53 latency alias record pointing to the two ALBs. Set the Evaluate Target Health value to Yes."
        }
      ],
      "correctAnswer": "D",
      "explanation": "To meet the company's requirements for serving game assets from the closest AWS Region and providing failover capability, the best solution is:<p><br></p><p>D. Create an Amazon Route 53 health check for each ALB. Create a Route 53 latency alias record pointing to the two ALBs. Set the Evaluate Target Health value to Yes.</p><p><br></p><p>Here's why this solution is the most appropriate:</p><p>1. Latency-based routing: Route 53 latency routing allows you to direct traffic to the Region with the lowest latency for each user, ensuring game assets are fetched from the closest Region.</p><p>2. Health checks: By creating health checks for each ALB, Route 53 can detect if a Region's assets become unavailable.</p><p>3. Failover capability: Setting \"Evaluate Target Health\" to Yes enables automatic failover. If the closest Region's ALB fails the health check, Route 53 will route traffic to the other Region.</p><p>4. Alias records: Using alias records allows you to route traffic directly to ALBs, which is more efficient and cost-effective than using non-alias records.</p><p>This solution provides several benefits:</p><p>- Improved performance by serving assets from the closest Region</p><p>- Automatic failover for high availability</p><p>- Cost-effective implementation using native AWS services</p><p>To implement this solution:</p><p><br></p><p>1. Create a Route 53 health check for each ALB to monitor their availability.</p><p>2. Set up a latency-based routing policy in Route 53.</p><p>3. Create alias records for each ALB, associating them with the latency-based routing policy.</p><p>4. Enable \"Evaluate Target Health\" for each record.</p><p><br></p><p>Remember to configure appropriate security groups and network ACLs for your ALBs and EC2 instances to ensure secure communication. Also, regularly review and update your routing policies and health check configurations to maintain optimal performance and reliability.</p><p>Sources</p><p>[1] [Use an Application Load Balancer for Amazon ECS - Amazon Elastic Container Service] (https://docs.aws.amazon.com/AmazonECS/latest/developerguide/alb.html)</p><p>[3] [Application Load Balancers - Elastic Load Balancing] (https://docs.aws.amazon.com/elasticloadbalancing/latest/application/application-load-balancers.html)</p><p>[6] [Load Balancing - Amazon EKS] (https://docs.aws.amazon.com/eks/latest/best-practices/load-balancing.html)</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "6c04e6cad13148978bf8c1914c0c240d",
      "questionNumber": 468,
      "type": "single",
      "content": "<p>Question #468</p><p>A company deploys workloads in multiple AWS accounts. Each account has a VPC with VPC flow logs published in text log format to a centralized Amazon S3 bucket. Each log file is compressed with gzip compression. The company must retain the log files indefinitely.</p><p>A security engineer occasionally analyzes the logs by using Amazon Athena to query the VPC flow logs. The query performance is degrading over time as the number of ingested logs is growing. A solutions architect must improve the performance of the log analysis and reduce the storage space that the VPC flow logs use.</p><p>Which solution will meet these requirements with the LARGEST performance improvement?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an AWS Lambda function to decompress the gzip files and to compress the files with bzip2 compression. Subscribe the Lambda function to an s3:ObjectCreated:Put S3 event notification for the S3 bucket."
        },
        {
          "label": "B",
          "content": "Enable S3 Transfer Acceleration for the S3 bucket. Create an S3 Lifecycle configuration to move files to the S3 Intelligent-Tiering storage class as soon as the files are uploaded."
        },
        {
          "label": "C",
          "content": "Update the VPC flow log configuration to store the files in Apache Parquet format. Specify hourly partitions for the log files."
        },
        {
          "label": "D",
          "content": "Create a new Athena workgroup without data usage control limits. Use Athena engine version 2."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Update the VPC flow log configuration to store the files in Apache Parquet format. Specify hourly partitions for the log files.</p><p>This solution will provide the largest performance improvement for the following reasons:</p><p>Columnar Storage Format: Apache Parquet is a columnar storage format that significantly improves query performance in Athena compared to text log formats. When querying VPC flow logs, you typically only need specific columns (like source IP, destination IP, or time ranges), and Parquet allows Athena to read only the required columns rather than scanning the entire dataset.</p><p>Compression Efficiency: Parquet provides better compression than gzip-compressed text files, typically reducing storage space by about 20%. This addresses the requirement to reduce storage space while also improving query performance by reducing the amount of data that needs to be scanned.</p><p>Hourly Partitioning: Partitioning the data by hour creates a time-based directory structure that allows Athena to skip irrelevant data blocks during queries. When the security engineer runs time-based queries (which are common for log analysis), Athena will only scan the partitions that match the time range in the query, dramatically reducing the amount of data processed.</p><p>Predicate Pushdown: Parquet format enables predicate pushdown, which pushes filtering operations closer to the data source. This means filtering happens before data is loaded into memory, significantly reducing the amount of data that needs to be processed.</p><p>Long-term Scalability: As the company needs to retain logs indefinitely, the performance benefits of Parquet format and partitioning become increasingly important over time as the dataset grows.</p><p>Option D (creating a new Athena workgroup without data usage control limits and using Athena engine version 2) would provide some performance improvements but doesn't address the fundamental issue of inefficient data format and organization. While Athena engine version 2 is faster than version 1, the performance gains would be much smaller compared to the optimization of the data format itself.</p><p>The Parquet format with hourly partitioning directly addresses both requirements: improving query performance and reducing storage space. This approach provides a more sustainable solution as the volume of log data continues to grow indefinitely.</p><p>Sources</p><p>Create tables for flow logs in Apache Parquet format - Amazon Athena （https://docs.aws.amazon.com/athena/latest/ug/vpc-flow-logs-parquet.html）</p><p>Create tables for flow logs in Apache Parquet format using partition projection - Amazon Athena （https://docs.aws.amazon.com/athena/latest/ug/vpc-flow-logs-partition-projection-parquet-example.html）</p><p>Monitoring and optimizing the data lake environment - Storage Best Practices for Data and Analytics Applications （https://docs.aws.amazon.com/whitepapers/latest/building-data-lakes/monitoring-optimizing-data-lake-environment.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "54cf7bbd57654d08a249427d123dd4ce",
      "questionNumber": 469,
      "type": "multiple",
      "content": "<p>Question #469</p><p>A company wants to establish a dedicated connection between its on-premises infrastructure and AWS. The company is setting up a 1 Gbps AWS Direct Connect connection to its account VPC. The architecture includes a transit gateway and a Direct Connect gateway to connect multiple VPCs and the on-premises infrastructure.</p><p><br></p><p>The company must connect to VPC resources over a transit VIF by using the Direct Connect connection.</p><p><br></p><p>Which combination of steps will meet these requirements? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Update the 1 Gbps Direct Connect connection to 10 Gbps."
        },
        {
          "label": "B",
          "content": "Advertise the on-premises network prefixes over the transit VIF."
        },
        {
          "label": "C",
          "content": "Advertise the VPC prefixes from the Direct Connect gateway to the on-premises network over the transit VIF."
        },
        {
          "label": "D",
          "content": "Update the Direct Connect connection&#39;s MACsec encryption mode attribute to must_encrypt."
        },
        {
          "label": "E",
          "content": "Associate a MACsec Connection Key Name/Connectivity Association Key (CKN/CAK) pair with the Direct Connect connection."
        }
      ],
      "correctAnswer": "BC",
      "explanation": "<p>Based on the information provided, the correct answers are:</p><p>B. Advertise the on-premises network prefixes over the transit VIF. </p><p>C. Advertise the VPC prefixes from the Direct Connect gateway to the on-premises network over the transit VIF.</p><p>These two steps are essential for establishing connectivity between the on-premises infrastructure and VPC resources over a transit VIF using the Direct Connect connection. Here's why these are the correct choices:</p><p>Advertise the on-premises network prefixes over the transit VIF (Option B):</p><p>This step is crucial because it informs the AWS network (specifically the Direct Connect gateway and transit gateway) about the reachable destinations in the on-premises network.</p><p>Without this advertisement, the transit gateway wouldn't know which traffic should be routed to the on-premises network via the Direct Connect gateway.</p><p>This enables traffic from VPC resources to reach the on-premises infrastructure by providing the necessary routing information.</p><p>Advertise the VPC prefixes from the Direct Connect gateway to the on-premises network over the transit VIF (Option C):</p><p>This step is equally important as it informs the on-premises network about the reachable AWS VPC destinations.</p><p>Without this advertisement, the on-premises routers wouldn't know which traffic should be sent over the Direct Connect connection to reach VPC resources.</p><p>This enables traffic from the on-premises infrastructure to reach the VPC resources by providing the necessary routing information.</p><p>The other options are not required for the specified setup:</p><p>Option A (Update the 1 Gbps Direct Connect connection to 10 Gbps): The existing 1 Gbps connection is sufficient for the setup. While increasing bandwidth might improve performance, it's not a requirement for establishing connectivity.</p><p>Option D (Update the Direct Connect connection's MACsec encryption mode attribute to must_encrypt) and Option E (Associate a MACsec Connection Key Name/Connectivity Association Key (CKN/CAK) pair): These options relate to MACsec encryption, which is only applicable for 10 Gbps and 100 Gbps Direct Connect connections at select locations. Since the company is using a 1 Gbps connection, MACsec encryption is not applicable.</p><p>The combination of options B and C establishes the bidirectional routing path necessary for connectivity between the on-premises infrastructure and VPC resources over the transit VIF using the Direct Connect connection, transit gateway, and Direct Connect gateway.</p><p>Sources</p><p>How to advertise remote VPC subnets into a local VPC Direct connect connection | AWS re:Post （https://repost.aws/questions/QU5X-SBJS1TOGg4o5VE-5WiA/how-to-advertise-remote-vpc-subnets-into-a-local-vpc-direct-connect-connection）</p><p>Allowed prefixes interactions for AWS Direct Connect gateways - AWS Direct Connect （https://docs.aws.amazon.com/directconnect/latest/UserGuide/allowed-to-prefixes.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "19855c38ff5c480abc4e2479a078f2a0",
      "questionNumber": 470,
      "type": "single",
      "content": "<p>Question #470</p><p>A company wants to use Amazon WorkSpaces in combination with thin client devices to replace aging desktops. Employees use the desktops to access applications that work with Clinical trial data. Corporate security policy states that access to the applications must be restricted to only company branch office locations. The company is considering adding an additional branch office in the next 6 months.</p><p>Which solution meets these requirements with the MOST operational efficiency?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an IP access control group rule with the list of public addresses from the branch offices. Associate the IP access control group with the WorkSpaces directory."
        },
        {
          "label": "B",
          "content": "Use AWS Firewall Manager to create a web ACL rule with an IPSet with the list of public addresses from the branch office locations. Associate the web ACL with the WorkSpaces directory."
        },
        {
          "label": "C",
          "content": "Use AWS Certificate Manager (ACM) to issue trusted device certificates to the machines deployed in the branch office locations. Enable restricted access on the WorkSpaces directory."
        },
        {
          "label": "D",
          "content": "Create a custom WorkSpace image with Windows Firewall configured to restrict access to the public addresses of the branch offices. Use the image to deploy the WorkSpaces."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Create an IP access control group rule with the list of public addresses from the branch offices. Associate the IP access control group with the WorkSpaces directory.</p><p>This solution meets the requirements with the most operational efficiency for the following reasons:</p><p>Purpose-Built Feature: IP access control groups are a native feature of Amazon WorkSpaces specifically designed to restrict access based on IP addresses. This makes it the most straightforward and efficient solution for the stated requirement.</p><p>Direct Integration: The IP access control groups integrate directly with the WorkSpaces directory, providing a clear and simple way to manage access restrictions without additional services or configurations.</p><p>Easy Management: When the company adds the new branch office in the next 6 months, updating the IP access control group will be a simple process of adding the new office's public IP address range to the existing group. This can be done quickly through the WorkSpaces console.</p><p>No Additional Services: This solution doesn't require setting up or managing any additional AWS services, reducing operational complexity and potential points of failure.</p><p>Cost-Effective: IP access control groups are included with WorkSpaces at no additional cost, making this solution more cost-effective than alternatives that might require additional services.</p><p>Compliance Support: This approach provides a clear and auditable method for enforcing the corporate security policy that restricts access to clinical trial data applications to only company branch office locations.</p><p>Option B (using AWS Firewall Manager with a web ACL) would be more complex and less operationally efficient because:</p><p>It involves setting up and managing additional AWS services (Firewall Manager and AWS WAF).</p><p>Web ACLs are primarily designed for web applications, not desktop access scenarios.</p><p>It introduces unnecessary complexity for a requirement that can be met with a simpler, built-in feature.</p><p>It would likely require more administrative overhead when adding the new branch office.</p><p>The IP access control group solution provides the most direct and operationally efficient way to meet the security requirements while also accommodating the planned expansion to an additional branch office in the near future.</p><p>Sources</p><p>IP access control groups for WorkSpaces Personal - Amazon WorkSpaces （https://docs.aws.amazon.com/workspaces/latest/adminguide/amazon-workspaces-ip-access-control-groups.html）</p><p>Workspaces - IP Restriction and Access via Corporate VPN | AWS re:Post （https://repost.aws/questions/QUJrRjN05oRaydKYEXhfcF-Q/workspaces-ip-restriction-and-access-via-corporate-vpn）</p><p>Managing IP access controls in Amazon WorkSpaces Secure Browser - Amazon WorkSpaces Secure Browser （https://docs.aws.amazon.com/workspaces-web/latest/adminguide/ip-access-controls.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "b3db23fcf5864b18abba2d58bae47f12",
      "questionNumber": 471,
      "type": "multiple",
      "content": "<p>Question #471</p><p>A company uses AWS Organizations. The company runs two firewall appliances in a centralized networking account. Each firewall appliance runs on a manually configured highly available Amazon EC2 instance. A transit gateway connects the VPC from the centralized networking account to VPCs of member accounts. Each firewall appliance uses a static private IP address that is then used to route traffic from the member accounts to the internet.</p><p>During a recent incident, a badly configured script initiated the termination of both firewall appliances. During the rebuild of the firewall appliances, the company wrote a new script to configure the firewall appliances at startup.</p><p>The company wants to modernize the deployment of the firewall appliances. The firewall appliances need the ability to scale horizontally to handle increased traffic when the network expands. The company must continue to use the firewall appliances to comply with company policy. The provider of the firewall appliances has confirmed that the latest version of the firewall code will work with all AWS services.</p><p>Which combination of steps should the solutions architect recommend to meet these requirements MOST cost-effectively? (Choose three.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Deploy a Gateway Load Balancer in the centralized networking account. Set up an endpoint service that uses AWS PrivateLink."
        },
        {
          "label": "B",
          "content": "Deploy a Network Load Balancer in the centralized networking account. Set up an endpoint service that uses AWS PrivateLink."
        },
        {
          "label": "C",
          "content": "Create an Auto Scaling group and a launch template that uses the new script as user data to configure the firewall appliances. Create a target group that uses the instance target type."
        },
        {
          "label": "D",
          "content": "Create an Auto Scaling group. Configure an AWS Launch Wizard deployment that uses the new script as user data to configure the firewall appliances. Create a target group that uses the IP target type."
        },
        {
          "label": "E",
          "content": "Create VPC endpoints in each member account. Update the route tables to point to the VPC endpoints."
        },
        {
          "label": "F",
          "content": "Create VPC endpoints in the centralized networking account. Update the route tables in each member account to point to the VPC endpoints."
        }
      ],
      "correctAnswer": "ACF",
      "explanation": "<p>The company needs a scalable, highly available, and cost-effective solution for deploying firewall appliances while maintaining compliance. Here's why these choices are the best: &nbsp;</p><p> A. Deploy a Gateway Load Balancer (GWLB) in the centralized networking account. Set up an endpoint service that uses AWS PrivateLink. &nbsp;</p><p>- Gateway Load Balancer (GWLB) is specifically designed for deploying third-party virtual appliances (like firewalls) in a scalable way. &nbsp;</p><p>- It integrates with AWS PrivateLink, allowing member accounts to securely route traffic through the centralized firewalls. &nbsp;</p><p>- This is more efficient than a Network Load Balancer (NLB) for this use case because GWLB handles traffic inspection and forwarding at scale. &nbsp;</p><p> C. Create an Auto Scaling group and a launch template that uses the new script as user data to configure the firewall appliances. Create a target group that uses the instance target type. &nbsp;</p><p>- Auto Scaling ensures high availability and horizontal scaling of firewall instances. &nbsp;</p><p>- The launch template with user data automates the configuration of new instances, eliminating manual setup. &nbsp;</p><p>- The target group (instance type) registers the firewall instances with the GWLB for traffic distribution. &nbsp;</p><p> F. Create VPC endpoints in the centralized networking account. Update the route tables in each member account to point to the VPC endpoints. &nbsp;</p><p>- VPC endpoints (Gateway Load Balancer endpoints, GWLBE) allow member accounts to route traffic to the centralized firewalls securely. &nbsp;</p><p>- Updating route tables in member accounts ensures all traffic passes through the firewall for inspection. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- B (Network Load Balancer + PrivateLink): NLB is not optimized for firewall traffic inspection like GWLB. &nbsp;</p><p>- D (AWS Launch Wizard + IP target type): Launch Wizard is unnecessary here, and IP target type is less efficient than instance target type for this scenario. &nbsp;</p><p>- E (VPC endpoints in member accounts): The endpoints should be in the centralized account, not member accounts, for proper traffic flow. &nbsp;</p><p> Conclusion: &nbsp;</p><p>A, C, F provide the most scalable, automated, and cost-effective solution while maintaining compliance. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "882518934fed47aa85aaf830e42a1c04",
      "questionNumber": 472,
      "type": "multiple",
      "content": "<p>Question #472</p><p>A solutions architect must implement a multi-Region architecture for an Amazon RDS for PostgreSQL database that supports a web application.</p><p>The database launches from an AWS CloudFormation template that includes AWS services and features that are present in both the primary and secondary Regions.</p><p>The database is configured for automated backups, and it has an RTO of 15 minutes and an RPO of 2 hours. The web application is configured to use an Amazon Route 53 record to route traffic to the database.</p><p>Which combination of steps will result in a highly available architecture that meets all the requirements? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a cross-Region read replica of the database in the secondary Region. Configure an AWS Lambda function in the secondary Region to promote the read replica during a failover event."
        },
        {
          "label": "B",
          "content": "In the primary Region, create a health check on the database that will invoke an AWS Lambda function when a failure is detected. Program the Lambda function to recreate the database from the latest database snapshot in the secondary Region and update the Route 53 host records for the database."
        },
        {
          "label": "C",
          "content": "Create an AWS Lambda function to copy the latest automated backup to the secondary Region every 2 hours."
        },
        {
          "label": "D",
          "content": "Create a failover routing policy in Route 53 for the database DNS record. Set the primary and secondary endpoints to the endpoints in each Region."
        },
        {
          "label": "E",
          "content": "Create a hot standby database in the secondary Region. Use an AWS Lambda function to restore the secondary database to the latest RDS automatic backup in the event that the primary database fails."
        }
      ],
      "correctAnswer": "AD",
      "explanation": "<p>Based on the information provided, the correct answers are:</p><p>A. Create a cross-Region read replica of the database in the secondary Region. Configure an AWS Lambda function in the secondary Region to promote the read replica during a failover event.</p><p>D. Create a failover routing policy in Route 53 for the database DNS record. Set the primary and secondary endpoints to the endpoints in each Region.</p><p>This combination of steps will result in a highly available multi-Region architecture that meets all the requirements for the following reasons:</p><p>Cross-Region Read Replica with Lambda-Automated Promotion (Option A):</p><p>This addresses the RTO requirement of 15 minutes by providing a standby database in the secondary Region that can be quickly promoted to primary status.</p><p>The Lambda function automates the promotion process, minimizing human intervention and reducing the time to failover.</p><p>Cross-Region replication for RDS PostgreSQL typically has very low replication lag (often seconds), which easily meets the RPO requirement of 2 hours.</p><p>This approach leverages the CloudFormation template mentioned in the scenario, as the same services and features are present in both Regions.</p><p>Route 53 Failover Routing Policy (Option D):</p><p>This ensures that the web application can seamlessly connect to the database after a failover event.</p><p>The failover routing policy automatically routes traffic to the healthy endpoint based on health checks.</p><p>When the primary database fails, Route 53 will detect this through health checks and automatically route traffic to the secondary Region's database endpoint.</p><p>This complements the database failover mechanism by handling the DNS resolution aspect of the architecture.</p><p>Together, these steps create a comprehensive disaster recovery solution that:</p><p>Maintains a synchronized copy of the database in a secondary Region (meeting the RPO requirement)</p><p>Provides automated promotion of the read replica during failover (meeting the RTO requirement)</p><p>Automatically redirects application traffic to the new primary database (ensuring application continuity)</p><p>Leverages the existing CloudFormation template and services available in both Regions</p><p>This combination provides the necessary components for a highly available multi-Region architecture that can withstand Region-level failures while meeting the specified RTO of 15 minutes and RPO of 2 hours.</p><p>Sources</p><p>How a large financial AWS customer implemented high availability and fast disaster recovery for Amazon Aurora PostgreSQL using Global Database and Amazon RDS Proxy | AWS Database Blog （https://aws.amazon.com/blogs/database/how-a-large-financial-aws-customer-implemented-ha-and-dr-for-amazon-aurora-postgresql-using-global-database-and-amazon-rds-proxy/）</p><p>AWS Backup vs global table/database for cost-effective RTO,RPO requirements | AWS re:Post（https://repost.aws/questions/QUWf0GuonKSvW8U2N4o8huHw/aws-backup-vs-global-table-database-for-cost-effective-rto-rpo-requirements） </p><p>Choosing the right database for your RTO and RPO requirements - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/strategy-database-disaster-recovery/choosing-database.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "71c4d9ea68f141a3b6e5c86e49f43393",
      "questionNumber": 473,
      "type": "single",
      "content": "<p>Question #473</p><p>An ecommerce company runs an application on AWS. The application has an Amazon API Gateway API that invokes an AWS Lambda function. The data is stored in an Amazon RDS for PostgreSQL DB instance.</p><p>During the company’s most recent flash sale, a sudden increase in API calls negatively affected the application's performance. A solutions architect reviewed the Amazon CloudWatch metrics during that time and noticed a significant increase in Lambda invocations and database connections. The CPU utilization also was high on the DB instance.</p><p>What should the solutions architect recommend to optimize the application's performance?</p>",
      "options": [
        {
          "label": "A",
          "content": "Increase the memory of the Lambda function. Modify the Lambda function to close the database connections when the data is retrieved."
        },
        {
          "label": "B",
          "content": "Add an Amazon ElastiCache for Redis cluster to store the frequently accessed data from the RDS database."
        },
        {
          "label": "C",
          "content": "Create an RDS proxy by using the Lambda console. Modify the Lambda function to use the proxy endpoint."
        },
        {
          "label": "D",
          "content": "Modify the Lambda function to connect to the database outside of the function&#39;s handler. Check for an existing database connection before creating a new connection."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Create an RDS proxy by using the Lambda console. Modify the Lambda function to use the proxy endpoint.</p><p>This solution will best optimize the application's performance for the following reasons:</p><p>Connection Pooling: During the flash sale, the sudden increase in API calls led to a significant increase in Lambda invocations, which in turn created many database connections. RDS Proxy provides efficient connection pooling, allowing multiple Lambda invocations to share a smaller pool of database connections rather than each creating its own connection.</p><p>Reduced Database Load: The high CPU utilization on the DB instance was likely caused by the overhead of establishing and managing numerous database connections. RDS Proxy reduces this overhead by maintaining persistent connections to the database and reusing them across multiple Lambda invocations.</p><p>Improved Scalability: RDS Proxy can handle connection surges during flash sales by queuing connection requests when the database reaches its connection limit, preventing connection errors and allowing the application to scale more effectively.</p><p>Minimal Code Changes: Implementing RDS Proxy requires only changing the database endpoint in the Lambda function, making it a relatively simple solution to implement compared to more complex connection management logic.</p><p>Automatic Failover Support: RDS Proxy provides seamless handling of database failovers, which improves the overall reliability of the application during high-traffic events.</p><p>Option D (modifying the Lambda function to connect outside the handler and check for existing connections) would provide some benefits by reusing connections within the same Lambda container instance. However, this approach has limitations:</p><p>It only works within a single Lambda container instance, and during high-traffic events, many new container instances would still be created, each establishing its own connection.</p><p>It requires more complex code changes and careful management of connection timeouts and error handling.</p><p>It doesn't address the fundamental issue of too many simultaneous connections to the database during traffic spikes.</p><p>RDS Proxy is specifically designed to address the exact issues observed during the flash sale: high numbers of Lambda invocations creating too many database connections and causing high CPU utilization on the database. By implementing RDS Proxy, the solutions architect can optimize the application's performance for future flash sales with minimal code changes and maximum effectiveness.</p><p>Sources</p><p>Which is preferred, Lambda-&gt;RDS Proxy-&gt;RDS or Lambda-&gt;RDS? | AWS re:Post （https://repost.aws/questions/QU1U72jyYWTcisPaIF3CSSqg/which-is-preferred-lambda-rds-proxy-rds-or-lambda-rds）</p><p>Amazon RDS Proxy - Amazon Relational Database Service （https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-proxy.html）</p><p>Reuse of database connections by a lambda. | AWS re:Post （https://repost.aws/questions/QUABFESymvR1aJ7MGijjp5SA/reuse-of-database-connections-by-a-lambda）</p><p>RDS Proxy causing a consistent increase in DB Connections from Lambda | AWS re:Post （https://repost.aws/questions/QU2hiJKCLUQcq5ApSro0ixMQ/rds-proxy-causing-a-consistent-increase-in-db-connections-from-lambda）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "5a7054921abb423c90d84dd03d127c0d",
      "questionNumber": 474,
      "type": "single",
      "content": "<p>Question #474</p><p>A retail company wants to improve its application architecture. The company's applications register new orders, handle returns of merchandise, and provide analytics. The applications store retail data in a MySQL database and an Oracle OLAP analytics database. All the applications and databases are hosted on Amazon EC2 instances. Each application consists of several components that handle different parts of the order process. These components use incoming data from different sources. A separate ETL job runs every week and copies data from each application to the analytics database.<br><br></p><p>A solutions architect must redesign the architecture into an event-driven solution that uses serverless services. The solution must provide updated analytics in near real time.<br><br>Which solution will meet these requirements? </p>",
      "options": [
        {
          "label": "A",
          "content": "Migrate the individual applications as microservices to Amazon Elastic Container Service (Amazon ECS) containers that use AWS Fargate. Keep the retail MySQL database on Amazon EC2. Move the analytics database to Amazon Neptune. Use Amazon Simple Queue Service (Amazon SQS) to send all the incoming data to the microservices and the analytics database."
        },
        {
          "label": "B",
          "content": "Create an Auto Scaling group for each application. Specify the necessary number of EC2 instances in each Auto Scaling group. Migrate the retail MySQL database and the analytics database to Amazon Aurora MySQL. Use Amazon Simple Notification Service (Amazon SNS) to send all the incoming data to the correct EC2 instances and the analytics database."
        },
        {
          "label": "C",
          "content": "Migrate the individual applications as microservices to Amazon Elastic Kubernetes Service (Amazon EKS) containers that use AWS Fargate. Migrate the retail MySQL database to Amazon Aurora Serverless MySQL. Migrate the analytics database to Amazon Redshift Serverless. Use Amazon EventBridge to send all the incoming data to the microservices and the analytics database."
        },
        {
          "label": "D",
          "content": "Migrate the individual applications as microservices to Amazon AppStream 2.0. Migrate the retail MySQL database to Amazon Aurora MySQL. Migrate the analytics database to Amazon Redshift Serverless. Use AWS IoT Core to send all the incoming data to the microservices and the analytics database."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>The company needs an event-driven, serverless architecture that provides near-real-time analytics. Here’s why Option C is the best choice: &nbsp;</p><p> 1. Microservices on Amazon EKS with AWS Fargate &nbsp;</p><p>- Amazon EKS (Elastic Kubernetes Service) is ideal for running microservices in a scalable way. &nbsp;</p><p>- AWS Fargate provides serverless compute, eliminating the need to manage EC2 instances. &nbsp;</p><p>- This setup ensures automatic scaling and cost efficiency for the order-processing applications. &nbsp;</p><p> 2. Serverless Databases (Aurora MySQL & Redshift Serverless) &nbsp;</p><p>- Amazon Aurora Serverless (MySQL-compatible) scales automatically, reducing management overhead for the retail database. &nbsp;</p><p>- Amazon Redshift Serverless is optimized for near-real-time analytics and scales based on demand. &nbsp;</p><p>- Both databases support event-driven updates, eliminating the need for weekly ETL jobs. &nbsp;</p><p> 3. Event-Driven Data Flow with Amazon EventBridge &nbsp;</p><p>- Amazon EventBridge is a serverless event bus that can route events from different sources (e.g., order processing, returns) to microservices and the analytics database. &nbsp;</p><p>- This enables real-time data processing instead of relying on batch ETL jobs. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A (ECS + Neptune + SQS): &nbsp;</p><p> &nbsp;- Amazon Neptune is a graph database, not suitable for OLAP analytics. &nbsp;</p><p> &nbsp;- SQS is for queuing, not real-time event processing. &nbsp;</p><p>- B (Auto Scaling EC2 + Aurora MySQL + SNS): &nbsp;</p><p> &nbsp;- Still relies on EC2 instances, not fully serverless. &nbsp;</p><p> &nbsp;- SNS is for pub/sub messaging, not as flexible as EventBridge for event-driven workflows. &nbsp;</p><p>- D (AppStream 2.0 + IoT Core): &nbsp;</p><p> &nbsp;- AppStream 2.0 is for virtual desktop streaming, not microservices. &nbsp;</p><p> &nbsp;- AWS IoT Core is for IoT devices, not retail order processing. &nbsp;</p><p> Conclusion: &nbsp;</p><p>Option C provides a fully serverless, event-driven, and scalable solution with real-time analytics, meeting all requirements. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "231ddc74ca28442a9d2cfc25e383e6bd",
      "questionNumber": 475,
      "type": "single",
      "content": "<p>Question #475</p><p>A company is planning a migration from an on-premises data center to the AWS Cloud. The company plans to use multiple AWS accounts that are managed in an organization in AWS Organizations. The company will create a small number of accounts initially and will add accounts as needed. A solutions architect must design a solution that turns on AWS CloudTrail in all AWS accounts.<br><br>What is the MOST operationally efficient solution that meets these requirements? </p>",
      "options": [
        {
          "label": "A",
          "content": "Create an AWS Lambda function that creates a new CloudTrail trail in all AWS accounts in the organization. Invoke the Lambda function daily by using a scheduled action in Amazon EventBridge."
        },
        {
          "label": "B",
          "content": "Create a new CloudTrail trail in the organization&#39;s management account. Configure the trail to log all events for all AWS accounts in the organization."
        },
        {
          "label": "C",
          "content": "Create a new CloudTrail trail in all AWS accounts in the organization. Create new trails whenever a new account is created. Define an SCP that prevents deletion or modification of trails. Apply the SCP to the root OU."
        },
        {
          "label": "D",
          "content": "Create an AWS Systems Manager Automation runbook that creates a CloudTrail trail in all AWS accounts in the organization. Invoke the automation by using Systems Manager State Manager."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Create a new CloudTrail trail in the organization's management account. Configure the trail to log all events for all AWS accounts in the organization.</p><p>This solution is the most operationally efficient for the following reasons:</p><p>Centralized Management: Creating an organization trail in the management account provides a single point of management for CloudTrail across all accounts in the organization. This significantly reduces the operational overhead compared to managing individual trails in each account.</p><p>Automatic Coverage for New Accounts: When new AWS accounts are added to the organization, they are automatically included in the organization trail without any additional configuration. This is particularly important since the company plans to \"add accounts as needed.\"</p><p>Consistent Logging: An organization trail ensures uniform event logging across all accounts, providing a comprehensive and consistent audit trail for the entire organization.</p><p>No Custom Code Required: Unlike the Lambda function approach, this solution uses native AWS functionality and doesn't require writing, testing, and maintaining custom code.</p><p>No Scheduling or Monitoring Overhead: There's no need to set up and monitor scheduled EventBridge rules or Lambda functions, eliminating potential points of failure.</p><p>Immediate Activation: The organization trail begins logging events for new accounts as soon as they're added to the organization, without any delay or manual intervention.</p><p>Cost Efficiency: While both solutions incur CloudTrail and S3 storage costs, the Lambda function approach would add additional costs for Lambda executions and potential API call charges.</p><p>Option A (using a Lambda function with EventBridge) is less efficient because:</p><p>It introduces unnecessary complexity with custom code</p><p>It requires ongoing maintenance of the Lambda function</p><p>There could be a delay between account creation and CloudTrail enablement</p><p>It might miss newly created accounts if they're added between scheduled Lambda executions</p><p>It creates additional points of failure in the logging system</p><p>The organization trail approach (Option B) is specifically designed for multi-account environments in AWS Organizations and represents the AWS recommended best practice for enabling CloudTrail across an organization.</p><p>Sources</p><p>CloudTrail concepts - AWS CloudTrail （https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html）</p><p>AWS CloudTrail and AWS Organizations - AWS Organizations （https://docs.aws.amazon.com/organizations/latest/userguide/services-that-can-integrate-cloudtrail.html）</p><p>Working with CloudTrail trails - AWS CloudTrail（https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-trails.html） </p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": true,
      "noteContent": "Organization Trail是Organizations的原生功能"
    },
    {
      "id": "1229443d006b437e8eedbd3d346ed1a7",
      "questionNumber": 476,
      "type": "single",
      "content": "<p>Question #476</p><p>A software development company has multiple engineers who are working remotely. The company is running Active Directory Domain Services (AD DS) on an Amazon EC2 instance. The company's security policy states that all internal, nonpublic services that are deployed in a VPC must be accessible through a VPN. Multi-factor authentication (MFA) must be used for access to a VPN.<br><br>What should a solutions architect do to meet these requirements? </p>",
      "options": [
        {
          "label": "A",
          "content": "Create an AWS Site-to-Site VPN connection. Configure integration between a VPN and AD DS. Use an Amazon WorkSpaces client with MFA support enabled to establish a VPN connection."
        },
        {
          "label": "B",
          "content": "Create an AWS Client VPN endpoint. Create an AD Connector directory for integration with AD DS. Enable MFA for AD Connector. Use AWS Client VPN to establish a VPN connection."
        },
        {
          "label": "C",
          "content": "Create multiple AWS Site-to-Site VPN connections by using AWS VPN CloudHub. Configure integration between AWS VPN CloudHub and AD DS. Use AWS Copilot to establish a VPN connection."
        },
        {
          "label": "D",
          "content": "Create an Amazon WorkLink endpoint. Configure integration between Amazon WorkLink and AD DS. Enable MFA in Amazon WorkLink. Use AWS Client VPN to establish a VPN connection."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Create an AWS Client VPN endpoint. Create an AD Connector directory for integration with AD DS. Enable MFA for AD Connector. Use AWS Client VPN to establish a VPN connection.</p><p>This solution meets the requirements for the following reasons:</p><p>Remote Access Requirement: The company has multiple engineers working remotely who need access to internal, non-public services deployed in a VPC. AWS Client VPN is specifically designed for this scenario, allowing individual users to establish secure connections to AWS resources from any location.</p><p>VPN Access Requirement: The security policy states that all internal services must be accessible through a VPN. AWS Client VPN provides this secure tunnel for remote users to access resources within the VPC.</p><p>Active Directory Integration: The company is running AD DS on an EC2 instance. AD Connector provides a way to integrate AWS Client VPN with the existing Active Directory infrastructure, allowing users to authenticate using their existing AD credentials.</p><p>Multi-Factor Authentication (MFA) Requirement: The security policy mandates MFA for VPN access. AD Connector supports enabling MFA, which adds an additional layer of security beyond just username and password authentication.</p><p>Scalability: As the company has multiple engineers working remotely, this solution scales well to accommodate individual user connections without requiring additional hardware or complex configurations.</p><p>Option A (AWS Site-to-Site VPN with WorkSpaces) is not appropriate for this scenario because:</p><p>Site-to-Site VPN is designed to connect entire networks (like an on-premises data center to AWS), not individual remote users. It would require remote engineers to first connect to a central location before accessing AWS resources.</p><p>Amazon WorkSpaces is a virtual desktop solution, not a VPN solution. While it can be used with MFA, it doesn't fulfill the requirement that \"all internal services must be accessible through a VPN.\"</p><p>This approach would be more complex and costly than necessary for the given requirements.</p><p>AWS Client VPN with AD Connector and MFA provides the most direct and appropriate solution for enabling remote engineers to securely access internal services in the VPC while meeting all the security requirements specified in the scenario.</p><p>Sources</p><p>Active Directory authentication in Client VPN - AWS Client VPN （https://docs.aws.amazon.com/vpn/latest/clientvpn-admin/ad.html）</p><p>Enabling multi-factor authentication for AWS Managed Microsoft AD - AWS Directory Service （https://docs.aws.amazon.com/directoryservice/latest/admin-guide/ms_ad_mfa.html）</p><p>Connect on-prem AD to VPC, is it a AD connector required? | AWS re:Post （https://repost.aws/questions/QUNPG4paiiRJWzFobuUtbyTQ/connect-on-prem-ad-to-vpc-is-it-a-ad-connector-required）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "55e2a7c93e954686a8aa618a2f80d94f",
      "questionNumber": 477,
      "type": "multiple",
      "content": "<p>Question #477</p><p><br></p><p>A company is running a three-tier web application in an on-premises data center. The frontend is served by an Apache web server, the middle tier is a monolithic Java application, and the storage tier is a PostgreSQL database. During a recent marketing promotion, customers could not place orders through the application because the application crashed. An analysis showed that all three tiers were overloaded. The application became unresponsive, and the database reached its capacity limit because of read operations. The company already has several similar promotions scheduled in the near future.</p><p><br></p><p>A solutions architect must develop a plan for migration to AWS to resolve these issues. The solution must maximize scalability and must minimize operational effort.</p><p><br></p><p>Which combination of steps will meet these requirements? (Choose three.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Refactor the frontend so that static assets can be hosted on Amazon S3. Use Amazon CloudFront to serve the frontend to customers. Connect the frontend to the Java application."
        },
        {
          "label": "B",
          "content": "Rehost the Apache web server of the frontend on Amazon EC2 instances that are in an Auto Scaling group. Use a load balancer in front of the Auto Scaling group. Use Amazon Elastic File System (Amazon EFS) to host the static assets that the Apache web server needs."
        },
        {
          "label": "C",
          "content": "Rehost the Java application in an AWS Elastic Beanstalk environment that includes auto scaling."
        },
        {
          "label": "D",
          "content": "Refactor the Java application, Develop a Docker container to run the Java application. Use AWS Fargate to host the container."
        },
        {
          "label": "E",
          "content": "Use AWS Database Migration Service (AWS DMS) to replatform the PostgreSQL database to an Amazon Aurora PostgreSQL database. Use Aurora Auto Scaling for read replicas."
        },
        {
          "label": "F",
          "content": "Rehost the PostgreSQL database on an Amazon EC2 instance that has twice as much memory as the on-premises server. "
        }
      ],
      "correctAnswer": "ACE",
      "explanation": "<p>The company needs a scalable, highly available, and low-maintenance solution on AWS. Here’s why Option A, C, E is the best choice: &nbsp;</p><p> A. Refactor the frontend to use Amazon S3 + CloudFront &nbsp;</p><p>- Amazon S3 is ideal for hosting static assets (HTML, CSS, JS, images) with high durability and scalability. &nbsp;</p><p>- Amazon CloudFront (CDN) improves performance by caching content globally, reducing load on backend servers. &nbsp;</p><p>- This reduces the need for Apache web servers to serve static files, improving scalability. &nbsp;</p><p> C. Rehost the Java application in AWS Elastic Beanstalk with Auto Scaling &nbsp;</p><p>- Elastic Beanstalk automates deployment, scaling, and monitoring of Java applications. &nbsp;</p><p>- Auto Scaling ensures the application can handle traffic spikes during promotions. &nbsp;</p><p>- Since the Java app is monolithic, rehosting (lift-and-shift) is faster than refactoring into microservices. &nbsp;</p><p> E. Replatform PostgreSQL to Amazon Aurora with Auto Scaling for read replicas &nbsp;</p><p>- Amazon Aurora PostgreSQL is fully compatible but offers better performance, scalability, and reliability. &nbsp;</p><p>- Aurora Auto Scaling automatically adds read replicas during high traffic, preventing database overload. &nbsp;</p><p>- This solves the issue of read capacity limits during promotions. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- B (Rehost Apache on EC2 + EFS): &nbsp;</p><p> &nbsp;- Still requires managing EC2 instances (not fully serverless). &nbsp;</p><p> &nbsp;- EFS is overkill for static assets (S3 + CloudFront is better). &nbsp;</p><p>- D (Refactor Java into Docker + Fargate): &nbsp;</p><p> &nbsp;- While Fargate is serverless, refactoring a monolithic Java app into containers is time-consuming and not necessary for immediate scalability. &nbsp;</p><p> &nbsp;- Elastic Beanstalk (Option C) provides a simpler, faster solution. &nbsp;</p><p>- F (Rehost PostgreSQL on a larger EC2 instance): &nbsp;</p><p> &nbsp;- Vertical scaling (bigger EC2 instance) is not as scalable as Aurora with read replicas (Option E). &nbsp;</p><p> &nbsp;- Requires manual intervention to scale, unlike Aurora Auto Scaling. &nbsp;</p><p> Conclusion: &nbsp;</p><p>A, C, E provides the best balance of scalability, minimal operational effort, and AWS-managed services to handle future promotions. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "332fe63629804bf9af124232c798e7ab",
      "questionNumber": 478,
      "type": "single",
      "content": "<p>Question #478</p><p><br></p><p>A company is deploying a new application on AWS. The application consists of an Amazon Elastic Kubernetes Service (Amazon EKS) cluster and an Amazon Elastic Container Registry (Amazon ECR) repository. The EKS cluster has an AWS managed node group.</p><p><br></p><p>The company's security guidelines state that all resources on AWS must be continuously scanned for security vulnerabilities.</p><p><br></p><p>Which solution will meet this requirement with the LEAST operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Activate AWS Security Hub. Configure Security Hub to scan the EKS nodes and the ECR repository."
        },
        {
          "label": "B",
          "content": "Activate Amazon Inspector to scan the EKS nodes and the ECR repository."
        },
        {
          "label": "C",
          "content": "Launch a new Amazon EC2 instance and install a vulnerability scanning tool from AWS Marketplace. Configure the EC2 instance to scan the EKS nodes. Configure Amazon ECR to perform a basic scan on push."
        },
        {
          "label": "D",
          "content": "Install the Amazon CloudWatch agent on the EKS nodes. Configure the CloudWatch agent to scan continuously. Configure Amazon ECR to perform a basic scan on push."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Activate Amazon Inspector to scan the EKS nodes and the ECR repository.</p><p>This solution will meet the requirement for continuous security vulnerability scanning with the least operational overhead for the following reasons:</p><p>Purpose-Built for Vulnerability Scanning: Amazon Inspector is specifically designed for automated vulnerability management and continuous scanning of AWS resources, including EKS nodes and ECR repositories.</p><p>Native Container Support: Amazon Inspector has built-in capabilities to scan container images in ECR repositories for software vulnerabilities, identifying security issues in the container images that will be deployed to the EKS cluster.</p><p>EKS Node Scanning: Amazon Inspector can automatically scan the EC2 instances in the AWS managed node group for the EKS cluster, detecting operating system vulnerabilities and misconfigurations.</p><p>Minimal Configuration: Once activated, Amazon Inspector automatically discovers supported resources and begins scanning them without requiring extensive setup or configuration.</p><p>Continuous Monitoring: Amazon Inspector provides continuous, automated scanning that detects new vulnerabilities as they emerge, meeting the company's requirement for continuous security scanning.</p><p>Low Operational Overhead: As a fully managed service, Amazon Inspector requires minimal ongoing maintenance and automatically updates its vulnerability database, reducing the operational burden on the company's team.</p><p>Option A (AWS Security Hub) would not be the optimal choice for this specific requirement because:</p><p>Security Hub is primarily an aggregation and management service that collects security findings from various AWS services, including Amazon Inspector.</p><p>Security Hub itself doesn't perform the actual vulnerability scanning of EKS nodes and ECR repositories - it would still rely on other services like Amazon Inspector to perform the actual scans.</p><p>Setting up Security Hub to scan these resources would require additional configuration and integration with other services, increasing the operational overhead.</p><p>By activating Amazon Inspector, the company can implement continuous vulnerability scanning for both their EKS cluster nodes and ECR repository with minimal setup and ongoing maintenance, making it the solution with the least operational overhead that meets the security requirement.</p><p>Sources</p><p>Scan images for software vulnerabilities in Amazon ECR - Amazon ECR （https://docs.aws.amazon.com/AmazonECR/latest/userguide/image-scanning.html）</p><p>Automated Vulnerability Management - Amazon Inspector Features - AWS （https://aws.amazon.com/cn/inspector/features/）</p><p>Amazon Inspector enhances container security by mapping Amazon ECR images to running containers | AWS News Blog （https://aws.amazon.com/cn/blogs/aws/amazon-inspector-enhances-container-security-by-mapping-amazon-ecr-images-to-running-containers/）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "63f7e65c02354cf2a25851ce0a41a5c4",
      "questionNumber": 479,
      "type": "single",
      "content": "<p>Question #479</p><p><br></p><p>A company needs to improve the reliability of its ticketing application. The application runs on an Amazon Elastic Container Service (Amazon ECS) cluster. The company uses Amazon CloudFront to serve the application. A single ECS service of the ECS cluster is the CloudFront distribution’s origin.</p><p><br></p><p>The application allows only a specific number of active users to enter a ticket purchasing flow. These users are identified by an encrypted attribute in their JSON Web Token (JWT). All other users are redirected to a waiting room module until there is available capacity for purchasing.</p><p><br></p><p>The application is experiencing high loads. The waiting room module is working as designed, but load on the waiting room is disrupting the application's availability.</p><p><br></p><p>This disruption is negatively affecting the application's ticket sale transactions.</p><p><br></p><p>Which solution will provide the MOST reliability for ticket sale transactions during periods of high load?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a separate service in the ECS cluster for the waiting room. Use a separate scaling configuration. Ensure that the ticketing service uses the JWT information and appropriately forwards requests to the waiting room service."
        },
        {
          "label": "B",
          "content": "Move the application to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. Split the waiting room module into a pod that is separate from the ticketing pod. Make the ticketing pod part of a StatefulSet. Ensure that the ticketing pod uses the JWT information and appropriately forwards requests to the waiting room pod."
        },
        {
          "label": "C",
          "content": "Create a separate service in the ECS cluster for the waiting room. Use a separate scaling configuration. Create a CloudFront function that inspects the JWT information and appropriately forwards requests to the ticketing service or the waiting room service."
        },
        {
          "label": "D",
          "content": "Move the application to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. Split the waiting room module into a pod that is separate from the ticketing pod. Use AWS App Mesh by provisioning the App Mesh controller for Kubernetes. Enable mTLS authentication and service-to-service authentication for communication between the ticketing pod and the waiting room pod. Ensure that the ticketing pod uses the JWT information and appropriately forwards requests to the waiting room pod."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>The company needs to improve reliability by isolating the waiting room traffic from the ticketing service while efficiently routing users based on their JWT. Here’s why Option C is the best choice: &nbsp;</p><p> 1. Separate ECS Service for the Waiting Room + Independent Scaling &nbsp;</p><p>- Isolating the waiting room into its own ECS service prevents it from consuming resources needed for ticket sales. &nbsp;</p><p>- Independent scaling allows the waiting room to handle high loads without affecting the ticketing service. &nbsp;</p><p> 2. CloudFront Function for JWT-Based Routing &nbsp;</p><p>- CloudFront Functions (edge compute) can inspect the JWT and route requests before they hit the origin. &nbsp;</p><p> &nbsp;- If the JWT allows ticket purchasing → forward to the ticketing service. &nbsp;</p><p> &nbsp;- If the user must wait → forward to the waiting room service. &nbsp;</p><p>- This reduces load on the ECS cluster by offloading routing logic to CloudFront. &nbsp;</p><p> 3. Why This is the Most Reliable Solution &nbsp;</p><p>- No unnecessary migration (unlike Options B & D, which suggest moving to EKS). &nbsp;</p><p>- Serverless routing (CloudFront Functions) is low-latency and scalable. &nbsp;</p><p>- Decouples the waiting room from the ticketing service, ensuring ticket sales remain stable. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A (Separate ECS service but no CloudFront routing): &nbsp;</p><p> &nbsp;- Still relies on the ticketing service to route traffic, which adds unnecessary load. &nbsp;</p><p>- B (Move to EKS + StatefulSet): &nbsp;</p><p> &nbsp;- Overkill—ECS is sufficient, and StatefulSet is unnecessary for this stateless app. &nbsp;</p><p> &nbsp;- No CloudFront optimization, so routing still happens at the cluster level. &nbsp;</p><p>- D (Move to EKS + App Mesh + mTLS): &nbsp;</p><p> &nbsp;- Complexity is unnecessary—CloudFront Functions provide a simpler, more scalable solution. &nbsp;</p><p> &nbsp;- mTLS and App Mesh add overhead without solving the core issue (waiting room load). &nbsp;</p><p> Conclusion: &nbsp;</p><p>Option C provides the most reliable, scalable, and cost-effective solution by: &nbsp;</p><p>✅ Isolating the waiting room (separate ECS service + scaling). &nbsp;</p><p>✅ Offloading routing logic to CloudFront (JWT inspection at the edge). &nbsp;</p><p>✅ Minimizing changes (no migration to EKS). &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "184134b953d044688a49b9fe4cc9a02a",
      "questionNumber": 480,
      "type": "single",
      "content": "<p>Question #480</p><p><br></p><p>A solutions architect is creating an AWS CloudFormation template from an existing manually created non-production AWS environment. The CloudFormation template can be destroyed and recreated as needed. The environment contains an Amazon EC2 instance. The EC2 instance has an instance profile that the EC2 instance uses to assume a role in a parent account.</p><p><br></p><p>The solutions architect recreates the role in a CloudFormation template and uses the same role name. When the CloudFormation template is launched in the child account, the EC2 instance can no longer assume the role in the parent account because of insufficient permissions.</p><p><br></p><p>What should the solutions architect do to resolve this issue?</p>",
      "options": [
        {
          "label": "A",
          "content": "In the parent account, edit the trust policy for the role that the EC2 instance needs to assume. Ensure that the target role ARN in the existing statement that allows the sts:AssumeRole action is correct. Save the trust policy."
        },
        {
          "label": "B",
          "content": "In the parent account, edit the trust policy for the role that the EC2 instance needs to assume. Add a statement that allows the sts:AssumeRole action for the root principal of the child account. Save the trust policy."
        },
        {
          "label": "C",
          "content": "Update the CloudFormation stack again. Specify only the CAPABILITY_NAMED_IAM capability."
        },
        {
          "label": "D",
          "content": "Update the CloudFormation stack again. Specify the CAPABILITY_IAM capability and the CAPABILITY_NAMED_IAM capability."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>The issue occurs because the trust policy in the parent account’s role must explicitly allow the EC2 instance’s instance profile (or its role) in the child account to assume it. When the role was recreated in the CloudFormation template, the trust relationship was not updated to include the new role’s ARN (even if the name is the same, the ARN changes when recreated). &nbsp;</p><p> Why Option A is Correct: &nbsp;</p><p>1. The root cause is the trust policy in the parent account’s role. &nbsp;</p><p>2. The existing `sts:AssumeRole` statement must be updated to include the new role ARN from the child account. &nbsp;</p><p>3. This ensures the EC2 instance’s instance profile (or its role) can still assume the parent account’s role. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- B (Add a statement allowing the child account’s root principal): &nbsp;</p><p> &nbsp;- Security risk: Granting `sts:AssumeRole` to the entire child account’s root principal is overly permissive. &nbsp;</p><p> &nbsp;- Best practice: Only allow specific roles (like the EC2 instance profile) to assume the role. &nbsp;</p><p>- C (Specify `CAPABILITY_NAMED_IAM`): &nbsp;</p><p> &nbsp;- This is irrelevant—the issue is not about CloudFormation permissions, but about IAM trust relationships. &nbsp;</p><p>- D (Specify both `CAPABILITY_IAM` and `CAPABILITY_NAMED_IAM`): &nbsp;</p><p> &nbsp;- Again, this is not the issue—CloudFormation can create the role, but the trust policy must be manually updated in the parent account. &nbsp;</p><p> Conclusion: &nbsp;</p><p>The trust policy in the parent account must be updated to allow the newly recreated role in the child account to assume it. This is a manual step because CloudFormation cannot modify IAM policies in another account. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "892ae1d204244d168d24e0b4c364459d",
      "questionNumber": 481,
      "type": "single",
      "content": "<p>Question #481</p><p><br></p><p>A company's web application has reliability issues. The application serves customers globally. The application runs on a single Amazon EC2 instance and performs read-intensive operations on an Amazon RDS for MySQL database.</p><p><br></p><p>During high load, the application becomes unresponsive and requires a manual restart of the EC2 instance. A solutions architect must improve the application's reliability.</p><p><br></p><p>Which solution will meet this requirement with the LEAST development effort?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an Amazon CloudFront distribution. Specify the EC2 instance as the distribution&rsquo;s origin. Configure a Multi-AZ deployment for the RDS for MySQL database. Use the standby DB instance for the read-intensive operations."
        },
        {
          "label": "B",
          "content": "Run the application on EC2 instances that are in an Auto Scaling group. Place the EC2 instances behind an Elastic Load Balancing (ELB) load balancer. Replace the database service with Amazon Aurora. Use Aurora Replicas for the read-intensive operations."
        },
        {
          "label": "C",
          "content": "Deploy AWS Global Accelerator. Configure a Multi-AZ deployment for the RDS for MySQL database. Use the standby DB instance for the read-intensive operations."
        },
        {
          "label": "D",
          "content": "Migrate the application to AWS Lambda functions. Create read replicas for the RDS for MySQL database. Use the read replicas for the read-intensive operations."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Run the application on EC2 instances that are in an Auto Scaling group. Place the EC2 instances behind an Elastic Load Balancing (ELB) load balancer. Replace the database service with Amazon Aurora. Use Aurora Replicas for the read-intensive operations.</p><p>This solution will improve the application's reliability with the least development effort for the following reasons:</p><p>Addressing the Single Point of Failure: The current setup has a single EC2 instance that becomes unresponsive during high load. An Auto Scaling group with multiple EC2 instances behind an ELB eliminates this single point of failure by:</p><p>Automatically distributing traffic across multiple instances</p><p>Automatically replacing unhealthy instances</p><p>Scaling the number of instances based on demand</p><p>Handling Read-Intensive Database Operations: The application performs read-intensive operations on the database, which likely contributes to the performance issues. Aurora with Aurora Replicas provides:</p><p>Better read scalability by offloading read operations to multiple Aurora Replicas</p><p>Higher performance compared to standard RDS MySQL</p><p>Automatic failover capabilities</p><p>Global Customer Base: Since the application serves customers globally, the ELB and Auto Scaling group will ensure better availability and performance across different regions.</p><p>Minimal Development Effort: While both solutions require some changes, option B requires less development effort because:</p><p>Aurora is MySQL-compatible, so the application's database queries would require minimal changes</p><p>The application code itself wouldn't need significant modifications to work with an Auto Scaling group and ELB</p><p>The migration from RDS MySQL to Aurora can be done with minimal downtime using AWS migration tools</p><p>Option A (CloudFront with Multi-AZ RDS MySQL) would not be as effective because:</p><p>It doesn't address the EC2 instance becoming unresponsive under high load, as it still relies on a single instance</p><p>Multi-AZ RDS MySQL provides high availability but doesn't improve read performance as effectively as Aurora Replicas</p><p>Using the standby instance for read operations in Multi-AZ RDS MySQL is more complex and less efficient than using Aurora Replicas</p><p>The Auto Scaling group with ELB and Aurora solution provides comprehensive improvements to both the application tier and database tier, addressing the root causes of the reliability issues while requiring minimal development changes to the application code.</p><p>Sources</p><p>Key AWS services - Financial Services Industry Lens （https://docs.aws.amazon.com/wellarchitected/latest/financial-services-industry-lens/key-aws-services-rel.html）</p><p>High availability for Amazon Aurora - Amazon Aurora （https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html）</p><p>Enhance performance of generative language models with self-consistency prompting on Amazon Bedrock | AWS Machine Learning Blog （https://aws.amazon.com/cn/blogs/machine-learning/enhance-performance-of-generative-language-models-with-self-consistency-prompting-on-amazon-bedrock/）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "fd20d74d49af4384a702cecaaaa651c3",
      "questionNumber": 482,
      "type": "single",
      "content": "<p>Question #482</p><p><br></p><p>A company needs to use an AWS Transfer Family SFTP-enabled server with an Amazon S3 bucket to receive updates from a third-party data supplier. The data is encrypted with Pretty Good Privacy (PGP) encryption. The company needs a solution that will automatically decrypt the data after the company receives the data.</p><p><br></p><p>A solutions architect will use a Transfer Family managed workflow. The company has created an IAM service role by using an IAM policy that allows access to AWS Secrets Manager and the S3 bucket. The role’s trust relationship allows the transfer `amazonaws.com` service to assume the role.</p><p><br></p><p>What should the solutions architect do next to complete the solution for automatic decryption?</p>",
      "options": [
        {
          "label": "A",
          "content": "Store the PGP public key in Secrets Manager. Add a nominal step in the Transfer Family managed workflow to decrypt files. Configure PGP encryption parameters in the nominal step. Associate the workflow with the Transfer Family server."
        },
        {
          "label": "B",
          "content": "Store the PGP private key in Secrets Manager. Add an exception-handling step in the Transfer Family managed workflow to decrypt files. Configure PGP encryption parameters in the exception handler. Associate the workflow with the SFTP user."
        },
        {
          "label": "C",
          "content": "Store the PGP private key in Secrets Manager. Add a nominal step in the Transfer Family managed workflow to decrypt files. Configure PGP decryption parameters in the nominal step. Associate the workflow with the Transfer Family server."
        },
        {
          "label": "D",
          "content": "Store the PGP public key in Secrets Manager. Add an exception-handling step in the Transfer Family managed workflow to decrypt files. Configure PGP decryption parameters in the exception handler. Associate the workflow with the SFTP user."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Store the PGP private key in Secrets Manager. Add a nominal step in the Transfer Family managed workflow to decrypt files. Configure PGP decryption parameters in the nominal step. Associate the workflow with the Transfer Family server.</p><p>This is the correct next step to complete the solution for automatic decryption because:</p><p>PGP Private Key Storage: The PGP private key needs to be securely stored in AWS Secrets Manager. This is a critical step because the key will be used to decrypt the files received from the third-party data supplier. Secrets Manager provides secure storage with encryption at rest and access control through IAM policies.</p><p>Managed Workflow with Decrypt Step: AWS Transfer Family managed workflows include predefined \"nominal steps\" that can be configured for specific actions. Adding a decrypt step is necessary to automatically process the encrypted files as they arrive via SFTP.</p><p>PGP Decryption Parameters: The decrypt step needs specific configuration parameters, including:</p><p>Reference to the PGP private key stored in Secrets Manager</p><p>Source file location (the encrypted file uploaded to the S3 bucket)</p><p>Destination for the decrypted file</p><p>Error handling options</p><p>Workflow Association: The configured workflow needs to be associated with the Transfer Family SFTP server to ensure it runs automatically when files are uploaded.</p><p>This solution leverages the existing IAM service role that has already been created with the necessary permissions to access Secrets Manager and the S3 bucket, and with the trust relationship allowing the transfer.amazonaws.com service to assume the role.</p><p>The solution provides a fully automated process where:</p><p>The third-party uploads PGP-encrypted files to the SFTP server</p><p>Files are stored in the S3 bucket</p><p>The Transfer Family managed workflow automatically triggers</p><p>The decrypt step retrieves the private key from Secrets Manager</p><p>The files are decrypted and stored back in the S3 bucket</p><p>This approach requires minimal ongoing maintenance and provides a secure, automated solution for handling the encrypted data from the third-party supplier.</p><p>Sources</p><p>Architecting secure and compliant managed file transfers with AWS Transfer Family SFTP connectors and PGP encryption | AWS Storage Blog （https://aws.amazon.com/cn/blogs/storage/architecting-secure-and-compliant-managed-file-transfers-with-aws-transfer-family-sftp-connectors-and-pgp-encryption/）</p><p>AWS Transfer Family does not use S3 default encryption - SFTP - access denied | AWS re:Post （https://repost.aws/questions/QUYHwOd1zqQfqfwVNogoAvVQ/aws-transfer-family-does-not-use-s3-default-encryption-sftp-access-denied）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "8feea4ede319437dbf0589196cd40c21",
      "questionNumber": 483,
      "type": "single",
      "content": "Question #483<p><br></p><p>A company is migrating infrastructure for its massive multiplayer game to AWS. The game’s application features a leaderboard where players can see rankings in real time. The leaderboard requires microsecond reads and single-digit-millisecond write latencies. The datasets are single-digit terabytes in size and must be available to accept writes in less than a minute if a primary node failure occurs.</p><p><br></p><p>The company needs a solution in which data can persist for further analytical processing through a data pipeline.</p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an Amazon RDS database with a read replica. Configure the application to point writes to the writer endpoint. Configure the application to point reads to the reader endpoint."
        },
        {
          "label": "B",
          "content": "Create an Amazon MemoryDB for Redis cluster in Multi-AZ mode. Configure the application to interact with the primary node."
        },
        {
          "label": "C",
          "content": "Create multiple Redis nodes on Amazon EC2 instances that are spread across multiple Availability Zones. Configure backups to Amazon S3."
        }
      ],
      "correctAnswer": "B",
      "explanation": "Based on the company's requirements for their massive multiplayer game's leaderboard, the solution that will meet these requirements with the least operational overhead is:<p>B. Create an Amazon MemoryDB for Redis cluster in Multi-AZ mode. Configure the application to interact with the primary node.</p><p>Here's why this solution is the most suitable:</p><p>1. Performance: Amazon MemoryDB for Redis offers microsecond read latencies and single-digit millisecond write latencies, which meets the game's leaderboard requirements.</p><p>2. Scalability: MemoryDB can handle up to 160 million transactions per second (TPS) per cluster, which is suitable for a massive multiplayer game.</p><p>3. Data persistence: MemoryDB ensures data durability through a Multi-AZ transactional log, allowing data to persist for further analytical processing through a data pipeline.</p><p>4. High availability: Multi-AZ mode provides automatic failover and node recovery, ensuring that the system can accept writes in less than a minute if a primary node failure occurs.</p><p>5. Data size: MemoryDB supports up to 100 TB of storage per cluster, which is more than sufficient for the single-digit terabyte datasets mentioned.</p><p>6. Operational overhead: As a fully managed service, MemoryDB requires minimal operational overhead compared to self-managed solutions.</p><p>7. Compatibility: MemoryDB supports Redis data structures and APIs, making it easy to integrate with existing game applications.</p><p>To implement this solution:</p><p>1. Create a MemoryDB cluster in Multi-AZ mode within your preferred AWS region.</p><p>2. Configure the cluster with appropriate node types and settings to handle your expected workload.</p><p>3. Set up encryption at rest and in transit for enhanced security.</p><p>4. Implement proper access controls using AWS Identity and Access Management (IAM) and Access Control Lists (ACLs).</p><p>5. Configure your game application to interact with the primary node of the MemoryDB cluster for both reads and writes.</p><p>6. Set up monitoring and alerting using Amazon CloudWatch to track the cluster's performance and health.</p><p>For data pipeline integration:</p><p>1. Use Amazon Kinesis Data Firehose to capture and stream data from MemoryDB to Amazon S3 for long-term storage.</p><p>2. Utilize Amazon Glue for ETL processes to prepare the data for analysis.</p><p>3. Consider using Amazon Redshift or Amazon Athena for analytical processing of the persisted data.</p><p>Remember to review the AWS documentation for the most up-to-date information on MemoryDB features, best practices, and any limitations. Additionally, always test your configuration in a non-production environment before deploying to production.</p><p>Sources</p><p>[1] [Managing clusters - Amazon MemoryDB for Redis] (https://docs.aws.amazon.com/memorydb/latest/devguide/clusters.html)</p><p>[3] [Launch a scalable serverless data pipeline to ingest, store, and analyze telemetry data generated from games and services - Game Analytics Pipeline] (https://docs.aws.amazon.com/solutions/latest/game-analytics-pipeline/welcome.html)</p><p>[6] [Features of MemoryDB - Amazon MemoryDB] (https://docs.aws.amazon.com/memorydb/latest/devguide/servicename-feature-overview.html)</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "5b53e917e50448fe8570ad0c23b55cd9",
      "questionNumber": 484,
      "type": "single",
      "content": "<p>Question #484</p><p><br></p><p>A company is running several applications in the AWS Cloud. The applications are specific to separate business units in the company. The company is running the components of the applications in several AWS accounts that are in an organization in AWS Organizations.</p><p><br></p><p>Every cloud resource in the company’s organization has a tag named BusinessUnit. Every tag already has the appropriate value of the business unit name.</p><p><br></p><p>The company needs to allocate its cloud costs to different business units. The company also needs to visualize the cloud costs for each business unit.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "In the organization&#39;s management account, create a cost allocation tag named BusinessUnit. Also in the management account, create an Amazon S3 bucket and an AWS Cost and Usage Report (AWS CUR). Configure the S3 bucket as the destination for the AWS CUR. From the management account, query the AWS CUR data by using Amazon Athena. Use Amazon QuickSight for visualization."
        },
        {
          "label": "B",
          "content": "In each member account, create a cost allocation tag named BusinessUnit. In the organization&rsquo;s management account, create an Amazon S3 bucket and an AWS Cost and Usage Report (AWS CUR). Configure the S3 bucket as the destination for the AWS CUR. Create an Amazon CloudWatch dashboard for visualization."
        },
        {
          "label": "C",
          "content": "In the organization&#39;s management account, create a cost allocation tag named BusinessUnit. In each member account, create an Amazon S3 bucket and an AWS Cost and Usage Report (AWS CUR). Configure each S3 bucket as the destination for its respective AWS CUR."
        },
        {
          "label": "D",
          "content": "&nbsp;In each member account, create a cost allocation tag named BusinessUnit. Also in each member account, create an Amazon S3 bucket and an AWS Cost and Usage Report (AWS CUR). Configure each S3 bucket as the destination for its respective AWS CUR. From the management account, query the AWS CUR data by using Amazon Athena. Use Amazon QuickSight for visualization."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>The question describes a scenario where a company needs to allocate and visualize AWS costs by business units using the existing `BusinessUnit` tag. Here's why option A is the best solution: &nbsp;</p><p>1. Cost Allocation Tag in Management Account: &nbsp;</p><p> &nbsp; - The `BusinessUnit` tag already exists on resources. Activating it as a cost allocation tag in the management account ensures that AWS Cost and Usage Reports (CUR) will include this tag for cost allocation. &nbsp;</p><p> &nbsp; - Cost allocation tags only need to be activated once in the management account (for organization-wide tagging). &nbsp;</p><p>2. AWS Cost and Usage Report (CUR): &nbsp;</p><p> &nbsp; - The CUR provides detailed cost and usage data, including tags. &nbsp;</p><p> &nbsp; - Configuring the CUR in the management account with an S3 bucket as the destination ensures consolidated billing data is available for analysis. &nbsp;</p><p>3. Amazon Athena & QuickSight for Analysis & Visualization: &nbsp;</p><p> &nbsp; - Amazon Athena can query the CUR data stored in S3. &nbsp;</p><p> &nbsp; - Amazon QuickSight is a powerful visualization tool that integrates with Athena to create dashboards for business unit cost tracking. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- B: Incorrect because CloudWatch is not designed for cost visualization (it’s for monitoring). Also, cost allocation tags don’t need to be activated in each member account. &nbsp;</p><p>- C: Incorrect because creating separate CURs in each member account is unnecessary and complicates cost aggregation. CloudWatch is not the right tool for cost visualization. &nbsp;</p><p>- D: Incorrect because activating cost allocation tags in each member account is redundant, and managing multiple CURs is inefficient. &nbsp;</p><p> Conclusion: &nbsp;</p><p>A is the correct choice because it efficiently centralizes cost allocation tag activation, uses CUR for detailed reporting, and leverages Athena + QuickSight for visualization. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "a9091f1d74ab4e858b78c8fbcf2a2a74",
      "questionNumber": 485,
      "type": "multiple",
      "content": "<p>Question #485</p><p><br></p><p>A utility company wants to collect usage data every 5 minutes from its smart meters to facilitate time-of-use metering. When a meter sends data to AWS, the data is sent to Amazon API Gateway, processed by an AWS Lambda function, and stored in an Amazon DynamoDB table. During the pilot phase, the Lambda functions took from 3 to 5 seconds to complete.</p><p><br></p><p>As more smart meters are deployed, the engineers notice the Lambda functions are taking from 1 to 2 minutes to complete. The functions are also increasing in duration as new types of metrics are collected from the devices. There are many `ProvisionedThroughputExceededException` errors while performing PUT operations on DynamoDB, and there are also many `TooManyRequestsException` errors from Lambda.</p><p><br></p><p>Which combination of changes will resolve these issues? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Increase the write capacity units to the DynamoDB table."
        },
        {
          "label": "B",
          "content": "Increase the memory available to the Lambda functions."
        },
        {
          "label": "C",
          "content": "Increase the payload size from the smart meters to send more data."
        },
        {
          "label": "D",
          "content": "Stream the data into an Amazon Kinesis data stream from API Gateway and process the data in batches."
        },
        {
          "label": "E",
          "content": "Collect data in an Amazon SQS FIFO queue, which triggers a Lambda function to process each message."
        }
      ],
      "correctAnswer": "AD",
      "explanation": "<p>The problem describes two key issues: &nbsp;</p><p>1. DynamoDB Throttling (`ProvisionedThroughputExceededException`) → Solution: Increase write capacity (A) &nbsp;</p><p> &nbsp; - The DynamoDB table is unable to handle the increased write load, leading to throttling errors. &nbsp;</p><p> &nbsp; - Increasing write capacity units (WCUs) will allow DynamoDB to handle more write requests per second. &nbsp;</p><p>2. Lambda Throttling (`TooManyRequestsException`) & Slow Processing → Solution: Stream data via Kinesis (D) &nbsp;</p><p> &nbsp; - Lambda is struggling with high concurrency and long execution times (1-2 minutes). &nbsp;</p><p> &nbsp; - Instead of processing each request individually via API Gateway → Lambda, streaming data into Kinesis allows: &nbsp;</p><p> &nbsp; &nbsp; - Batching of records for more efficient processing. &nbsp;</p><p> &nbsp; &nbsp; - Better scaling (Kinesis can handle high throughput). &nbsp;</p><p> &nbsp; &nbsp; - Reduced Lambda invocations (processing multiple records per invocation). &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- B (Increase Lambda memory): While this might speed up individual executions, it doesn’t solve throttling or DynamoDB issues. &nbsp;</p><p>- C (Increase payload size): This would worsen Lambda execution time and DynamoDB throttling. &nbsp;</p><p>- E (SQS FIFO queue): FIFO queues have lower throughput (300 msg/sec) and aren’t ideal for high-volume IoT data. Kinesis (D) is better for streaming. &nbsp;</p><p> Conclusion: &nbsp;</p><p>A (Increase DynamoDB WCUs) and D (Use Kinesis for batching) are the best solutions to resolve both DynamoDB throttling and Lambda scaling issues. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "8bb6a2cc70234f17879df5f822637baf",
      "questionNumber": 486,
      "type": "single",
      "content": "<p>Question #486</p><p><br></p><p>A company recently completed a successful proof of concept of Amazon WorkSpaces. A solutions architect needs to make the solution highly available across two AWS Regions. Amazon WorkSpaces is deployed in a failover Region, and a hosted zone is deployed in Amazon Route 53.</p><p><br></p><p>What should the solutions architect do to configure high availability for the solution?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a connection alias in the primary Region and in the failover Region. Associate the connection aliases with a directory in each Region. Create a Route 53 failover routing policy. Set Evaluate Target Health to Yes."
        },
        {
          "label": "B",
          "content": "Create a connection alias in the primary Region and in the failover Region. Associate the connection aliases with a directory in the primary Region. Create a Route 53 multivalue answer routing policy."
        },
        {
          "label": "C",
          "content": "Create a connection alias in the primary Region. Associate the connection alias with a directory in the primary Region. Create a Route 53 weighted routing policy."
        },
        {
          "label": "D",
          "content": "Create a connection alias in the primary Region. Associate the connection alias with a directory in the failover Region. Create a Route 53 failover routing policy. Set Evaluate Target Health to Yes."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Create a connection alias in the primary Region and in the failover Region. Associate the connection aliases with a directory in each Region. Create a Route 53 failover routing policy. Set Evaluate Target Health to Yes.</p><p>This solution provides the most effective high availability configuration for Amazon WorkSpaces across two AWS Regions for the following reasons:</p><p>Complete Regional Independence: By creating connection aliases in both the primary and failover Regions and associating each with its own directory, you establish fully independent WorkSpaces environments in each Region. This ensures that if one Region experiences an outage, the other can continue to operate without dependency on the affected Region.</p><p>Automated Health-Based Routing: The Route 53 failover routing policy with \"Evaluate Target Health\" set to Yes actively monitors the health of your WorkSpaces environment. If the primary Region becomes unavailable or unhealthy, Route 53 will automatically redirect users to the failover Region without requiring manual intervention.</p><p>True Disaster Recovery: This configuration provides genuine disaster recovery capabilities, as users can be redirected to a completely functional WorkSpaces environment in the failover Region if the primary Region experiences issues.</p><p>Minimal Downtime: The automatic failover mechanism ensures that users experience minimal disruption during a regional outage, as they are quickly redirected to the healthy Region.</p><p>Option B (creating connection aliases in both Regions but associating them with a directory only in the primary Region and using a multivalue answer routing policy) is less effective because:</p><p>Single Point of Failure: Having the directory only in the primary Region creates a dependency that could prevent the failover Region from functioning if the primary Region becomes unavailable.</p><p>No Health-Based Routing: The multivalue answer routing policy doesn't consider the health of the endpoints. It simply returns multiple values in a random order, which doesn't guarantee that users will be directed to the available Region during an outage.</p><p>Inconsistent User Experience: Without health checks, some users might be directed to the unavailable Region, resulting in connection failures and a poor user experience.</p><p>The solution in option A follows AWS best practices for creating a highly available WorkSpaces deployment across multiple Regions, providing both redundancy and automated failover capabilities to ensure business continuity.</p><p>Sources</p><p>Amazon WorkSpaces FAQs | Persistent Desktop Virtualization （https://aws.amazon.com/cn/workspaces-family/workspaces/faqs/）</p><p>Business continuity for WorkSpaces Personal - Amazon WorkSpaces （https://docs.aws.amazon.com/workspaces/latest/adminguide/business-continuity.html）</p><p>https://docs.aws.amazon.com/workspaces/latest/adminguide/cross-region-redirection.html#cross-region-redirection-associate-connection-aliases</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "87534d3a30d143289d62341cbba2a61f",
      "questionNumber": 487,
      "type": "single",
      "content": "Question #487<p><br></p><p>A company plans to migrate many VMs from an on-premises environment to AWS. The company requires an initial assessment of the on-premises environment before the migration, a visualization of the dependencies between applications that run on the VMs, and a report that provides an assessment of the on-premises environment.</p><p><br></p><p>To get this information, the company has initiated a Migration Evaluator assessment request. The company has the ability to install collector software in its on-premises environment without any constraints.</p><p><br></p><p>Which solution will provide the company with the required information with the LEAST operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Install the AWS Application Discovery Agent on each on-premises VM. After the data collection period ends, use AWS Migration Hub to view the application dependencies. Download the Quick insights assessment report from Migration Hub."
        },
        {
          "label": "B",
          "content": "Install the Migration Evaluator Collector on each on-premises VM. After the data collection period ends, use Migration Evaluator to view the application dependencies. Download and export the discovered server list from Migration Evaluator. Upload the list to Amazon QuickSight. When the QuickSight report is generated, download the Quick Insights assessment report."
        },
        {
          "label": "C",
          "content": "Setup the AWS Application Discovery Service Agentless Collector in the on-premises environment. After the data collection period ends, use AWS Migration Hub to view the application dependencies. Export the discovered server list from Application Discovery Service. Upload the list to Migration Evaluator. When the Migration Evaluator report is generated, download the Quick Insights assessment."
        },
        {
          "label": "D",
          "content": "Set up the Migration Evaluator Collector in the on-premises environment. Install the AWS Application Discovery Agent on each VM. After the data collection period ends, use AWS Migration Hub to view the application dependencies. Download the Quick Insights assessment report from Migration Evaluator."
        }
      ],
      "correctAnswer": "A",
      "explanation": "The company is looking for a solution that provides an initial assessment of their on-premises environment, visualizes application dependencies, and offers a report with an assessment, all with minimal operational overhead. Let's evaluate the options:<p><br></p><p>A. Install the AWS Application Discovery Agent on each on-premises VM. After the data collection period ends, use AWS Migration Hub to view the application dependencies. Download the Quick insights assessment report from Migration Hub.</p><p>   - This option is straightforward and uses a single agent for data collection. AWS Migration Hub integrates with the AWS Application Discovery Service to provide a visualization of application dependencies and an assessment report with minimal effort.</p><p><br></p><p>B. Install the Migration Evaluator Collector on each on-premises VM. After the data collection period ends, use Migration Evaluator to view the application dependencies. Download and export the discovered server list from Migration Evaluator. Upload the list to Amazon QuickSight. When the QuickSight report is generated, download the Quick Insights assessment report.</p><p>   - This option involves additional steps, such as exporting and uploading server lists to Amazon QuickSight, which adds operational overhead.</p><p><br></p><p>C. Set up the AWS Application Discovery Service Agentless Collector in the on-premises environment. After the data collection period ends, use AWS Migration Hub to view the application dependencies. Export the discovered server list from Application Discovery Service. Upload the list to Migration Evaluator. When the Migration Evaluator report is generated, download the Quick Insights assessment.</p><p>   - This option is more complex because it involves setting up an agentless collector and additional steps to upload the server list to Migration Evaluator.</p><p><br></p><p>D. Set up the Migration Evaluator Collector in the on-premises environment. Install the AWS Application Discovery Agent on each VM. After the data collection period ends, use AWS Migration Hub to view the application dependencies. Download the Quick Insights assessment report from Migration Evaluator.</p><p>   - This option is not practical as it suggests using two different collectors, which is unnecessary and would create additional work.</p><p><br></p><p>Option A is the most efficient because it requires the least operational overhead by using a single agent and leveraging AWS Migration Hub for dependency visualization and report generation.</p><p><br></p><p>For the most up-to-date information on AWS services and best practices, refer to the official AWS documentation.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "fd8533389cf947afb11a4402271f4769",
      "questionNumber": 488,
      "type": "single",
      "content": "<p>Question #488</p><p><br></p><p>A company hosts its primary API on AWS by using an Amazon API Gateway API and AWS Lambda functions that contain the logic for the API methods. The company’s internal applications use the API for core functionality and business logic. The company’s customers use the API to access data from their accounts. Several customers also have access to a legacy API that is running on a single standalone Amazon EC2 instance.</p><p><br></p><p>The company wants to increase the security for these APIs to better prevent denial of service (DoS) attacks, check for vulnerabilities, and guard against common exploits.</p><p><br></p><p>What should a solutions architect do to meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use AWS WAF to protect both APIs. Configure Amazon Inspector to analyze the legacy API. Configure Amazon GuardDuty to monitor for malicious attempts to access the APIs."
        },
        {
          "label": "B",
          "content": "Use AWS WAF to protect the API Gateway API. Configure Amazon Inspector to analyze both APIs. Configure Amazon GuardDuty to block malicious attempts to access the APIs."
        },
        {
          "label": "C",
          "content": "Use AWS WAF to protect the API Gateway API. Configure Amazon Inspector to analyze the legacy API. Configure Amazon GuardDuty to monitor for malicious attempts to access the APIs."
        },
        {
          "label": "D",
          "content": "Use AWS WAF to protect the API Gateway API. Configure Amazon Inspector to protect the legacy API. Configure Amazon GuardDuty to block malicious attempts to access the APIs."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>The question requires a solution that: &nbsp;</p><p>1. Increases security for both the API Gateway-based API and the legacy EC2-hosted API. &nbsp;</p><p>2. Prevents DoS attacks, checks for vulnerabilities, and guards against common exploits. &nbsp;</p><p>Option C provides the most complete and accurate approach: &nbsp;</p><p>- AWS WAF (Web Application Firewall): &nbsp;</p><p> &nbsp;- Protects the API Gateway API from common web exploits (e.g., SQL injection, XSS) and helps mitigate DoS attacks. &nbsp;</p><p> &nbsp;- *Note: AWS WAF cannot be directly applied to the legacy EC2 API (since it’s not behind an ALB, CloudFront, or API Gateway).* &nbsp;</p><p>- Amazon Inspector: &nbsp;</p><p> &nbsp;- Scans the legacy EC2 instance for vulnerabilities (OS/app-level misconfigurations, CVEs). &nbsp;</p><p>- Amazon GuardDuty: &nbsp;</p><p> &nbsp;- Monitors for malicious activity (e.g., unusual API access patterns, brute force attempts) but does not block traffic directly (it provides threat detection, not enforcement). &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- A: Incorrect because AWS WAF cannot protect the legacy EC2 API (unless it’s behind a supported service like ALB/CloudFront). &nbsp;</p><p>- B: Incorrect because: &nbsp;</p><p> &nbsp;- Inspector cannot analyze API Gateway (it’s for EC2, ECR, Lambda, etc., not managed services). &nbsp;</p><p> &nbsp;- GuardDuty cannot block traffic (only detects threats; enforcement requires integration with other services). &nbsp;</p><p>- D: Incorrect because: &nbsp;</p><p> &nbsp;- Inspector does not \"protect\" (it only assesses vulnerabilities). &nbsp;</p><p> &nbsp;- GuardDuty cannot block traffic directly (same issue as B). &nbsp;</p><p> Conclusion: &nbsp;</p><p>C is the correct choice because it: &nbsp;</p><p>✔ Uses AWS WAF for API Gateway (protection against exploits/DoS). &nbsp;</p><p>✔ Uses Inspector for legacy EC2 (vulnerability assessment). &nbsp;</p><p>✔ Uses GuardDuty for monitoring (threat detection). &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "b328e0d6d395489692e1ceaec1ad2122",
      "questionNumber": 489,
      "type": "single",
      "content": "<p>Question #489</p><p><br></p><p>A company is running a serverless ecommerce application on AWS. The application uses Amazon API Gateway to invoke AWS Lambda Java functions. The Lambda functions connect to an Amazon RDS for MySQL database to store data.</p><p><br></p><p>During a recent sale event, a sudden increase in web traffic resulted in poor API performance and database connection failures. The company needs to implement a solution to minimize the latency for the Lambda functions and to support bursts in traffic.</p><p><br></p><p>Which solution will meet these requirements with the LEAST amount of change to the application?</p>",
      "options": [
        {
          "label": "A",
          "content": "Update the code of the Lambda functions so that the Lambda functions open the database connection outside of the function handler. Increase the provisioned concurrency for the Lambda functions."
        },
        {
          "label": "B",
          "content": "Create an RDS Proxy endpoint for the database. Store database secrets in AWS Secrets Manager. Set up the required IAM permissions. Update the Lambda functions to connect to the RDS Proxy endpoint. Increase the provisioned concurrency for the Lambda functions."
        },
        {
          "label": "C",
          "content": "Create a custom parameter group. Increase the value of the max_connections parameter. Associate the custom parameter group with the RDS DB instance and schedule a reboot. Increase the reserved concurrency for the Lambda functions."
        },
        {
          "label": "D",
          "content": "Create an RDS Proxy endpoint for the database. Store database secrets in AWS Secrets Manager. Set up the required IAM permissions. Update the Lambda functions to connect to the RDS Proxy endpoint. Increase the reserved concurrency for the Lambda functions."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>The main issues during the sale event were: &nbsp;</p><p>1. Database connection failures due to high Lambda concurrency overwhelming the RDS MySQL database. &nbsp;</p><p>2. Increased API latency due to Lambda cold starts and connection management overhead. &nbsp;</p><p> Why Option B is the Best Solution? &nbsp;</p><p>- RDS Proxy helps manage database connections efficiently by pooling and reusing connections, preventing exhaustion when Lambda scales up. &nbsp;</p><p>- Secrets Manager securely stores database credentials, which RDS Proxy integrates with. &nbsp;</p><p>- Provisioned Concurrency reduces Lambda cold starts, improving latency during traffic bursts. &nbsp;</p><p>- This solution requires minimal code changes—only updating the Lambda functions to point to the RDS Proxy endpoint. &nbsp;</p><p> Why Other Options Are Not Ideal? &nbsp;</p><p>- A: While moving the connection outside the handler and using provisioned concurrency helps, it doesn’t solve the RDS connection exhaustion issue. &nbsp;</p><p>- C: Increasing `max_connections` in RDS may help, but it doesn’t efficiently manage connection pooling like RDS Proxy. Reserved concurrency (unlike provisioned concurrency) doesn’t prevent cold starts. &nbsp;</p><p>- D: Reserved concurrency limits Lambda scaling, which is counterproductive during traffic bursts. Provisioned concurrency (as in B) is the correct choice for reducing latency. &nbsp;</p><p> Conclusion: &nbsp;</p><p>B is the best solution because it combines RDS Proxy (for connection management) and provisioned concurrency (to reduce latency) with minimal changes. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "a0b3f17263664833b52edc2a209dca24",
      "questionNumber": 490,
      "type": "single",
      "content": "<p>Question #490</p><p><br></p><p>A company requires that all internal application connectivity use private IP addresses. To facilitate this policy, a solutions architect has created interface endpoints to connect to AWS Public services. Upon testing, the solutions architect notices that the service names are resolving to public IP addresses, and that internal services cannot connect to the interface endpoints.</p><p><br></p><p>Which step should the solutions architect take to resolve this issue?</p>",
      "options": [
        {
          "label": "A",
          "content": "Update the subnet route table with a route to the interface endpoint."
        },
        {
          "label": "B",
          "content": "Enable the private DNS option on the VPC attributes."
        },
        {
          "label": "C",
          "content": "Configure the security group on the interface endpoint to allow connectivity to the AWS services."
        },
        {
          "label": "D",
          "content": "Configure an Amazon Route 53 private hosted zone with a conditional forwarder for the internal application."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Enable the private DNS option on the VPC attributes.</p><p>This is the correct solution because:</p><p>The problem described is that service names are resolving to public IP addresses instead of private IP addresses, even though interface endpoints have been created. This indicates a DNS resolution issue.</p><p>When you create interface endpoints to AWS services, by default they come with their own endpoint-specific DNS names that resolve to the private IP addresses of the endpoint network interfaces in your VPC. However, applications typically use the standard public DNS names of AWS services (like s3.amazonaws.com).</p><p>To make the standard public DNS names of AWS services resolve to the private IP addresses of your interface endpoints, you need to enable the private DNS option. This creates a private hosted zone in Route 53 that overrides the default public DNS resolution within your VPC.</p><p>When the private DNS option is enabled, requests from within your VPC to the standard AWS service DNS names will resolve to the private IP addresses of your interface endpoints, allowing your internal applications to connect to AWS services using private IP addresses as required by the company policy.</p><p>The other options are incorrect for the following reasons:</p><p>A. Updating the subnet route table with a route to the interface endpoint - Interface endpoints don't require specific routes in the route table. They work at the DNS level by resolving service names to private IP addresses.</p><p>C. Configuring the security group on the interface endpoint - While security groups are important for controlling access to interface endpoints, they don't affect DNS resolution. The issue described is about DNS resolution, not access control.</p><p>D. Configuring an Amazon Route 53 private hosted zone with a conditional forwarder - This is a more complex solution than necessary. Enabling the private DNS option automatically creates the required private hosted zone with the correct DNS records.</p><p>By enabling the private DNS option, the solutions architect will ensure that AWS service names resolve to the private IP addresses of the interface endpoints, allowing internal applications to connect to AWS services using private IP addresses as required by the company policy.</p><p>Sources</p><p>Centralized access to VPC private endpoints - Building a Scalable and Secure Multi-VPC AWS Network Infrastructure （https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/centralized-access-to-vpc-private-endpoints.html）</p><p>Troubleshoot resolving domain name over a VPC peering connection | AWS re:Post（https://repost.aws/knowledge-center/vpc-peering-troubleshoot-dns-resolution） </p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "da5c5c8e10544597a1cbd549c9fdb0ab",
      "questionNumber": 491,
      "type": "single",
      "content": "<p>Question #491</p><p><br></p><p>A company is developing a latency-sensitive application. Part of the application includes several AWS Lambda functions that need to initialize as quickly as possible. The Lambda functions are written in Java and contain initialization code outside the handlers to load libraries, initialize classes, and generate unique IDs.</p><p><br></p><p>Which solution will meet the startup performance requirement MOST cost-effectively?</p>",
      "options": [
        {
          "label": "A",
          "content": "Move all the initialization code to the handlers for each Lambda function. Activate Lambda SnapStart for each Lambda function. Configure SnapStart to reference the $LATEST version of each Lambda function."
        },
        {
          "label": "B",
          "content": "Publish a version of each Lambda function. Create an alias for each Lambda function. Configure each alias to point to its corresponding version. Set up a provisioned concurrency configuration for each Lambda function to point to the corresponding alias."
        },
        {
          "label": "C",
          "content": "&nbsp;Publish a version of each Lambda function. Set up a provisioned concurrency configuration for each Lambda function to point to the corresponding version. Activate Lambda SnapStart for the published versions of the Lambda functions."
        },
        {
          "label": "D",
          "content": "Update the Lambda functions to add a pre-snapshot hook. Move the code that generates unique IDs into the handlers. Publish a version of each Lambda function. Activate Lambda SnapStart for the published versions of the Lambda functions."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. Update the Lambda functions to add a pre-snapshot hook. Move the code that generates unique IDs into the handlers. Publish a version of each Lambda function. Activate Lambda SnapStart for the published versions of the Lambda functions.</p><p>This solution will meet the startup performance requirement most cost-effectively for the following reasons:</p><p>Lambda SnapStart for Java: Lambda SnapStart significantly improves the cold start performance of Java functions by taking a snapshot of the initialized execution environment and reusing it for subsequent invocations. This directly addresses the requirement for Lambda functions to \"initialize as quickly as possible.\"</p><p>Moving Unique ID Generation to Handlers: The current Lambda functions generate unique IDs during initialization (outside the handlers). This is problematic with SnapStart because the same snapshot is used for multiple function invocations, which would result in duplicate IDs. Moving the unique ID generation code into the handlers ensures that each function invocation generates a new, unique ID.</p><p>Pre-snapshot Hooks: Adding pre-snapshot hooks allows for proper preparation of the execution environment before the snapshot is taken. This ensures that libraries and classes are properly initialized and optimized before being captured in the snapshot, leading to better performance when the function is invoked.</p><p>Cost-Effectiveness: This solution is more cost-effective than option C because it doesn't use provisioned concurrency, which incurs charges even when the functions aren't being invoked. With option D, you only pay for the actual function invocations and the standard SnapStart costs.</p><p>Option C (using provisioned concurrency with SnapStart) would also improve startup performance but at a higher cost. Provisioned concurrency keeps execution environments warm and ready to respond, but you pay for this capacity regardless of whether it's being used. This makes it less cost-effective, especially for applications with variable or unpredictable traffic patterns.</p><p>By optimizing the code structure for SnapStart (moving unique ID generation to handlers and using pre-snapshot hooks) and avoiding the ongoing costs of provisioned concurrency, option D provides the best balance of meeting the startup performance requirements while minimizing costs.</p><p>Sources</p><p>What Are The Best Practices for Managing Cold Starts in AWS Lambda ? | AWS re:Post （https://repost.aws/questions/QUCMh4YyniS4WieB786GGM9A/what-are-the-best-practices-for-managing-cold-starts-in-aws-lambda）</p><p>Reduce SDK startup time for AWS Lambda - AWS SDK for Java 2.x （https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/lambda-optimize-starttime.html）</p><p>Improving API performance at Sonar with Lambda SnapStart and Micronaut | AWS Open Source Blog （https://aws.amazon.com/cn/blogs/opensource/improving-api-performance-at-sonar-with-lambda-snapstart-and-micronaut/）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "04c89f669cd6487dba265ad16d021173",
      "questionNumber": 492,
      "type": "multiple",
      "content": "<p>Question #492</p><p><br></p><p>A solutions architect is importing a VM from an on-premises environment by using the Amazon EC2 VM Import feature of AWS Import/Export. The solutions architect has created an AMI and has provisioned an Amazon EC2 instance that is based on that AMI. The EC2 instance runs inside a public subnet in a VPC and has a public IP address assigned.</p><p><br></p><p>The EC2 instance does not appear as a managed instance in the AWS Systems Manager console.</p><p><br></p><p>Which combination of steps should the solutions architect take to troubleshoot this issue? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Verify that Systems Manager Agent is installed on the instance and is running."
        },
        {
          "label": "B",
          "content": "Verify that the instance is assigned an appropriate IAM role for Systems Manager."
        },
        {
          "label": "C",
          "content": "Verify the existence of a VPC endpoint on the VPC."
        },
        {
          "label": "D",
          "content": "Verify that the AWS Application Discovery Agent is configured."
        },
        {
          "label": "E",
          "content": "Verify the correct configuration of service-linked roles for Systems Manager."
        }
      ],
      "correctAnswer": "AB",
      "explanation": "<p>Based on the information provided, the correct answers are:</p><p>A. Verify that Systems Manager Agent is installed on the instance and is running. B. Verify that the instance is assigned an appropriate IAM role for Systems Manager.</p><p>These are the two critical steps needed to troubleshoot why the EC2 instance doesn't appear as a managed instance in AWS Systems Manager:</p><p>Systems Manager Agent (SSM Agent) Verification (Option A):</p><p>When importing a VM from an on-premises environment using EC2 VM Import, the SSM Agent is not automatically installed on the resulting AMI.</p><p>The SSM Agent is essential for Systems Manager to communicate with and manage the EC2 instance.</p><p>The solutions architect needs to verify that the SSM Agent is properly installed and running on the imported instance.</p><p>Without the SSM Agent, the instance cannot communicate with the Systems Manager service and won't appear in the console.</p><p>IAM Role Assignment (Option B):</p><p>EC2 instances require an IAM role with appropriate permissions to communicate with Systems Manager.</p><p>The imported VM likely doesn't have the necessary IAM role attached to it.</p><p>The solutions architect needs to create and attach an IAM role with the AmazonSSMManagedInstanceCore managed policy (or equivalent permissions) to the instance.</p><p>Without this role, the instance won't have permission to communicate with the Systems Manager service.</p><p>The other options are not necessary or relevant for this specific issue:</p><p>C. Verify the existence of a VPC endpoint on the VPC - While VPC endpoints can enhance security by keeping traffic within the AWS network, they are optional. Since the instance is in a public subnet with a public IP address, it can communicate with Systems Manager over the internet without requiring VPC endpoints.</p><p>D. Verify that the AWS Application Discovery Agent is configured - The Application Discovery Agent is part of AWS Application Discovery Service, which is used for migration planning. It's not related to or required for Systems Manager functionality.</p><p>E. Verify the correct configuration of service-linked roles for Systems Manager - Service-linked roles are primarily used for AWS services to interact with other services on your behalf. They are not typically the cause of an instance not appearing in Systems Manager.</p><p>The most common reasons for an EC2 instance not appearing in Systems Manager are the absence of the SSM Agent or missing IAM permissions, which is why options A and B are the correct troubleshooting steps.</p><p>Sources</p><p>Troubleshooting managed node issues in Systems Manager with SAW | AWS re:Post （https://repost.aws/articles/ARRZ_rHRGiRQ2_MaNPDhISSw/troubleshooting-managed-node-issues-in-systems-manager-with-saw）</p><p>Configure instance permissions required for Systems Manager - AWS Systems Manager （https://docs.aws.amazon.com/systems-manager/latest/userguide/setup-instance-permissions.html）</p><p>Unable to Connect to EC2 Instance via SSM Session Manager | AWS re:Post（https://repost.aws/questions/QUmw-Dgnm0RuaCyCLsOVuo5Q/unable-to-connect-to-ec2-instance-via-ssm-session-manager） </p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "a2e86d096f194d628aaeca34dba6a0a5",
      "questionNumber": 493,
      "type": "single",
      "content": "<p>Question #493</p><p><br></p><p>A company is using AWS CloudFormation as its deployment tool for all applications. It stages all application binaries and templates within Amazon S3 buckets with versioning enabled. Developers have access to an Amazon EC2 instance that hosts the integrated development environment (IDE). The developers download the application binaries from Amazon S3 to the EC2 instance, make changes, and upload the binaries to an S3 bucket after running the unit tests locally. The developers want to improve the existing deployment mechanism and implement CI/CD using AWS CodePipeline.</p><p><br></p><p>The developers have the following requirements:</p><p><br></p><p>- Use AWS CodeCommit for source control.</p><p>- Automate unit testing and security scanning.</p><p>- Alert the developers when unit tests fail.</p><p>- Turn application features on and off, and customize deployment dynamically as part of CI/CD.</p><p>- Have the lead developer provide approval before deploying an application.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use AWS CodeBuild to run unit tests and security scans. Use an Amazon EventBridge rule to send Amazon SNS alerts to the developers when unit tests fail. Write AWS Cloud Development Kit (AWS CDK) constructs for different solution features, and use a manifest file to turn features on and off in the AWS CDK application. Use a manual approval stage in the pipeline to allow the lead developer to approve applications."
        },
        {
          "label": "B",
          "content": "Use AWS Lambda to run unit tests and security scans. Use Lambda in a subsequent stage in the pipeline to send Amazon SNS alerts to the developers when unit tests fail. Write AWS Amplify plugins for different solution features and utilize user prompts to turn features on and off. Use Amazon SES in the pipeline to allow the lead developer to approve applications."
        },
        {
          "label": "C",
          "content": "Use Jenkins to run unit tests and security scans. Use an Amazon EventBridge rule in the pipeline to send Amazon SES alerts to the developers when unit tests fail. Use AWS CloudFormation nested stacks for different solution features and parameters to turn features on and off. Use AWS Lambda in the pipeline to allow the lead developer to approve applications."
        },
        {
          "label": "D",
          "content": "Use AWS CodeDeploy to run unit tests and security scans. Use an Amazon CloudWatch alarm in the pipeline to send Amazon SNS alerts to the developers when unit tests fail. Use Docker images for different solution features and the AWS CLI to turn features on and off. Use a manual approval stage in the pipeline to allow the lead developer to approve applications."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>The developers want to implement a CI/CD pipeline using AWS CodePipeline with the following key requirements: &nbsp;</p><p>1. Use AWS CodeCommit for source control. &nbsp;</p><p>2. Automate unit testing and security scanning (AWS CodeBuild is ideal for this). &nbsp;</p><p>3. Alert developers when unit tests fail (Amazon EventBridge + SNS can send notifications). &nbsp;</p><p>4. Turn features on/off dynamically (AWS CDK constructs + manifest file allow feature toggling). &nbsp;</p><p>5. Lead developer approval before deployment (manual approval stage in CodePipeline). &nbsp;</p><p> Why Option A is the Best Solution? &nbsp;</p><p>✅ AWS CodeBuild – Runs unit tests and security scans efficiently. &nbsp;</p><p>✅ Amazon EventBridge + SNS – Sends alerts when tests fail. &nbsp;</p><p>✅ AWS CDK constructs + manifest file – Enables dynamic feature toggling. &nbsp;</p><p>✅ Manual approval stage – Allows lead developer to review before deployment. &nbsp;</p><p> Why Other Options Are Not Ideal? &nbsp;</p><p>❌ B: &nbsp;</p><p>- Using Lambda for tests/scans is inefficient (CodeBuild is purpose-built). &nbsp;</p><p>- AWS Amplify plugins are not suitable for CloudFormation-based deployments. &nbsp;</p><p>- Amazon SES is for email, not pipeline approvals. &nbsp;</p><p>❌ C: &nbsp;</p><p>- Jenkins is not a fully managed AWS-native solution (CodeBuild is preferred). &nbsp;</p><p>- Amazon SES is not ideal for alerts (SNS is better). &nbsp;</p><p>- Lambda for approvals is unnecessary (CodePipeline has built-in manual approvals). &nbsp;</p><p>❌ D: &nbsp;</p><p>- AWS CodeDeploy is for deployment, not running tests/scans (CodeBuild is correct). &nbsp;</p><p>- Docker images + AWS CLI are not the best way to toggle features (AWS CDK is better). &nbsp;</p><p> Conclusion: &nbsp;</p><p>A is the best solution because it uses AWS-native tools (CodeBuild, CDK, EventBridge, SNS) and meets all requirements with minimal complexity. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "8401936c2ef541de944e6dd61c37be9a",
      "questionNumber": 494,
      "type": "single",
      "content": "<p>Question #494</p><p><br></p><p>A global e-commerce company has many data centers around the world. With the growth of its stored data, the company needs to set up a solution to provide scalable storage for legacy on-premises file applications. The company must be able to take point-in-time copies of volumes using AWS Backup and must retain low-latency access to frequently accessed data. The company also needs to have storage volumes that can be mounted as Internet Small Computer System Interface (iSCSI) devices from the company’s on-premises application servers.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Provision an AWS Storage Gateway tape gateway. Configure the tape gateway to store data in an Amazon S3 bucket. Deploy AWS Backup to take point-in-time copies of the volumes."
        },
        {
          "label": "B",
          "content": "Provision an Amazon FSx File Gateway and an Amazon S3 File Gateway. Deploy AWS Backup to take point-in-time copies of the data."
        },
        {
          "label": "C",
          "content": "Provision an AWS Storage Gateway volume gateway in cache mode. Back up the on-premises Storage Gateway volumes with AWS Backup."
        },
        {
          "label": "D",
          "content": "Provision an AWS Storage Gateway file gateway in cache mode. Deploy AWS Backup to take point-in-time copies of the volumes."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Provision an AWS Storage Gateway volume gateway in cache mode. Back up the on-premises Storage Gateway volumes with AWS Backup.</p><p>This solution meets all the requirements stated in the question for the following reasons:</p><p>iSCSI Connectivity Requirement: The company needs storage volumes that can be mounted as iSCSI devices from on-premises application servers. AWS Storage Gateway volume gateway specifically provides iSCSI connectivity, allowing on-premises applications to access cloud storage using the iSCSI protocol. File gateway, on the other hand, uses NFS or SMB protocols, not iSCSI.</p><p>Point-in-Time Copies with AWS Backup: The company needs to take point-in-time copies of volumes using AWS Backup. AWS Storage Gateway volume gateway integrates with AWS Backup, allowing the creation and management of point-in-time snapshots of the volumes. These snapshots are stored as Amazon EBS snapshots in AWS.</p><p>Low-Latency Access to Frequently Accessed Data: The cache mode of the volume gateway keeps frequently accessed data locally on-premises while storing the primary data in Amazon S3. This provides low-latency access to commonly used data, meeting the company's requirement for fast access to frequently accessed information.</p><p>Scalable Storage for Legacy Applications: Volume gateway in cache mode provides virtually unlimited storage capacity by leveraging Amazon S3 for primary storage, while only requiring local storage for the cache. This addresses the company's need for scalable storage as their data continues to grow.</p><p>Option D (AWS Storage Gateway file gateway in cache mode) is incorrect because:</p><p>File gateway does not provide iSCSI connectivity, which is a specific requirement mentioned in the question.</p><p>File gateway uses file-level protocols (NFS/SMB) rather than block-level protocols like iSCSI.</p><p>While file gateway can work with data stored in S3, it doesn't integrate with AWS Backup in the same way that volume gateway does for taking point-in-time copies of volumes.</p><p>The volume gateway in cache mode is the appropriate solution for this scenario as it provides the required iSCSI connectivity, works with AWS Backup for point-in-time copies, offers low-latency access to frequently accessed data, and provides scalable storage for the company's growing data needs.</p><p>Sources</p><p>Backup and recovery from on-premises infrastructure to AWS - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/backup-recovery/on-premises-to-aws.html）</p><p>Storage Gateway - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/configuring-storage-offload-vmware-cloud-aws/storage-gateway-option.html）</p><p>Volume Gateway | Amazon Web Services （https://aws.amazon.com/cn/storagegateway/volume/）</p><p>Backing up your volumes - AWS Storage Gateway （https://docs.aws.amazon.com/storagegateway/latest/vgw/backing-up-volumes.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "f36a7c666b394425a64bc53e93fff45d",
      "questionNumber": 495,
      "type": "single",
      "content": "<p>Question #495</p><p><br></p><p>A company has an application that uses AWS Key Management Service (AWS KMS) to encrypt and decrypt data. The application stores data in an Amazon S3 bucket in an AWS Region. Company security policies require the data to be encrypted before the data is placed into the S3 bucket. The application must decrypt the data when the application reads files from the S3 bucket.</p><p><br></p><p>The company replicates the S3 bucket to other Regions. A solutions architect must design a solution so that the application can encrypt and decrypt data across Regions. The application must use the same key to decrypt the data in each Region.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a KMS multi-Region primary key. Use the KMS multi-Region primary key to create a KMS multi-Region replica key in each additional Region where the application is running. Update the application code to use the specific replica key in each Region."
        },
        {
          "label": "B",
          "content": "Create a new customer managed KMS key in each additional Region where the application is running. Update the application code to use the specific KMS key in each Region."
        },
        {
          "label": "C",
          "content": "Use AWS Private Certificate Authority to create a new certificate authority (CA) in the primary Region. Issue a new private certificate from the CA for the application&rsquo;s website URL. Share the CA with the additional Regions by using AWS Resource Access Manager (AWS RAM). Update the application code to use the shared CA certificates in each Region."
        },
        {
          "label": "D",
          "content": "Use AWS Systems Manager Parameter Store to create a parameter in each additional Region where the application is running. Export the key material from the KMS key in the primary Region. Store the key material in the parameter in each Region. Update the application code to use the key data from the parameter in each Region."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>The company needs a solution that: &nbsp;</p><p>1. Encrypts/decrypts data in multiple Regions using the same KMS key. &nbsp;</p><p>2. Maintains security compliance (data must be encrypted before being stored in S3). &nbsp;</p><p>3. Avoids managing multiple keys (to simplify decryption across Regions). &nbsp;</p><p> Why Option A is the Best Solution? &nbsp;</p><p>✅ KMS Multi-Region Keys (MRK) allow: &nbsp;</p><p> &nbsp; - A primary key in one Region. &nbsp;</p><p> &nbsp; - Replica keys in other Regions that share the same key material. &nbsp;</p><p> &nbsp; - The application can use the local replica key in each Region, ensuring low latency while maintaining consistent encryption/decryption (since the keys are cryptographically identical). &nbsp;</p><p>✅ Minimal code changes – The application only needs to reference the replica key in its current Region. &nbsp;</p><p> Why Other Options Are Not Ideal? &nbsp;</p><p>❌ B: &nbsp;</p><p>- Different KMS keys per Region mean the application cannot decrypt data across Regions without re-encrypting it with the new key. &nbsp;</p><p>- Violates the requirement of using the same key. &nbsp;</p><p>❌ C: &nbsp;</p><p>- AWS Private CA is for TLS certificates, not KMS encryption. &nbsp;</p><p>- Does not solve the KMS key synchronization problem. &nbsp;</p><p>❌ D: &nbsp;</p><p>- Exporting KMS key material is highly discouraged (security risk). &nbsp;</p><p>- Parameter Store is not designed for secure key material storage (KMS is the correct service). &nbsp;</p><p> Conclusion: &nbsp;</p><p>A is the best solution because KMS Multi-Region Keys allow the same encryption key to be used across Regions securely, with minimal application changes. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "d7707098ce384a1b9deff6d55a06d8bc",
      "questionNumber": 496,
      "type": "single",
      "content": "<p>Question #496</p><p><br></p><p>A company hosts an application that uses several Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB). During the initial startup of the EC2 instances, the EC2 instances run user data scripts to download critical content for the application from an Amazon S3 bucket.</p><p><br></p><p>The EC2 instances are launching correctly. However, after a period of time, the EC2 instances are terminated with the following error message: “An instance was taken out of service in response to an ELB system health check failure.” EC2 instances continue to launch and be terminated because of Auto Scaling events in an endless loop.</p><p><br></p><p>The only recent change to the deployment is that the company added a large amount of critical content to the S3 bucket. The company does not want to alter the user data scripts in production.</p><p><br></p><p>What should a solutions architect do so that the production environment can deploy successfully?</p>",
      "options": [
        {
          "label": "A",
          "content": "&nbsp;Increase the size of the EC2 instances."
        },
        {
          "label": "B",
          "content": "Increase the health check timeout for the ALB."
        },
        {
          "label": "C",
          "content": "Change the health check path for the ALB."
        },
        {
          "label": "D",
          "content": "&nbsp;Increase the health check grace period for the Auto Scaling group."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>The issue occurs because: &nbsp;</p><p>1. EC2 instances take longer to start due to downloading a large amount of content from S3 during user data execution. &nbsp;</p><p>2. ALB health checks start too early before the instances are fully ready, causing them to be marked as unhealthy and terminated. &nbsp;</p><p>3. This creates a loop where new instances launch, fail health checks, and get terminated repeatedly. &nbsp;</p><p> Why Option D is the Best Solution? &nbsp;</p><p>✅ Increase the health check grace period for the Auto Scaling group: &nbsp;</p><p> &nbsp; - This gives instances more time to complete their startup tasks (downloading from S3) before health checks begin. &nbsp;</p><p> &nbsp; - Prevents premature termination while keeping the existing user data scripts unchanged. &nbsp;</p><p> &nbsp; - No changes to ALB settings or instance size are needed. &nbsp;</p><p> Why Other Options Are Not Ideal? &nbsp;</p><p>❌ A: Increasing EC2 instance size &nbsp;</p><p> &nbsp; - Might help with download speed, but does not guarantee instances will be ready before health checks start. &nbsp;</p><p> &nbsp; - Unnecessary cost increase if the issue is timing, not compute power. &nbsp;</p><p>❌ B: Increasing ALB health check timeout &nbsp;</p><p> &nbsp; - This only extends how long the ALB waits for a health check response, not when health checks start. &nbsp;</p><p> &nbsp; - Does not solve the root cause (instances need more time to initialize). &nbsp;</p><p>❌ C: Changing the ALB health check path &nbsp;</p><p> &nbsp; - If the application is not yet running, changing the path won’t help—the instance still fails checks. &nbsp;</p><p> &nbsp; - Does not address the delay in startup tasks. &nbsp;</p><p> Conclusion: &nbsp;</p><p>D is the best solution because it delays health checks until instances are fully initialized, breaking the termination loop without modifying user data scripts. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "c38e009c80f24dcda485eb1b6cca3831",
      "questionNumber": 497,
      "type": "single",
      "content": "<p>Question #497</p><p><br></p><p>A company needs to move some on-premises Oracle databases to AWS. The company has chosen to keep some of the databases on premises for business compliance reasons.</p><p><br></p><p>The on-premises databases contain spatial data and run cron jobs for maintenance. The company needs to connect to the on-premises systems directly from AWS to query data as a foreign table.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create Amazon DynamoDB global tables with auto scaling enabled. Use the AWS Schema Conversion Tool (AWS SCT) and AWS Database Migration Service (AWS DMS) to move the data from on premises to DynamoDB. Create an AWS Lambda function to move the spatial data to Amazon S3. Query the data by using Amazon Athena. Use Amazon EventBridge to schedule jobs in DynamoDB for maintenance. Use Amazon API Gateway for foreign table support."
        },
        {
          "label": "B",
          "content": "Create an Amazon RDS for Microsoft SQL Server DB instance. Use native replication to move the data from on premises to the DB instance. Use the AWS Schema Conversion Tool (AWS SCT) to modify the SQL Server schema as needed after replication. Move the spatial data to Amazon Redshift. Use stored procedures for system maintenance. Create AWS Glue crawlers to connect to the on-premises Oracle databases for foreign table support."
        },
        {
          "label": "C",
          "content": "Launch Amazon EC2 instances to host the Oracle databases. Place the EC2 instances in an Auto Scaling group. Use AWS Application Migration Service to move the data from on premises to the EC2 instances and for real-time bidirectional change data capture (CDC) synchronization. Use Oracle native spatial data support. Create an AWS Lambda function to run maintenance jobs as part of an AWS Step Functions workflow. Create an internet gateway for foreign table support."
        },
        {
          "label": "D",
          "content": "Create an Amazon RDS for PostgreSQL DB instance. Use the AWS Schema Conversion Tool (AWS SCT) and AWS Database Migration Service (AWS DMS) to move the data from on premises to the DB instance. Use PostgreSQL native spatial data support. Run cron jobs on the DB instance for maintenance. Use AWS Direct Connect to connect the DB instance to the on-premises environment for foreign table support."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>The requirements are: &nbsp;</p><p>1. Migrate Oracle databases to AWS while keeping some on-premises for compliance. &nbsp;</p><p>2. Spatial data support – PostgreSQL has native support for spatial data (PostGIS extension). &nbsp;</p><p>3. Cron jobs for maintenance – PostgreSQL allows scheduled jobs (via `pg_cron` or external schedulers). &nbsp;</p><p>4. Query on-premises data as foreign tables – AWS Direct Connect provides a secure, high-speed connection to on-premises systems, and PostgreSQL supports Foreign Data Wrappers (FDW) to query remote databases. &nbsp;</p><p> Why not the other options? &nbsp;</p><p>- A: DynamoDB is not suitable for spatial data or complex SQL queries. Athena is for S3-based analytics, not Oracle foreign tables. &nbsp;</p><p>- B: SQL Server is not the best choice for migrating from Oracle, and Redshift is for analytics, not OLTP workloads. &nbsp;</p><p>- C: Running Oracle on EC2 is costly and doesn’t leverage AWS managed services. An internet gateway is not secure for direct database connections. &nbsp;</p><p> Why D is correct: &nbsp;</p><p>- Amazon RDS for PostgreSQL is a managed database service with PostGIS for spatial data. &nbsp;</p><p>- AWS SCT + DMS handles schema conversion and migration. &nbsp;</p><p>- Cron jobs can be implemented via `pg_cron` or Amazon EventBridge. &nbsp;</p><p>- AWS Direct Connect ensures secure, low-latency connectivity for querying on-premises Oracle databases as foreign tables. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "ef9cef22625043e1a303f9db2109f1cb",
      "questionNumber": 498,
      "type": "single",
      "content": "<p>Question #498</p><p><br></p><p>Accompany runs an application on Amazon EC2 and AWS Lambda. The application stores temporary data in Amazon S3. The S3 objects are deleted after 24 hours.</p><p><br></p><p>The company deploys new versions of the application by launching AWS CloudFormation stacks. The stacks create the required resources. After validating a new version, the company deletes the old stack. The deletion of an old development stack recently failed.</p><p><br></p><p>A solutions architect needs to resolve this issue without major architecture changes.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a Lambda function to delete objects from an S3 bucket. Add the Lambda function as a custom resource in the CloudFormation stack with a DependsOn attribute that points to the S3 bucket resource."
        },
        {
          "label": "B",
          "content": "Modify the CloudFormation stack to attach a DeletionPolicy attribute with a value of Delete to the S3 bucket."
        },
        {
          "label": "C",
          "content": "Update the CloudFormation stack to add a DeletionPolicy attribute with a value of Snapshot for the S3 bucket resource."
        },
        {
          "label": "D",
          "content": "Update the CloudFormation template to create an Amazon Elastic File System (Amazon EFS) file system to store temporary files instead of Amazon S3. Configure the Lambda functions to run in the same VPC as the EFS file system."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Create a Lambda function to delete objects from an S3 bucket. Add the Lambda function as a custom resource in the CloudFormation stack with a DependsOn attribute that points to the S3 bucket resource.</p><p>This solution will resolve the CloudFormation stack deletion failure issue for the following reasons:</p><p>Root Cause of the Problem: The stack deletion is failing because CloudFormation cannot delete an S3 bucket that contains objects. Even though the application is designed to delete objects after 24 hours, there might still be objects in the bucket at the time of stack deletion.</p><p>Why Option A Works: Creating a Lambda function as a custom resource with a DependsOn attribute ensures that:</p><p>The Lambda function will run during the stack deletion process</p><p>It will delete all objects from the S3 bucket before CloudFormation attempts to delete the bucket itself</p><p>The DependsOn attribute ensures proper sequencing, making sure the Lambda function executes before CloudFormation tries to delete the bucket</p><p>Advantages of This Approach:</p><p>It handles the cleanup of objects automatically during stack deletion</p><p>It works even if there are versioned objects or objects that were missed by the application's 24-hour deletion process</p><p>It's a more robust solution that will work consistently across different environments</p><p>Option B (Modify the CloudFormation stack to attach a DeletionPolicy attribute with a value of Delete to the S3 bucket) is incorrect because:</p><p>The DeletionPolicy attribute with a value of Delete only tells CloudFormation to delete the bucket during stack deletion, but it doesn't automatically delete the objects within the bucket.</p><p>If the bucket contains any objects, the stack deletion will still fail even with this policy in place.</p><p>CloudFormation requires buckets to be empty before they can be deleted, regardless of the DeletionPolicy setting.</p><p>The Lambda function approach (Option A) is more comprehensive as it actively empties the bucket before CloudFormation attempts to delete it, addressing the root cause of the stack deletion failure. This solution meets the requirement of resolving the issue without major architecture changes, as it only adds a custom resource to the existing CloudFormation stack without modifying the core application architecture.</p><p>Sources</p><p>Issue deleting S3 Bucket via CloudFormation | AWS re:Post （https://repost.aws/questions/QUBbQap76TQ8-lp-j8Gr7XQw/issue-deleting-s3-bucket-via-cloudformation）</p><p>S3 Bucket Not Deleted When Deleting CloudFormation Stack | AWS re:Post（https://repost.aws/questions/QUWQxEijWhSS6TtNh_Qu8gwg/s3-bucket-not-deleted-when-deleting-cloudformation-stack） </p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "4f24ce36c17e4beebbc2a9fd7f47bf7d",
      "questionNumber": 499,
      "type": "multiple",
      "content": "<p>Question #499</p><p><br></p><p>A company has an application that stores user-uploaded videos in an Amazon S3 bucket that uses S3 Standard storage. Users access the videos frequently in the first 180 days after the videos are uploaded. Access after 180 days is rare. Named users and anonymous users access the videos.</p><p><br></p><p>Most of the videos are more than 100 MB in size. Users often have poor internet connectivity when they upload videos, resulting in failed uploads.</p><p>The company uses multipart uploads for the videos.</p><p>A solutions architect needs to optimize the S3 costs of the application.</p><p>Which combination of actions will meet these requirements? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Configure the S3 bucket to be a Requester Pays bucket."
        },
        {
          "label": "B",
          "content": "Use S3 Transfer Acceleration to upload the videos to the S3 bucket."
        },
        {
          "label": "C",
          "content": "Create an S3 Lifecycle configuration to expire incomplete multipart uploads 7 days after initiation."
        },
        {
          "label": "D",
          "content": "Create an S3 Lifecycle configuration to transition objects to S3 Glacier Instant Retrieval after 1 day."
        },
        {
          "label": "E",
          "content": "Create an S3 Lifecycle configuration to transition objects to S3 Standard-infrequent Access (S3 Standard-IA) after 180 days."
        }
      ],
      "correctAnswer": "CE",
      "explanation": "<p>Based on the information provided, the correct answers are:</p><p>C. Create an S3 Lifecycle configuration to expire incomplete multipart uploads 7 days after initiation. E. Create an S3 Lifecycle configuration to transition objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 180 days.</p><p>These two actions together will optimize the S3 costs for the company's application for the following reasons:</p><p>Expiring incomplete multipart uploads (Option C):</p><p>The scenario mentions that users often have poor internet connectivity when uploading videos, resulting in failed uploads.</p><p>When multipart uploads fail or are abandoned, the partially uploaded parts remain in the S3 bucket and continue to incur storage costs.</p><p>Creating a lifecycle configuration to expire incomplete multipart uploads after 7 days will automatically clean up these abandoned upload parts, preventing unnecessary storage costs.</p><p>This is especially important for large files like videos (over 100MB), as multipart uploads consist of multiple parts that can add up to significant storage costs if left unmanaged.</p><p>Transitioning to S3 Standard-IA after 180 days (Option E):</p><p>The scenario states that videos are frequently accessed in the first 180 days after upload, but access after that is rare.</p><p>S3 Standard-IA is designed for data that is accessed less frequently but requires rapid access when needed.</p><p>S3 Standard-IA offers a lower storage cost compared to S3 Standard, making it ideal for the rarely accessed older videos.</p><p>Since the videos are larger than 100MB, they easily meet the 128KB minimum object size for cost-effective use of S3 Standard-IA.</p><p>The transition after 180 days aligns perfectly with the described access pattern, keeping the videos in the most cost-effective storage class based on their access frequency.</p><p>Together, these two lifecycle configurations address both the cost optimization for long-term storage and the cleanup of failed uploads, which are the two main cost concerns mentioned in the scenario. The first ensures you're not paying for abandoned upload parts, while the second ensures you're using the most cost-effective storage class based on the access patterns of the videos.</p><p>Sources</p><p>How to Optimize AWS S3 Storage Costs? | AWS re:Post （https://repost.aws/questions/QU97-ATukcRz-XashRXJSkHA/how-to-optimize-aws-s3-storage-costs）</p><p>Cost-optimized log aggregation and archival in Amazon S3 using s3tar | AWS Storage Blog （https://aws.amazon.com/cn/blogs/storage/cost-optimized-log-aggregation-and-archival-in-amazon-s3-using-s3tar/）</p><p>Secure archive storage – Amazon S3 Glacier storage classes – AWS （https://aws.amazon.com/cn/s3/storage-classes/glacier/?ams%23interactive-card-vertical%23pattern-data.filter=%257B%2522filters%2522%253A%255B%255D%257D）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "9acf48a07cc64a67be8383b8c71cd54e",
      "questionNumber": 500,
      "type": "single",
      "content": "<p>Question #500</p><p><br></p><p>A company runs an e-commerce web application on AWS. The web application is hosted as a static website on Amazon S3 with Amazon CloudFront for content delivery. An Amazon API Gateway API invokes AWS Lambda functions to handle user requests and order processing for the web application. The Lambda functions store data in an Amazon RDS for MySQL DB cluster that uses On-Demand instances. The DB cluster usage has been consistent in the past 12 months.</p><p><br></p><p>Recently, the website has experienced SQL injection and web exploit attempts. Customers also report that order processing time has increased during periods of peak usage. During these periods, the Lambda functions often have cold starts. As the company grows, the company needs to ensure scalability and low-latency access during traffic peaks. The company also must optimize the database costs and add protection against the SQL injection and web exploit attempts.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Configure the Lambda functions to have an increased timeout value during peak periods. Use RDS Reserved Instances for the database. Use CloudFront and subscribe to AWS Shield Advanced to protect against the SQL injection and web exploit attempts."
        },
        {
          "label": "B",
          "content": "Increase the memory of the Lambda functions, Transition to Amazon Redshift for the database. Integrate Amazon Inspector with CloudFront to protect against the SQL injection and web exploit attempts."
        },
        {
          "label": "C",
          "content": "&nbsp;Use Lambda functions with provisioned concurrency for compute during peak periods, Transition to Amazon Aurora Serverless for the database. Use CloudFront and subscribe to AWS Shield Advanced to protect against the SQL injection and web exploit attempts."
        },
        {
          "label": "D",
          "content": "Use Lambda functions with provisioned concurrency for compute during peak periods. Use RDS Reserved Instances for the database. Integrate AWS WAF with CloudFront to protect against the SQL injection and web exploit attempts."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>The requirements are: &nbsp;</p><p>1. Improve scalability & reduce latency during peak traffic (cold starts in Lambda, slow order processing). &nbsp;</p><p>2. Optimize database costs (consistent usage over 12 months suggests Reserved Instances can save costs). &nbsp;</p><p>3. Protect against SQL injection & web exploits (security vulnerabilities). &nbsp;</p><p> Why Option D is Correct: &nbsp;</p><p>- Lambda Provisioned Concurrency eliminates cold starts during peak traffic, improving performance. &nbsp;</p><p>- RDS Reserved Instances reduce costs for the MySQL DB cluster (since usage is consistent). &nbsp;</p><p>- AWS WAF + CloudFront blocks SQL injection and web exploits (WAF inspects HTTP/HTTPS requests). &nbsp;</p><p> Why Other Options Are Wrong: &nbsp;</p><p>- A: Increasing Lambda timeout doesn’t solve cold starts. AWS Shield Advanced protects against DDoS, not SQL injection. &nbsp;</p><p>- B: Amazon Redshift is for analytics, not transactional workloads. Amazon Inspector scans for vulnerabilities but doesn’t block attacks in real time. &nbsp;</p><p>- C: Aurora Serverless is good for unpredictable workloads, but Reserved Instances are better for consistent usage. Shield Advanced doesn’t block SQL injection (WAF does). &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "b771608c216a4c99932e00188517ee5c",
      "questionNumber": 501,
      "type": "single",
      "content": "<p>Question #501</p><p><br></p><p>A company runs a web application on a single Amazon EC2 instance. End users experience slow application performance during times of peak usage, when CPU utilization is consistently more than 95%.</p><p><br></p><p>A user data script installs required custom packages on the EC2 instance. The process of launching the instance takes several minutes.</p><p><br></p><p>The company is creating an Auto Scaling group that has mixed instance groups, varied CPUs, and a maximum capacity limit. The Auto Scaling group will use a launch template for various configuration options. The company needs to decrease application latency when new instances are launched during auto scaling.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use a predictive scaling policy. Use an instance maintenance policy to run the user data script. Set the default instance warmup time to 0 seconds."
        },
        {
          "label": "B",
          "content": "Use a dynamic scaling policy. Use lifecycle hooks to run the user data script. Set the default instance warmup time to 0 seconds."
        },
        {
          "label": "C",
          "content": "Use a predictive scaling policy. Enable warm pools for the Auto Scaling group. Use an instance maintenance policy to run the user data script."
        },
        {
          "label": "D",
          "content": "Use a dynamic scaling policy. Enable warm pools for the Auto Scaling group. Use lifecycle hooks to run the user data script."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. Use a dynamic scaling policy. Enable warm pools for the Auto Scaling group. Use lifecycle hooks to run the user data script.</p><p>This solution will most effectively decrease application latency when new instances are launched during auto scaling for the following reasons:</p><p>Warm Pools: The key advantage of this solution is the use of warm pools, which maintain pre-initialized EC2 instances that are ready to quickly respond to scaling events. Since the scenario mentions that \"launching the instance takes several minutes\" due to the installation of required custom packages, warm pools directly address this issue by having instances already prepared and waiting in a \"warmed\" state.</p><p>Lifecycle Hooks: These allow the user data script to run during the instance initialization process before the instance is put into service. This ensures that all custom packages are properly installed and the instance is fully ready before it starts handling application traffic.</p><p>Dynamic Scaling Policy: This type of policy responds to real-time metrics (like the 95% CPU utilization mentioned in the scenario) to trigger scaling actions. When combined with warm pools, the response to high CPU utilization will be much faster since the instances are already pre-initialized.</p><p>Reduced Latency: Together, these features significantly reduce the time between when a scaling event is triggered and when new capacity is actually available to serve requests, directly addressing the requirement to \"decrease application latency when new instances are launched.\"</p><p>Option A (using predictive scaling policy with instance maintenance policy and setting default instance warmup time to 0 seconds) has several limitations:</p><p>Setting the warmup time to 0 seconds would incorrectly mark instances as ready immediately after launch, even though they actually require several minutes to install custom packages. This could lead to routing traffic to instances that aren't fully initialized.</p><p>Predictive scaling is useful for anticipated load patterns but doesn't address the fundamental issue of slow instance initialization.</p><p>Instance maintenance policies are primarily designed for replacing unhealthy instances, not for improving scaling speed.</p><p>The warm pools approach in Option D directly addresses the core issue by having instances already initialized and ready to go, which is exactly what's needed when \"the process of launching the instance takes several minutes\" and you need to \"decrease application latency when new instances are launched.\"</p><p>Sources</p><p>Instance maintenance policy for Auto Scaling group - Amazon EC2 Auto Scaling （https://docs.aws.amazon.com/autoscaling/ec2/userguide/instance-maintenance-policy-overview-and-considerations.html）</p><p>Set the default instance warmup for an Auto Scaling group - Amazon EC2 Auto Scaling （https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-default-instance-warmup.html）</p><p>Decrease latency for applications with long boot times using warm pools - Amazon EC2 Auto Scaling （https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-warm-pools.html）</p><p>Auto Scaling groups - Amazon EC2 Auto Scaling （https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-groups.html）</p><p>Use lifecycle hooks with a warm pool in Auto Scaling group - Amazon EC2 Auto Scaling （https://docs.aws.amazon.com/autoscaling/ec2/userguide/warm-pool-instance-lifecycle.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "af52810219c84903b25ba337b129992d",
      "questionNumber": 502,
      "type": "multiple",
      "content": "<p>Question #502</p><p><br></p><p>A company needs to migrate its on-premises database fleet to Amazon RDS. The company is currently using a mixture of Microsoft SQL Server, MySQL, and Oracle databases. Some of the databases have custom schemas and stored procedures.</p><p><br></p><p>Which combination of steps should the company take for the migration? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Use Migration Evaluator Quick Insights to analyze the source databases and to identify the stored procedures that need to be migrated."
        },
        {
          "label": "B",
          "content": "Use AWS Application Migration Service to analyze the source databases and to identify the stored procedures that need to be migrated."
        },
        {
          "label": "C",
          "content": "Use the AWS Schema Conversion Tool (AWS SCT) to analyze the source databases for changes that are required."
        },
        {
          "label": "D",
          "content": "Use AWS Database Migration Service (AWS DMS) to migrate the source databases to Amazon RDS."
        },
        {
          "label": "E",
          "content": "Use AWS DataSync to migrate the data from the source databases to Amazon RDS."
        }
      ],
      "correctAnswer": "CD",
      "explanation": "<p>Based on the information provided, the correct answers are:</p><p>C. Use the AWS Schema Conversion Tool (AWS SCT) to analyze the source databases for changes that are required</p><p>D. Use AWS Database Migration Service (AWS DMS) to migrate the source databases to Amazon RDS.</p><p>These two tools together provide the most effective combination for migrating on-premises databases with custom schemas and stored procedures to Amazon RDS. Here's why:</p><p>AWS Schema Conversion Tool (AWS SCT):</p><p>Specifically designed to analyze database schemas and convert them from one database engine to another</p><p>Essential for handling the custom schemas and stored procedures mentioned in the scenario</p><p>Provides detailed assessment reports that identify objects that can be automatically converted and those that require manual intervention</p><p>Supports all the database types mentioned (SQL Server, MySQL, and Oracle)</p><p>Creates target schema scripts that can be applied to the RDS instances</p><p>AWS Database Migration Service (AWS DMS):</p><p>Handles the actual data migration from source databases to target RDS instances</p><p>Supports both homogeneous (same engine) and heterogeneous (different engine) migrations</p><p>Provides minimal downtime through continuous data replication</p><p>Works seamlessly with the schemas converted by AWS SCT</p><p>Can migrate data directly to RDS instances</p><p>Together, these tools address both aspects of the migration challenge:</p><p>SCT handles the schema conversion, including custom schemas and stored procedures</p><p>DMS handles the data migration with minimal disruption</p><p>The other options have significant limitations:</p><p>A. Migration Evaluator Quick Insights is primarily a discovery and assessment tool for estimating AWS costs and planning migrations. While useful in early planning stages, it doesn't handle schema conversion or data migration.</p><p>B. AWS Application Migration Service is designed for migrating applications and servers (lift-and-shift), not specifically for database schema conversion or data migration.</p><p>E. AWS DataSync is designed for transferring files between on-premises storage and AWS storage services like S3, EFS, or FSx. It's not suitable for database migrations, especially when schema conversion is required.</p><p>For a comprehensive database migration that involves custom schemas and stored procedures across multiple database engines, the combination of AWS SCT for schema conversion and AWS DMS for data migration is the most appropriate solution.</p><p>Sources</p><p>Categorizing and Prioritizing a Large-Scale Move to an Open Source Database | AWS Database Blog （https://aws.amazon.com/cn/blogs/database/categorizing-and-prioritizing-a-large-scale-move-to-an-open-source-database/）</p><p>AWS Schema Conversion Tool - Amazon Web Services （https://aws.amazon.com/cn/dms/features/）</p><p>Planning your database migration process - Migrating Your Databases to Amazon Aurora （https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.AuroraMySQL.html）</p><p>Choosing AWS migration services and tools - Choosing AWS migration services and tools （https://docs.aws.amazon.com/decision-guides/latest/migration-on-aws-how-to-choose/migration-on-aws-how-to-choose.html）</p><p>Tools for heterogeneous database migrations - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-sql-server/heterogeneous-migration-tools.html）</p><p>Migrating Databases to the Amazon Web Services Cloud Using the Database Migration Service - Database Migration Guide （https://docs.aws.amazon.com/dms/latest/sbs/chap-dms.html）</p><p>Refactoring recommendations - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/oracle-exadata-blueprint/performing-refactor.html）</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "1314a071d9d349708148c49e5978d7f5",
      "questionNumber": 503,
      "type": "multiple",
      "content": "<p>Question #503</p><p><br></p><p>A company is migrating its blog platform to AWS. The company's on-premises servers connect to AWS through an AWS Site-to-Site VPN connection. The blog content is updated several times a day by multiple authors and is served from a file share on a network-attached storage (NAS) server.</p><p><br></p><p>The company needs to migrate the blog platform without delaying the content updates. The company has deployed Amazon EC2 instances across multiple Availability Zones to run the blog platform behind an Application Load Balancer. The company also needs to move 200 TB of archival data from its on-premises servers to Amazon S3 as soon as possible.</p><p><br></p><p>Which combination of steps will meet these requirements? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a weekly cron job in Amazon EventBridge. Use the cron job to invoke an AWS Lambda function to update the EC2 instances from the NAS server."
        },
        {
          "label": "B",
          "content": "Configure an Amazon Elastic Block Store (Amazon EBS) Multi-Attach volume for the EC2 instances to share for content access. Write code to synchronize the EBS volume with the NAS server weekly."
        },
        {
          "label": "C",
          "content": "Mount an Amazon Elastic File System (Amazon EFS) file system to the on-premises servers to act as the NAS server. Copy the blog data to the EFS file system. Mount the EFS file system to the EC2 instances to serve the content."
        },
        {
          "label": "D",
          "content": "Order an AWS Snowball Edge Storage Optimized device. Copy the static data artifacts to the device. Ship the device to AWS."
        },
        {
          "label": "E",
          "content": "Order an AWS Snowball Snowcone device. Copy the static data artifacts to the device. Ship the device to AWS."
        }
      ],
      "correctAnswer": "CD",
      "explanation": "<p> For the blog platform migration (ensuring real-time content updates):</p><p>- Option C is correct because Amazon EFS provides a shared, scalable, and highly available file system that can be mounted both on on-premises servers (via AWS Direct Connect or VPN) and EC2 instances. This allows multiple authors to continue updating blog content in real-time while the EC2 instances serve the latest content seamlessly.</p><p> For transferring 200 TB of archival data to Amazon S3 quickly:</p><p>- Option D is correct because AWS Snowball Edge Storage Optimized is designed for large-scale data transfers (like 200 TB) efficiently and securely. It is a better choice than Snowcone SSD (Option E), which is meant for smaller datasets (up to 8 TB).</p><p> Why not the other options?</p><p>- Option A: A weekly cron job would introduce delays in content updates, which violates the requirement of real-time synchronization.</p><p>- Option B: EBS Multi-Attach volumes are limited to a single Availability Zone and require manual synchronization, making them unsuitable for a multi-AZ setup and real-time updates.</p><p>- Option E: Snowcone SSD is too small (max 8 TB) for 200 TB of data, whereas Snowball Edge Storage Optimized can handle much larger datasets.</p><p> Final Answer:</p><p>✅ C (Use Amazon EFS for real-time blog content updates) &nbsp;</p><p>✅ D (Use AWS Snowball Edge Storage Optimized for large archival data transfer)</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "22c69b5772974bb9ac3af0ecd4e50e56",
      "questionNumber": 504,
      "type": "single",
      "content": "<p>Question #504</p><p><br></p><p>A company plans to migrate a legacy on-premises application to AWS. The application is a Java web application that runs on Apache Tomcat with a PostgreSQL database.</p><p><br></p><p>The company does not have access to the source code but can deploy the application Java Archive (JAR) files. The application has increased traffic at the end of each month.</p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Launch Amazon EC2 instances in multiple Availability Zones. Deploy Tomcat and PostgreSQL to all the instances by using Amazon Elastic File System (Amazon EFS) mount points. Use AWS Step Functions to deploy additional EC2 instances to scale for increased traffic."
        },
        {
          "label": "B",
          "content": "Provision Amazon Elastic Kubernetes Service (Amazon EKS) in an Auto Scaling group across multiple AWS Regions. Deploy Tomcat and PostgreSQL in the container images. Use a Network Load Balancer to scale for increased traffic."
        },
        {
          "label": "C",
          "content": "Refactor the Java application into Python-based containers. Use AWS Lambda functions for the application logic. Store application data in Amazon DynamoDB global tables. Use AWS Storage Gateway and Lambda concurrency to scale for increased traffic."
        },
        {
          "label": "D",
          "content": "Use AWS Elastic Beanstalk to deploy the Tomcat servers with auto scaling in multiple Availability Zones. Store application data in an Amazon RDS for PostgreSQL database. Deploy Amazon CloudFront and an Application Load Balancer to scale for increased traffic."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. Use AWS Elastic Beanstalk to deploy the Tomcat servers with auto scaling in multiple Availability Zones. Store application data in an Amazon RDS for PostgreSQL database. Deploy Amazon CloudFront and an Application Load Balancer to scale for increased traffic.</p><p>This solution will meet the requirements with the LEAST operational overhead for the following reasons:</p><p>Managed Services Approach: Elastic Beanstalk is a platform-as-a-service (PaaS) offering that handles the underlying infrastructure management, including capacity provisioning, load balancing, auto-scaling, and application health monitoring. This significantly reduces operational overhead compared to solutions where you need to manage these components yourself.</p><p>Simplified Deployment: Elastic Beanstalk provides a streamlined way to deploy Java applications on Tomcat without needing to configure the underlying infrastructure. Since the company only has access to JAR files and not the source code, Elastic Beanstalk's support for deploying packaged applications is ideal.</p><p>Managed Database: Amazon RDS for PostgreSQL provides a fully managed database service that handles routine database tasks such as backups, patch management, and high availability. This eliminates the need to manage a PostgreSQL database manually.</p><p>Automatic Scaling: The combination of Elastic Beanstalk's auto scaling capabilities and the Application Load Balancer allows the application to automatically handle the increased traffic at the end of each month without manual intervention.</p><p>Content Delivery Optimization: CloudFront integration improves performance and reduces latency for end users by caching content at edge locations, which is beneficial for web applications.</p><p>High Availability: Deploying across multiple Availability Zones ensures the application remains available even if one zone experiences issues.</p><p>Option B (Amazon EKS in an Auto Scaling group across multiple AWS Regions) would involve significantly more operational overhead because:</p><p>Kubernetes Complexity: Managing Kubernetes clusters requires specialized knowledge and ongoing maintenance.</p><p>Container Management: Creating and maintaining container images for Tomcat and PostgreSQL would require additional operational effort.</p><p>Multi-Region Deployment: Managing deployments across multiple AWS Regions adds complexity that isn't necessary for the stated requirements.</p><p>Database Management: Running PostgreSQL in containers rather than using a managed service like RDS would require additional operational overhead for database administration tasks.</p><p>The Elastic Beanstalk solution (Option D) provides a much more streamlined approach that aligns perfectly with the company's constraints (no source code access, only JAR files) while minimizing operational overhead and still providing the necessary scalability to handle variable traffic patterns.</p><p>Sources</p><p>Migrate from IBM WebSphere Application Server to Apache Tomcat on Amazon EC2 with Auto Scaling - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-from-ibm-websphere-application-server-to-apache-tomcat-on-amazon-ec2-with-auto-scaling.html）</p><p>Migrate from Oracle WebLogic to Apache Tomcat (TomEE) on Amazon ECS - AWS Prescriptive Guidance（https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-from-oracle-weblogic-to-apache-tomcat-tomee-on-amazon-ecs.html） </p><p>Migrate on-premises Java applications to AWS using AWS App2Container - AWS Prescriptive Guidance（https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-on-premises-java-applications-to-aws-using-aws-app2container.html） </p><p>Considering online migration options - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-databases-postgresql-ec2/considering-online-migration-options.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "408467a29a2f4f35a7161d4f81c4f3e0",
      "questionNumber": 505,
      "type": "multiple",
      "content": "<p>Question #505</p><p><br></p><p>A company is migrating its on-premises IoT platform to AWS. The platform consists of the following components:</p><p><br></p><p>- A MongoDB cluster as a data store for all collected and processed IoT data.</p><p>- An application that uses Message Queuing Telemetry Transport (MQTT) to connect to IoT devices every 5 minutes to collect data.</p><p>- An application that runs jobs periodically to generate reports from the IoT data. The jobs take 120-600 seconds to finish running.</p><p>- A web application that runs on a web server. End users use the web application to generate reports that are accessible to the general public.</p><p><br></p><p>The company needs to migrate the platform to AWS to reduce operational overhead while maintaining performance.</p><p><br></p><p>Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose three.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Create AWS Step Functions state machines with AWS Lambda tasks to prepare the reports and to write the reports to Amazon S3. Configure an Amazon CloudFront distribution that has an S3 origin to serve the reports."
        },
        {
          "label": "B",
          "content": "Create an AWS Lambda function. Program the Lambda function to connect to the IoT devices, process the data, and write the data to the data store. Configure a Lambda layer to temporarily store messages for processing."
        },
        {
          "label": "C",
          "content": "Configure an Amazon Elastic Kubernetes Service (Amazon EKS) cluster with Amazon EC2 instances to prepare the reports. Create an ingress controller on the EKS cluster to serve the reports."
        },
        {
          "label": "D",
          "content": "Connect the IoT devices to AWS IoT Core to publish messages. Create an AWS IoT rule that runs when a message is received. Configure the rule to call an AWS Lambda function. Program the Lambda function to parse, transform, and store device message data to the data store."
        },
        {
          "label": "E",
          "content": "Migrate the MongoDB cluster to Amazon DocumentDB (with MongoDB compatibility)."
        },
        {
          "label": "F",
          "content": "Migrate the MongoDB cluster to Amazon EC2 instances."
        }
      ],
      "correctAnswer": "ADE",
      "explanation": "<p>The goal is to migrate the IoT platform to AWS with the least operational overhead while maintaining performance. Here are the best options:</p><p> Correct Choices:</p><p>D. Connect the IoT devices to AWS IoT Core to publish messages. Create an AWS IoT rule that runs when a message is received. Configure the rule to call an AWS Lambda function. Program the Lambda function to parse, transform, and store device message data to the data store. &nbsp;</p><p>- AWS IoT Core is a fully managed service for IoT device communication, reducing operational overhead. &nbsp;</p><p>- AWS IoT Rules can trigger Lambda functions for processing, eliminating the need for a separate MQTT application. &nbsp;</p><p>E. Migrate the MongoDB cluster to Amazon DocumentDB (with MongoDB compatibility). &nbsp;</p><p>- Amazon DocumentDB is a fully managed MongoDB-compatible database, reducing operational overhead compared to self-managing MongoDB on EC2. &nbsp;</p><p>A. Create AWS Step Functions state machines with AWS Lambda tasks to prepare the reports and to write the reports to Amazon S3. Configure an Amazon CloudFront distribution that has an S3 origin to serve the reports. &nbsp;</p><p>- Step Functions can orchestrate periodic report generation jobs (120-600 sec) with Lambda. &nbsp;</p><p>- Storing reports in S3 with CloudFront provides scalable and cost-effective public access. &nbsp;</p><p> Why Not the Others? &nbsp;</p><p>- B: While Lambda can process IoT data, using AWS IoT Core (D) is more efficient and fully managed. &nbsp;</p><p>- C: Amazon EKS introduces unnecessary operational overhead compared to serverless (Step Functions + Lambda). &nbsp;</p><p>- F: Self-managing MongoDB on EC2 has higher operational overhead than Amazon DocumentDB (E). &nbsp;</p><p> Final Answer: &nbsp;</p><p>✅ D, E, A (AWS IoT Core + Lambda for device data, Amazon DocumentDB for data storage, Step Functions + Lambda + S3/CloudFront for reports)</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "2e3d05eea5734d41aa2f8f64d91aee37",
      "questionNumber": 506,
      "type": "single",
      "content": "<p>Question #506</p><p><br></p><p>A company creates an Amazon API Gateway API and shares the API with an external development team. The API uses AWS Lambda functions and is deployed to a stage named \"Production.\"</p><p><br></p><p>The external development team is the sole consumer of the API. The API experiences sudden increases in usage at specific times, leading to concerns about increased costs. The company needs to limit cost and usage without reworking the Lambda functions.</p><p><br></p><p>Which solution will meet these requirements MOST cost-effectively?</p>",
      "options": [
        {
          "label": "A",
          "content": "Configure the API to send requests to Amazon Simple Queue Service (Amazon SQS) queues instead of directly to the Lambda functions. Update the Lambda functions to consume messages from the queues and to process the requests. Set up the queues to invoke the Lambda functions when new messages arrive."
        },
        {
          "label": "B",
          "content": "Configure provisioned concurrency for each Lambda function. Use AWS Application Auto Scaling to register the Lambda functions as targets. Set up scaling schedules to increase and decrease capacity to match changes in API usage."
        },
        {
          "label": "C",
          "content": "Create an API Gateway API key and an AWS WAF Regional web ACL. Associate the web ACL with the Production stage. Add a rate-based rule to the web ACL. In the rule, specify the rate limit and a custom request aggregation that uses the X-API-Key header. Share the API key with the external development team."
        },
        {
          "label": "D",
          "content": "Create an API Gateway API Key and usage plan. Define throttling limits and quotas in the usage plan. Associate the usage plan with the Production stage and the API key. Share the API key with the external development team."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>The correct answer is D. Create an API Gateway API Key and usage plan. Define throttling limits and quotas in the usage plan. Associate the usage plan with the Production stage and the API key. Share the API key with the external development team.</p><p>- Option A suggests using Amazon SQS queues to decouple API Gateway from Lambda, but this requires reworking the Lambda functions to consume messages from the queue, which violates the requirement of not modifying the Lambda functions.</p><p>- Option B proposes using provisioned concurrency and Application Auto Scaling, but this does not limit API usage or costs—it only helps manage Lambda concurrency. Additionally, it does not address sudden spikes in API usage.</p><p>- Option C suggests using AWS WAF with a rate-based rule, but this is more complex than necessary and is typically used for security (e.g., DDoS protection) rather than cost control. It also requires managing WAF rules, which is not the most straightforward solution.</p><p>- Option D is the best choice because:</p><p> &nbsp;- API Gateway usage plans allow you to set throttling limits (rate limits) and quotas (daily/monthly limits) to control how much the API can be called.</p><p> &nbsp;- API keys help track and restrict usage to authorized consumers (the external team).</p><p> &nbsp;- This solution does not require changes to the Lambda functions and is the most cost-effective way to limit usage and prevent unexpected cost spikes.</p><p>Thus, D is the correct answer.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "dc7162fc911845ed900876bfebf19c50",
      "questionNumber": 507,
      "type": "single",
      "content": "<p>Question #507</p><p><br></p><p>An entertainment company hosts a ticketing service on a fleet of Linux Amazon EC2 instances that are in an Auto Scaling group. The ticketing service uses a pricing file. The pricing file is stored in an Amazon S3 bucket that has S3 Standard storage. A central pricing solution hosted by a third party updates the pricing file.</p><p><br></p><p>The pricing file is updated every 1-15 minutes and has several thousand line items. The pricing file is downloaded to each EC2 instance when the instance launches.</p><p><br></p><p>The EC2 instances occasionally use outdated pricing information that can result in incorrect charges for customers.</p><p><br></p><p>Which solution will resolve this problem MOST cost-effectively?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an AWS Lambda function to update an Amazon DynamoDB table with new prices each time the pricing file is updated. Update the ticketing service to use DynamoDB to look up pricing."
        },
        {
          "label": "B",
          "content": "Create an AWS Lambda function to update an Amazon Elastic File System (Amazon EFS) file share with the pricing file each time the file is updated. Update the ticketing service to use Amazon EFS to access the pricing file."
        },
        {
          "label": "C",
          "content": "Load Mountpoint for Amazon S3 onto the AMI of the EC2 instances. Configure Mountpoint for Amazon S3 to mount the S3 bucket that contains the pricing file. Update the ticketing service to point to the mount point and path to access the S3 object."
        },
        {
          "label": "D",
          "content": "Create an Amazon Elastic Block Store (Amazon EBS) volume. Use EBS Multi-Attach to attach the volume to every EC2 instance. When a new EC2 instance launches, configure the new instance to update the pricing file on the EBS volume. Update the ticketing service to point to the new local source."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>The most cost-effective solution to ensure that the EC2 instances always use the most up-to-date pricing information is:</p><p>C. Load Mountpoint for Amazon S3 onto the AMI of the EC2 instances. Configure Mountpoint for Amazon S3 to mount the S3 bucket that contains the pricing file. Update the ticketing service to point to the mount point and path to access the S3 object.</p><p> Explanation:</p><p>1. Problem Analysis:</p><p> &nbsp; - The pricing file is frequently updated (every 1-15 minutes) in an S3 bucket.</p><p> &nbsp; - EC2 instances download the file only at launch, leading to outdated pricing.</p><p> &nbsp; - The solution must ensure real-time access to the latest pricing file without manual intervention.</p><p>2. Why Option C?:</p><p> &nbsp; - Mountpoint for Amazon S3 allows EC2 instances to directly access the S3 bucket as a local file system.</p><p> &nbsp; - The ticketing service can read the pricing file in real-time from the mounted S3 bucket, eliminating the need for periodic downloads or synchronization.</p><p> &nbsp; - This is cost-effective because:</p><p> &nbsp; &nbsp; - No additional services (Lambda, DynamoDB, EFS, or EBS Multi-Attach) are required.</p><p> &nbsp; &nbsp; - S3 is already being used, and Mountpoint for S3 is a lightweight, open-source solution.</p><p> &nbsp; - Ensures immediate consistency—any updates to the pricing file in S3 are immediately available to all EC2 instances.</p><p>3. Why Not Other Options?:</p><p> &nbsp; - A (DynamoDB): Overkill for a pricing file that doesn’t require NoSQL capabilities. Adds complexity and cost (DynamoDB read/write operations).</p><p> &nbsp; - B (EFS): EFS is expensive for this use case (pay for storage + throughput). Requires Lambda to sync updates, adding complexity.</p><p> &nbsp; - D (EBS Multi-Attach): EBS Multi-Attach is not cost-effective (limited to a single Availability Zone, requires manual updates, and incurs additional costs for Multi-Attach volumes).</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "fb96c817d943452688b62a2bb8657f83",
      "questionNumber": 508,
      "type": "single",
      "content": "<p>Question #508</p><p><br></p><p>A company has an application that uses Amazon EC2 instances in an Auto Scaling group. The quality assurance (QA) department needs to launch a large number of short-lived environments to test the application. The application environments are currently launched by the manager of the department using an AWS CloudFormation template. To launch the stack, the manager uses a role with permission to use CloudFormation, EC2, and Auto Scaling APIs. The manager wants to allow testers to launch their own environments, but does not want to grant broad permissions to each user.</p><p><br></p><p>Which setup would achieve these goals?</p>",
      "options": [
        {
          "label": "A",
          "content": "Upload the AWS CloudFormation template to Amazon S3. Give users in the QA department permission to assume the manager&rsquo;s role and add a policy that restricts the permissions to the template and the resources it creates. Train users to launch the template from the CloudFormation console."
        },
        {
          "label": "B",
          "content": "Create an AWS Service Catalog product from the environment template. Add a launch constraint to the product with the existing role. Give users in the QA department permission to use AWS Service Catalog APIs only. Train users to launch the template from the AWS Service Catalog console."
        },
        {
          "label": "C",
          "content": "Upload the AWS CloudFormation template to Amazon S3. Give users in the QA department permission to use CloudFormation and S3 APIs, with conditions that restrict the permissions to the template and the resources it creates. Train users to launch the template from the CloudFormation console."
        },
        {
          "label": "D",
          "content": "Create an AWS Elastic Beanstalk application from the environment template. Give users in the QA department permission to use Elastic Beanstalk permissions only. Train users to launch Elastic Beanstalk environments with the Elastic Beanstalk CLI, passing the existing role to the environment as a service role."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Option B suggests using AWS Service Catalog, which is specifically designed for this use case. It allows organizations to create and manage catalogs of approved IT services that users can deploy with predefined permissions and constraints. </p><p>- AWS Service Catalog Product: You can create a product from the CloudFormation template, which standardizes the environment setup.</p><p>- Launch Constraint: This allows you to specify the existing role (the manager's role) that Service Catalog will use to launch the product, without granting users direct access to that role.</p><p>- Permissions: Users only need permissions to access AWS Service Catalog APIs (not CloudFormation, EC2, or Auto Scaling directly), ensuring they cannot modify or launch resources outside the approved template.</p><p>- Self-Service: Users can launch their own environments via the AWS Service Catalog console without requiring elevated permissions.</p><p> Why not the other options?</p><p>- A: Allowing users to assume the manager’s role is risky, even with restrictions, because role assumption grants temporary broad permissions.</p><p>- C: Granting users direct CloudFormation and S3 permissions (even with conditions) still exposes more risk than necessary, as users could potentially modify templates or launch unauthorized stacks.</p><p>- D: Elastic Beanstalk is not the right tool here, as it’s meant for deploying applications rather than managing short-lived testing environments with strict permission controls.</p><p>Thus, B is the most secure and scalable solution. </p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "7e30c4e219ad4b9abed7bed297bee86c",
      "questionNumber": 509,
      "type": "multiple",
      "content": "<p>Question #509</p><p><br></p><p>A company is using a single AWS Region for its e-commerce website. The website includes a web application that runs on several Amazon EC2 instances behind an Application Load Balancer (ALB). The website also includes an Amazon DynamoDB table. A custom domain name in Amazon Route 53 is linked to the ALB. The company created an SSL/TLS certificate in AWS Certificate Manager (ACM) and attached the certificate to the ALB. The company is not using a content delivery network as part of its design.</p><p><br></p><p>The company wants to replicate its entire application stack in a second Region to provide disaster recovery, plan for future growth, and provide improved access time to users. A solutions architect needs to implement a solution that achieves these goals and minimizes administrative overhead.</p><p><br></p><p>Which combination of steps should the solutions architect take to meet these requirements? (Choose three.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an AWS CloudFormation template for the current infrastructure design. Use parameters for important system values, including Region. Use the CloudFormation template to create the new infrastructure in the second Region."
        },
        {
          "label": "B",
          "content": "Use the AWS Management Console to document the existing infrastructure design in the first Region and to create the new infrastructure in the second Region."
        },
        {
          "label": "C",
          "content": "Update the Route 53 hosted zone record for the application to use weighted routing. Send 50% of the traffic to the ALB in each Region."
        },
        {
          "label": "D",
          "content": "Update the Route 53 hosted zone record for the application to use latency-based routing. Send traffic to the ALB in each Region."
        },
        {
          "label": "E",
          "content": "Update the configuration of the existing DynamoDB table by enabling DynamoDB Streams. Add the second Region to create a global table."
        },
        {
          "label": "F",
          "content": "Create a new DynamoDB table. Enable DynamoDB Streams for the new table. Add the second Region to create a global table. Copy the data from the existing DynamoDB table to the new table as a one-time operation."
        }
      ],
      "correctAnswer": "ADE",
      "explanation": "<p>1. A. Use AWS CloudFormation to replicate the infrastructure in the second Region &nbsp;</p><p> &nbsp; - CloudFormation allows infrastructure-as-code (IaC), enabling consistent deployment across Regions with minimal administrative overhead. &nbsp;</p><p> &nbsp; - Using parameters (like Region) makes the template reusable. &nbsp;</p><p>2. D. Update Route 53 to use latency-based routing &nbsp;</p><p> &nbsp; - Latency-based routing improves access time by directing users to the Region with the lowest latency. &nbsp;</p><p> &nbsp; - This is better than weighted routing (Option C) because it optimizes performance rather than splitting traffic arbitrarily. &nbsp;</p><p>3. E. Enable DynamoDB Streams and create a global table &nbsp;</p><p> &nbsp; - DynamoDB Global Tables provide multi-Region replication for disaster recovery and low-latency access. &nbsp;</p><p> &nbsp; - Enabling DynamoDB Streams is required before converting an existing table into a global table. &nbsp;</p><p> Why not the other options? &nbsp;</p><p>- B: Manually recreating infrastructure via the AWS Console is error-prone and increases administrative overhead. &nbsp;</p><p>- C: Weighted routing (50/50 split) doesn’t optimize performance like latency-based routing. &nbsp;</p><p>- F: While this would work, it’s unnecessary because you can directly convert the existing table into a global table (Option E is more efficient). &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "9a46e1f7d85242b6be029598eeff4dee",
      "questionNumber": 510,
      "type": "multiple",
      "content": "<p>Question #510</p><p><br></p><p>A company wants to create a single Amazon S3 bucket for its data scientists to store work-related documents. The company uses AWS IAM Identity Center to authenticate all users. A group for the data scientists was created.</p><p><br></p><p>The company wants to give the data scientists access to only their own work. The company also wants to create monthly reports that show which documents each user accessed.</p><p><br></p><p>Which combination of steps will meet these requirements? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a custom IAM Identity Center permission set to grant the data scientists access to an S3 bucket prefix that matches their username tag. Use a policy to limit access to paths with the ${aws:PrincipalTag/userName}/* condition."
        },
        {
          "label": "B",
          "content": "Create an IAM Identity Center role for the data scientists group that has Amazon S3 read access and write access. Add an S3 bucket policy that allows access to the IAM Identity Center role."
        },
        {
          "label": "C",
          "content": "Configure AWS CloudTrail to log S3 data events and deliver the logs to an S3 bucket. Use Amazon Athena to run queries on the CloudTrail logs in Amazon S3 and generate reports."
        },
        {
          "label": "D",
          "content": "Configure AWS CloudTrail to log S3 management events to CloudWatch. Use Amazon Athena&rsquo;s CloudWatch connector to query the logs and generate reports."
        },
        {
          "label": "E",
          "content": "Enable S3 access logging to EMR File System (EMRFS). Use Amazon S3 Select to query logs and generate reports."
        }
      ],
      "correctAnswer": "AC",
      "explanation": "<p>Based on the information provided, the correct answers are:</p><p>A. Create a custom IAM Identity Center permission set to grant the data scientists access to an S3 bucket prefix that matches their username tag. Use a policy to limit access to paths with the ${aws:PrincipalTag/userName}/* condition.</p><p>C. Configure AWS CloudTrail to log S3 data events and deliver the logs to an S3 bucket. Use Amazon Athena to run queries on the CloudTrail logs in Amazon S3 and generate reports.</p><p>These two steps together provide a complete solution for both requirements:</p><p>Personalized Access Control (Option A):</p><p>This approach uses IAM Identity Center permission sets with a policy that leverages the ${aws:PrincipalTag/userName} variable to restrict each data scientist's access to only their own prefix in the S3 bucket.</p><p>When data scientists authenticate through IAM Identity Center, their username tag is passed to the IAM policy, which then dynamically restricts their access to only the S3 path that matches their username.</p><p>This creates a clean separation where each data scientist can only access their own work-related documents within the shared bucket.</p><p>The solution is scalable and requires minimal ongoing administration as new data scientists join the company.</p><p>Access Reporting (Option C):</p><p>CloudTrail with S3 data events provides comprehensive logging of all object-level operations within the S3 bucket, including who accessed which documents and when.</p><p>By delivering these logs to an S3 bucket, they become available for long-term storage and analysis.</p><p>Amazon Athena allows for SQL-based querying of the CloudTrail logs directly from S3, making it easy to generate the required monthly reports showing which documents each user accessed.</p><p>This approach provides flexibility in report creation and can be automated to run on a monthly schedule.</p><p>Together, these solutions provide:</p><p>Secure, personalized access control through IAM Identity Center</p><p>Detailed access logging through CloudTrail</p><p>Flexible reporting capabilities through Athena</p><p>A scalable architecture that requires minimal ongoing management</p><p>This combination effectively meets both the access control and reporting requirements specified by the company while leveraging AWS best practices for security and monitoring.</p><p>Sources</p><p>Connect users to data through your apps with Storage Browser for Amazon S3 | AWS News Blog（https://aws.amazon.com/cn/blogs/aws/connect-users-to-data-through-your-apps-with-storage-browser-for-amazon-s3/） </p><p>Sync users from Identity Center to AWS Account IAM | AWS re:Post （https://repost.aws/questions/QUgs5rAM3BT-aqdtaQkGY3bg/sync-users-from-identity-center-to-aws-account-iam）</p><p>Simplify data lake access control for your enterprise users with trusted identity propagation in AWS IAM Identity Center, AWS Lake Formation, and Amazon S3 Access Grants | AWS Big Data Blog （https://aws.amazon.com/cn/blogs/big-data/simplify-data-lake-access-control-for-your-enterprise-users-with-trusted-identity-propagation-in-aws-iam-identity-center-aws-lake-formation-and-amazon-s3-access-grants/）</p><p>Access management - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-access-management.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "f0b77e1830a844cba43e47675b1f067d",
      "questionNumber": 511,
      "type": "single",
      "content": "<p>Question #511</p><p><br></p><p>A company hosts a data-processing application on Amazon EC2 instances. The application polls an Amazon Elastic File System (Amazon EFS) file system for newly uploaded files. When a new file is detected, the application extracts data from the file and runs logic to select a Docker container image to process the file. The application starts the appropriate container image and passes the file location as a parameter.</p><p><br></p><p>The data processing that the container performs can take up to 2 hours. When the processing is complete, the code that runs inside the container writes the file back to Amazon EFS and exits.</p><p><br></p><p>The company needs to refactor the application to eliminate the EC2 instances that are running the containers.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an Amazon Elastic Container Service (Amazon ECS) cluster. Configure the processing to run as AWS Fargate tasks. Extract the container selection logic to run as an Amazon EventBridge rule that starts the appropriate Fargate task. Configure the EventBridge rule to run when files are added to the EFS file system."
        },
        {
          "label": "B",
          "content": "&nbsp;Create an Amazon Elastic Container Service (Amazon ECS) cluster. Configure the processing to run as AWS Fargate tasks. Update and containerize the container selection logic to run as a Fargate service that starts the appropriate Fargate task. Configure an EFS event notification to invoke the Fargate service when files are added to the EFS file system."
        },
        {
          "label": "C",
          "content": "Create an Amazon Elastic Container Service (Amazon ECS) cluster. Configure the processing to run as AWS Fargate tasks. Extract the container selection logic to run as an AWS Lambda function that starts the appropriate Fargate task. Migrate the storage of file uploads to an Amazon S3 bucket. Update the processing code to use Amazon S3. Configure an S3 event notification to invoke the Lambda function when objects are created."
        },
        {
          "label": "D",
          "content": "Create AWS Lambda container images for the processing. Configure Lambda functions to use the container images. Extract the container selection logic to run as a decision Lambda function that invokes the appropriate Lambda processing function. Migrate the storage of file uploads to an Amazon S3 bucket. Update the processing code to use Amazon S3. Configure an S3 event notification to invoke the decision Lambda function when objects are created."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p> Key Requirements:</p><p>1. Eliminate EC2 instances – The solution must replace the EC2-based container orchestration with a serverless or managed service.</p><p>2. Process files from EFS – The current setup uses EFS, but migrating to Amazon S3 is acceptable if it simplifies the architecture.</p><p>3. Event-driven execution – The solution should trigger processing when new files arrive.</p><p>4. Long-running processing (up to 2 hours) – AWS Fargate supports long-running tasks, whereas Lambda has a maximum timeout of 15 minutes (which rules out Option D).</p><p> Analysis of Options:</p><p>- Option A: &nbsp;</p><p> &nbsp;- Uses EventBridge to trigger Fargate tasks when files are added to EFS. &nbsp;</p><p> &nbsp;- Problem: EventBridge does not natively support EFS event notifications. EFS events must be detected via a Lambda function or another service, making this option incomplete.</p><p>- Option B: &nbsp;</p><p> &nbsp;- Uses an EFS event notification to trigger a Fargate service that selects and starts the appropriate task. &nbsp;</p><p> &nbsp;- Problem: EFS event notifications cannot directly invoke Fargate tasks. They require an intermediary (like Lambda) to process the event and start the task.</p><p>- Option C: &nbsp;</p><p> &nbsp;- Migrates file storage to Amazon S3 (simpler than EFS for event-driven workflows). &nbsp;</p><p> &nbsp;- Uses S3 event notifications to trigger a Lambda function, which selects and starts the appropriate Fargate task. &nbsp;</p><p> &nbsp;- Fargate supports long-running tasks (up to 2 hours). &nbsp;</p><p> &nbsp;- This is the most viable solution because it replaces EC2 with serverless (Lambda + Fargate) and uses S3 events for reliable triggering.</p><p>- Option D: &nbsp;</p><p> &nbsp;- Uses Lambda container images for processing. &nbsp;</p><p> &nbsp;- Problem: Lambda has a 15-minute maximum runtime, which is insufficient for 2-hour processing tasks. &nbsp;</p><p> Why Option C is Correct:</p><p>- Serverless architecture (no EC2 instances). &nbsp;</p><p>- S3 event notifications reliably trigger Lambda. &nbsp;</p><p>- Lambda invokes Fargate tasks, which can run for up to 2 hours. &nbsp;</p><p>- EFS is replaced with S3, simplifying event-driven workflows. &nbsp;</p><p>https://www.examtopics.com/discussions/amazon/view/143048-exam-aws-certified-solutions-architect-professional-sap-c02/</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "01c654c692074f56a702d11ecb388985",
      "questionNumber": 512,
      "type": "single",
      "content": "Question #512<p><br></p><p>A media company has a 30-TB repository of digital news videos. These videos are stored on tape in an on-premises tape library and referenced by a Media Asset Management (MAM) system. The company wants to enrich the metadata for these videos in an automated fashion and put them into a searchable catalog by using a MAM feature. The company must be able to search based on information in the video, such as objects, scenery items, or people’s faces.</p><p><br></p><p>A catalog is available that contains faces of people who have appeared in the videos that include an image of each person. The company would like to migrate these videos to AWS.</p><p><br></p><p>The company has a high-speed AWS Direct Connect connection with AWS and would like to move the MAM solution video content directly from its current file system.</p><p><br></p><p>How can these requirements be met by using the LEAST amount of ongoing management overhead and causing MINIMAL disruption to the existing system?</p><p><br></p>",
      "options": [
        {
          "label": "A",
          "content": "Set up an AWS Storage Gateway, file gateway appliance on-premises. Use the MAM solution to extract the videos from the current archive and push them into the file gateway. Use the catalog of faces to build a collection in Amazon Rekognition. Build an AWS Lambda function that invokes the Rekognition Javascript SDK to have Rekognition pull the video from the Amazon S3 files backing the file gateway, retrieve the required metadata, and push the metadata into the MAM solution."
        },
        {
          "label": "B",
          "content": "Set up an AWS Storage Gateway, tape gateway appliance on-premises. Use the MAM solution to extract the videos from the current archive and push them into the tape gateway. Use the catalog of faces to build a collection in Amazon Rekognition. Build an AWS Lambda function that invokes the Rekognition Javascript SDK to have Amazon Rekognition process the video in the tape gateway, retrieve the required metadata, and push the metadata into the MAM solution."
        },
        {
          "label": "C",
          "content": "Configure a video ingestion stream by using Amazon Kinesis Video Streams. Use the catalog of faces to build a collection in Amazon Rekognition. Stream the videos from the MAM solution into Kinesis Video Streams. Configure Amazon Rekognition to process the streamed videos. Then, use a stream consumer to retrieve the required metadata, and push the metadata into the MAM solution. Configure the stream to store the videos in Amazon S3."
        },
        {
          "label": "D",
          "content": "Set up an Amazon EC2 instance that runs the OpenCV libraries. Copy the videos, images, and face catalog from the on-premises library into an Amazon EBS volume mounted on this EC2 instance. Process the videos to retrieve the required metadata, and push the metadata into the MAM solution, while also copying the video files to an Amazon S3 bucket."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>- Option A: Setting up an AWS Storage Gateway file gateway on - premises is a good choice as it allows the company to use its existing file system to access the videos. The MAM solution can easily push videos into the file gateway. Using Amazon Rekognition to build a collection from the face catalog and a Lambda function to interact with the Rekognition SDK to process videos stored in the S3 files backing the file gateway is an efficient way. This approach has relatively low ongoing management overhead as AWS manages much of the underlying infrastructure of the Storage Gateway and Rekognition. It also causes minimal disruption to the existing system as it can work with the current MAM solution and file system. So, this option meets the requirements.</p><p>- Option B: A tape gateway in AWS Storage Gateway is mainly for migrating tape - based data to AWS and providing a tape - like interface. Processing videos directly in the tape gateway by Amazon Rekognition is not an efficient or common approach. It would likely have higher management overhead and may not work as smoothly as using a file gateway. So, this option is not the best choice.</p><p>- Option C: Configuring an Amazon Kinesis Video Streams for video ingestion would require significant changes to the existing system as it is not directly integrated with the on - premises tape - based storage and MAM solution. Also, managing the Kinesis Video Streams, stream consumers, and the overall ingestion process would add more ongoing management overhead compared to using a Storage Gateway. So, this option is not optimal.</p><p>- Option D: Running OpenCV libraries on an Amazon EC2 instance requires more hands - on management for setting up, configuring, and maintaining the EC2 instance, EBS volume, and the OpenCV environment. It also involves more complex operations to copy data and process videos compared to using AWS - managed services like Storage Gateway and Rekognition. So, this option has higher management overhead and is not the best fit for the requirements. </p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "2be799ab04cc4a538f576ad36c4a6a46",
      "questionNumber": 513,
      "type": "single",
      "content": "<p>Question #513</p><p><br></p><p>A company needs to optimize the cost of an AWS environment that contains multiple accounts in an organization in AWS Organizations. The company conducted cost optimization activities 3 years ago and purchased Amazon EC2 Standard Reserved Instances that recently expired. The company needs EC2 instances for 3 more years. Additionally, the company has deployed a new serverless workload.</p><p><br></p><p>Which strategy will provide the company with the MOST cost savings?</p>",
      "options": [
        {
          "label": "A",
          "content": "Purchase the same Reserved Instances for an additional 3-year term with All Upfront payment. Purchase a 3-year Compute Savings Plan with All Upfront payment in the management account to cover any additional compute costs."
        },
        {
          "label": "B",
          "content": "Purchase a 1-year Compute Savings Plan with No Upfront payment in each member account. Use the Savings Plans recommendations in the AWS Cost Management console to choose the Compute Savings Plan."
        },
        {
          "label": "C",
          "content": "Purchase a 3-year EC2 Instance Savings Plan with No Upfront payment in the management account to cover EC2 costs in each AWS Region. Purchase a 3-year Compute Savings Plan with No Upfront payment in the management account to cover any additional compute costs."
        },
        {
          "label": "D",
          "content": "Purchase a 3-year EC2 Instance Savings Plan with All Upfront payment in each member account. Use the Savings Plans recommendations in the AWS Cost Management console to choose the EC2 Instance Savings Plan."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Purchase the same Reserved Instances for an additional 3-year term with All Upfront payment. Purchase a 3-year Compute Savings Plan with All Upfront payment in the management account to cover any additional compute costs.</p><p>This strategy will provide the company with the MOST cost savings for the following reasons:</p><p>Maximum Discount Percentage: 3-year commitments with All Upfront payment provide the highest possible discount rates:</p><p>3-year Standard Reserved Instances with All Upfront payment offer up to 72% savings on EC2 instances</p><p>3-year Compute Savings Plans with All Upfront payment offer up to 66% savings on compute usage</p><p>Long-term Commitment Alignment: The company explicitly states they need EC2 instances for 3 more years, making the 3-year commitment period perfectly aligned with their stated requirements.</p><p>Comprehensive Coverage: This strategy provides a two-pronged approach:</p><p>Reserved Instances cover specific EC2 instance types, sizes, and regions that the company knows they'll need</p><p>Compute Savings Plans provide flexible coverage for the new serverless workload (including Lambda and Fargate) as well as any additional EC2 usage not covered by Reserved Instances</p><p>Organization-wide Benefits: Purchasing the Compute Savings Plan in the management account allows the savings to automatically apply across all member accounts in the organization, maximizing utilization and preventing wasted commitments.</p><p>Predictable Costs: With All Upfront payments, the company eliminates future price fluctuations for their committed usage, providing budget certainty for the next 3 years.</p><p>Option B (1-year Compute Savings Plans with No Upfront payment in each member account) would provide less overall savings for several reasons:</p><p>Lower Discount Rate: 1-year commitments offer significantly lower discount percentages compared to 3-year commitments (typically 31-40% vs. 60-72%).</p><p>No Upfront Payment: While this requires less initial capital, it results in a lower overall discount compared to All Upfront payment options.</p><p>Distributed Management: Managing separate Savings Plans in each member account creates administrative overhead and potential inefficiencies in utilization.</p><p>Misalignment with Needs: The company explicitly states they need EC2 instances for 3 more years, making the 1-year commitment period unnecessarily short and requiring annual renewals.</p><p>Given the company's stated 3-year requirement for EC2 instances and their previous experience with Reserved Instances, the combination of 3-year Reserved Instances and 3-year Compute Savings Plans with All Upfront payment will maximize their cost savings while providing the necessary coverage for both their traditional EC2 workloads and their new serverless applications.</p><p>Sources</p><p>Main Differences b/t EC2 Saving Plans (Compute and Instance) and Reserved Instances | AWS re:Post (https://repost.aws/questions/QUeethov1VSDm1QvziWqMHzw/main-differences-b-t-ec2-saving-plans-compute-and-instance-and-reserved-instances)</p><p>Decide on Savings Plans or Reserved Instances for EC2 instances | AWS re:Post (https://repost.aws/knowledge-center/ec2-savings-plan-reserved-instances)</p><p>COST07-BP01 Perform pricing model analysis - AWS Well-Architected Framework (https://docs.aws.amazon.com/wellarchitected/2025-02-25/framework/cost_pricing_model_analysis.html)</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "eddecc23061f4b468c7db1c4243b261c",
      "questionNumber": 514,
      "type": "multiple",
      "content": "<p>Question #514</p><p><br></p><p>A company operates a static content distribution platform that serves customers globally. The customers consume content from their own AWS accounts.</p><p><br></p><p>The company serves its content from an Amazon S3 bucket. The company uploads the content from its on-premises environment to the S3 bucket by using an S3 File Gateway.</p><p><br></p><p>The company wants to improve the platform’s performance and reliability by serving content from the AWS Region that is geographically closest to customers. The company must route the on-premises data to Amazon S3 with minimal latency and without public internet exposure.</p><p><br></p><p>Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Implement S3 Multi-Region Access Points"
        },
        {
          "label": "B",
          "content": "Use S3 Cross-Region Replication (CRR) to copy content to different Regions"
        },
        {
          "label": "C",
          "content": "Create an AWS Lambda function that tracks the routing of clients to Regions"
        },
        {
          "label": "D",
          "content": "Use an AWS Site-to-Site VPN connection to connect to a Multi-Region Access Point."
        },
        {
          "label": "E",
          "content": "Use AWS PrivateLink and AWS Direct Connect to connect to a Multi-Region Access Point."
        }
      ],
      "correctAnswer": "AE",
      "explanation": "<p>Based on the information provided, the correct answers are:</p><p>A. Implement S3 Multi-Region Access Points E. Use AWS PrivateLink and AWS Direct Connect to connect to a Multi-Region Access Point.</p><p>These two steps together provide the most effective solution with the least operational overhead for the following reasons:</p><p>S3 Multi-Region Access Points (MRAP):</p><p>Creates a single global endpoint that automatically routes requests to the S3 bucket in the AWS Region closest to customers</p><p>Improves performance by directing traffic to the nearest region, reducing latency</p><p>Enhances reliability through automatic failover if a region becomes unavailable</p><p>Simplifies management by providing a unified endpoint rather than requiring customers to connect to different regional endpoints</p><p>Works seamlessly with the existing S3 File Gateway setup for content uploads</p><p>AWS PrivateLink and AWS Direct Connect to connect to a Multi-Region Access Point:</p><p>Provides a private, dedicated network connection between the company's on-premises environment and AWS</p><p>Ensures data transfer doesn't traverse the public internet, meeting the requirement for \"without public internet exposure\"</p><p>Offers consistent, low-latency performance for uploading content from on-premises to S3, addressing the requirement for \"minimal latency\"</p><p>Direct Connect provides higher bandwidth and more reliable connectivity compared to VPN solutions</p><p>PrivateLink creates private endpoints within the VPC, further enhancing security and performance</p><p>Together, these options create an end-to-end solution that:</p><p>Enables global content distribution with optimal performance (using MRAP)</p><p>Provides secure, private connectivity from on-premises to AWS (using PrivateLink and Direct Connect)</p><p>Minimizes operational overhead through managed services</p><p>Meets all the stated requirements for performance, reliability, and security</p><p>Option D (Use an AWS Site-to-Site VPN connection to connect to a Multi-Region Access Point) is not optimal because:</p><p>VPN connections typically have lower bandwidth and higher latency compared to Direct Connect</p><p>VPN connections can be less reliable as they depend on internet connectivity</p><p>VPN requires more operational overhead to maintain compared to Direct Connect</p><p>For a production content distribution platform serving customers globally, Direct Connect provides better performance and reliability</p><p>The combination of S3 Multi-Region Access Points (A) and AWS PrivateLink with Direct Connect (E) provides the most comprehensive solution with the least operational overhead while meeting all the requirements specified in the scenario.</p><p>Sources</p><p>Multi Region Access Points – Amazon S3 Features – Amazon Web Services （https://aws.amazon.com/cn/s3/features/multi-region-access-points/）</p><p>Managing multi-Region traffic with Multi-Region Access Points - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/MultiRegionAccessPoints.html）</p><p>Performance design patterns for Amazon S3 - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance-design-patterns.html）</p><p>Configuring a Multi-Region Access Point for use with AWS PrivateLink - Amazon Simple Storage Service（https://docs.aws.amazon.com/AmazonS3/latest/userguide/MultiRegionAccessPointsPrivateLink.html） </p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "c3997a44f91342d99f0671aa702878ea",
      "questionNumber": 515,
      "type": "single",
      "content": "<p>Question #515</p><p><br></p><p>A company is migrating its data center to the AWS Cloud and needs to complete the migration as quickly as possible. The company has many applications running on hundreds of VMware VMs in the data center. Each VM is configured with a shared Windows folder that contains common shared files. The file share is larger than 100 GB in size.</p><p><br></p><p>The company’s compliance team requires a change request to be filed and approved for every software installation and modification to each VM.</p><p><br></p><p>The company has an AWS Direct Connect connection with 10 GB of bandwidth between AWS and the data center.</p><p><br></p><p>Which set of steps should the company take to complete the migration in the LEAST amount of time?</p>",
      "options": [
        {
          "label": "A",
          "content": "Use VM Import/Export to create images of each VM. Use AWS Application Migration Service to manage and view the images. Copy the Windows file share data to an Amazon Elastic File System (Amazon EFS) file system. After migration, remap the file share to the EFS file system."
        },
        {
          "label": "B",
          "content": "Deploy the AWS Application Discovery Service agentless appliance to VMware vCenter. Review the portfolio of discovered VMs in AWS Migration Hub."
        },
        {
          "label": "C",
          "content": "Deploy the AWS Application Migration Service agentless appliance to VMware vCenter. Copy the Windows file share data to a new Amazon FSx for Windows File Server file system. After migration, remap the file share on each VM to the FSx for Windows File Server file system."
        },
        {
          "label": "D",
          "content": "Create and review a portfolio in AWS Migration Hub. Order an AWS Snowcone device. Deploy AWS Application Migration Service to VMware vCenter and export all the VMs to the Snowcone device. Copy all Windows file share data to the Snowcone device. Ship the Snowcone device to AWS. Use Application Migration Service to deploy all the migrated instances."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>The correct answer is C. Here's the breakdown of why this is the best approach for the fastest migration while meeting compliance requirements:</p><p> Why Option C is Correct:</p><p>1. AWS Application Migration Service (Agentless Appliance) &nbsp;</p><p> &nbsp; - The agentless appliance is deployed to VMware vCenter, eliminating the need to install agents on each VM (which would require compliance approvals for each installation). &nbsp;</p><p> &nbsp; - This allows for lift-and-shift migration with minimal changes to the VMs.</p><p>2. Amazon FSx for Windows File Server &nbsp;</p><p> &nbsp; - The shared Windows folder (larger than 100 GB) is best migrated to FSx for Windows File Server, which is fully compatible with Windows file shares (supports SMB protocol, Active Directory integration, and Windows permissions). &nbsp;</p><p> &nbsp; - After migration, the file share can be remapped on each VM to point to FSx.</p><p>3. Fastest Migration Path &nbsp;</p><p> &nbsp; - Since there’s already a 10 Gbps Direct Connect connection, transferring data over the network is feasible. &nbsp;</p><p> &nbsp; - No need for Snowcone (which would introduce delays due to shipping). &nbsp;</p><p> &nbsp; - Agentless migration avoids compliance bottlenecks (no per-VM agent installations).</p><p> Why Other Options Are Incorrect:</p><p>- A: Uses VM Import/Export (manual process) and Amazon EFS (not ideal for Windows file shares). &nbsp;</p><p>- B: Only discovers VMs (does not perform migration). &nbsp;</p><p>- D: Requires installing agents on each hypervisor, which may need compliance approvals and slow down the process. &nbsp;</p><p>- E: Uses Snowcone, which is unnecessary given the high-bandwidth Direct Connect connection and would delay migration.</p><p> Final Answer:</p><p>C. Deploy the AWS Application Migration Service agentless appliance to VMware vCenter. Copy the Windows file share data to a new Amazon FSx for Windows File Server file system. After migration, remap the file share on each VM to the FSx for Windows File Server file system.</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "7a89120f6dce4ef2b1cca5fbd41ee551",
      "questionNumber": 516,
      "type": "single",
      "content": "<p>Question #516</p><p><br></p><p>A company has multiple AWS accounts that are in an organization in AWS Organizations. The company needs to store AWS account activity and query the data from a central location using SQL.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an AWS CloudTrail trail in each account. Specify CloudTrail management events for the trail. Configure CloudTrail to send the events to Amazon CloudWatch Logs. Configure CloudWatch cross-account observability. Query the data in CloudWatch Logs Insights."
        },
        {
          "label": "B",
          "content": "Use a delegated administrator account to create an AWS CloudTrail Lake data store. Specify CloudTrail management events for the data store. Enable the data store for all accounts in the organization. Query the data in CloudTrail Lake."
        },
        {
          "label": "C",
          "content": "Use a delegated administrator account to create an AWS CloudTrail trail. Specify CloudTrail management events for the trail. Enable the trail for all accounts in the organization. Keep all other settings as default. Query the CloudTrail data from the CloudTrail event history page."
        },
        {
          "label": "D",
          "content": "Use AWS CloudFormation StackSets to deploy AWS CloudTrail Lake data stores in each account. Specify CloudTrail management events for the data stores. Keep all other settings as default, Query the data in CloudTrail Lake."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Use a delegated administrator account to create an AWS CloudTrail Lake data store. Specify CloudTrail management events for the data store. Enable the data store for all accounts in the organization. Query the data in CloudTrail Lake.</p><p>This solution best meets the requirements for the following reasons:</p><p>SQL Querying Capability: CloudTrail Lake provides native SQL querying capabilities, which directly addresses the requirement to \"query the data from a central location by using SQL.\" CloudTrail Lake is specifically designed to support SQL-based analysis of AWS account activity.</p><p>Centralized Storage: By creating a CloudTrail Lake data store in a delegated administrator account and enabling it for all accounts in the organization, the company can collect and store AWS account activity from all member accounts in a single, centralized location. This eliminates the need to set up and manage separate trails in each account.</p><p>Organization-wide Coverage: Using a delegated administrator approach with CloudTrail Lake allows for seamless collection of management events from all accounts in the AWS Organization without having to configure each account individually.</p><p>Simplified Management: This solution reduces operational overhead by centralizing both the storage and querying of account activity in one place, rather than requiring cross-account configurations.</p><p>Purpose-Built for the Use Case: CloudTrail Lake is specifically designed for the long-term storage, analysis, and SQL-based querying of AWS account activity data across an organization.</p><p>Option A (using CloudTrail with CloudWatch Logs and CloudWatch cross-account observability) has several limitations:</p><p>CloudWatch Logs Insights uses its own query language, not standard SQL, which doesn't meet the requirement to query the data using SQL.</p><p>It requires setting up CloudTrail in each account and configuring CloudWatch cross-account observability, which is more complex and has higher operational overhead.</p><p>While CloudWatch Logs Insights is powerful for log analysis, it's not specifically optimized for querying AWS account activity across an organization in the way that CloudTrail Lake is.</p><p>CloudTrail Lake is the purpose-built solution for this exact use case - centralized storage and SQL-based querying of AWS account activity across an organization - making Option B the correct answer.</p><p>Sources</p><p>Logging strategies for security incident response | AWS Security Blog (https://aws.amazon.com/cn/blogs/security/logging-strategies-for-security-incident-response/)</p><p>AWS CloudTrail or Amazon CloudWatch? - AWS CloudTrail or Amazon CloudWatch? (https://docs.aws.amazon.com/decision-guides/latest/cloudtrail-or-cloudwatch/cloudtrail-or-cloudwatch.html)</p><p>Query AWS CloudTrail logs - Amazon Athena (https://docs.aws.amazon.com/athena/latest/ug/cloudtrail-logs.html)</p><p>SEC04-BP01 Configure service and application logging - Security Pillar (https://docs.aws.amazon.com/wellarchitected/latest/security-pillar/sec_detect_investigate_events_app_service_logging.html)</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "8d08ac3bacf24b2ba6fb62018595addd",
      "questionNumber": 517,
      "type": "single",
      "content": "<p>Question #517</p><p><br></p><p>A company is using AWS to develop and manage its production web application. The application includes an Amazon API Gateway HTTP API that invokes an AWS Lambda function. The Lambda function processes and then stores data in a database.</p><p><br></p><p>The company wants to implement user authorization for the web application in an integrated way. The company already uses a third-party identity provider that issues OAuth tokens for the company’s other applications.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Integrate the company&rsquo;s third-party identity provider with API Gateway. Configure an API Gateway Lambda authorizer to validate tokens from the identity provider. Require the Lambda authorizer on all API routes. Update the web application to get tokens from the identity provider and include the tokens in the Authorization header when calling the API Gateway HTTP API."
        },
        {
          "label": "B",
          "content": "Integrate the company&#39;s third-party identity provider with AWS Directory Service. Configure Directory Service as an API Gateway authorizer to validate tokens from the identity provider. Require the Directory Service authorizer on all API routes. Configure AWS IAM Identity Center as a SAML 2.0 identity Provider. Configure the web application as a custom SAML 2.0 application."
        },
        {
          "label": "C",
          "content": "Integrate the company&rsquo;s third-party identity provider with AWS IAM Identity Center. Configure API Gateway to use IAM Identity Center for zero-configuration authentication and authorization. Update the web application to retrieve AWS Security Token Service (AWS STS) tokens from IAM Identity Center and include the tokens in the Authorization header when calling the API Gateway HTTP API."
        },
        {
          "label": "D",
          "content": "Integrate the company&rsquo;s third-party identity provider with AWS IAM Identity Center. Configure IAM users with permissions to call the API Gateway HTTP API. Update the web application to extract request parameters from the IAM users and include the parameters in the Authorization header when calling the API Gateway HTTP API."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>The scenario requires implementing user authorization for an API Gateway HTTP API that invokes a Lambda function, using an existing third-party OAuth identity provider. Here’s why Option A is the best solution: &nbsp;</p><p>1. API Gateway Lambda Authorizer &nbsp;</p><p> &nbsp; - A Lambda authorizer can validate OAuth tokens issued by the third-party identity provider. &nbsp;</p><p> &nbsp; - The authorizer checks the token's validity and returns an IAM policy allowing or denying access to the API. &nbsp;</p><p>2. Token Handling &nbsp;</p><p> &nbsp; - The web application retrieves tokens from the third-party identity provider (as it already does for other apps). &nbsp;</p><p> &nbsp; - The tokens are included in the `Authorization` header when calling the API Gateway. &nbsp;</p><p>3. Secure API Access &nbsp;</p><p> &nbsp; - The Lambda authorizer is enforced on all API routes, ensuring only authenticated users can access them. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- Option B: AWS Directory Service is not designed to validate OAuth tokens, and using IAM Identity Center as a SAML provider is unnecessary for this use case. &nbsp;</p><p>- Option C: IAM Identity Center does not provide \"zero-configuration\" OAuth token validation for API Gateway. STS tokens are used for AWS service access, not OAuth-based API authorization. &nbsp;</p><p>- Option D: IAM users are not suitable for web application user authentication, and request parameters are not a secure way to handle authorization. &nbsp;</p><p>Thus, Option A is the most appropriate solution. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "51b9706e49804c8591ee99e15be2c623",
      "questionNumber": 518,
      "type": "single",
      "content": "<p>Question #518</p><p><br></p><p>A company has deployed applications to thousands of Amazon EC2 instances in an AWS account. A security audit discovers that several unencrypted Amazon Elastic Block Store (Amazon EBS) volumes are attached to the EC2 instances. The company’s security policy requires the EBS volumes to be encrypted.</p><p><br></p><p>The company needs to implement an automated solution to encrypt the EBS volumes. The solution also must prevent development teams from creating unencrypted EBS volumes.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Configure the AWS Config managed rule that identifies unencrypted EBS volumes. Configure an automatic remediation action. Associate an AWS Systems Manager Automation runbook that includes the steps to create a new encrypted EBS volume. Create an AWS Key Management Service (AWS KMS) customer managed key. In the key policy, include a statement to deny the creation of unencrypted EBS volumes."
        },
        {
          "label": "B",
          "content": "Use AWS Systems Manager Fleet Manager to create a list of unencrypted EBS volumes, Create a Systems Manager Automation runbook that includes the steps to create a new encrypted EBS volume. Create an SCP to deny the creation of unencrypted EBS volumes."
        },
        {
          "label": "C",
          "content": "Use AWS Systems Manager Fleet Manager to create a list of unencrypted EBS volumes. Create a Systems Manager Automation runbook that includes the steps to create a new encrypted EBS volume. Modify the AWS account setting for EBS encryption to always encrypt new EBS volumes."
        },
        {
          "label": "D",
          "content": "Configure the AWS Config managed rule that identifies unencrypted EBS volumes. Configure an automatic remediation action. Associate an AWS Systems Manager Automation runbook that includes the steps to create a new encrypted EBS volume. Modify the AWS account setting for EBS encryption to always encrypt new EBS volumes."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p> Requirements:</p><p>1. Automated solution to encrypt existing unencrypted EBS volumes.</p><p>2. Prevent creation of new unencrypted EBS volumes.</p><p> Why Option D?</p><p>- AWS Config Managed Rule: Can identify unencrypted EBS volumes (e.g., using the `ebs-encryption-by-default` or `ec2-encrypted-volumes` rule).</p><p>- Automatic Remediation: Can trigger a Systems Manager Automation runbook to create an encrypted copy of the unencrypted volume and attach it to the instance.</p><p>- AWS Account Setting for EBS Encryption: Enabling default EBS encryption ensures all new EBS volumes are automatically encrypted, preventing the creation of unencrypted volumes.</p><p> Why Not Other Options?</p><p>- Option A: Incorrect because a KMS key policy cannot deny the creation of unencrypted EBS volumes. KMS policies control key usage, not EC2/EBS API actions. You need SCPs or default encryption settings to enforce this.</p><p>- Option B: Incorrect because Fleet Manager is not the right tool for identifying unencrypted volumes (AWS Config is better suited). Also, while SCPs can deny unencrypted volume creation, the question emphasizes automated remediation, which is better handled via AWS Config + Systems Manager.</p><p>- Option C: Incorrect because Fleet Manager is not the optimal tool for this use case, and while enabling default EBS encryption helps, it doesn’t address existing unencrypted volumes as comprehensively as AWS Config + Systems Manager.</p><p> Key Takeaways:</p><p>- For existing volumes: AWS Config + Systems Manager Automation for remediation.</p><p>- For new volumes: Enable default EBS encryption at the account level.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "7529d9f38cff4cd78499395015d241a2",
      "questionNumber": 519,
      "type": "single",
      "content": "<p>Question #519</p><p><br></p><p>A company is running a large containerized workload in the AWS Cloud. The workload consists of approximately 100 different services. The company uses Amazon Elastic Container Service (Amazon ECS) to orchestrate the workload.</p><p><br></p><p>Recently the company’s development team started using AWS Fargate instead of Amazon EC2 instances in the ECS cluster. In the past, the workload has come close to running the maximum number of EC2 instances that are available in the account.</p><p><br></p><p>The company is worried that the workload could reach the maximum number of ECS tasks that are allowed. A solutions architect must implement a solution that will notify the development team when Fargate reaches 80% of the maximum number of tasks.</p><p><br></p><p>What should the solutions architect do to meet this requirement?</p>",
      "options": [
        {
          "label": "A",
          "content": "&nbsp;Use Amazon CloudWatch to monitor the Sample Count statistic for each service in the ECS cluster. Set an alarm for when the math expression sample count/SERVICE_QUOTA(service)*100 is greater than 80. Notify the development team by using Amazon Simple Notification Service (Amazon SNS)."
        },
        {
          "label": "B",
          "content": "Use Amazon CloudWatch to monitor service quotas that are published under the AWS/Usage metric namespace. Set an alarm for when the math expression metric/SERVICE_QUOTA(metric)*100 is greater than 80. Notify the development team by using Amazon Simple Notification Service (Amazon SNS)."
        },
        {
          "label": "C",
          "content": "Create an AWS Lambda function to poll detailed metrics from the ECS cluster. When the number of running Fargate tasks is greater than 80, invoke Amazon Simple Email Service (Amazon SES) to notify the development team."
        },
        {
          "label": "D",
          "content": "Create an AWS Config rule to evaluate whether the Fargate SERVICE_QUOTA is greater than 80. Use Amazon Simple Email Service (Amazon SES) to notify the development team when the AWS Config rule is not compliant."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>The correct answer is B. Here's the detailed explanation:</p><p> Why Option B is Correct:</p><p>1. AWS/Usage Namespace in CloudWatch: &nbsp;</p><p> &nbsp; - AWS Fargate service quotas (including the maximum number of tasks) are published as metrics under the `AWS/Usage` namespace in Amazon CloudWatch.</p><p> &nbsp; - These metrics track usage against service limits (quotas).</p><p>2. Setting Up the Alarm: &nbsp;</p><p> &nbsp; - You can create a CloudWatch alarm that monitors the metric value (current usage) relative to the `SERVICE_QUOTA` (maximum allowed tasks).</p><p> &nbsp; - The math expression `metric/SERVICE_QUOTA(metric)*100` calculates the percentage of quota usage.</p><p> &nbsp; - When this value exceeds 80%, the alarm triggers.</p><p>3. Notification via Amazon SNS: &nbsp;</p><p> &nbsp; - The alarm can be configured to send a notification to the development team using Amazon Simple Notification Service (SNS).</p><p> Why Other Options Are Incorrect:</p><p>- Option A: The `Sample Count` statistic in CloudWatch does not track service quotas. It counts the number of data points, not the actual usage vs. quota.</p><p>- Option C: Polling ECS metrics manually with Lambda is inefficient and unnecessary when CloudWatch already provides quota monitoring.</p><p>- Option D: AWS Config does not evaluate service quota usage in real-time; it checks for compliance with rules, not dynamic quota thresholds.</p><p> Conclusion: &nbsp;</p><p>Option B is the most efficient and AWS-recommended way to monitor Fargate task quotas and notify the team when nearing the limit. &nbsp;</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "ced546bd083b483ba519485ec8f882e4",
      "questionNumber": 520,
      "type": "multiple",
      "content": "<p>Question #520</p><p><br></p><p>A company has several AWS Lambda functions written in Python. The functions are deployed with the .zip package deployment type. The functions use a Lambda layer that contains common libraries and packages in a .zip file. The Lambda .zip packages and Lambda layer .zip file are stored in an Amazon S3 bucket.</p><p><br></p><p>The company must implement automatic scanning of the Lambda functions and the Lambda layer to identify CVEs. A subset of the Lambda functions must receive automated code scans to detect potential data leaks and other vulnerabilities. The code scans must occur only for selected Lambda functions, not all the Lambda functions.</p><p><br></p><p>Which combination of actions will meet these requirements? (Choose three.)</p><p style=\"text-align: center;\"> </p>",
      "options": [
        {
          "label": "A",
          "content": "Activate Amazon Inspector. Start automated CVE scans."
        },
        {
          "label": "B",
          "content": "Activate Lambda standard scanning and Lambda code scanning in Amazon Inspector."
        },
        {
          "label": "C",
          "content": "Enable Amazon GuardDuty. Enable the Lambda Protection feature in GuardDuty."
        },
        {
          "label": "D",
          "content": "Enable scanning in the Monitor settings of the Lambda functions that need code scans."
        },
        {
          "label": "E",
          "content": "Tag Lambda functions that do not need code scans. In the tag, include a key of InspectorCodeExclusion and a value of LambdaCodeScanning."
        },
        {
          "label": "F",
          "content": "Use Amazon Inspector to scan the S3 bucket that contains the Lambda .zip packages and the Lambda layer .zip file for code scans."
        }
      ],
      "correctAnswer": "ABE",
      "explanation": "<ul><li style=\"text-align: left;\">A. Activate Amazon Inspector. Start automated CVE scans. &nbsp;</li><li style=\"text-align: left;\">B. Activate Lambda standard scanning and Lambda code scanning in Amazon Inspector. &nbsp;</li><li style=\"text-align: left;\">E. Tag Lambda functions that do not need code scans. In the tag, include a key of `InspectorCodeExclusion` and a value of `LambdaCodeScanning`. &nbsp;</li><li style=\"text-align: left;\"> Explanation: &nbsp;</li><li style=\"text-align: left;\">1. Amazon Inspector provides automated vulnerability scanning for AWS Lambda functions, including CVE detection in dependencies (standard scanning) and code scanning for security issues like data leaks. &nbsp;</li><li style=\"text-align: left;\"> &nbsp; - A ensures CVE scanning is enabled. &nbsp;</li><li style=\"text-align: left;\"> &nbsp; - B activates both standard (CVE) and code scanning for Lambda. &nbsp;</li><li style=\"text-align: left;\">2. Selective code scanning is achieved by tagging Lambda functions that should be excluded (E). Amazon Inspector respects the `InspectorCodeExclusion` tag to skip code scanning on specified functions. &nbsp;</li><li style=\"text-align: left;\"> Why not the others? &nbsp;</li><li style=\"text-align: left;\">- C (GuardDuty) is for threat detection, not CVE or code scanning. &nbsp;</li><li style=\"text-align: left;\">- D (Monitor settings) is not a valid Lambda feature for enabling code scans. &nbsp;</li><li style=\"text-align: left;\">- F (S3 scanning) is incorrect because Inspector scans deployed Lambda functions, not the S3 .zip files directly. &nbsp;</li><li style=\"text-align: left;\">Thus, the correct answers are A, B, and E. </li></ul>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "5cd7b16f846d436f8a1d9c993ef9761e",
      "questionNumber": 521,
      "type": "single",
      "content": "<p>Question #521</p><p><br></p><p>A company is changing the way that it handles patching of Amazon EC2 instances in its application account. The company currently patches instances over the internet by using a NAT gateway in a VPC in the application account.</p><p><br></p><p>The company has EC2 instances set up as a patch source repository in a dedicated private VPC in a core account. The company wants to use AWS Systems Manager Patch Manager and the patch source repository in the core account to patch the EC2 instances in the application account.</p><p><br></p><p>The company must prevent all EC2 instances in the application account from accessing the internet.</p><p><br></p><p>The EC2 instances in the application account need to access Amazon S3, where the application data is stored. These EC2 instances need connectivity to Systems Manager and to the patch source repository in the private VPC in the core account.</p><p><br></p><p>Which solution will meet these requirements?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a network ACL that blocks outbound traffic on port 80. Associate the network ACL with all subnets in the application account. In the application account and the core account, deploy one EC2 instance that runs a custom VPN server. Create a VPN tunnel to access the private VPC. Update the route table in the application account."
        },
        {
          "label": "B",
          "content": "Create private VPC endpoints for Systems Manager and Amazon S3. Delete the NAT gateway from the VPC in the application account. Create a transit gateway to access the patch source repository EC2 instances in the core account. Update the route table in the core account."
        },
        {
          "label": "C",
          "content": "Create VPC endpoints for Systems Manager and Amazon S3. Delete the NAT gateway from the VPC in the application account. Create a VPC peering connection to access the patch source repository EC2 instances in the core account. Update the route tables in both accounts."
        },
        {
          "label": "D",
          "content": "Create a network ACL that blocks inbound traffic on port 80. Associate the network ACL with all subnets in the application account. Create a transit gateway to access the patch source repository EC2 instances in the core account. Update the route tables in both accounts."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p> Requirements:</p><p>1. Prevent EC2 instances in the application account from accessing the internet (must remove NAT gateway).</p><p>2. Allow EC2 instances to access Amazon S3 (for application data).</p><p>3. Allow connectivity to AWS Systems Manager (SSM) (for Patch Manager).</p><p>4. Allow connectivity to the patch source repository in the core account's private VPC.</p><p> Solution Breakdown:</p><p>- VPC Endpoints for Systems Manager and Amazon S3:</p><p> &nbsp;- Interface VPC endpoints (PrivateLink) for SSM allow private connectivity without internet access.</p><p> &nbsp;- Gateway VPC endpoint for Amazon S3 allows private access to S3 without NAT or internet.</p><p>- Delete the NAT gateway (since internet access is no longer allowed).</p><p>- VPC Peering Connection:</p><p> &nbsp;- Establishes a direct private connection between the application account VPC and the core account VPC.</p><p> &nbsp;- Update route tables in both VPCs to allow traffic to the patch source repository.</p><p> Why Not the Other Options?</p><p>- A: Using a custom VPN server is overly complex and introduces unnecessary management overhead. Network ACLs blocking port 80 won't fully prevent internet access (other ports could still be used).</p><p>- B: Private VIFs (Direct Connect) are expensive and unnecessary for this use case. Transit Gateway is overkill if VPC peering suffices.</p><p>- D: Blocking inbound port 80 doesn't prevent outbound internet access. Transit Gateway is not needed when VPC peering is simpler and cheaper.</p><p> Key Points:</p><p>- VPC endpoints (SSM & S3) ensure private connectivity to AWS services.</p><p>- VPC peering provides direct access to the patch repository in the core account.</p><p>- No internet access is maintained by removing the NAT gateway.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "642b0a6d3ff74e509f90b88b97431ee4",
      "questionNumber": 522,
      "type": "single",
      "content": "<p>Question #522</p><p><br></p><p>A company in the United States (US) has acquired a company in Europe. Both companies use the AWS Cloud. The US company has built a new application with a microservices architecture. The US company is hosting the application across five VPCs in the `us-east-2` Region. The application must be able to access resources in one VPC in the `eu-west-1` Region.</p><p><br></p><p>However, the application must not be able to access any other VPCs.</p><p><br></p><p>The VPCs in both Regions have no overlapping CIDR ranges. All accounts are already consolidated in one organization in AWS Organizations.</p><p><br></p><p>Which solution will meet these requirements MOST cost-effectively?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create one transit gateway in `eu-west-1`. Attach the VPCs in `us-east-2` and the VPC in `eu-west-1` to the transit gateway. Create the necessary route entries in each VPC so that the traffic is routed through the transit gateway."
        },
        {
          "label": "B",
          "content": "Create one transit gateway in each Region. Attach the involved subnets to the regional transit gateway. Create the necessary route entries in the associated route tables for each subnet so that the traffic is routed through the regional transit gateway. Peer the two transit gateways."
        },
        {
          "label": "C",
          "content": "Create a full mesh VPC peering connection configuration between all the VPCs. Create the necessary route entries in each VPC so that the traffic is routed through the VPC peering connection."
        },
        {
          "label": "D",
          "content": "Create one VPC peering connection for each VPC in `us-east-2` to the VPC in `eu-west-1`. Create the necessary route entries in each VPC so that the traffic is routed through the VPC peering connection."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>The most cost-effective solution that meets the requirements is:</p><p> Option D: Create one VPC peering connection for each VPC in us-east-2 to the VPC in eu-west-1. Create the necessary route entries in each VPC so that the traffic is routed through the VPC peering connection.</p><p> Why?</p><p>1. No Overlapping CIDRs: Since the VPCs in both regions have non-overlapping CIDR ranges, VPC peering is feasible.</p><p>2. Least Cost: </p><p> &nbsp; - Transit Gateway (Option A & B) incurs additional costs for inter-region data transfer and hourly fees.</p><p> &nbsp; - Full Mesh Peering (Option C) is unnecessary since only communication between the US VPCs and the single EU VPC is required.</p><p> &nbsp; - Direct Peering (Option D) is the cheapest option, as it only requires simple peering connections between the required VPCs.</p><p>3. Security & Compliance: </p><p> &nbsp; - The solution restricts access only to the specified EU VPC (via explicit peering and route entries).</p><p> &nbsp; - No additional access is granted to other VPCs.</p><p> Why Not Other Options?</p><p>- Option A: A single transit gateway in `eu-west-1` cannot directly attach VPCs from `us-east-2` (transit gateways are region-specific).</p><p>- Option B: Transit gateways with inter-region peering are more expensive than VPC peering for this simple use case.</p><p>- Option C: Full mesh peering is unnecessary and complex when only selective VPCs need access.</p><p>Thus, Option D is the most cost-effective while meeting all requirements.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "c0642e72b2f04997b3e1f757b1de3176",
      "questionNumber": 523,
      "type": "multiple",
      "content": "<p>Question #523</p><p><br></p><p>A travel company built a web application that uses Amazon Simple Email Service (Amazon SES) to send email notifications to users. The company needs to enable logging to help troubleshoot email delivery issues. The company also needs the ability to do searches that are based on recipient, subject, and time sent.</p><p><br></p><p>Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an Amazon SES configuration set with Amazon Data Firehose as the destination. Choose to send logs to an Amazon S3 bucket."
        },
        {
          "label": "B",
          "content": "Enable AWS CloudTrail logging. Specify an Amazon S3 bucket as the destination for the logs."
        },
        {
          "label": "C",
          "content": "Use Amazon Athena to query the logs in the Amazon S3 bucket for recipient, subject, and time sent."
        },
        {
          "label": "D",
          "content": "Create an Amazon CloudWatch log group. Configure Amazon SES to send logs to the log group."
        },
        {
          "label": "E",
          "content": "Use Amazon Athena to query the logs in Amazon CloudWatch for recipient, subject, and time sent."
        }
      ],
      "correctAnswer": "AC",
      "explanation": "<p>Based on the information provided, the correct answers are:</p><p>A. Create an Amazon SES configuration set with Amazon Data Firehose as the destination. Choose to send logs to an Amazon S3 bucket.</p><p>C. Use Amazon Athena to query the logs in the Amazon S3 bucket for recipient, subject, and time sent.</p><p>These two steps together provide the most effective solution for the travel company's requirements for the following reasons:</p><p>Amazon SES Configuration Set with Firehose to S3 (Option A):</p><p>This approach captures detailed email event data including delivery, bounce, complaint, and send events</p><p>Using a configuration set allows for customization of what data is logged, ensuring all necessary information (recipient, subject, time sent) is captured</p><p>Firehose provides a reliable, scalable way to stream the email event data to S3</p><p>S3 offers durable, long-term storage for the logs, which is important for historical troubleshooting</p><p>This method captures the most comprehensive data about email delivery, which directly addresses the requirement to \"help troubleshoot email delivery issues\"</p><p>Amazon Athena for Querying S3 Logs (Option C):</p><p>Athena provides powerful SQL-based querying capabilities against data stored in S3</p><p>This directly addresses the requirement for \"the ability to do searches that are based on recipient, subject, and time sent\"</p><p>Athena can efficiently query large volumes of log data without the need to load it into a database</p><p>It allows for ad-hoc queries and custom reports based on any combination of the required search criteria</p><p>The SQL interface makes it accessible to users with SQL knowledge without requiring specialized log analysis skills</p><p>Together, these options create an end-to-end solution:</p><p>Option A establishes the logging infrastructure to capture detailed email delivery data</p><p>Option C provides the search and analysis capabilities needed to troubleshoot issues</p><p>The other options have significant limitations:</p><p>B. CloudTrail logging primarily captures API calls, not detailed email delivery information. It wouldn't provide the necessary data about recipients, subjects, and delivery status needed for troubleshooting.</p><p>D. Using CloudWatch log groups for SES logs would work for capturing the data, but it has limitations for long-term storage and cost-effectiveness compared to the S3 approach.</p><p>E. Athena cannot directly query CloudWatch logs. CloudWatch Logs Insights is the service for querying CloudWatch logs, but it doesn't provide the same SQL capabilities as Athena and may be more limited for complex searches.</p><p>The combination of SES configuration set with Firehose to S3 (A) and Athena for querying (C) provides the most comprehensive and flexible solution for the travel company's requirements.</p><p>Sources</p><p>Logging and monitoring in Amazon SES - Amazon Simple Email Service （https://docs.aws.amazon.com/ses/latest/dg/security-monitoring-overview.html）</p><p>Amazon SES Statistics by SMTP User | AWS re:Post （https://repost.aws/questions/QUyiRM5B5ITwGRBzLbCV3jnA/amazon-ses-statistics-by-smtp-user）</p><p>Mail Manager logging - Amazon Simple Email Service （https://docs.aws.amazon.com/ses/latest/dg/eb-logging.html）</p><p>Amazon SES low info | AWS re:Post （https://repost.aws/questions/QUhziH4660R42sSfUZPvtX5g/amazon-ses-low-info）</p><p>Monitoring your Amazon SES sending activity - Amazon Simple Email Service （https://docs.aws.amazon.com/ses/latest/dg/monitor-sending-activity.html）</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "f6b2a350421145d3b29545f0f02b86da",
      "questionNumber": 524,
      "type": "single",
      "content": "<p>Question #524</p><p><br></p><p>A company migrated to AWS and uses AWS Business Support. The company wants to monitor the cost-effectiveness of Amazon EC2 instances across AWS accounts. The EC2 instances have tags for department, business unit, and environment. Development EC2 instances have high cost but low utilization.</p><p><br></p><p>The company needs to detect and stop any underutilized development EC2 instances. Instances are underutilized if they had 10% or less average daily CPU utilization and 5 MB or less network I/O for at least 4 of the past 14 days.</p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
      "options": [
        {
          "label": "A",
          "content": "Configure Amazon CloudWatch dashboards to monitor EC2 instance utilization based on tags for department, business unit, and environment. Create an Amazon EventBridge rule that invokes an AWS Lambda function to stop underutilized development EC2 instances."
        },
        {
          "label": "B",
          "content": "Configure AWS Systems Manager to track EC2 instance utilization and report underutilized instances to Amazon CloudWatch. Filter the CloudWatch data by tags for department, business unit, and environment. Create an Amazon EventBridge rule that invokes an AWS Lambda function to stop underutilized development EC2 instances."
        },
        {
          "label": "C",
          "content": "Create an Amazon EventBridge rule to detect low utilization of EC2 instances reported by AWS Trusted Advisor. Configure the rule to invoke an AWS Lambda function that filters the data by tags for department, business unit, and environment and stops underutilized development EC2 instances."
        },
        {
          "label": "D",
          "content": "Create an AWS Lambda function to run daily to retrieve utilization data for all EC2 instances. Save the data to an Amazon DynamoDB table. Create an Amazon QuickSight dashboard that uses the DynamoDB table as a data source to identify and stop underutilized development EC2 instances."
        }
      ],
      "correctAnswer": "C",
      "explanation": "<p>The correct answer is C. Create an Amazon EventBridge rule to detect low utilization of EC2 instances reported by AWS Trusted Advisor. Configure the rule to invoke an AWS Lambda function that filters the data by tags for department, business unit, and environment and stops underutilized development EC2 instances.</p><p> Explanation:</p><p>1. AWS Business Support includes AWS Trusted Advisor, which provides cost optimization checks, including identifying underutilized EC2 instances based on CPU utilization and network I/O metrics. This eliminates the need for manual monitoring or custom dashboards.</p><p>2. Least Operational Overhead: Trusted Advisor already monitors and reports underutilized instances, so you don't need to set up additional monitoring (unlike options A, B, or D).</p><p>3. Automation with EventBridge & Lambda: &nbsp;</p><p> &nbsp; - An EventBridge rule can trigger when Trusted Advisor detects underutilized instances. &nbsp;</p><p> &nbsp; - A Lambda function can filter instances by tags (department, business unit, environment) and stop development instances that meet the criteria. &nbsp;</p><p> Why Not the Other Options?</p><p>- A & B: These require setting up custom monitoring (CloudWatch dashboards or Systems Manager), which adds operational overhead.</p><p>- D: This involves a manual, daily Lambda function and DynamoDB storage, which is more complex and less efficient than using Trusted Advisor.</p><p> Key Takeaway:</p><p>AWS Trusted Advisor (included with Business Support) already provides underutilized EC2 instance detection, so leveraging it with EventBridge and Lambda is the most efficient solution. </p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "a980b840734944509fb9f4aa9716ce49",
      "questionNumber": 525,
      "type": "single",
      "content": "<p>Question #525</p><p><br></p><p>A company is hosting an application on AWS for a project that will run for the next 3 years. The application consists of 20 Amazon EC2 On-Demand Instances that are registered in a target group for a Network Load Balancer (NLB). The instances are spread across two Availability Zones. The application is stateless and runs 24 hours a day, 7 days a week.</p><p><br></p><p>The company receives reports from users who are experiencing slow responses from the application. Performance metrics show that the instances are at 10% CPU utilization during normal application use. However, the CPU utilization increases to 100% at busy times, which typically last for a few hours.</p><p><br></p><p>The company needs a new architecture to resolve the problem of slow responses from the application.</p><p><br></p><p>Which solution will meet these requirements MOST cost-effectively?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create an Auto Scaling group. Attach the Auto Scaling group to the target group of the NLB. Set the minimum capacity to 20 and the desired capacity to 28. Purchase Reserved Instances for 20 instances."
        },
        {
          "label": "B",
          "content": "Create a Spot Fleet that has a request type of &quot;request.&quot; Set the TotalTargetCapacity parameter to 20. Set the DefaultTargetCapacityType parameter to On-Demand. Specify the NLB when creating the Spot Fleet."
        },
        {
          "label": "C",
          "content": "Create a Spot Fleet that has a request type of &quot;maintain.&quot; Set the TotalTargetCapacity parameter to 20. Set the DefaultTargetCapacityType parameter to Spot. Replace the NLB with an Application Load Balancer."
        },
        {
          "label": "D",
          "content": "Create an Auto Scaling group. Attach the Auto Scaling group to the target group of the NLB. Set the minimum capacity to 4 and the maximum capacity to 28. Purchase Reserved Instances for four instances."
        }
      ],
      "correctAnswer": "D",
      "explanation": "<p>The most cost-effective solution to address the issue of slow responses during peak times while maintaining performance is:</p><p>D. Create an Auto Scaling group. Attach the Auto Scaling group to the target group of the NLB. Set the minimum capacity to 4 and the maximum capacity to 28. Purchase Reserved Instances for four instances.</p><p> Explanation:</p><p>1. Problem Analysis:</p><p> &nbsp; - The application runs 24/7 with 20 On-Demand Instances but experiences 100% CPU utilization during peak times, causing slow responses.</p><p> &nbsp; - Current setup is inefficient because the instances are underutilized (10% CPU normally) but cannot handle spikes.</p><p>2. Why Auto Scaling?</p><p> &nbsp; - Auto Scaling dynamically adjusts capacity based on demand, scaling out during peak times and scaling in during low usage.</p><p> &nbsp; - This ensures performance during spikes while reducing costs when demand is low.</p><p>3. Why Reserved Instances (RIs) for Baseline Capacity?</p><p> &nbsp; - Since the application runs 24/7, purchasing RIs for the minimum capacity (4 instances) saves costs compared to On-Demand pricing.</p><p> &nbsp; - The remaining instances (scaled up to 28) can use On-Demand or Spot Instances (though the option doesn't explicitly mention Spot, Auto Scaling can optimize costs).</p><p>4. Why Option D Over Others?</p><p> &nbsp; - Option A suggests a fixed high capacity (min 20, desired 28) with RIs for 20 instances. This is not cost-effective because it doesn’t scale down during low usage.</p><p> &nbsp; - Option B uses a Spot Fleet with On-Demand as default, but Spot Fleets are better for fault-tolerant workloads, and this doesn’t address dynamic scaling.</p><p> &nbsp; - Option C suggests replacing NLB with ALB and using Spot Instances, but Spot Instances can be interrupted, which is risky for a 24/7 application. Also, ALB is unnecessary since the app is stateless and NLB is sufficient.</p><p>5. Cost Optimization in Option D:</p><p> &nbsp; - Min capacity (4 instances) covered by RIs for long-term savings.</p><p> &nbsp; - Auto Scaling handles spikes (up to 28 instances) without over-provisioning.</p><p> &nbsp; - No need for ALB (stateless app works fine with NLB).</p><p> Conclusion:</p><p>Option D provides the best balance of cost savings and performance by leveraging Auto Scaling for dynamic demand and RIs for baseline capacity. </p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "54e5887ec37c48d7a707b4f4160050ba",
      "questionNumber": 526,
      "type": "single",
      "content": "<p>Question #526</p><p><br></p><p>Accompany is building an application to collect and transmit sensor data from a factory. The application will use AWS IoT Core to send data from hundreds of devices to an Amazon S3 data lake. The company must enrich the data before loading the data into Amazon S3.</p><p><br></p><p>The application will transmit the sensor data every 5 seconds. New sensor data must be available in Amazon S3 less than 30 minutes after the application collects the data. No other applications are processing the sensor data from AWS IoT Core.</p><p><br></p><p>Which solution will meet these requirements MOST cost-effectively?</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a topic in AWS IoT Core to ingest the sensor data. Create an AWS Lambda function to enrich the data and to write the data to Amazon S3. Configure an AWS IoT rule action to invoke the Lambda function."
        },
        {
          "label": "B",
          "content": "Use AWS IoT Core Basic Ingest to ingest the sensor data. Configure an AWS IoT rule action to write the data to Amazon Kinesis Data Firehose. Set the Kinesis Data Firehose buffering interval to 900 seconds. Use Kinesis Data Firehose to invoke an AWS Lambda function to enrich the data, Configure Kinesis Data Firehose to deliver the data to Amazon S3."
        },
        {
          "label": "C",
          "content": "Create a topic in AWS IoT Core to ingest the sensor data. Configure an AWS IoT rule action to send the data to an Amazon Timestream table. Create an AWS Lambda function to read the data from Timestream. Configure the Lambda function to enrich the data and to write the data to Amazon S3."
        },
        {
          "label": "D",
          "content": "Use AWS IoT Core Basic Ingest to ingest the sensor data. Configure an AWS IoT rule action to write the data to Amazon Kinesis Data Streams. Create a consumer AWS Lambda function to process the data from Kinesis Data Streams and to enrich the data. Call the S3 PutObject API operation from the Lambda function to write the data to Amazon S3."
        }
      ],
      "correctAnswer": "B",
      "explanation": "<p>he most cost-effective solution that meets the requirements is Option B. Here's why:</p><p> Key Requirements:</p><p>1. Data collection every 5 seconds (high frequency).</p><p>2. Enrichment before storing in Amazon S3.</p><p>3. Data must be available in S3 within 30 minutes.</p><p>4. Cost-effectiveness (minimize unnecessary processing/storage).</p><p> Analysis of Options:</p><p> Option A: AWS IoT Rule → Lambda → S3</p><p>- Pros: Simple, direct invocation of Lambda for enrichment.</p><p>- Cons: </p><p> &nbsp;- High cost: Lambda is invoked every 5 seconds (high frequency), leading to many invocations.</p><p> &nbsp;- No batching: Each Lambda call writes individually to S3, increasing costs.</p><p> Option B: AWS IoT Rule → Kinesis Data Firehose (+ Lambda Enrichment) → S3</p><p>- Pros:</p><p> &nbsp;- Buffering: Firehose can batch data (set to 900 seconds = 15 minutes) before writing to S3, reducing PUT requests.</p><p> &nbsp;- Built-in Lambda integration: Firehose can invoke Lambda for enrichment only when needed, optimizing cost.</p><p> &nbsp;- S3 delivery is managed by Firehose, which is cost-effective for high-frequency data.</p><p>- Cons: None significant for this use case.</p><p>- Best fit: Balances cost, performance, and meets the 30-minute SLA.</p><p> Option C: AWS IoT Rule → Timestream → Lambda → S3</p><p>- Pros: Timestream is good for time-series data.</p><p>- Cons: </p><p> &nbsp;- Overkill for this use case: No need for time-series database since data is just being enriched and stored in S3.</p><p> &nbsp;- Higher cost: Timestream charges for storage and queries, which are unnecessary here.</p><p> Option D: AWS IoT Rule → Kinesis Data Streams → Lambda → S3</p><p>- Pros: Kinesis can handle streaming data.</p><p>- Cons: </p><p> &nbsp;- More expensive than Firehose: Kinesis Data Streams requires managing shards (additional cost).</p><p> &nbsp;- Lambda must poll Kinesis, increasing complexity and cost compared to Firehose’s built-in integration.</p><p> Why Option B Wins:</p><p>- Kinesis Data Firehose is optimized for buffering and batching, reducing S3 PUT costs.</p><p>- Lambda is only used for enrichment (not per message), reducing invocation costs.</p><p>- Fully serverless and managed, minimizing operational overhead.</p><p>- Meets the 30-minute SLA with the 15-minute buffering setting.</p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": false,
      "hasNote": false
    },
    {
      "id": "d8c9c527cea74db19c655bc13aa874e3",
      "questionNumber": 527,
      "type": "multiple",
      "content": "<p>Question #527</p><p><br></p><p>A company is collecting data from a large set of IoT devices. The data is stored in an Amazon S3 data lake. Data scientists perform analytics on Amazon EC2 instances that run in two public subnets in a VPC in a separate AWS account.</p><p><br></p><p>The data scientists need access to the data lake from the EC2 instances. The EC2 instances already have an assigned role with permissions to access Amazon S3.</p><p><br></p><p>According to company policies, only authorized networks are allowed to have access to the IoT data.</p><p><br></p><p>Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a gateway VPC endpoint for Amazon S3 in the data scientists&rsquo; VPC."
        },
        {
          "label": "B",
          "content": "Create an S3 access point in the data scientists&#39; AWS account for the data lake."
        },
        {
          "label": "C",
          "content": "Update the EC2 instance role. Add a policy with a condition that allows the s3:GetObject action when the value for the s3:DataAccessPointArn condition key is a valid access point ARN."
        },
        {
          "label": "D",
          "content": "Update the VPC route table to route S3 traffic to an S3 access point."
        },
        {
          "label": "E",
          "content": "Add an S3 bucket policy with a condition that allows the s3:GetObject action when the value for the s3:DataAccessPointArn condition key is a valid access point ARN."
        }
      ],
      "correctAnswer": "BE",
      "explanation": "<p><span style=\"color: rgb(31, 35, 41); background-color: rgb(255, 255, 255); font-size: 16px;\">Option B: Create an S3 access point for the data lake in the data scientists' AWS account. S3 access points support cross-account access configuration and can be associated with specific VPCs (corresponding to the \"authorized network\"), serving as the foundational component for implementing network scope control. </span></p><p><span style=\"color: rgb(31, 35, 41); background-color: rgb(255, 255, 255); font-size: 16px;\">Option E: Add a condition to the S3 bucket policy (allowing the `s3:GetObject` action only when the request comes from the specified access point). This restricts data access at the bucket level to only requests through the authorized access point, and when combined with the VPC association rules of the access point, ensures that access is limited to the authorized network. </span></p><p><span style=\"color: rgb(31, 35, 41); background-color: rgb(255, 255, 255); font-size: 16px;\"> Option A: Gateway VPC endpoints are applicable for private subnets to access S3 via the VPC internal network. However, in the question, the EC2 instances are located in public subnets (which use the public network by default), so this configuration is unnecessary. </span></p><p><span style=\"color: rgb(31, 35, 41); background-color: rgb(255, 255, 255); font-size: 16px;\">Option C: The EC2 instance role already has S3 access permissions. The requirement is to restrict the \"network source\" rather than the role permissions, so modifying the role policy cannot achieve network control. </span></p><p><span style=\"color: rgb(31, 35, 41); background-color: rgb(255, 255, 255); font-size: 16px;\">Option D: VPC route tables cannot route traffic to an S3 access point (access points are not routing targets), so this operation has no practical significance.</span></p><p><br></p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "1d4debe3495944d3b361f951b9943ef9",
      "questionNumber": 528,
      "type": "single",
      "content": "<p>Question #528</p><p><br></p><p>A company wants to migrate its website to AWS. The website uses containers that are deployed in an on-premises, self-managed Kubernetes cluster. All data for the website is stored in an on-premises PostgreSQL database.</p><p><br></p><p>The company has decided to migrate the on-premises Kubernetes cluster to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The EKS cluster will use EKS managed node groups with a static number of nodes. The company will also migrate the on-premises database to an Amazon RDS for PostgreSQL database.</p><p><br></p><p>A solutions architect needs to estimate the total cost of ownership (TCO) for this workload before the migration.</p><p><br></p><p>Which solution will provide the required TCO information?</p>",
      "options": [
        {
          "label": "A",
          "content": "Request access to Migration Evaluator. Run the Migration Evaluator Collector and import the data. Configure a scenario. Export a Quick Insights report from Migration Evaluator."
        },
        {
          "label": "B",
          "content": "Launch AWS Database Migration Service (AWS DMS) for the on-premises database. Generate an assessment report. Create an estimate in AWS Pricing Calculator for the costs of the EKS migration."
        },
        {
          "label": "C",
          "content": "Initialize AWS Application Migration Service. Add the on-premises servers as source servers. Launch a test instance. Output a TCO report from Application Migration Service."
        },
        {
          "label": "D",
          "content": "Access the AWS Cloud Economics Center webpage to assess the AWS Cloud Value Framework. Create an AWS Cost and Usage report from the Cloud Value Framework."
        }
      ],
      "correctAnswer": "A",
      "explanation": "<p>The correct answer is A. Request access to Migration Evaluator. Run the Migration Evaluator Collector and import the data. Configure a scenario. Export a Quick Insights report from Migration Evaluator.</p><p> Explanation:</p><p>1. Migration Evaluator (formerly TSO Logic) is an AWS tool designed to help customers analyze their on-premises workloads and estimate the Total Cost of Ownership (TCO) when migrating to AWS. &nbsp;</p><p>2. The process involves: &nbsp;</p><p> &nbsp; - Running the Migration Evaluator Collector on-premises to gather data about the current infrastructure (Kubernetes cluster, PostgreSQL database, etc.). &nbsp;</p><p> &nbsp; - Importing this data into Migration Evaluator to configure a migration scenario (e.g., moving to Amazon EKS and Amazon RDS for PostgreSQL). &nbsp;</p><p> &nbsp; - Generating a Quick Insights report, which provides a TCO comparison between on-premises and AWS. &nbsp;</p><p>3. This approach is the most accurate way to estimate costs for migrating both the Kubernetes cluster and the database to AWS.</p><p> Why Not the Other Options?</p><p>- B: AWS DMS is for database migration, not TCO estimation. AWS Pricing Calculator alone doesn’t account for on-premises costs. &nbsp;</p><p>- C: AWS Application Migration Service (MGN) is for rehosting servers (lift-and-shift), not for Kubernetes or TCO analysis. &nbsp;</p><p>- D: The Cloud Economics Center provides general best practices, not workload-specific TCO estimates. The Cost and Usage Report (CUR) is for existing AWS spend, not on-premises comparisons.</p><p>Thus, Migration Evaluator (Option A) is the correct choice for estimating TCO before migration.</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    },
    {
      "id": "280a51a6926c42cf972955863237149e",
      "questionNumber": 529,
      "type": "multiple",
      "content": "<p>Question #529</p><p><br></p><p>An events company runs a ticketing platform on AWS. The company’s customers configure and schedule their events on the platform. The events result in large increases of traffic to the platform. The company knows the date and time of each customer’s events.</p><p><br></p><p>The company runs the platform on an Amazon Elastic Container Service (Amazon ECS) cluster. The ECS cluster consists of Amazon EC2 On-Demand Instances that are in an Auto Scaling group. The Auto Scaling group uses a predictive scaling policy.</p><p><br></p><p>The ECS cluster makes frequent requests to an Amazon S3 bucket to download ticket assets. The ECS cluster and the S3 bucket are in the same AWS Region and the same AWS account. Traffic between the ECS cluster and the S3 bucket flows across a NAT gateway.</p><p><br></p><p>The company needs to optimize the cost of the platform without decreasing the platform's availability.</p><p><br></p><p>Which combination of steps will meet these requirements? (Choose two.)</p>",
      "options": [
        {
          "label": "A",
          "content": "Create a gateway VPC endpoint for the S3 bucket."
        },
        {
          "label": "B",
          "content": "Add another ECS capacity provider that uses an Auto Scaling group of Spot Instances. Configure the new capacity provider strategy to have the same weight as the existing capacity provider strategy."
        },
        {
          "label": "C",
          "content": "Create On-Demand Capacity Reservations for the applicable instance type for the time period of the scheduled scaling policies."
        },
        {
          "label": "D",
          "content": "Enable S3 Transfer Acceleration on the S3 bucket."
        },
        {
          "label": "E",
          "content": "Replace the predictive scaling policy with scheduled scaling policies for the scheduled events."
        }
      ],
      "correctAnswer": "AE",
      "explanation": "<p> Option A: Create a gateway VPC endpoint for the S3 bucket &nbsp;</p><p>- Currently, traffic between the ECS cluster and S3 flows through a NAT gateway, which incurs data processing costs. &nbsp;</p><p>- A Gateway VPC endpoint for S3 allows private connectivity between the VPC and S3 without using a NAT gateway, reducing costs while maintaining availability. &nbsp;</p><p> Option E: Replace the predictive scaling policy with scheduled scaling policies for the scheduled events &nbsp;</p><p>- Since the company knows the exact date and time of each event, scheduled scaling policies are more cost-effective than predictive scaling. &nbsp;</p><p>- Predictive scaling uses machine learning to forecast demand, which may lead to over-provisioning (increasing costs) or under-provisioning (reducing availability). Scheduled scaling ensures resources are ready exactly when needed. &nbsp;</p><p> Why not the other options? &nbsp;</p><p>- B (Spot Instances with equal weight): While Spot Instances can reduce costs, they are not reliable for time-sensitive, high-availability workloads like ticketing platforms. Using them equally with On-Demand could risk availability. &nbsp;</p><p>- C (On-Demand Capacity Reservations): This ensures capacity but does not optimize costs—it actually locks in higher expenses by reserving instances in advance. &nbsp;</p><p>- D (S3 Transfer Acceleration): This is used to speed up transfers over long distances, but since the S3 bucket and ECS cluster are in the same region, it provides no benefit and adds unnecessary cost. &nbsp;</p><p> Final Answer: &nbsp;</p><p>✅ A (Reduce NAT gateway costs with a VPC endpoint for S3) &nbsp;</p><p>✅ E (Replace predictive scaling with scheduled scaling for known events)</p>",
      "subQuestions": null,
      "caseId": null,
      "caseOrder": null,
      "case": null,
      "caseContent": null,
      "bookmarked": true,
      "hasNote": false
    }
  ]
}